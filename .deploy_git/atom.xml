<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Karobben</title>
  
  <subtitle>Engjoy~</subtitle>
  <link href="https://karobben.github.io/atom.xml" rel="self"/>
  
  <link href="https://karobben.github.io/"/>
  <updated>2024-05-24T15:08:56.927Z</updated>
  <id>https://karobben.github.io/</id>
  
  <author>
    <name>Karobben</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Birds Ig</title>
    <link href="https://karobben.github.io/2024/05/23/LearnNotes/birdig/"/>
    <id>https://karobben.github.io/2024/05/23/LearnNotes/birdig/</id>
    <published>2024-05-23T18:37:09.000Z</published>
    <updated>2024-05-24T15:08:56.927Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Specific-in-Birds">Specific in Birds</h2><p>Until 2024/5/23, the only annotated bird in IMGT is Chicken. According to the IMGT, chicken heavy chain gene has only <a href="https://www.imgt.org/IMGTrepertoire/index.php?section=LocusGenes&amp;repertoire=genetable&amp;species=Chicken&amp;group=IGHV">1 functional V gene </a>, <a href="https://www.imgt.org/IMGTrepertoire/index.php?section=LocusGenes&amp;repertoire=genetable&amp;species=Chicken&amp;group=IGHV">3 functional D genes</a>, and <a href="https://www.imgt.org/IMGTrepertoire/index.php?section=LocusGenes&amp;repertoire=genetable&amp;species=Chicken&amp;group=IGHJ">1 J gene</a>. This is hugely difference from human or other mammals.</p><p>Birds have three classes of antibody: <strong>IgM</strong> (the only class in all vertebrates), <strong>IgY</strong>, and <strong>IgA</strong>. Not been directly determined but inferred from size estimates of the intact molecules of <strong>IgM</strong> and <strong>IgA</strong>, they could form <mark>polypeptide chains</mark><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>. In addition, ducks have a smaller form of IgY, called IgY (ΔFc).</p><ul><li>IgM is the only class of antibody that is found in all vertebrate. IgM is larger than that of a true tetrameric IgM, such as occurs in teleost fish<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> and thus both are likely to be pentameric.</li><li>IgY is the major low-molecular weight form of antibody found circulating in birds, where it has sometimes been referred to as IgG. An IgY-like molecule is likely to have been the evolutionary precursor of both IgG and IgE immunoglobulins<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>.</li></ul><h2 id="Chicken">Chicken</h2><p>The generation of antibody binding-site diversity is very well understood for the chicken:</p><ul><li>Parvari R, Avivi A, Lentner F, Ziv E, Tel-Or S, Burstein Y, et al. Chicken immunoglobulin gamma-heavy chains: limited VH gene repertoire, combinatorial diversification by D gene segments and evolution of the heavy chain locus. EMBO J. 1988;7:739–44.</li><li>Reynaud CA, Anquez V, Dahan A, Weill JC. A single rearrangement event generates most of the chicken immunoglobulin light chain diversity. Cell. 1985;40:283–91.</li><li>Reynaud CA, Anquez V, Grimal H, Weill JC. A hyperconversion mechanism generates the chicken light chain preimmune repertoire. Cell. 1987;48:379–88.</li><li>Reynaud CA, Dahan A, Anquez V, Weill JC. Somatic hyperconversion diversifies the single Vh gene of the chicken with a high incidence in the D region. Cell. 1989;59:171–83.</li></ul><h2 id="Duck">Duck</h2><p>Anseriform birds (ducks and their relatives) are the closest relatives of the chickens which was well understood and be studied.</p><p>Ducks have the same hematopoietic tissues as chickens, including bone marrow, gut associated lymphoid tissue, spleen, thymus and the Bursa of Fabricius, a specialized organ for B lymphoid development. However, there is one notable difference. Ducks have lymph nodes, which are completely absent in chickens<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>.</p><blockquote><ol><li>a single functional rearrangement of the variable (V) region, like chicken.</li><li>generate diversity through gene conversion from a pool of pseudogenes.</li><li>V region element and the pseudogenes appear to consist of a single gene family (The same as the Chicken)</li><li>Further analysis of 26 heavy chain joining (JH) and 27 light chain JL segments shows there is use of a single J segment in ducks.<br>From: Lundqvist M L, et al.<sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1:1]</a></sup></li></ol></blockquote><p>The overwhelming evidence is that all birds express only a single class of immunoglobulin light (L) chain<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup> most closely related to the λ chain of the mammals <sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup><sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup>. The suggestion of additional classes of L chain in the birds <sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup> has not been substantiated at the amino acid sequence or genetic level.</p><h3 id="Duck-IgY">Duck IgY</h3><p>Ducks IgY:</p><ul><li>secreted form</li><li>a receptor form with a hydrophobic membrane-spanning C-terminus</li><li>truncated form termed IgY(ΔFc)</li></ul><p>Difference and similarities in other species:</p><ul><li>No truncated form: Chickens express only the full-length and membrane-receptor forms of IgY<sup class="footnote-ref"><a href="#fn3" id="fnref3:1">[3:1]</a></sup>.</li><li>Has truncated form: A small form of IgY (5.7S) is produced by some species of turtles<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup>.</li></ul><p>Duck immune response inept:</p><ul><li>lacking: precipitation, agglutination, complement fixation and opsonization<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup><sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup> (related the lacks an Fc region).</li></ul><h3 id="Duck-IgA">Duck IgA</h3><p><strong>IgA</strong> has been described to date only in mammals and birds</p><ul><li><p>secretions of the gut, respiratory and reproductive tracts, as well as in tears, bile and (in mammals) the milk</p></li><li><p>IgA of mammals is typically a dimer.</p></li><li><p><strong>Duck IGH</strong>:</p><ol><li>a single family of VH sequences</li><li>a single expressed JH element</li><li>JH is immediately downstream of a D segment</li></ol></li><li><p><strong>Duck IGL</strong>:</p><ol><li>there are few functional coding VL and JL elements in the germline</li><li>diversification most likely arises in large measure from gene conversion events from an extensive suite of germline VL-related sequences; (Genomic Southern blot analysis showed that there is a large family of germline VL-related sequences in the mallard duck<sup class="footnote-ref"><a href="#fn6" id="fnref6:1">[6:1]</a></sup>)</li><li>It is not known whether there is a single functional V and J element in the duck L chain locus, however, that is the simplest explanation of the results.</li><li>muscovy duck were at least two functional VL genes<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup>.</li><li>a single family of VL sequences</li></ol></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Lundqvist M L, Middleton D L, Radford C, et al. Immunoglobulins of the non-galliform birds: antibody expression and repertoire in the duck[J]. Developmental &amp; Comparative Immunology, 2006, 30(1-2): 93-100. <a href="#fnref1" class="footnote-backref">↩︎</a> <a href="#fnref1:1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Leslie GA, Clem LW. Phylogeny of immunoglobulin structure and function. 3. Immunoglobulins of the chicken. J Exp Med. 1969;130:1337–52. <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Parvari R, Avivi A, Lentner F, Ziv E, Tel-Or S, Burstein Y, et al. Chicken immunoglobulin gamma-heavy chains: limited VH gene repertoire, combinatorial diversification by D gene segments and evolution of the heavy chain locus. EMBO J. 1988;7:739–44. <a href="#fnref3" class="footnote-backref">↩︎</a> <a href="#fnref3:1" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>Magor KE, Higgins DA, Middleton DL, Warr GW. One gene encodes the heavy chains for three different forms of IgY in the duck. J Immonol. 1994;153:5549–55. <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>Flajnik MF, Miller KM, Du Pasquier L. Evolution of the immune system. In: Paul WE, editor. Fundamental immunology. 5th ed. Philadelphia: Lippincott Williams and Wilkins; 2003. p. 519–70. <a href="#fnref5" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p>Magor KE, Higgins DA, Middleton DL, Warr GW. cDNA sequence and organization of the immunoglobulin light chain gene of the duck, Anas platyrhynchos. Dev Comp Immunol. 1994;18:523–31. <a href="#fnref6" class="footnote-backref">↩︎</a> <a href="#fnref6:1" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p>Reynaud CA, Dahan A, Weill JC. Complete sequence of a chicken lambda light chain immunoglobulin derived from the nucleotide sequence of its mRNA. Proc Natl Acad Sci USA. 1983;80:4099–103. <a href="#fnref7" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p>Grant JA, Sanders B, Hood L. Partial amino acid sequences of chicken and turkey immunoglobulin light chains. Homology with mammalian lambda chains. Biochemistry. 1971;10:3123–32. <a href="#fnref8" class="footnote-backref">↩︎</a></p></li><li id="fn9" class="footnote-item"><p>Kubo RT, Rosenblum IY, Benedict AA. The unblocked N-terminal sequence of chicken IgG lambda-like light chains. J Immunol. 1970;105:534–6. <a href="#fnref9" class="footnote-backref">↩︎</a></p></li><li id="fn10" class="footnote-item"><p>Leslie GA. Evidence for a second avian light chain isotype. Immunochemistry. 1977;14:149–51. <a href="#fnref10" class="footnote-backref">↩︎</a></p></li><li id="fn11" class="footnote-item"><p>Leslie GA, Clem LW. Phylogeny of immunoglobulin structure and function, VI. 17S, 7.5S and 5.7S anti-DNP of the turtle, Pseudamys scripta. J Immunol. 1972;108:1656–64. <a href="#fnref11" class="footnote-backref">↩︎</a></p></li><li id="fn12" class="footnote-item"><p>Grey HM. Duck immunoglobulins. II. Biologic and immunochemical studies. J Imunol. 1967;98:820–6. <a href="#fnref12" class="footnote-backref">↩︎</a></p></li><li id="fn13" class="footnote-item"><p>Humphrey BD, Calvert CC, Klasing KC. The ratio of full length IgY to truncated IgY in immune complexes affects macrophage phagocytosis and the acute phase response of mallard ducks (Anas platyrhynchos) Dev Comp Immunol. 2004;28:665–72. <a href="#fnref13" class="footnote-backref">↩︎</a></p></li><li id="fn14" class="footnote-item"><p>McCormack WT, Carlson LM, Tjoelker LW, Thompson CB. Evolutionary comparison of the avian IgL locus: combinatorial diversity plays a role in the generation of the antibody repertoire in some avian species. Int Immunol. 1989;1:332–41. <a href="#fnref14" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">The avian immune system exhibits a unique immunoglobulin (Ig) system characterized by distinct features absent in other vertebrates. Birds possess a specialized IgY, which serves as the functional equivalent to mammalian IgG and IgE, but with significant structural and functional differences. Unlike mammalian systems, birds utilize a limited number of germline gene segments and rely on gene conversion within the bursa of Fabricius to generate antibody diversity. This mechanism allows for a rapid and diverse immune response, showcasing the evolutionary adaptation of birds to their ecological niches and pathogen challenges.</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Notes/Biology/"/>
    
    <category term="Immunity" scheme="https://karobben.github.io/categories/Notes/Biology/Immunity/"/>
    
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Immunology" scheme="https://karobben.github.io/tags/Immunology/"/>
    
    <category term="Antibody" scheme="https://karobben.github.io/tags/Antibody/"/>
    
  </entry>
  
  <entry>
    <title>Antibody 12/23 rule</title>
    <link href="https://karobben.github.io/2024/05/18/LearnNotes/ab1223rule/"/>
    <id>https://karobben.github.io/2024/05/18/LearnNotes/ab1223rule/</id>
    <published>2024-05-18T14:49:22.000Z</published>
    <updated>2024-05-24T19:24:12.785Z</updated>
    
    <content type="html"><![CDATA[<h2 id="How-Does-Antibody-Fragments-Jointed-Together">How Does Antibody Fragments Jointed Together?</h2><p>We all know that antibodies are composed of V, D, and J segments, which originate from different locations on the chromosome. But how are they connected together after post-transcriptional modification? The 12/23 rule is the fundamental mechanism that ensures proper recombination of these segments.</p><blockquote><p>The V region, or V domain, of an immunoglobulin heavy or light chain is encoded by more than one gene segment. For the light chain, the V domain is encoded by two separate DNA segments. The first segment encodes the first 95–101 amino acids of the light chain and is termed a V gene segment because it encodes most of the V domain. The second segment encodes the remainder of the V domain (up to 13 amino acids) and is termed a joining or J gene segment<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</p></blockquote><h2 id="What-Is-12-23-Rule">What Is 12/23 Rule</h2><p>Video tutorial: <a href="https://www.youtube.com/watch?v=QTOBSFJWogE">Daniel Levy; 2013. VDJ Gene Recombination</a></p><p>The 12/23 rule is a principle in V(D)J recombination, a process crucial for generating the diversity of antibodies and T-cell receptors. It states that recombination can only occur between gene segments flanked by recombination signal sequences (RSS) with spacers of 12 base pairs (bp) and 23 bp. This ensures proper alignment and prevents inappropriate recombination events, maintaining the integrity and functionality of the immune system’s response.</p><table><thead><tr><th style="text-align:center"><img src="https://www.ncbi.nlm.nih.gov/books/NBK27140/bin/CH4F5.jpg" alt="12/23 rule illustration"></th></tr></thead><tbody><tr><td style="text-align:center">© Charles A. Janeway</td></tr></tbody></table><p>This image illustrates the 12/23 rule in the context of V(D)J recombination.</p><ol><li><p><strong>Recombination Signal Sequences (RSS)</strong>:</p><ul><li><strong>Heptamer</strong>: A conserved sequence of 7 base pairs.</li><li><strong>Nonamer</strong>: A conserved sequence of 9 base pairs.</li><li><strong>Spacer</strong>: The region between the heptamer and nonamer, either 12 or 23 base pairs long.</li></ul></li><li><p><strong>V(D)J Recombination Process</strong>:</p><ul><li><strong>Segments</strong>: V (Variable), D (Diversity), and J (Joining) segments.</li><li><strong>Rule Application</strong>: The 12/23 rule ensures that only segments with RSS of different spacers (12 and 23 bp) recombine, facilitating the correct assembly of these segments.</li></ul></li></ol><p>The image visually represents how the rule guides the alignment and recombination of V, D, and J gene segments, which is essential for generating the diversity of antibodies and T-cell receptors.</p><h2 id="How-It-Worked">How It Worked</h2><p>The heptamer and nonamer is the recombination signal sequences (RSSs). The (RAG1-RAG2)2 endonuclease complex (RAG) specifically recognizes and cleaves a pair of RSSs<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p><table><thead><tr><th style="text-align:center"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0959440X18301143-gr3.jpg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">© Ru H<sup class="footnote-ref"><a href="#fn2" id="fnref2:1">[2:1]</a></sup></td></tr></tbody></table><h2 id="How-Does-It-Applied">How Does It Applied</h2><h3 id="IgDetective">IgDetective</h3><p><a href="https://github.com/Immunotools/IgDetective">IgDetective</a>, published by Vikram Sirupurapu and Yana Safonova<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>, is a tool for detecting and naming antibodies based on the 12/23 rule. This tool leverages the stringent application of the 12/23 rule among mammals, making it suitable primarily for mammalian species.</p><ul><li>Test: not suitable for birds like chicken</li></ul><h3 id="williamdlees-Digger">williamdlees-Digger</h3><p>This is another <a href="https://github.com/williamdlees/digger">open source</a> tool developed by William D. Lees<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>, aimed at annotating the positions of V/D/J genes on newly assembled genomes. According to the results shown in the <a href="https://williamdlees.github.io/digger/_build/html/examples/human_igh.html">documentation</a>, it has very high accuracy.</p><p>Prerequisite:</p><ul><li>a set of known core coding region allele sequences</li><li>Position-weighted matrices</li></ul><p>Some technic details worthy to know:</p><ul><li>The search for V-sequences uses the parameters gapopen 5, gapextend 5, penalty -1, word_size 11.</li><li>D and J searches, word_size is reduced to 7 to reflect the shorter sequences</li><li>D sequences the evalue is set to 100 rather than the default of 10 to widen the search.<br>How it find the heptamers and nonamers:</li></ul><table><thead><tr><th>Type</th><th>Functionality criteria</th></tr></thead><tbody><tr><td>V</td><td>RSS nonamer and heptamer pass PWM thresholds, and match canonical consensus, if defined.<br>L-PART1 and L-PART2 pass PWM thresholds and splice to form a coding sequence with no STOP-CODONs that is in-frame with the V sequence.<br>V-REGION is in-frame and has first and second cysteines at the correct positions in the IMGT alignment.<br>No STOP-CODONs are present in the V-REGION before the second cysteine.</td></tr><tr><td>D</td><td>RSS nonamers and heptamers match canonical consensus, if defined.</td></tr><tr><td>J</td><td>RSS nonamer and heptamer pass PWM thresholds, and match canonical consensus, if defined.<br>The J-motif is found at the expected position relative to the end of the J sequence.<br>The donor splice is found at the expected position, given the length of the matched sequence.</td></tr></tbody></table><p><strong>Limitation:</strong> It only supports <mark>IMGT well-annotated species</mark> because it relies on germline annotation from the IMGT database.<br><strong>Merits:</strong> Easy used and to be understood (write by python).</p><table><thead><tr><th style="text-align:center"><img src="https://williamdlees.github.io/digger/_build/html/_images/igh_results.jpg" alt="Digger Results"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://williamdlees.github.io/digger/_build/html/examples/human_igh.html">© William D. Leeszs</a></td></tr></tbody></table><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">extract_refs -L IGH <span class="hljs-string">&quot;Gallus gallus&quot;</span><br>fix_macaque_gaps Gallus_gallus_IGHV_gapped.fasta \<br>    Gallus_gallus_IGHV_gapped_fixed.fasta IGH<br>cat Gallus_gallus_IGHV.fasta Gallus_gallus_IGHD.fasta Gallus_gallus_IGHJ.fasta \<br>    &gt; Gallus_gallus_IGHVDJ.fasta<br><br>parse_imgt_annotations --save_sequence IMGT000014.fasta \<br>   <span class="hljs-string">&quot;https://www.imgt.org/ligmdb/view.action?format=IMGT&amp;id=IMGT000014&quot;</span> \<br>      IMGT000014_genes.csv IGH<br><br>cat IMGT000014.fasta &gt; Mmul_051212.fasta<br><br><br>mkdir motifs<br><span class="hljs-built_in">cd</span> motifs<br>parse_imgt_annotations \<br>        <span class="hljs-string">&quot;https://www.imgt.org/ligmdb/view?format=IMGT&amp;id=IMGT000014&quot;</span> \<br>        IMGT000014_genes.csv IGH<br>calc_motifs IGH IMGT000014_genes.csv<br><span class="hljs-built_in">cd</span> ..<br><br>makeblastdb -<span class="hljs-keyword">in</span> Gallus_gallus_IGHV.fasta -dbtype nucl<br>makeblastdb -<span class="hljs-keyword">in</span> Gallus_gallus_IGHD.fasta -dbtype nucl<br>makeblastdb -<span class="hljs-keyword">in</span> Gallus_gallus_IGHJ.fasta -dbtype nucl<br><br>blastn -db Gallus_gallus_IGHV.fasta -query Mmul_051212.fasta -out mmul_IGHV.out \<br> -outfmt 7 -gapopen 5 -gapextend 5 -penalty -1 -word_size 11<br>blastn -db Gallus_gallus_IGHD.fasta -query Mmul_051212.fasta -out mmul_IGHD.out \<br> -outfmt 7 -gapopen 5 -gapextend 5 -penalty -1 -word_size 4 -evalue 100<br>blastn -db Gallus_gallus_IGHJ.fasta -query Mmul_051212.fasta -out mmul_IGHJ.out \<br> -outfmt 7 -gapopen 5 -gapextend 5 -penalty -1 -word_size 4<br><br><br><br>blastresults_to_csv mmul_IGHV.out mmul_ighvdj_<br>blastresults_to_csv mmul_IGHD.out mmul_ighvdj_ -a<br>blastresults_to_csv mmul_IGHJ.out mmul_ighvdj_ -a<br><br>find_alignments Gallus_gallus_IGHVDJ.fasta \<br>       Mmul_051212.fasta \<br>       <span class="hljs-string">&quot;mmul_ighvdj_nw_*.csv&quot;</span> \<br>       -ref imgt,Gallus_gallus_IGHVDJ.fasta \<br>       -align Gallus_gallus_IGHV_gapped_fixed.fasta \<br>       -motif_dir motifs \<br>       Mmul_051212.csv<br><br>digger ../../../Duck/data/GCA_015476345.1_ZJU1.0_genomic.fna \<br>   -v_ref Homo_sapiens_IGHV.fasta \<br>   -d_ref Homo_sapiens_IGHD.fasta \<br>   -j_ref Homo_sapiens_IGHJ.fasta \<br>   -v_ref_gapped Homo_sapiens_IGHV_gapped.fasta \<br>   -ref imgt,Homo_sapiens_IGHVDJ.fasta \<br>   -species Chicken  \<br>   -locus IGH \<br>   IMGT000035.csv<br><br><br></code></pre></td></tr></table></figure></div><h2 id="How-It-Really-Looks-Like-in-Human-Genome">How It Really Looks Like in Human Genome</h2><p>I randomly checked a few sequences from the homo genome and found that those regions from different V, D, and J genes are very similar. It could because that the 12/23 rule is not very stringent but flexible. But it could also caused by they are prevented to be recombined.</p><p><img src="https://imgur.com/jqUHyK3.png" alt=""></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Janeway C, Travers P, Walport M, et al. Immunobiology: the immune system in health and disease[M]. New York: Garland Pub., 2001. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Ru H, Zhang P, Wu H. Structural gymnastics of RAG-mediated DNA cleavage in V (D) J recombination[J]. Current opinion in structural biology, 2018, 53: 178-186. <a href="#fnref2" class="footnote-backref">↩︎</a> <a href="#fnref2:1" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Sirupurapu V, Safonova Y, Pevzner P A. Gene prediction in the immunoglobulin loci[J]. Genome research, 2022, 32(6): 1152-1169. <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>Lees W D, Saha S, Yaari G, et al. Digger: directed annotation of immunoglobulin and T cell receptor V, D, and J gene sequences and assemblies[J]. Bioinformatics, 2024, 40(3): btae144. <a href="#fnref4" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">The 12/23 rule is fundamental in the V(D)J recombination process</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Notes/Biology/"/>
    
    <category term="Immunity" scheme="https://karobben.github.io/categories/Notes/Biology/Immunity/"/>
    
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Immunology" scheme="https://karobben.github.io/tags/Immunology/"/>
    
    <category term="Antibody" scheme="https://karobben.github.io/tags/Antibody/"/>
    
  </entry>
  
  <entry>
    <title>Multi-layer Neural Nets</title>
    <link href="https://karobben.github.io/2024/02/18/LearnNotes/ai-multilayer/"/>
    <id>https://karobben.github.io/2024/02/18/LearnNotes/ai-multilayer/</id>
    <published>2024-02-19T05:49:17.000Z</published>
    <updated>2024-02-20T22:00:24.634Z</updated>
    
    <content type="html"><![CDATA[<h2 id="From-linear-to-nonlinear-classifiers">From linear to nonlinear classifiers</h2><ul><li>Linear classifier<ul><li>a linear classifier computes $f(x) = argmax\ Wx$</li><li>The resulting classifier divides the x-space into Voronoi regions: convex regions with piece-wise linear boundaries</li></ul></li><li>Nonlinear classifier<ul><li>Not all classification problems have convex decision regions with PWL boundaries!</li><li>Here’s an example problem in which class 0 (blue) includes values of x near [0.8,0]<sup>T</sup>, but it also includes some values of x near [0.4,0.9]<sup>T</sup></li><li>You can’t compute this function using: $f(x) = argmax\ Wx$</li></ul></li><li>The solution: Piece-wise linear functions<ul><li>Nonlinear classifiers, can be learned using piece-wise linear classification boundaries</li><li>Nonlinear regression problems, can be learned using piece-wise linear regression</li><li>In the limit, as the number of pieces goes to infinity, the approximation approaches the desired solution</li></ul></li></ul><h3 id="Multi-layer-network">Multi-layer network</h3><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/JWTPy3Q.png" alt=""></th></tr></thead><tbody></tbody></table><p>A piece-wise linear function $f(x)$ can be represented by a two-layer neural network. First, the hidden nodes compute:</p><p>$$<br>h_j(x) = \max(0, w_j^{(1)T} x + b_j^{(1)})<br>$$</p><p>Then for PWL regression, the output is a weighted sum of the hidden nodes:</p><p>$$<br>f(x) = w^{(2)T}x + b^{(2)}<br>$$</p><p>…while for PWL classification, the output is the softmax or argmax of such a sum:</p><p>$$<br>f(x) = \text{softmax}(0, W^{(2)T} x + b^{(2)})<br>$$</p><h2 id="Training-a-two-layer-network-Back-propagation">Training a two-layer network: Back-propagation</h2><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Multi-layer Neural Nets</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Hidden Markov Model</title>
    <link href="https://karobben.github.io/2024/02/12/LearnNotes/ai-hmm/"/>
    <id>https://karobben.github.io/2024/02/12/LearnNotes/ai-hmm/</id>
    <published>2024-02-12T06:59:06.000Z</published>
    <updated>2024-02-20T22:00:24.632Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://imgur.com/oMmL1Ln.png" alt=""></p><p>A Hidden Markov Model is a Bayes Network with these assumptions:<br>• <em>Y<sub>t</sub></em> depends only on <em>Y<sub>t-1</sub></em><br>• <em>X<sub>t</sub></em> depends only on <em>Y<sub>t</sub></em></p><p>The belief network conveys the independence assumption:<br>$$<br>for\ all\ i \geq 0, P(S_{i+1}|S_i) = P (S_1|S_0)<br>$$</p><p>$$<br>P(S_i = s) = \sum_{s’} P(S_{i+1} = s \mid S_i = s’) * P(S_i = s’)<br>$$</p><p>In the context of the equation you’re referring to, $ s $ and $ s’ $ represent states in a Markov chain. Typically, $ s $ is used to denote the current state, while $ s’ $ (read as “s prime”) denotes a subsequent or different state that the system can transition into from the current state $ s $.</p><p>The summation over $ s’ $ in the equation indicates that you’re summing over all possible subsequent states that the system can transition to from the current state $ s $. This is part of the definition of a stationary distribution for a Markov chain, where the probability of being in any given state $ s $ is equal to the sum of the probabilities of transitioning to state $ s $ from all possible previous states $ s’ $, weighted by the probability of being in state $ s’ $ at the previous time step.</p><h3 id="Key-advantage-of-a-hidden-Markov-model-Polynomial-time-complexity">Key advantage of a hidden Markov model: Polynomial-time complexity</h3><ul><li>Suppose there are <em>|y|</em> different speech sounds in English, and the length of the utterance is <em>d</em> centiseconds (<em>|y| ≈ 50, d ≈ 100</em>)</li><li>Without the HMM assumptions, to compute <em>f(x)= argmaxP(y<sub>1</sub>, … , y<sub>d</sub>|x<sub>1</sub>, … , x<sub>d</sub>)</em> requires a time complexity of<br><em>O{|y|<sup>d</sup>} ≈ 50<sup>100</sup></em></li><li>With an HMM, each variable has only one parent, so inference is <em>O{|y|<sup>d</sup>} ≈ 50<sup>2</sup></em></li><li>The computationally efficient algorithm that we use to compute <em>f(x)= argmaxP(y<sub>1</sub>, … , y<sub>d</sub>|x<sub>1</sub>, … , x<sub>d</sub>)</em> is called the Viterbi algorithm, named after the electrical engineer who first applied it to error correction coding.</li></ul><div class="admonition note"><p class="admonition-title">it works much better than bayes</p><p>Text generated by a naïve Bayes model (unigram model):</p><ul><li>Representing and speedily is an good apt or come can different natural here he the a in came the to of to expert gray come to furnishes the line message had be these…</li></ul><p>Text generated by a HMM (bigram model):</p><ul><li>The head and in frontal attack on an English writer that the character of this point is therefore another for the letters that the time of who ever told the problem for an unexpected…</li></ul></div><h3 id="Applications-of-HMMs">Applications of HMMs</h3><ul><li>Speech recognition HMMs:<ul><li>Observations are acoustic signals (continuous valued)</li><li>States are specific positions in specific words (so, tens of thousands)</li></ul></li><li>Machine translation HMMs:<ul><li>Observations are words (tens of thousands)</li><li>States are cross-lingual alignments</li></ul></li><li>Robot tracking:<ul><li>Observations are range readings (continuous)</li><li>States are positions on a map</li></ul></li></ul><h3 id="Viterbi-Algorithm">Viterbi Algorithm</h3><p>The Viterbi algorithm is a computationally efficient algorithm for computing the maximum a posteriori (MAP) state sequence<br>$$<br>f(x)= argmax_{y_1, … , y_d}P(y_1, … , y_d|x_1, … , x_d)<br>$$</p><h3 id="Notation">Notation</h3><ul><li><p>Initial State Probability:</p><ul><li>$ \pi_i = P(Y_1 = i)$</li></ul></li><li><p>Transition Probability:</p><ul><li>$a_{i,j} = P(Y_t=j| Y_{t-1} = i)$</li></ul></li><li><p>Observation Probabilities:</p><ul><li>$b_j(x_t) = P(X_t = x_t|Y_t=j)$</li></ul></li><li><p>Node Probability: Probability of the best path until node $ j $ at time $ t $</p></li></ul><p>$$<br>v_t(j) = \max_{y_1,…,y_{t-1}} P(Y_1 = y_1 ,…, Y_{t-1} = y_{t-1}, Y_t = j, X_0 = x_0, …, X_t = x_t)<br>$$</p><ul><li>Backpointer: which node precedes node $ j $ on the best path?</li></ul><p>$$<br>\psi_t(j) = \arg\max_{y_{t-1}} \max_{y_1,…,y_{t-2}} P(Y_1 = y_1 ,…, Y_{t-1} = y_{t-1}, Y_t = j, X_0 = x_0, …, X_t = x_t)<br>$$</p><h3 id="How-HMM-Worked">How HMM Worked</h3><p>Initiation → Iteration → Termination → Back-Tracing</p><ul><li>Initiation<br>$v_1(i) = \pi_ib_i(x_1)$</li><li>Iteration<br>$v_t(j) = max v_{t-1}(i)a_{i,j}b_j(x_t)$</li><li>Termination<br>$y_d = argmax v_d{i}$</li><li>Back-Tracing<br>$y_t = \psi_{t+1}(y_{t+1})$</li></ul><h3 id="Example-Question">Example Question</h3><p>Richard Feynman is an AI. He cannot see the weather, but he can see whether or not his creator, Elspeth Dunsany, brings an umbrella to work. Let $ R_t $ denote the event “it is raining on day $ t $,” and let $ U_t $ denote the event “Dr. Dunsany brings her umbrella on day $ t $.” Dr. Dunsany is an absent-minded professor; she often brings her umbrella when it’s not raining, and often forgets her umbrella when it’s raining. Richard’s model of Dr. Dunsany’s behavior includes the parameter $ P(R_1 = T) = 0.5 $, and the parameters shown in the tables below. What is $ P(R_2 = F, U_1 = T, U_2 = T) $?</p><table><thead><tr><th style="text-align:center">$ P(R_t = T | R_{t-1} = r_{t-1}) $</th><th style="text-align:center">$ r_{t-1} = F $</th><th style="text-align:center">$ r_{t-1} = T $</th></tr></thead><tbody><tr><td style="text-align:center">$ r_t = F $</td><td style="text-align:center">0.4</td><td style="text-align:center">0.1</td></tr><tr><td style="text-align:center">$ r_t = T $</td><td style="text-align:center">0.8</td><td style="text-align:center">0.7</td></tr></tbody></table><table><thead><tr><th style="text-align:left">$P(U_t = T | R_t = r_t)$</th><th style="text-align:center">$r_t = F$</th><th style="text-align:center">$r_t = T$</th></tr></thead><tbody><tr><td style="text-align:left">$ r_{t-1} = F $</td><td style="text-align:center">0.4</td><td style="text-align:center">0.1</td></tr><tr><td style="text-align:left">$ r_{t-1} = T $</td><td style="text-align:center">0.8</td><td style="text-align:center">0.7</td></tr></tbody></table><ul><li><p>Initial State Probability:</p><ul><li>$ P(R_1 = T) = 0.5 $</li></ul></li><li><p>Transition Probabilities:</p><ul><li>$ P(R_t = T | R_{t-1} = F) = 0.8 $</li><li>$ P(R_t = T | R_{t-1} = T) = 0.7 $</li><li>$ P(R_t = F | R_{t-1} = F) = 0.4 $</li><li>$ P(R_t = F | R_{t-1} = T) = 0.1 $</li></ul></li><li><p>Observation Probabilities (Probability of Dr. Dunsany bringing her umbrella given the weather):</p><ul><li>$ P(U_t = T | R_t = F) = 0.4 $ (Probability she brings an umbrella when it’s not raining)</li><li>$ P(U_t = T | R_t = T) = 0.1 $ (Probability she brings an umbrella when it is raining)</li></ul></li></ul><p>These probabilities define the initial state distribution, the transition dynamics, and the observation model of a system, which are essential components of probabilistic models like Hidden Markov Models (HMMs).</p><p>The scenario you’ve presented involves conditional probabilities and is a typical example used in Bayesian inference or probabilistic models. Given the information in the image, we can calculate the probability that Dr. Dunsany brings her umbrella on day 2 given that it’s not raining on day 2, but it was raining on day 1.</p><p>The information given includes:</p><ol><li>The initial probability that it’s raining on day 1: $ P(R_1 = T) = 0.5 $</li><li>The conditional probabilities of bringing an umbrella given the weather:<ul><li>$ P(R_t = T | R_{t-1} = r_{t-1}) $: The probability that Dr. Dunsany brings an umbrella on day $ t $ given the weather on day $ t-1 $.</li><li>$ P(U_t = T | R_t = r_t) $: The probability that Dr. Dunsany brings an umbrella on day $ t $ given the weather on day $ t $.</li></ul></li></ol><p>With the tables provided, we can calculate the probability that Dr. Dunsany brings her umbrella on day 2 given the conditions specified.</p><p>We can apply the law of total probability to consider all possible weather scenarios from the previous day. Here is the formula based on the total probability theorem:</p><p>$ P(R_2 = F, U_1 = T, U_2 = T) = \\<br>\sum_{r_{t-1} \in {T, F}} P(R_2 = F | R_1 = r_{t-1}) \cdot P(U_1 = T | R_1 = r_{t-1}) \cdot P(U_2 = T | R_2 = F)<br>$</p><p>We would then substitute the values from the tables into the formula to calculate the desired probability. Would you like to proceed with this calculation?</p><style>pre {  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Hidden Markov Model</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Basic Mathematics Calculating</title>
    <link href="https://karobben.github.io/2024/02/11/LearnNotes/math-cal/"/>
    <id>https://karobben.github.io/2024/02/11/LearnNotes/math-cal/</id>
    <published>2024-02-12T04:58:46.000Z</published>
    <updated>2024-02-14T16:06:42.182Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sum">Sum</h2><p>The sum symbol, represented by the Greek letter sigma (Σ), is widely used in mathematics to denote the summation of a sequence of numbers or expressions. When you see this symbol, it means you should add up a series of numbers according to the specified rule. Here’s a breakdown of how it’s typically used:</p><h3 id="Basic-Structure">Basic Structure</h3><p>The summation symbol is written as:</p><p>$$<br>\sum_{i=a}^{b} f(i)<br>$$</p><p>where:</p><ul><li>$i$ is the index of summation, which takes on each integer value from $a$ to $b$, inclusive.</li><li>$a$ is the lower limit of summation, the starting value of $i$.</li><li>$b$ is the upper limit of summation, the ending value of $i$.</li><li>$f(i)$ is the function of $i$ to be summed over the range from $a$ to $b$.</li></ul><h3 id="Examples">Examples</h3><ol><li><p><strong>Sum of the first 5 natural numbers</strong>:<br>$$<br>\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15<br>$$<br>Here, $f(i) = i$, and you sum the values of $i$ from 1 to 5.</p></li><li><p><strong>Sum of the squares of the first 3 positive integers</strong>:<br>$$<br>\sum_{i=1}^{3} i^2 = 1^2 + 2^2 + 3^2 = 1 + 4 + 9 = 14<br>$$<br>In this example, $f(i) = i^2$, so you square each $i$ from 1 to 3 and then add them together.</p></li><li><p><strong>Sum of a constant over a range</strong>:<br>Suppose you want to add the number 4, five times. The expression would be:<br>$$<br>\sum_{i=1}^{5} 4 = 4 + 4 + 4 + 4 + 4 = 20<br>$$</p><p>Here, $f(i) = 4$, which doesn’t depend on $i$. You’re essentially multiplying 4 by the number of terms (5 in this case).</p></li><li><p><strong>Two sums</strong><br>$$<br>\sum_{i=1}^ {5}\sum_{j=2}^ {6} ij<br>$$<br>For this, you sum over $j$ from 2 to 6 for each value of $i$ from 1 to 5, and then sum those results. It’s like computing a series within another series. The operation proceeds as follows:</p><ol><li>First, fix $i$ at its starting value, 1.</li><li>Then, for $i = 1$, sum over $j$ from 2 to 6, calculating $1 \cdot j$ for each $j$ and adding them together.</li><li>Repeat this process for each value of $i$ up to 5.</li><li>Finally, sum all the results from the inner summations together.</li></ol><p>Let’s compute this step-by-step to see the result.<br>The result of the double summation $\sum_{i=1}^ {5}\sum_{j=2}^ {6} ij$ is 300. This means that when you sum the product of $i$ and $j$ for each $i$ from 1 to 5 and each $j$ from 2 to 6, the total sum is 300.<br>PS: in python:</p> <div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">N= <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>):<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>,<span class="hljs-number">7</span>):<br>        N += i*j<br></code></pre></td></tr></table></figure></div></li></ol><h3 id="How-to-Use">How to Use</h3><ul><li><strong>Identify the sequence</strong> you need to sum. This could be a series of numbers, functions of an index, or even a constant value repeated several times.</li><li><strong>Determine the starting and ending indices</strong> ($a$ and $b$, respectively) for your summation.</li><li><strong>Write down the function or value</strong> to be summed as $f(i)$ for each $i$ in the range from $a$ to $b$.</li><li><strong>Compute each term</strong> in the series and <strong>add them together</strong> to find the total sum.</li></ul><p>Summation notation is a powerful tool in mathematics, especially for dealing with sequences and series, and it’s widely used in various fields such as statistics, physics, and finance.</p><h2 id="Product-Notation">Product Notation</h2><p>Similarly, we have <mark>product notation</mark>, too. The product symbol is represented by the Greek letter pi (Π), not to be confused with the mathematical constant $\pi$ (pi) used for the ratio of a circle’s circumference to its diameter. The product symbol is used to denote the multiplication of a sequence of numbers or expressions, just like the sum symbol is used for addition.</p><p>$$<br>\prod_{i=a}^{b} f(i)<br>$$</p><p>where:</p><ul><li>$i$ is the index of multiplication, taking on each integer value from $a$ to $b$, inclusive.</li><li>$a$ is the lower limit of the product, the starting value of $i$.</li><li>$b$ is the upper limit of the product, the ending value of $i$.</li><li>$f(i)$ is the function of $i$ to be multiplied over the range from $a$ to $b$.</li></ul><h3 id="Examples-v2">Examples</h3><ol><li><p><strong>Product of the first 5 natural numbers</strong> (also known as $5!$, factorial of 5):</p><p>$$<br>\prod_{i=1}^{5} i = 1 \times 2 \times 3 \times 4 \times 5 = 120<br>$$</p><p>This multiplies the values of $i$ from 1 to 5.</p></li></ol><p>In mathematics and particularly in machine learning, besides the summation (Σ) and product (Π) notations, another frequently used notation is the integral symbol (∫). While the summation and product notations deal with discrete sequences, the integral symbol is used for continuous functions and is fundamental in calculus. Integrals play a crucial role in various aspects of machine learning, especially in optimization, probability distributions, and understanding the area under curves (such as ROC curves).</p><h2 id="Integral-Notation">Integral Notation</h2><p>The basic structure of an integral is:</p><p>$$<br>\int_{a}^{b} f(x) , dx<br>$$</p><p>where:</p><ul><li>$a$ and $b$ are the lower and upper limits of integration, respectively, defining the interval over which the function $f(x)$ is integrated.</li><li>$f(x)$ is the function to be integrated over $x$.</li><li>$dx$ represents an infinitesimally small increment of $x$, indicating that the integration is performed with respect to $x$.</li></ul><h3 id="Importance-in-Machine-Learning">Importance in Machine Learning</h3><ol><li><p><strong>Optimization</strong>: Many machine learning models involve optimization problems where the goal is to minimize or maximize some function (e.g., a loss function in neural networks or a cost function in logistic regression). Integrals are essential in solving continuous optimization problems, especially when calculating gradients or understanding the behavior of functions over continuous intervals.</p></li><li><p><strong>Probability Distributions</strong>: In the context of probabilistic models and statistics, integrals are used to calculate probabilities, expected values, and variances of continuous random variables. For example, the area under the probability density function (PDF) of a continuous random variable over an interval gives the probability of the variable falling within that interval.</p></li><li><p><strong>Feature Extraction and Signal Processing</strong>: In machine learning applications involving signal processing or feature extraction from continuous data, integrals are used to calculate various features and transform signals into more useful forms.</p></li><li><p><strong>Kernel Methods</strong>: In machine learning, kernel methods (e.g., support vector machines) utilize integrals in the formulation of kernel functions, which are essential in mapping input data into higher-dimensional spaces for classification or regression tasks.</p></li><li><p><strong>Deep Learning</strong>: In the training of deep neural networks, integrals may not be explicitly visible but are conceptually present in the form of continuous optimization and in the calculation of gradients during backpropagation.</p></li></ol><h3 id="Example">Example</h3><p>Consider the problem of finding the area under a curve, which is a fundamental concept in machine learning for evaluating model performance (e.g., calculating the area under the ROC curve (AUC) for classification problems). If $f(x)$ represents the curve, the area under $f(x)$ from $a$ to $b$ can be computed by the integral:</p><p>$$<br>\text{Area} = \int_{a}^{b} f(x) , dx<br>$$</p><h2 id="Other-Frequently-Used-Notations">Other Frequently Used Notations</h2><p>This integral computes the total area under $f(x)$ between $a$ and $b$, providing a measure of the model’s performance over that interval.</p><p>Integrals, along with summation and product notations, form the backbone of many mathematical operations in machine learning, from theoretical underpinnings to practical applications in data analysis, model evaluation, and optimization strategies.</p><p>Beyond summation (Σ), product (Π), and integral (∫) notations, there are several other mathematical symbols and concepts that are frequently used in machine learning and statistics. These include:</p><h3 id="Gradient-∇">Gradient (∇)</h3><p>The gradient is a vector operation that represents the direction and rate of the fastest increase of a scalar function. In machine learning, the gradient is crucial for optimization algorithms like gradient descent, which is used to minimize loss functions. The gradient of a function $f(x_1, x_2, \ldots, x_n)$ with respect to its variables is denoted by:</p><p>$$<br>\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)<br>$$</p><h3 id="Partial-Derivative-∂">Partial Derivative (∂)</h3><p>The partial derivative represents the rate of change of a function of multiple variables with respect to one of those variables, keeping the others constant. It’s denoted by the symbol ∂. Partial derivatives are essential in the calculation of gradients and in the optimization of machine learning models.</p><h3 id="Expectation-E">Expectation (E)</h3><p>The expectation or expected value of a random variable is a fundamental concept in probability and statistics, denoted by $E[X]$ for a random variable $X$. It represents the average or mean value that $X$ takes over its probability distribution and is crucial in understanding the behavior of models, especially in probabilistic settings.</p><h3 id="Variance-Var-and-Standard-Deviation-σ">Variance (Var) and Standard Deviation (σ)</h3><p>Variance measures the spread of a random variable’s values and is denoted by $Var(X)$ or $\sigma^2$ for a random variable $X$. The standard deviation, $\sigma$, is the square root of the variance and provides a measure of the dispersion of data points around their mean value. These concepts are vital in assessing the reliability and performance of models.</p><h3 id="Covariance-and-Correlation">Covariance and Correlation</h3><p>Covariance and correlation measure the relationship between two random variables. Covariance indicates the direction of the linear relationship between variables, while correlation measures both the strength and direction of this linear relationship. Understanding these relationships is essential in features selection and in modeling the interactions between variables.</p><h3 id="Big-O-Notation-O">Big O Notation (O)</h3><p>Big O notation is used to describe the computational complexity of algorithms, which is crucial in machine learning for understanding the scalability and efficiency of models and algorithms. For example, an algorithm with a complexity of $O(n^2)$ means its execution time or space requirements increase quadratically as the input size $n$ increases.</p><h3 id="Matrix-Notations-and-Operations">Matrix Notations and Operations</h3><p>Matrices and vectors are fundamental in machine learning for representing and manipulating data. Operations such as matrix multiplication, transpose, and inversion are essential for linear algebra, which underpins many machine learning algorithms, including neural networks, PCA (Principal Component Analysis), and SVMs (Support Vector Machines).</p><p>Each of these mathematical concepts plays a crucial role in the formulation, analysis, and implementation of machine learning algorithms. They provide the theoretical foundation for understanding model behavior, optimizing performance, and evaluating outcomes in a wide range of applications.</p><h2 id="Matrix-Calculating">Matrix Calculating</h2><p>Matrix multiplication is a fundamental operation in linear algebra with extensive applications in mathematics, physics, engineering, computer science, and particularly in machine learning and data analysis. The way matrix multiplication is defined—by taking the dot product of rows and columns—might seem arbitrary at first, but it’s designed to capture several important mathematical and practical concepts.</p><p>Understanding how to perform basic operations with matrices—addition, subtraction, multiplication, and division (in a sense)—is crucial in linear algebra, which is foundational for many areas of mathematics, physics, engineering, and especially machine learning. Here’s a brief overview of each operation:</p><p>$$<br>\begin{pmatrix}<br>a_{11} &amp; \cdots &amp; a_{1j} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>a_{i1} &amp; \cdots &amp; a_{ij}<br>\end{pmatrix}<br>$$</p><p>Each element within the matrix is a pair $(i,j)$, where $i$ is the row index and $j$ is the column index.</p><h3 id="Matrix-Addition-and-Subtraction">Matrix Addition and Subtraction</h3><p>Matrix addition and subtraction are straightforward operations that are performed element-wise. This means you add or subtract the corresponding elements of the matrices. For these operations to be defined, the matrices must be of the same dimensions.</p><ul><li><p><strong>Addition</strong>: If $A$ and $B$ are matrices of the same size, their sum $C = A + B$ is a matrix where each element $c_{ij}$ is the sum of $a_{ij} + b_{ij}$.</p></li><li><p><strong>Subtraction</strong>: Similarly, the difference $C = A - B$ is a matrix where each element $c_{ij}$ is the difference $a_{ij} - b_{ij}$.</p></li></ul><h4 id="Example-v2">Example:</h4><p>If $A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}$ and $B = \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix}$, then</p><ul><li>$A + B = \begin{pmatrix} 1+5 &amp; 2+6 \\ 3+7 &amp; 4+8 \end{pmatrix} = \begin{pmatrix} 6 &amp; 8 \\ 10 &amp; 12 \end{pmatrix}$</li><li>$A - B = \begin{pmatrix} 1-5 &amp; 2-6 \\ 3-7 &amp; 4-8 \end{pmatrix} = \begin{pmatrix} -4 &amp; -4 \\ -4 &amp; -4 \end{pmatrix}$</li></ul><h3 id="Matrix-Multiplication">Matrix Multiplication</h3><p>Matrix multiplication is more complex and involves a dot product of rows and columns. For two matrices $A$ and $B$ to be multiplied, the number of columns in $A$ must equal the number of rows in $B$. If $A$ is an $m \times n$ matrix and $B$ is an $n \times p$ matrix, the resulting matrix $C = AB$ will be an $m \times p$ matrix where each element $c_{ij}$ is computed as the dot product of the $i$th row of $A$ and the $j$th column of $B$.</p><h4 id="Example-v3">Example:</h4><p>If $A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}$ and $B = \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix}$, then</p><ul><li>$AB = \begin{pmatrix} (1× 5 + 2× 7) &amp; (1× 6 + 2× 8) \\ (3× 5 + 4× 7) &amp; (3× 6 + 4× 8) \end{pmatrix} = \begin{pmatrix} 19 &amp; 22 \\ 43 &amp; 50 \end{pmatrix}$</li></ul><h3 id="Matrix-Division">Matrix Division</h3><p>Matrix division as such doesn’t exist in the way we think of division for real numbers. Instead, we talk about the inverse of a matrix. For matrix $A$ to “divide” another matrix $B$, you would multiply $B$ by the inverse of $A$, denoted as $A^{-1}$. This operation is only defined for square matrices (same number of rows and columns), and not all square matrices have an inverse.</p><ul><li><strong>Multiplying by the Inverse</strong>: If you want to solve for $X$ in $AX = B$, you can multiply both sides by $A^{-1}$, assuming $A^{-1}$ exists, to get $X = A^{-1}B$.</li></ul><h4 id="Example-v4">Example:</h4><p>If $A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}$, and its inverse $A^{-1} = \begin{pmatrix} -2 &amp; 1 \\ 1.5 &amp; -0.5 \end{pmatrix}$, and you want to “divide” $B = \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix}$ by $A$, you would compute $A^{-1}B$.</p><h3 id="Key-Points">Key Points</h3><ul><li><strong>Addition/Subtraction</strong>: Element-wise operation requiring matrices of the same dimensions.</li><li><strong>Multiplication</strong>: Involves the dot product of rows and columns, requiring the number of columns in the first matrix to equal the number of rows in the second.</li><li><strong>Division</strong>: Not directly defined, but involves multiplying by the inverse of a matrix.</li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Basic Mathematics Calculating</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Perceptron</title>
    <link href="https://karobben.github.io/2024/02/07/LearnNotes/ai-perceptron/"/>
    <id>https://karobben.github.io/2024/02/07/LearnNotes/ai-perceptron/</id>
    <published>2024-02-07T19:03:23.000Z</published>
    <updated>2024-02-14T16:06:42.181Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Perceptron">Perceptron</h2><p>Perceptron is invented before the loss function</p><p>Linear classifier: Notation<br>• The observation x<sup>T</sup> = [x<sub>1</sub>, … , x<sub>n</sub>] is a real-valued vector (d is the number of feature dimensions)<br>• The class label y ∈ Y is drawn from some finite set of class labels.<br>• Usually the output vocabulary, Y, is some set of strings. For<br>convenience, though, we usually map the class labels to a sequence<br>of integers, Y = [1, … , v} , where � is the vocabulary size</p><h2 id="Linear-classifier-Definition">Linear classifier: Definition</h2><p>A linear classifier is defined by</p><p>$$<br>f(x) = \text{argmax } Wx + b<br>$$</p><p>where:</p><p><img src="https://imgur.com/zuycCx8.png" alt=""></p><p>$w_k, b_k$ are the weight vector and bias corresponding to class $k$, and the argmax function finds the element of the vector $wx$ with the largest value.</p><p>There are a total of $v(d + 1)$ trainable parameters: the elements of the matrix $w$.</p><p><img src="https://imgur.com/undefined.png" alt=""></p><h1>Example</h1><p>Notice that in the two-class case, the equation</p><p>$$<br>f(x) = \text{argmax } Wx + b<br>$$</p><p>Simplifies to</p><p><img src="https://imgur.com/QAGB3Ur.png" alt=""></p><p>The class boundary is the line whose equation is</p><p>$$<br>(w_2 - w_1)^T x + (b_2 - b_1) = 0<br>$$</p><h2 id="Gradient-descent">Gradient descent</h2><h1>Gradient descent</h1><p>Suppose we have training tokens $(x_i, y_i)$, and we have some initial class vectors $w_1$ and $w_2$. We want to update them as</p><p>$$<br>w_1 \leftarrow w_1 - \eta \frac{\partial \mathcal{L}}{\partial w_1}<br>$$</p><p>$$<br>w_2 \leftarrow w_2 - \eta \frac{\partial \mathcal{L}}{\partial w_2}<br>$$</p><p>…where $\mathcal{L}$ is some loss function. What loss function makes sense?</p><p><img src="https://imgur.com/YaSOBI6.png" alt=""></p><h3 id="Zero-one-loss-function">Zero-one loss function</h3><h1>Zero-one loss function</h1><p>The most obvious loss function for a classifier is its classification error rate,</p><p>$$<br>\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \ell(f(x_i), y_i)<br>$$</p><p>Where $\ell(\hat{y}, y)$ is the zero-one loss function,</p><p>$$<br>\ell(f(x), y) =<br>\begin{cases}<br>0 &amp; \text{if } f(x) = y \<br>1 &amp; \text{if } f(x) \neq y<br>\end{cases}</p><h3 id="Non-differentiable">Non-differentiable!</h3><p>The problem with the zero -one loss function is that it’s not differentiable:<br><img src="https://imgur.com/tuIgHI9.png" alt=""></p><h3 id=""></h3><p>Integer vectors: One-hot vectors, A one-hot vector is a binary vector in which all elements are 0 except for a single element that’s equal to 1.</p><p>Drive the perceptron</p><h1>The perceptron learning algorithm</h1><p>a mistake happens here (function)</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Perceptron</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>FLUORESCENCE SPECTROSCOPY</title>
    <link href="https://karobben.github.io/2024/02/06/LearnNotes/fluorescence/"/>
    <id>https://karobben.github.io/2024/02/06/LearnNotes/fluorescence/</id>
    <published>2024-02-06T07:22:42.000Z</published>
    <updated>2024-02-08T18:05:42.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="FLUORESCENCE-SPECTROSCOPY">FLUORESCENCE SPECTROSCOPY</h2><div class="admonition note"><p class="admonition-title">What happens after the molecule is excited?</p><p><img src="https://imgur.com/nD2T2sw.png" alt="" />Fluorescence properties depend on what happens to the molecule during the ~10-8 sec during which it is excited. The decay after absorption includes 1. radiative decay ($K_f$) 2. Non-radiative decay($k_NR$)<br>Fluorescence happens very fast because it back to the ground state very fast. In general, the decay brings the electron from excited state to the ground state (decay defines as events per sec ($k_f$))<br>Nan-radiative decay (exp: form of heat), the decay faster and does not generate photon. The energy transfer into solid molecules and spreed away. They don't went to exited state and generate photons.</p></div><ul><li>the quantum yields for phosphorescence are usually very low because <mark>the radiative decay rates are slow compared to typical nonradiative rates</mark> and quenching processes<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</li></ul><h3 id="What-are-the-processes-of-non-radiative-decay">What are the processes of non-radiative decay?</h3><table><thead><tr><th style="text-align:center"></th><th style="text-align:left">Unit: sec<sup>-1</sup></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/uYOhyLM.png" alt=""></td><td style="text-align:left">Black = Non-radiative<br> Red = Radiative (photon)<br> ABS = absorption (10<sup>15</sup>)<br><strong>IC</strong> = internal conversion <br> (k<sub>IC</sub> ≈ 10<sup>11~12</sup>)<br> <strong>Q</strong> = quenching<br><strong>IX</strong>=intersystem crossing<br>S<sub>1</sub>→T<sub>0</sub>: 10<sup>8</sup>; T<sub>1</sub>→S<sub>0</sub>: 10<sup>2</sup><br> <strong>Chem</strong>=photochemistry<br>k<sub>f</sub> ≈ 10<sup>8</sup>; k<sub>p</sub> ≈ 10<sup>2</sup><br> <strong>F</strong> = fluorescence<br> <strong>P</strong> = phosphorescence<br> Trans = energy transfer<br>k<sub>collision</sub> ≈ 10<sup>10</sup> M<sup>-1</sup>sec<sup>-1</sup></td></tr></tbody></table><p>Only apart of electron went to the S<sub>1</sub> and they decay back to the ground state to generate fluorescence. Most of them when to S<sub>2</sub> and decay faster., In this case, less energy lost through fluorescence. Those change are internal Change. When they when to T<sub>1</sub> it transferred to other states (intersystem crossing) and generate phosphorescence. This state state decay very slow (phosphorescence decays for a few seconds or even more slow)</p><ul><li>Internal Conversion: energy loss due to collisions with solvent molecules<ul><li>collision rate = k<sub>coll</sub> [solvent]<ul><li>k<sub>coll</sub> ~10<sup>10</sup>M<sup>-1</sup>sec<sup>-1</sup></li><li>[slovent]: 55M for water</li><li>The rate of collision of a single molecule is ≈ 10<sup>11</sup>-10<sup>12</sup>sec<sup>-1</sup></li></ul></li><li><mark>S1-S2</mark>: Heat. Fast IC (10<sup>-11</sup>sec): heat loss to solvent, <strong>all excited molecules are in the lowest vibrational state</strong> of S<sub>1</sub></li><li><mark>S0-S1</mark>: Heat. Slow IC (10<sup>-8</sup>sec): due to larger energy gap, therefore fluorescence is possible.</li></ul></li></ul><div class="admonition note"><p class="admonition-title">What is the concentration of pure water?</p><li> 1 L water = 1000 g<li> water molecule = 18 g/mol<li> So 1 L water = 1000/18 =55 mol<li> [M] = 55 mol/1 L = 55 M</div><p>The processes from the S<sub>2</sub> to S<sub>1</sub> is very fast and easily be absorbed and stored in the solvent.<br>The processes from the S<sub>1</sub> to the S<sub>0</sub>, the processes relatively slow.<br>Concentration of bonds for solvent like water is very high, so it could store lots of energy.</p><h4 id="Solvent-reorganization-and-the-Stokes-Shift">Solvent reorganization and the Stokes Shift</h4><p>Measure fluorescence at fixed λ<sub>ex</sub> as a function of <em><strong>λ<sub>em</sub></strong></em><br><strong>Stokes shift</strong>: Emission spectrum is always red-shifted (lower energy) compared to the absorption spectrum</p><table><thead><tr><th style="text-align:center">Vibrational relaxation</th><th style="text-align:center">Solvent reorganization</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://chem.libretexts.org/@api/deki/files/79075/%253DScreen_shot_2011-03-14_at_11.08.58_AM.png?revision=1&amp;size=bestfit&amp;width=224&amp;height=274" alt=""></td><td style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/f/fc/Stokes_shift_diagram.svg" alt=""></td></tr><tr><td style="text-align:center"><a href="https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Supplemental_Modules_(Physical_and_Theoretical_Chemistry)/Spectroscopy/Electronic_Spectroscopy/Jablonski_diagram">© libretexts.org</a></td><td style="text-align:center"><a href="https://en.wikipedia.org/wiki/Stokes_shift">© wikipedia</a></td></tr><tr><td style="text-align:center"></td><td style="text-align:center">Franck-Condon Overlap Factors<br>Prob (0’→2’) ≅ Prob (2’←0’) etc</td></tr></tbody></table><p>The vibrational relaxation looks almost symitry. So, the peaks from the solvent reorganization should corresponded the states change in the vibrational relaxation.<br>The emission shift (Stokes Shift) always as red-shifted (into right). So, the fluorescence always as less energy as the excited state.</p><table><thead><tr><th style="text-align:center">Solvent Effect</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/3c7E6y8.png" alt=""></td></tr></tbody></table><p>The larger, the more effects (?)</p><ol><li>Dipole changes after absorption. But the solvent dipole doesn’t change. But this is not the favorite result, the dipole of the molecule changes</li><li>(right) the solvent changes with the molecule. When the molecule back to the ground state and emitting fluorescence. But the solvent delay and change the ground the state of the molecule. It means the path from the S<sub>1</sub> to S<sub>0</sub> became shorter and the energy would used lesser. This phenomenon could be intensify by using more polar solvent.</li></ol><p>Absorption: ~ 10<sup>-15</sup> sec<br>Solvent reorganization (relaxation): ~ 10<sup>-10</sup> sec<br>Fluorescence: ~ 10<sup>-8</sup> sec<br>Step 1: Permanent Dipoles of solvent re-orient to adjust to the altered dipole of the excited fluorophore.<br>Step 2: The dipole-dipole interaction in turn stabilizes S1 and destabilizes S0.<br><strong>Requires</strong>:<br>1. solvent polarity (dielectric constant, ε)<br>2. mobility of solvent (reorientation of solvent dipoles)</p><h3 id="How-long-can-a-molecule-stay-in-its-excited-state">How long can a molecule stay in its excited state?</h3><p>Excite some molecules to $ S_1 $ with a brief pulse of light at $ t = 0 $, $ N_0^ * $ excited state molecules<br>Decay of the excited state population is exponential:<br>$$ \frac{dN^ *(t)}{dt} = -(k_ f + k_{NR})N^ *(t) $$</p><p>left: ?. right: Chemical rate total decay rate * total concentration</p><p>So: $ N ^ * (t) = N_ 0 ^ * e ^ {-(k_f+k_ {NR})t} = N_0 ^ * e^ {-t/\tau} \quad$<br>where $N ^ * (t)$ is the number of excited molecules at time <em><strong>t</strong></em>.</p><p>Define: fluorescence lifetime $ \tau $<br>$ \tau = \frac{1}{k_f + k_{NR}} $<br>the processes when 1/e ?</p><p>Hence, the equation for the decay of the excited state population:<br>$ N^ *(t) = N_0^ * e^ {-t/\tau} $</p><div class="admonition note"><p class="admonition-title">The meaning of the fluorescence lifetime</p><p><em><strong>τ</strong></em> has units of time (seconds) <br>$[N_ {t=\tau}^ *] = \frac{N_ 0^ *}{e} \approx 0/37N_ 0^ *$<br>After one lifetime following excitation, the probability of a molecule still being in the excited state is about 37%. The shorter the <em><strong>τ</strong></em> the faster the decay.<br>Commonly used fluorescence in biological system, the  <em><strong>τ</strong></em> ~ 1-10 ns</p></div><h4 id="How-bright-can-a-molecule-be">How bright can a molecule be?</h4><p><strong>Rate up</strong>: $S_ 0 \rightarrow S_ 1 = I_ 0$, unit: [# of photons absorbed/sec]<br><strong>Rate down</strong>: $S_ 1 \rightarrow S_ 0 = -(k_ f + k_ {NR}) \cdot N ^*(t)$<br><em>Unit</em>: [1/sec][# of photons]</p><p>$N^ *(t)$ = concentration of excited state molecules at any time, $t$<br>$k_  {NR}$ = sum of all non-radiative rate constants<br>$N^ *(t) = [S_1(t)]$, [# of molecules]</p><p>Steady state: rate up = rate down<br>$0 = \frac{dN ^ *(t)}{dt} = I_ 0 - (k_ f + k_ {NR}) N ^ *(t)$</p><p>In the steady state $N ^ *(t)$ is constant $= N ^ * _ {SS}$<br>$I_0 = (k_f + k_{NR}) N ^ *_{SS}$</p><h4 id="Fluorescence-quantum-yield-QY">Fluorescence quantum yield (QY)</h4><p>$Q_f$ (QY): fraction of excited-state molecules that relax to the ground state by emitting a photon.</p><p>photons/sec emitted in steady state</p><p>$$ Q_f = \frac{k_ f N^ *_ {SS}}{I_ 0} = \frac{k_ f N^ *_ {SS}}{(k_ f + k_{NR})N^ *_ {SS}} $$</p><p>Since $I_0 = (k_f + k_{NR})N^*_{SS}$</p><p>photons/sec absorbed in steady state</p><p><mark>Quantum yield</mark>: $Q_f = \frac{k_f}{(k_f + k_{NR})} = k_f \times \tau$</p><p>Recall $\tau = \frac{1}{(k_f + k_{NR})}$</p><h3 id="What-are-the-fluorophores-in-biological-systems">What are the fluorophores in biological systems?</h3><ul><li>Intrinsic Fluorescence of Proteins</li></ul><p>Absorption spectra Fluorescence spectra of amino acids in water “About 300 papers per year abstracted in Biological Abstracts report work that exploits or studies tryptophan (Trp) fluorescence in proteins…”<br>Vivian et al. Biophysical Journal 2001</p><table><thead><tr><th></th><th>Lifetime (nsec)</th><th>Absorption</th><th></th><th>Fluorescence</th><th></th></tr></thead><tbody><tr><td></td><td></td><td>Wavelength (nm)</td><td>Absorptivity (ε, M<sup>-1cm</sup>-1)</td><td>Wavelength (λmax, nm)</td><td>Quantum Yield (25°C)</td></tr><tr><td>Tryptophan</td><td>2.6</td><td>280</td><td>5,600</td><td>348</td><td>0.20</td></tr><tr><td>Tyrosine</td><td>3.6</td><td>274</td><td>1,400</td><td>303</td><td>0.14</td></tr><tr><td>Phenylalanine</td><td>6.4</td><td>257</td><td>200</td><td>282</td><td>0.04</td></tr></tbody></table><h2 id="Maturation-of-GFP">Maturation of GFP</h2><table><thead><tr><th style="text-align:center"><img src="https://zeiss-campus.magnet.fsu.edu/articles/probes/images/aequoreafpintrofigure1.jpg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://zeiss-campus.magnet.fsu.edu/articles/probes/images/aequoreafpintrofigure2.jpg" alt=""></td></tr><tr><td style="text-align:center"><img src="https://zeiss-campus.magnet.fsu.edu/articles/probes/images/aequoreafpintrofigure4.jpg" alt=""></td></tr><tr><td style="text-align:center"><a href="https://zeiss-campus.magnet.fsu.edu/print/probes/jellyfishfps-print.html">© zeiss</a></td></tr></tbody></table><table><thead><tr><th>Compound</th><th>Lifetime (nsec)</th><th>Wavelength (nm)(Absorption)</th><th>Absorptivity (ε, M<sup>-1</sup>cm<sup>-1</sup>)</th><th>Wavelength (λ<sub>max</sub>, nm) (Emission)</th><th>Quantum Yield (25°C)    (Emission)</th></tr></thead><tbody><tr><td>Tryptophan</td><td>2.6</td><td>280</td><td>5,600</td><td>348</td><td>0.20</td></tr><tr><td>Tyrosine</td><td>3.6</td><td>274</td><td>1,400</td><td>303</td><td>0.14</td></tr><tr><td>Phenylalanine</td><td>6.4</td><td>257</td><td>200</td><td>282</td><td>0.04</td></tr><tr><td>wtGFP</td><td>3.3/2.8</td><td>395/475</td><td>21,000</td><td>509</td><td>0.77</td></tr><tr><td>(Enhanced)</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>EGFP (F64L, S65T)</td><td>2.7</td><td>484</td><td>56,000</td><td>507</td><td>0.60</td></tr></tbody></table><h3 id="Fluorescence-Quenching">Fluorescence Quenching</h3><p>Quenching: reduce the fluorescent signal. Static and dynamic quenching causing the similar result. But the processes are totally different.</p><ol><li>Static quenching:</li></ol><ul><li>Formation of a <strong>“dark” complex of the ground state</strong> of the fluorophore and another molecule.</li></ul><ol start="2"><li>Dynamic quenching:</li></ol><ul><li>Collision between the <strong>excited state of the fluorophore</strong> and another molecule</li><li><strong>Enhancing the non-radiative decay</strong> to the ground state</li></ul><p>$$F = \sigma × 𝐼 × 𝑄Y$$</p><p>$F$: fluorescence intensity (photons/sec)<br>$\sigma$: absorption cross-section (cm<sup>2</sup>)<br>$I$: excitation light flux - photons/(cm<sup>2</sup> sec)<br>$QY$: Quantum yield (unitless)</p><p>Mirror-image rule: between the citation and the shifting spectrum, they are symmetry.</p><h3 id="Static-quenching-ground-state">Static quenching (ground state)</h3><p>Fluorescent species $A$ can associate with <strong>quencher</strong> $Q$ to form a non-fluorescent complex $AQ$:</p><p>$$<br>A + Q \leftrightarrow AQ<br>$$</p><p>The association constant $K_a$ is defined as:</p><p>$$<br>K_a = \frac{[AQ]}{[A][Q]}<br>$$</p><p>The ratio of the fluorescence intensities without and with the quencher present is given by:</p><p>$$<br>\frac{F_0}{F} = \frac{A_{tot}}{A} = \frac{[A] + [AQ]}{[A]}  = \frac{[A] + [A][Q]K_a}{[A]} \\ = 1 + [Q]K_a<br>$$</p><ul><li>Fluorescence depends on the concentration of the quencher, $[Q]$</li><li>Data analysis yields the association constant</li></ul><p>F is after quenching<br>It could quenching black radioactive object.</p><h3 id="Dynamic-quenching-collision-with-the-excited-state">Dynamic quenching: collision with the excited state</h3><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left"><li> $A + hv \rightarrow A^ *$ (excitation) <li> $ A^ * + Q \xrightarrow{k_Q} A + Q + \text{heat}$ (quenching) <li> $A ^ * \xrightarrow{k_f} A + hv’$ (fluorescence) <li> $k_Q$: second order rate constant for collisional quenching</td><td style="text-align:center"><img src="https://imgur.com/4rrQGDW.png" alt=""></td></tr></tbody></table><p>The diagram illustrates the energy levels $S_1$ and $S_0$, with $k_f$ representing the rate of fluorescence, $k_{NR}$ the non-radiative decay, and $[Q]k_Q$ the rate of quenching by the quencher $Q$. There’s also an illustrative depiction of a molecule $A^*$ being quenched by $Q$ within a radius of 50Å.</p><p>Only quenching excited molecule</p><p><img src="https://imgur.com/lcjPXNW.png" alt=""></p><p>The energy levels $S_1$ and $S_0$ are shown with $k_f$ representing the rate of fluorescence, $k_{NR}$ the non-radiative decay, and $[Q]k_Q$ the rate of quenching by the quencher $Q$.</p><ul><li><p>Rate of decay due to collision:<br>$$<br>\frac{d[S_1]}{dt} = -k_Q[Q][S_1]<br>$$</p></li><li><p>Total rate of decay: $S_1 \rightarrow S_0$:</p><ul><li>$ \frac{d[S_1]}{dt} = -k_f[S_1] - k_{NR}[S_1] - k_Q[Q][S_1] $</li><li>$ \frac{d[S_1]}{dt} = -(k_f + k_{NR} + k_Q[Q])[S_1] $</li></ul></li></ul><div class="admonition note"><p class="admonition-title">The quantum yield in the presence of a quencher:</p><p>$$ Q_f^{\theta} = \frac{k_f}{k_f + k_{NR} + k_Q[Q]} $$</p></div><p><strong>No Quencher:</strong>$Q_f^0 = \frac{k_f}{k_f + k_{NR}}$</p><p><strong>Plus quencher:</strong>$Q_f^{\theta} = \frac{k_f}{k_f + k_{NR} + k_Q[Q]}$</p><p>The ratio of fluorescence intensities without and with the quencher is given by:</p><p>$$<br>\frac{F_0}{F} = \frac{Q_f^ 0}{Q_f^ 0 + Q_f} \\ =  \frac{k_ f}{k_ f + k_ {NR}} \times \frac{k_ f + k_ {NR} + k_ Q[Q]}{k_ f}  \\<br>= 1 + \frac{k_Q[Q]}{k_f + k_{NR}} \\ = 1 + \tau_0 k_Q[Q]<br>$$</p><p>Where $\tau_0 = \frac{1}{k_f + k_{NR}}$ is the fluorescence lifetime (without quencher).</p><div class="admonition note"><p class="admonition-title">Define: the Stern-Volmer constant</p><p>$K_{SV} = k_Q \tau_0$<br>$ \frac{F_0}{F} = 1 + K_{SV} [Q] $</p></div><p>K<sub>SV</sub> measures <mark>the rate of quencher colliding</mark> into fluorophores at the excited state.<br>The more the fluorophore is protected from solvent, the smaller the value of K<sub>SV</sub>.</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/HOnMo3v.png" alt=""></td><td style="text-align:left"><li>Slope: $K_{SV} = 8 M^{-1}$ <br> (It descripts how strong the quencher it is. The larger (sharper), the stronger.)<li>Separately measure $\tau_0 = 4 ns$                           <li>Calculate $k_Q = \frac{K_{SV}}{\tau_0} = 2 \times 10^9 M^{-1} sec^{-1}$</td></tr></tbody></table><h3 id="Dynamic-and-Static-quenching-have-the-same-dependence-on-Q">Dynamic and Static quenching have the same dependence on [Q]</h3><p>Dynamic quenching: $\frac{F_0}{F} = 1 + K_{SV} [Q]$<br>Static quenching: $\frac{F_0}{F} = 1 + K_a [Q]$</p><p>In each case, you will get a straight line if you plot $\frac{F_0}{F}$ vs $[Q]$</p><h3 id="How-can-one-distinguish-between-static-quenching-and-dynamic-quenching">How can one distinguish between static quenching and dynamic quenching?</h3><p>The differential equation for the decay of excited state molecules $N^*$ is given by:</p><p>$$<br>\frac{dN^ * (t)}{dt} = -(k_f + k_{NR} + k_Q[Q])N^ * (t)<br>$$</p><p>This leads to the solution:</p><p>$$<br>N^ * (t) = N ^  * _ 0 e ^ {-(k_f+k_{NR}+k_Q[Q])t}<br>$$</p><p>And equivalently:</p><p>$$<br>N^ * (t) = N ^ * _0 e^ {-\frac{t}{\tau}}<br>$$</p><p><strong>DYNAMIC QUENCHING:</strong> The <strong>lifetime of the excited state decreases</strong> as the concentration of the quencher is increased.</p><p>In the presence of a quencher, the lifetime $\tau$ is given by:</p><p>$$<br>\tau = \frac{1}{(k_f + k_{NR} + k_Q[Q])}<br>$$</p><h3 id="Dynamic-quenching">Dynamic quenching</h3><table><thead><tr><th style="text-align:center">Plus quencher</th><th style="text-align:center"><strong>No quencher</strong></th></tr></thead><tbody><tr><td style="text-align:center">$\tau = \frac{1}{(k_f + k_{NR} + k_Q[Q])}$</td><td style="text-align:center">$ \tau_0 = \frac{1}{(k_f + k_{NR})} $</td></tr><tr><td style="text-align:center">$Q_f^{+Q} = \frac{k_f}{k_f + k_{NR} + k_Q[Q]} $</td><td style="text-align:center">$ Q_f^{0} = \frac{k_f}{k_f + k_{NR}} $</td></tr></tbody></table><p>hence</p><p>$$<br>\frac{\tau_0}{\tau} = \frac{Q_f^ {0}}{Q_f^ {+Q}} \approx \frac{F_0}{F}<br>$$</p><p>Stern-Volmer plot will be the same if you plot lifetimes or fluorescence intensity</p><h3 id="Static-quenching-DOES-NOT-affect-the-lifetime-of-the-excited-state">Static quenching DOES NOT affect the lifetime of the excited state</h3><p>Static quenching (ground state)</p><p>$$<br>A + Q \leftrightarrow AQ<br>$$</p><p>$$<br>K_a = \frac{[AQ]}{[A][Q]}<br>$$</p><p>Fluorescent species $A$ can form a non-fluorescent complex $AQ$ with quencher $Q$. The association constant $K_a$ is defined as the ratio of the concentration of the complex to the product of the concentrations of $A$ and $Q$.</p><p>The ratio of the fluorescence intensities without and with the quencher is given by:</p><p>$$<br>\frac{F_0}{F} = \frac{A_{tot}}{A} = \frac{[A] + [AQ]}{[A]} = \frac{[A] + [A][Q]K_a}{[A]} = 1 + [Q]K_a<br>$$</p><p>The excited state species $A^*$ has the same properties in the presence of the static quencher. But there is less of it, so the fluorescence intensity decreases.</p><h3 id="If-both-static-and-dynamic-quenching-are-occurring-in-the-same-sample">If both static and dynamic quenching are occurring in the same sample</h3><p><img src="https://imgur.com/26OTvpP.png" alt=""></p><p>$$<br>\frac{F_0}{F} = (1 + k_Q \tau_0 [Q])(1 + K_a [Q]) = (1 + K_{SV} [Q])(1 + K_a [Q])<br>$$</p><h3 id="Trp94-H±His18-form-a-dark-complex-no">Trp94-(H±His18) form a dark complex: no</h3><p>fluorescence</p><p><strong>Trp94 + His18 ⇌ Trp94·(H+His18) DARK</strong></p><p>At acidic pH:</p><ul><li>Quantum yield of W94 (Qf) is <strong>decreased</strong>;</li><li>Fluorescence lifetime of W94 (τ₀) is <strong>unchanged</strong></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Hurtubise RJ (1990) Phosphorimetry: Theory, Instrumentation, and Applications, VCH, New York. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">FLUORESCENCE SPECTROSCOPY</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>SeqKit: A Cross-Platform and Ultrafast Toolkit for FASTA/Q File Manipulation</title>
    <link href="https://karobben.github.io/2024/02/05/Bioinfor/seqkit/"/>
    <id>https://karobben.github.io/2024/02/05/Bioinfor/seqkit/</id>
    <published>2024-02-05T21:00:06.000Z</published>
    <updated>2024-02-14T16:06:42.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Install">Install</h2><p>GitHub: <a href="https://github.com/shenwei356/seqkit">shenwei356/seqkit</a></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">wget https://github.com/shenwei356/seqkit/releases/download/v2.7.0/seqkit_linux_amd64.tar.gz<br>tar -zxvf seqkit_linux_amd64.tar.gz<br></code></pre></td></tr></table></figure></div><h3 id="Convert-the-Fastq-to-Fasta">Convert the Fastq to Fasta</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">seqkit fq2fa output_directory/output_prefix.extendedFrags.fastq -o output_directory/output_prefix.merged.fasta<br></code></pre></td></tr></table></figure></div><h3 id="Remove-Duplicated-Sequence">Remove Duplicated Sequence</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">seqkit rmdup -s sequences.fasta -o unique_sequences.fasta -D counts.tsv<br></code></pre></td></tr></table></figure></div><ul><li><code>-s</code>: Specifies that duplicates should be identified based on sequence content.</li><li><code>[input_file]</code>: Replace this with the path to your input FASTA or FASTQ file.</li><li><code>-o [output_file]</code>: Specifies the output file. Replace <code>[output_file]</code> with the desired path for the file containing the sequences after duplicate removal.</li><li><code>-D</code>: write all removed duplicates (and counts) to this specified file.</li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">SeqKit provides a comprehensive suite of utilities for the efficient and high-throughput processing of FASTA/Q files. This toolkit allows for format conversion, subsequence extraction, quality control, and much more.</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/"/>
    
    <category term="Fasta/q" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/Fasta-q/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="Fasta" scheme="https://karobben.github.io/tags/Fasta/"/>
    
    <category term="Sequencing" scheme="https://karobben.github.io/tags/Sequencing/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression</title>
    <link href="https://karobben.github.io/2024/02/05/LearnNotes/ai-linear/"/>
    <id>https://karobben.github.io/2024/02/05/LearnNotes/ai-linear/</id>
    <published>2024-02-05T18:26:13.000Z</published>
    <updated>2024-02-20T22:00:24.634Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Linear-Regression">Linear Regression</h2><h3 id="Vectors-and-Matrix">Vectors and Matrix</h3><p>In numpy, the dot product can be written np.dot(w,x) or w@x.<br>Vectors will always be column vectors. Thus:</p><p>$$<br>x =<br>\begin{bmatrix}<br>x_{1} \\<br>\vdots \\<br>x_{n}<br>\end{bmatrix}<br>, \quad w^T = [w_{1}, \ldots, w_{n}]<br>$$</p><p>$$<br>w^Tx = [w_{1}, \ldots, w_{n}]<br>\begin{bmatrix}<br>x_{1} \\<br>\vdots \\<br>x_{n}<br>\end{bmatrix}<br>= \sum_{i=1}^{n} w_{i}x_{i}<br>$$</p><p><br><br><br>$$<br>x =<br>\begin{bmatrix}<br>x_{1} \\<br>\vdots \\<br>x_{n}<br>\end{bmatrix}<br>, \quad<br>W =<br>\begin{bmatrix}<br>w_{1,1} &amp; \ldots &amp; w_{1,n} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>w_{m,1} &amp; \ldots &amp; w_{m,n}<br>\end{bmatrix}<br>$$</p><p>$$Wx =<br>\begin{bmatrix}<br>w_{1,1} &amp; \ldots &amp; w_{1,n} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>w_{m,1} &amp; \ldots &amp; w_{m,n}<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_{1} \\<br>\vdots \\<br>x_{n}<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\sum_{i=1}^{n} w_{1,i}x_{i} \\<br>\vdots \\<br>\sum_{i=1}^{n} w_{m,i}x_{i}<br>\end{bmatrix}<br>$$</p><h3 id="Vector-and-Matrix-Gradients">Vector and Matrix Gradients</h3><p>The gradient of a scalar function with respect to a vector or matrix is:<br>The symbol $\frac{\sigma f}{\sigma x_ 1}$ means “partial derivative of f with respect to <em>x<sub>1</sub></em>”</p><p>$$<br>\frac{\partial f}{\partial x} =<br>\begin{bmatrix}<br>\frac{\partial f}{\partial x_1} \\<br>\vdots \\<br>\frac{\partial f}{\partial x_n}<br>\end{bmatrix}<br>,<br>\quad<br>\frac{\partial f}{\partial W} =<br>\begin{bmatrix}<br>\frac{\partial f}{\partial w_{1,1}} &amp; \cdots &amp; \frac{\partial f}{\partial w_{1,n}} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>\frac{\partial f}{\partial w_{m,1}} &amp; \cdots &amp; \frac{\partial f}{\partial w_{m,n}}<br>\end{bmatrix}<br>$$</p><table><thead><tr><th style="text-align:center"><img src="https://www.researchgate.net/profile/Vladimir-Nasteski/publication/328146111/figure/fig4/AS:702757891751937@1544561946700/Visual-representation-of-the-linear-regression-22.ppm" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.researchgate.net/publication/328146111_An_overview_of_the_supervised_machine_learning_methods?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Vladimir Nasteski</a></td></tr></tbody></table><p>$$ f(x) = w^ T x + b = \sum_{j=0} ^{D-1} w_ j x_ j + b $$</p><ul><li>$f(x) = y$</li><li>Generally, we want to choose the weights and bias, <em>w</em> and <em>b</em>, in order to minimize the errors.</li><li>The errors are the vertical green bars in the figure at right, <em>ε = f(x) − y</em>.</li><li>Some of them are positive, some are negative. What does it mean to “minimize” them?<ul><li>$ f(x) = w^ T x + b = \sum_{j=0} ^{D-1} w_ j x_ j + b $</li></ul></li><li>Training token errors Using that notation, we can define a signed error term for every training token: <em>ε = f(x<sub>i</sub>) - y<sub>i</sub></em></li><li>The error term is positive for some tokens, negative for other tokens. What does it mean to minimize it?</li></ul><h3 id="Mean-squared-error">Mean-squared error</h3><p>Squared: tends to notice the big values and trying ignor small values.</p><p>One useful criterion (not the only useful criterion, but perhaps the most common) of “minimizing the error” is to minimize the mean squared error:<br>$$  \mathcal{L} = \frac{1}{2n} \sum_{i=1}^ {n} \varepsilon_i^ 2 = \frac{1}{2n} \sum_{i=1}^ {n} (f(x_ i) - y_ i)^ 2  $$<br>The factor $\frac{1}{2}$ is included so that, so that when you differentiate ℒ , the 2 and the $\frac{1}{2}$ can cancel each other.</p><div class="admonition note"><p class="admonition-title">MSE = Parabola </p><p>Notice that MSE is a non -negative quadratic function of <em>f(<strong>x</strong>~i~) = <strong>w</strong>^T^ x~i~ + b</em>, therefore it’s a non negative quadratic function of <em><strong>w</strong></em> . Since it’s a non -negative quadratic function of <em><strong>w</strong></em>, it has a unique minimum that you can compute in closed form! We won’t do that today.$\mathcal{L} = \frac{1}{2n} \sum_{i=1}^ {n} (f(x_ i) - y_ i)^ 2$</p></div><h3 id="The-iterative-solution-to-linear-regression-gradient-descent">The iterative solution to linear regression (gradient descent):</h3><ul><li>Instead of minimizing MSE in closed form, we’re going to use an iterative algorithm called gradient descent. It works like this:<ul><li>Start: random initial <em><strong>w</strong></em> and <em>b</em> (at <em>t=0</em>)</li><li>Adjust <em><strong>w</strong></em> and <em>b</em> to reduce MSE (<em>t=1</em>)</li><li>Repeat until you reach the optimum (<em>t = ∞</em>).</li></ul></li></ul><p>$ w \leftarrow w - \eta \frac{\partial \mathcal{L}}{\partial w} $<br>$ b \leftarrow b - \eta \frac{\partial \mathcal{L}}{\partial b} $</p><h4 id="Finding-the-gradient">Finding the gradient</h4><p>The loss function $ \mathcal{L} $ is defined as:<br>$$ \mathcal{L} = \frac{1}{2n} \sum_{i=1}^{n} L_i, \quad L_i = \varepsilon_i^2, \quad \varepsilon_i = w^T x_i + b - y_i $$<br>To find the gradient, we use the chain rule of calculus:<br>$$ \frac{\partial \mathcal{L}}{\partial w} = \frac{1}{2n} \sum_{i=1}^{n} \frac{\partial L_i}{\partial w}, \quad \frac{\partial L_i}{\partial w} = 2\varepsilon_i \frac{\partial \varepsilon_i}{\partial w}, \quad \frac{\partial \varepsilon_i}{\partial w} = x_i $$</p><p>Putting it all together,<br>$$ \frac{\partial \mathcal{L}}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} \varepsilon_i x_i $$</p><p>• Start from random initial values of $w$ and $b(at\ t= 0)$.<br>• Adjust $w$ and $b$ according to:</p><p>$$ w \leftarrow w - \frac{\eta}{n} \sum_{i=1}^{n} \varepsilon_i x_i $$<br>$$ b \leftarrow b - \frac{\eta}{n} \sum_{i=1}^{n} \varepsilon_i $$</p><h4 id="Intuition">Intuition:</h4><ul><li>Notice the sign:<ul><li>$ w \leftarrow w - \frac{\eta}{n} \sum_{i=1}^{n} \varepsilon_i x_i $</li></ul></li><li>If $ \varepsilon_i $ is positive ($ f(x_i) &gt; y_i $), then we want to <mark>reduce</mark> $ f(x_i) $, so we make $ w $ less like $ x_i $</li><li>If $ \varepsilon_i $ is negative ($ f(x_i) &lt; y_i $), then we want to <mark>increase</mark> $ f(x_i) $, so we make $ w $ more like $ x_i $</li></ul><h3 id="Gradient-Descent">Gradient Descent</h3><ul><li>If $n$ is large, computing or differentiating MSE can be expensive.</li><li>The stochastic gradient descent algorithm picks one training token $(x_i, y_i)$ at random (“stochastically”), and adjusts $w$ in order to reduce the error a little bit for that one token:<br>$$ w \leftarrow w - \eta \frac{\partial \mathcal{L}_i}{\partial w} $$<br>…where<br>$$ \mathcal{L}_i = \varepsilon_i^2 = \frac{1}{2}(f(x_i) - y_i)^2 $$</li></ul><h3 id="Stochastic-gradient-descent">Stochastic gradient descent</h3><p>$$<br>\mathcal{L}_i = \varepsilon_i^2 = \frac{1}{2}(w^T x_i + b - y_i)^2<br>$$</p><p>If we differentiate that, we discover that:</p><p>$$<br>\frac{\partial \mathcal{L}_i}{\partial w} = \varepsilon_i x_i,<br>\quad<br>\frac{\partial \mathcal{L}_i}{\partial b} = \varepsilon_i<br>$$</p><p>So the stochastic gradient descent algorithm is:</p><p>$$<br>w \leftarrow w - \eta \varepsilon_i x_i,<br>\quad<br>b \leftarrow b - \eta \varepsilon_i<br>$$</p><h3 id="Code-Example">Code Example</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial<br><span class="hljs-keyword">from</span> matplotlib.animation <span class="hljs-keyword">import</span> FuncAnimation<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">updatew</span>(<span class="hljs-params">x, y, W, b, e=<span class="hljs-number">0.01</span></span>):</span><br>    E = np.<span class="hljs-built_in">sum</span>(W*x +b - y)<br>    W -= np.<span class="hljs-built_in">sum</span>(e*E*x)<br>    b -= e*E<br>    <span class="hljs-keyword">return</span> W, b<br><br>slope = <span class="hljs-number">1</span><br>intercept = <span class="hljs-number">3</span><br>std_dev = <span class="hljs-number">1</span><br>size = <span class="hljs-number">100</span>  <span class="hljs-comment"># Size of the dataset</span><br><br><span class="hljs-comment"># Generate x values</span><br>X = np.random.uniform(low=-<span class="hljs-number">10</span>, high=<span class="hljs-number">10</span>, size=size)<br><br><span class="hljs-comment"># Generate y values based on the equation y = x + 3</span><br><span class="hljs-comment"># Add normal distributed noise with standard deviation of 0.4</span><br>Y = slope * X + intercept + np.random.normal(<span class="hljs-number">0</span>, std_dev, size)<br><br>W = <span class="hljs-number">0</span><br>b = <span class="hljs-number">0</span><br>XX = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(X)):<br>    W,b = updatew(X[i], Y[i], W, b, <span class="hljs-number">.01</span>)<br>    XX +=[[W, b]]    <br><br>plt.plot(X, Y, <span class="hljs-string">&#x27;o&#x27;</span>)<br>plt.plot(X, X*W + b)<br>plt.text(-<span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-string">f&#x27;slop = <span class="hljs-subst">&#123;<span class="hljs-built_in">round</span>(W, <span class="hljs-number">2</span>)&#125;</span>\nintercept = <span class="hljs-subst">&#123;<span class="hljs-built_in">round</span>(b, <span class="hljs-number">2</span>)&#125;</span>&#x27;</span>)<br>plt.show()<br><br><span class="hljs-comment"># Your update function for the animation</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span>(<span class="hljs-params">frame</span>):</span><br>    <span class="hljs-comment"># Update the data for the animated line plot, for example</span><br>    ln.set_data(X, X * XX[frame][<span class="hljs-number">0</span>] + XX[frame][<span class="hljs-number">1</span>] )<br>     <span class="hljs-comment"># Update the text for the current frame</span><br>    txt.set_text(<span class="hljs-built_in">str</span>(<span class="hljs-built_in">int</span>(frame)) +<span class="hljs-string">&#x27;: $y = &#123;:.2f&#125;x + &#123;:.2f&#125;$&#x27;</span>.<span class="hljs-built_in">format</span>(XX[frame][<span class="hljs-number">0</span>], XX[frame][<span class="hljs-number">1</span>]))<br>    <span class="hljs-keyword">return</span> ln, txt<br><span class="hljs-comment"># Set up the figure and the line to animate</span><br>fig, ax = plt.subplots()<br>ln, = ax.plot([], [], <span class="hljs-string">&#x27;r-&#x27;</span>, animated=<span class="hljs-literal">True</span>)<br>txt = ax.text(-<span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-string">&#x27;&#x27;</span>, animated=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># Create a text object at (-9, 9)</span><br><span class="hljs-comment"># Plot the background points</span><br>ax.plot(X, Y, <span class="hljs-string">&#x27;o&#x27;</span>)  <span class="hljs-comment"># Static background points</span><br><span class="hljs-comment"># Init function to set up the background of each frame (if necessary)</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init</span>():</span><br>    ax.set_xlim(<span class="hljs-built_in">min</span>(X), <span class="hljs-built_in">max</span>(X))<br>    ax.set_ylim(<span class="hljs-built_in">min</span>(Y), <span class="hljs-built_in">max</span>(Y))<br>    txt.set_text(<span class="hljs-string">&#x27;&#x27;</span>)<br>    <span class="hljs-keyword">return</span> ln,<br><span class="hljs-comment"># Create the animation</span><br>ani = FuncAnimation(fig, update, frames=<span class="hljs-number">100</span>,<br>                    init_func=init, blit=<span class="hljs-literal">True</span>)<br>ani.save(<span class="hljs-string">&#x27;animation_drawing.gif&#x27;</span>, writer=<span class="hljs-string">&#x27;imagemagick&#x27;</span>, fps=<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure></div><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/1O0dz1d.png" alt="linear reguression"></th></tr></thead><tbody></tbody></table><h2 id="Perceptron">Perceptron</h2><p>Perceptron is invented before the loss function</p><p>Linear classifier: Notation<br>• The observation x<sup>T</sup> = [x<sub>1</sub>, … , x<sub>n</sub>] is a real-valued vector (d is the number of feature dimensions)<br>• The class label y ∈ Y is drawn from some finite set of class labels.<br>• Usually the output vocabulary, Y, is some set of strings. For<br>convenience, though, we usually map the class labels to a sequence<br>of integers, Y = [1, … , v} , where � is the vocabulary size</p><h3 id="Linear-classifier-Definition">Linear classifier: Definition</h3><p>A linear classifier is defined by</p><p>$$<br>f(x) = \text{argmax } Wx + b<br>$$</p><p>where:</p><p>$$<br>Wx + b =<br>\begin{bmatrix}<br>w_{1,1} &amp; \ldots &amp; w_{1,d} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>w_{v,1} &amp; \ldots &amp; w_{v,d}<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_{1} \\<br>\vdots \\<br>x_{d}<br>\end{bmatrix}<br>+<br>\begin{bmatrix}<br>b_{1} \\<br>\vdots \\<br>b_{v}<br>\end{bmatrix}=<br>\begin{bmatrix}<br>w_{1}^T x + b_{1} \\<br>\vdots \\<br>w_{v}^T x + b_{v}<br>\end{bmatrix}<br>$$</p><p>$w_k, b_k$ are the weight vector and bias corresponding to class $k$, and the argmax function finds the element of the vector $wx$ with the largest value.<br>There are a total of $v(d + 1)$ trainable parameters: the elements of the matrix $w$.</p><h3 id="Example">Example</h3><p><img src="https://imgur.com/9kmw2fe.png" alt=""></p><p>Consider a two -class classification problem, with</p><ul><li>$W^T_1 = [w_{1,1}, w_{1,2}] = [2,1]$</li><li>$W^T_2 = [w_{2,1}, w_{2,2}] = [1,2]$</li></ul><p>Notice that in the two-class case, the equation</p><p>$$<br>f(x) = \text{argmax } Wx + b<br>$$</p><p>Simplifies to</p><p>$$<br>f(x) =<br>\begin{cases}<br>1 &amp; \ if\ w_1^T x + b_1 &gt; w_2^T x + b_2 \\<br>2 &amp; \ if\ w_1^T x + b_1 \leq w_2^T x + b_2<br>\end{cases}<br>$$</p><p>The class boundary is the line whose equation is<br>$$<br>(w_2 - w_1)^T x + (b_2 - b_1) = 0<br>$$</p><div class="admonition note"><p class="admonition-title">Extend: Multi-class linear classifier </p><p><img src="https://imgur.com/TNWvhKX.png" alt="" /></p><p>The boundary between class $k$ and class $l$ is the line (or plane, or hyperplane) given by the equation</p></div><table><thead><tr><th style="text-align:center">$f(x) = argmax Wx + b$</th><th style="text-align:center">$(w_k - w_l)^T x + (b_k - b_l) = 0$</th></tr></thead><tbody></tbody></table><p>The classification regions in a linear classifier are called Voronoi regions.<br>A <strong>Voronoi region</strong> is a region that is<br>• Convex (if $u$ and $v$ are points in the region, then every point on the line segment $\bar{u}\bar{v}$ connecting them is also in the region)<br>• Bounded by piece-wise linear boundaries</p><h3 id="Gradient-descent">Gradient descent</h3><p>Suppose we have training tokens $(x_i, y_i)$, and we have some initial class vectors $w_1$ and $w_2$. We want to update them as</p><p>$$<br>w_1 \leftarrow w_1 - \eta \frac{\partial \mathcal{L}}{\partial w_1}<br>$$</p><p>$$<br>w_2 \leftarrow w_2 - \eta \frac{\partial \mathcal{L}}{\partial w_2}<br>$$</p><p>…where $\mathcal{L}$ is some loss function. What loss function makes sense?</p><p><img src="https://imgur.com/YaSOBI6.png" alt=""></p><h4 id="Zero-one-loss-function">Zero-one loss function</h4><p>The most obvious loss function for a classifier is its classification error rate,</p><p>$$<br>\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \ell(f(x_i), y_i)<br>$$</p><p>Where $\ell(\hat{y}, y)$ is the zero-one loss function,</p><p>$$<br>\ell(f(x), y) =<br>\begin{cases}<br>0 &amp; \text{if } f(x) = y \\<br>1 &amp; \text{if } f(x) \neq y<br>\end{cases}<br>$$</p><h4 id="Non-differentiable">Non-differentiable!</h4><p>The problem with the zero -one loss function is that it’s not differentiable:</p><p>$$<br>\frac{\partial \ell(f(x), y)}{\partial f(x)} =<br>\begin{cases}<br>0 &amp; \text{if } f(x) \neq y \\<br>+\infty &amp; \text{if } f(x) = y^+ \\<br>-\infty &amp; \text{if } f(x) = y^-<br>\end{cases}<br>$$</p><p>Integer vectors: One-hot vectors, A one-hot vector is a binary vector in which all elements are 0 except for a single element that’s equal to 1.</p><h3 id="One-hot-vectors">One-hot vectors</h3><p>A one-hot vector is a binary vector in which all elements are 0 except for a single element that’s equal to 1.</p><h4 id="Exp1-Binary-classifier">Exp1: Binary classifier</h4><p>$$<br>f(x) =<br>\begin{bmatrix}<br>f_1(x) \\<br>f_2(x)<br>\end{bmatrix} =<br>\begin{bmatrix}<br>1_{\arg\max Wx=1} \\<br>1_{\arg\max Wx=2}<br>\end{bmatrix}<br>$$</p><p>…where $1$ is called the “indicator function,” and it means:</p><p>$$<br>1_P =<br>\begin{cases}<br>1\ \ \ P\ is\ true\\<br>0\ \ \ P\ is\ false<br>\end{cases}<br>$$</p><h4 id="Exp2-Multi-Class">Exp2: Multi-Class</h4><p>Consider the classifier</p><p>$$<br>f(x) =<br>\begin{bmatrix}<br>f_1(x) \\<br>\vdots \\<br>f_v(x)<br>\end{bmatrix} =<br>\begin{bmatrix}<br>1_{\arg\max Wx=1} \\<br>\vdots \\<br>1_{\arg\max Wx=v}<br>\end{bmatrix}<br>$$</p><p>… with 20 classes. Then some of the classifications might look like this.</p><h4 id="One-hot-ground-truth">One-hot ground truth</h4><p>We can also use one-hot vectors to describe the ground truth. Let’s call the one-hot vector $y$, and the integer label $y$, thus</p><p>$$<br>y = \begin{bmatrix}<br>y_1 \\<br>y_2 \\ \end{bmatrix} = \begin{bmatrix}<br>1_{y=1} \\<br>2_{y=2} \end{bmatrix}<br>$$</p><p>Ground truth might differ from classifier output.</p><p>Instead of a one-zero loss, the perceptron uses a weird loss function that gives great results when differentiated. The perceptron loss function is:</p><p>$$<br>\ell(x, y) = (f(x) - y)^T (Wx + b)<br>$$</p><p>$$<br>= \left[ f_1(x) - y_1, \ldots, f_v(x) - y_v \right]<br>\left(\begin{bmatrix}<br>W_{1,1} &amp; \ldots &amp; W_{1,d} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>W_{v,1} &amp; \ldots &amp; W_{v,d}<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_{1} \\<br>\vdots \\<br>x_{d}<br>\end{bmatrix}<br>+<br>\begin{bmatrix}<br>b_{1} \\<br>\vdots \\<br>b_{v}<br>\end{bmatrix}\right)<br>$$</p><p>$$<br>= \sum_{k=1}^{v} (f_k(x) - y_k)(W_k^T x + b_k)<br>$$</p><h4 id="The-perceptron-loss">The perceptron loss</h4><p>The perceptron loss function is defined as:</p><p>$$<br>\ell(x, y) = \sum_{k=1}^{v} (f_k(x) - y_k)(W_k^T x + b_k)<br>$$</p><p>Notice that:</p><p>$$<br>(f_k(x) - y_k) =<br>\begin{cases}<br>+1 &amp; \text{if } f_k(x) = 1, y_k = 0 \\<br>-1 &amp; \text{if } f_k(x) = 0, y_k = 1 \\<br>0 &amp; \text{otherwise}<br>\end{cases}<br>$$</p><p>So what the loss really means is:</p><p>$$<br>\ell(x, y) = (w_{\hat{y}}^T x + b_{\hat{y}}) - (w_y^T x + b_y)<br>$$</p><p>Where:</p><ul><li>$y$ is the correct class label for this training token</li><li>$\hat{y} = \arg\max_k (w_k^T x + b_k)$ is the classifier output</li><li>$\ell(x, y) &gt; 0$ if $\hat{y} \neq y$</li><li>$\ell(x, y) = 0$ if $\hat{y} = y$</li></ul><h3 id="Perceptron-learning-algorithm">Perceptron learning algorithm</h3><h4 id="Gradient-of-the-perceptron-loss">Gradient of the perceptron loss</h4><p>The perceptron loss function is:</p><p>$$<br>\ell(x, y) = (w_{\hat{y}}^T x + b_{\hat{y}}) - (w_y^T x + b_y)<br>$$</p><p>Its derivative is:</p><p>$$<br>\frac{\partial \ell(x, y)}{\partial w_k} =<br>\begin{cases}<br>x &amp; \text{if } k = \hat{y} \\<br>-x &amp; \text{if } k = y \\<br>0 &amp; \text{otherwise}<br>\end{cases}<br>$$</p><h4 id="The-perceptron-learning-algorithm">The perceptron learning algorithm</h4><ol><li><p>Compute the classifier output $\hat{y} = \arg\max_k (w_k^T x + b_k)$</p></li><li><p>Update the weight vectors as:</p></li></ol><p>$$<br>w_k \leftarrow w_k - \eta \frac{\partial \ell(x, y)}{\partial w_k} =<br>\begin{cases}<br>w_k - \eta x &amp; \text{if } k = \hat{y} \\<br>w_k + \eta x &amp; \text{if } k = y \\<br>w &amp; \text{otherwise}<br>\end{cases}<br>$$</p><p>where $\eta \approx 0.01$ is the learning rate.</p><h4 id="Special-case-two-classes">Special case: two classes</h4><p>If there are only two classes, then we only need to learn one weight vector, $w = w_1 - w_2$. We can learn it as:</p><ol><li><p>Compute the classifier output $\hat{y} = \arg\max_k (w_k^T x + b_k)$</p></li><li><p>Update the weight vectors as:</p></li></ol><p>$$<br>w \leftarrow<br>\begin{cases}<br>w - \eta x &amp; \text{if } \hat{y} \neq y, y = 2 \\<br>w + \eta x &amp; \text{if } \hat{y} \neq y, y = 1 \\<br>w &amp; \text{if } \hat{y} = y<br>\end{cases}<br>$$</p><p>where $\eta \approx 0.01$ is the learning rate. Sometimes we say $y \in {1, -1}$ instead of $y \in {1,2}$.</p><h2 id="Softmax">Softmax</h2><p>Key idea: $f_c(x) =$ posterior probability of cass $c$</p><ul><li>A perceptron has a one-hot output vector, in which $f_c(x) = 1$ if the<br>neural net thinks $c$ is the most likely value of $y$, and 0 otherwise</li><li>A softmax computes $f_c(x) \approx Pr(Y =c |x)$. The conditions for this to be true are:<ul><li>It needs to satisfy the axioms of probability:<br>$$ 0 \leq f_c(x) \leq 1, \quad \sum_{c=1}^{v} f_c(x) = 1$$</li><li>The weight matrix, $W$, is trained using a loss function that encourages $f(x)$ to approximate posterior probability of the labels on some training dataset:<br>$$f_c(x) \approx \Pr(Y = c|x)$$</li></ul></li></ul><h3 id="Softmax-satisfies-the-axioms-of-probability">Softmax satisfies the axioms of probability</h3><ul><li><p>Axiom #1, probabilities are non-negative $(f_k(x) \geq 0)$. There are many ways to do this, but one way that works is to choose:</p><p>$$<br>f_c(x) \propto \exp(w_c^T x + b_c)<br>$$</p></li><li><p>Axiom #2, probabilities should sum to one $(\sum_{k=1}^{v} f_k(x) = 1)$. This can be done by normalizing:</p></li></ul><p>$$<br>f(x) = [f_1(x), …, f_v(x)]^T<br>$$<br>$$<br>f_c(x) = \frac{\exp(w_c^T x + b_c)}{\sum_{k=0}^{v-1} \exp(w_k^T x + b_k)}<br>$$</p><p>where $w_k^T$ is the $k^{th}$ row of the matrix $W$.</p><h3 id="The-logistic-sigmoid-function">The logistic sigmoid function</h3><p>For a two-class classifier, we don’t really need the vector label. If we define $w = w_2 - w_1$ and $b = b_2 - b_1$, then the softmax simplifies to:</p><p>$$<br>f(Wx + b) =<br>\begin{bmatrix}<br>\text{Pr}(Y = 1|x) \\<br>\text{Pr}(Y = 2|x)<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\frac{1}{1+e^ {-(w^ Tx+b)}} \\<br>\frac{e^ {-(w^ Tx+b)}}{1+e^ {-(w^ Tx+b)}}<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\sigma(w^Tx + b) \\<br>1 - \sigma(w^Tx + b)<br>\end{bmatrix}<br>$$</p><p>… so instead of the softmax, we use a scalar function called the logistic sigmoid function:</p><p>$$<br>\sigma(z) = \frac{1}{1+e^{-z}}<br>$$</p><p>This function is called sigmoid because it is S-shaped.</p><p>For $z \to -\infty$, $\sigma(z) \to 0$</p><p>For $z \to +\infty$, $\sigma(z) \to 1$</p><h3 id="Gradient-descent-v2">Gradient descent</h3><p>Suppose we have training tokens $(x_i, y_i)$, and we have some initial class vectors $w_1$ and $w_2$. We want to update them as</p><p>$$<br>w_1 \leftarrow w_1 - \eta \frac{\partial \mathcal{L}}{\partial w_1}<br>$$</p><p>$$<br>w_2 \leftarrow w_2 - \eta \frac{\partial \mathcal{L}}{\partial w_2}<br>$$</p><p>…where $\mathcal{L}$ is some loss function. What loss function makes sense?</p><h3 id="Zero-one-loss-function-v2">Zero-one loss function</h3><p>The most obvious loss function for a classifier is its classification error rate,</p><p>$$<br>\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \ell(\hat{f}(x_i), y_i)<br>$$</p><p>Where $\ell(\hat{y}, y)$ is the zero-one loss function,</p><p>$$<br>\ell(f(x), y) =<br>\begin{cases}<br>0 &amp; \text{if } f(x) = y \\<br>1 &amp; \text{if } f(x) \neq y<br>\end{cases}<br>$$</p><p>The problem with zero-one loss is that it’s not differentiable.</p><h3 id="A-loss-function-that-learns-probabilities">A loss function that learns probabilities</h3><p>Suppose we have a softmax output, so we want $f_c(x) \approx \Pr(Y = c|x)$. We can train this by learning $W$ and $b$ to maximize the probability of the training corpus. If we assume all training tokens are independent, we get:</p><p>$$<br>W, b = \underset{W,b}{\text{argmax}} \prod_{i=1}^{n} \Pr(Y = y_i|x_i) = \underset{W,b}{\text{argmax}} \sum_{i=1}^{n} \ln \Pr(Y = y_i|x_i)<br>$$</p><p>But remember that $f_c(x) \approx \Pr(Y = c|x)$! Therefore, maximizing the log probability of training data is the same as minimizing the cross entropy between the neural net and the ground truth:</p><p>$$<br>W, b = \underset{W,b}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} \mathcal{L}_ i, \quad \mathcal{L}_ i = - \log f_ {y_ i}(x_ i)<br>$$</p><h3 id="Cross-entropy">Cross-entropy</h3><p>This loss function:</p><p>$$<br>\mathcal{L} = - \ln f_{y}(x)<br>$$</p><p>is called cross-entropy. It measures the difference in randomness between:</p><ul><li>Truth: $Y = y$ with probability 1.0, $\ln(1.0) = 0$, minus the</li><li>Neural net estimate: $Y = y$ with probability $f_{y}(x)$.</li></ul><p>Thus</p><p>$$<br>\mathcal{L} = 0 - \ln f_{y}(x)<br>$$</p><h3 id="Gradient-of-the-cross-entropy-of-the-softmax">Gradient of the cross-entropy of the softmax</h3><p>Since we have these definitions:</p><p>$$<br>\mathcal{L} = - \ln f_{y}(x), \quad f_{y}(x) = \frac{\exp(z_{y})}{\sum_{k=1}^{v} \exp(z_{k})}, \quad z_{c} = w_c^T x + b_c<br>$$</p><p>Then:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w_c} = \left( \frac{\partial \mathcal{L}}{\partial z_c} \right) \left( \frac{\partial z_c}{\partial w_c} \right) = \left( \frac{\partial \mathcal{L}}{\partial z_c} \right) x<br>$$</p><p>…where:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial z_c} =<br>\begin{cases}<br>f_{c}(x_i) - 1 &amp; c = y \\<br>f_{c}(x_i) &amp; c \neq y<br>\end{cases}<br>$$</p><h3 id="Similarity-to-linear-regression">Similarity to linear regression</h3><p>For linear regression, we had:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w} = \epsilon x, \quad \epsilon = f(x) - y<br>$$</p><p>For the softmax classifier with cross-entropy loss, we have</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w_c} = \epsilon_c x<br>$$</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y \text{ (output should be 1)} \\<br>f_c(x_i) &amp; \text{otherwise (output should be 0)}<br>\end{cases}<br>$$</p><h3 id="Similarity-to-perceptron">Similarity to perceptron</h3><p>Suppose we have a training token $(x, y)$, and we have some initial class vectors $w_c$. Using softmax and cross-entropy loss, we can update the weight vectors as</p><p>$$<br>w_c \leftarrow w_c - \eta \epsilon_c x<br>$$</p><p>…where</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y_i \\<br>f_c(x_i) &amp; \text{otherwise}<br>\end{cases}<br>$$</p><p>In other words, like a perceptron,</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>\epsilon_c &lt; 0 &amp; c = y_i \\<br>\epsilon_c &gt; 0 &amp; \text{otherwise}<br>\end{cases}<br>$$</p><h3 id="Outline">Outline</h3><ul><li><p>Softmax:<br>$$ f_c(x) = \frac{\exp(w_c^T x + b_c)}{\sum_{k=1}^{v} \exp(w_k^T x + b_k)} \approx \Pr(Y = c|x) $$</p></li><li><p>Cross-entropy:<br>$$ \mathcal{L} = - \ln f_{y}(x) $$</p></li><li><p>Derivative of the cross-entropy of a softmax:<br>$$ \frac{\partial \mathcal{L}}{\partial w_c} = \epsilon_c x, \quad \epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y \text{ (output should be 1)} \\<br>f_c(x_i) &amp; \text{otherwise (output should be 0)}<br>\end{cases} $$</p></li><li><p>Gradient descent:<br>$$ w_c \leftarrow w_c - \eta \epsilon_c x $$</p></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Linear Regression</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Learning Progress</title>
    <link href="https://karobben.github.io/2024/02/02/LearnNotes/ai-learning/"/>
    <id>https://karobben.github.io/2024/02/02/LearnNotes/ai-learning/</id>
    <published>2024-02-02T18:58:50.000Z</published>
    <updated>2024-02-14T16:06:42.177Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Learning">Learning</h2><div class="admonition note"><p class="admonition-title">Biological inspiration: Long-term potentiation</p><ol><li>A synapse is repeatedly stimulated<br></li><li>More dendritic receptors<br></li><li>More neurotransmitters<br></li><li>A stronger link between neurons</li></ol></div><ul><li>Mathematical Model this Biological Learning Model:<ul><li><em><strong>X</strong></em> = input signal; <em><strong>f(X)</strong></em> = output signal</li><li><strong>Learning</strong> = adjust the parameters of the learning machine so that <em><strong>f(x)</strong></em> becomes the function we want</li></ul></li><li>Mathematical Model of Supervised Learning<ul><li><em><strong>D = {(x<sub>1</sub>, y<sub>1</sub>), …, (x<sub>n</sub>, y<sub>n</sub>)}</strong></em> = training dataset containing pairs of (example signal <em><strong>x<sub>i</sub></strong></em>, desired system output <em><strong>y<sub>i</sub></strong></em>)</li><li><strong>Supervised Learning</strong> = adjust parameters of the learner to minimize <em><strong>E[ℓ(Y, f(X))]</strong></em></li><li><em><strong>ℓ</strong></em>: loss function</li></ul></li></ul><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/228ikan.png" alt="loss-function"></th></tr></thead><tbody></tbody></table><h3 id="Decision-tree-learning-An-example">Decision tree learning: An example</h3><p>The Titanic sank. You were rescued. You want to know if your friend was also rescued. You can’t find them. Can you use machine learning methods to estimate the probability that your friend survived? (Calculate the possibility of your friend also be rescued)</p><ol><li>Gather data about as many of the passengers as you can.<ul><li>X = variables that describe the passenger, e.g., age, gender, number of siblings on board.</li><li>Y = 1 if the person is known to have survived</li></ul></li><li>Learn a function, f(X), that matches the known data as well as possible</li><li>Apply f(x) to your friend’s facts, to estimate their probability of survival</li></ol><p><strong>Decision-tree learning</strong>*:</p><ul><li>1st branch = variable that best distinguishes between groups with higher vs. lower survival rates (e.g., gender)</li><li>2nd branch = variable that best subdivides the remaining group</li><li>Quit when all people in a group have the same outcome, or when the group is too small to be reliably subdivided.</li></ul><table><thead><tr><th style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/e/eb/Decision_Tree.jpg" alt="Decision-tree for Titanic"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://en.m.wikipedia.org/wiki/File:Decision_Tree.jpg">© wikipedia</a></td></tr></tbody></table><p>In each leaf node of this tree:</p><ul><li><p>Number on the left = probability of survival</p></li><li><p>Number on the right = percentage of all known cases that are explained by this node</p></li><li><p>A decision tree is an example of a parametric learner</p></li><li><p>The function <em><strong>f(x)</strong></em> is determined by some learned parameters</p></li><li><p>In this case, the parameters are:</p></li><li><p>Should this node split, or not?</p></li><li><p>If so, which tokens go to the right-hand child?</p></li><li><p>If not, what is <em><strong>f(x)</strong></em> at the current node?</p></li><li><p>Titanic shipwreck example:</p><ul><li>Θ = [Y, female, Y, age ≤ 9.5, N, f(x) = 0.73, …]</li></ul></li></ul><div class="admonition note"><p class="admonition-title">A mathematical definition of learning</p><ul><li>Environment: there are two random variables, X and Y, that are jointly distributed according to</li><li><em><strong>P(X,Y)</strong></em></li><li>Data: <em><strong>P(X, Y)</strong></em> is unknown, but we have a sample of training data</li><li><em><strong>D = {(x~1~, y~1~), ..., (x~n~, y~n~)}</strong></em></li><li>Objective: We would like a function � that minimizes the expected value of some loss function, <em><strong>ℓ(Y , f(x))</strong></em> :</li><li><em><strong>ℛ = E[ℓ(Y, f(x))]</strong></em></li><li>Definition of learning: Learning is the task of estimating the function <em><strong>f</strong></em>, given knowledge of <em><strong>D</strong></em>.</li></ul></div><h3 id="Training-vs-Test-Corpora">Training vs. Test Corpora</h3><ul><li><strong>Training Corpus</strong> = a set of data that you use in order to optimize the parameters of your classifier (for example, optimize which features you measure, how you use those features to make decisions, and so on).<ul><li>Measuring the training corpus accuracy is important for debugging: if your training algorithm is working, then training corpus error rate should always go down.</li></ul></li><li><strong>Test Corpus</strong> = a set of data that is non-overlapping with the training set (none of the test tokens are also in the training dataset) that you can use to measure the error rate.<ul><li>Measuring the test corpus error rate is the only way to estimate how your classifier will work on new data (data that you’ve never yet seen)</li></ul></li><li><strong>Training error</strong> is sometimes called “optimization error”. It happens because you haven’t finished optimizing your parameters.</li><li><strong>Test error</strong> = <mark>optimization error + generalization error</mark></li><li><strong>Evaluation Test Corpus</strong> = a dataset that is used only to test the ONE classifier that does best on DevTest. From this corpus, you learn how well your classifier will perform in the real world.</li></ul><h3 id="Early-stopping">Early stopping</h3><ul><li><p><strong>Learning</strong>: Given $\mathcal{D} = {(x_1, y_1), \ldots, (x_n, y_n)}$, find the function $f(X)$ that minimizes some measure of risk.</p></li><li><p><strong>Empirical risk</strong>: a.k.a. training corpus error:</p><ul><li>$R_{\text{emp}} = \frac{1}{n} \sum_{i=1}^{n} \ell(y_i, f(x_i))$</li></ul></li><li><p><strong>True risk</strong>, a.k.a. expected test corpus error:</p><ul><li>$R = \mathbb{E}[\ell(Y, f(X))] = R_{\text{emp}} + R_{\text{generalization}}$</li></ul></li><li><p>Usually, minimum test error and minimum dev error don’t occur at the same time</p></li><li><p>… but early stopping based on the test set is cheating,</p></li><li><p>… so early stopping based on the dev set is the best we can do w/o cheating.</p></li></ul><h2 id="Summary">Summary</h2><ul><li><strong>Biological inspiration</strong>: Neurons that fire together wire together. Given enough training examples <em><strong>(x<sub>i</sub>, y<sub>i</sub>)</strong></em>, can we learn a desired function so that <em><strong>f(x) ≈ y</strong></em>?</li><li><strong>Classification tree</strong>: Learn a sequence of if-then statements that computes <em><strong>f(x) ≈ y</strong></em></li><li><strong>Mathematical definition of supervised learning</strong>: Given a training dataset, <em><strong>D = {(x<sub>1</sub>, y<sub>1</sub>), …, (x<sub>n</sub>, y<sub>n</sub>)}</strong></em> , find a function <em><strong>f</strong></em> that minimizes the risk, <em><strong>ℛ = E[ℓ(Y, f(x))]</strong></em>.</li><li><strong>Overtraining</strong>: $ℛ_ {emp} = \frac{1}{n} \sum^n_{i=1} ℓ*y_i, f(x_i))$ reaches zero if you train long enough.</li><li><strong>Early Stopping</strong>: Stop when error rate on the dev set reaches a minimum</li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Learning Progress</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Naive Bayes and Bayes NetWork</title>
    <link href="https://karobben.github.io/2024/02/01/LearnNotes/ai-bayes/"/>
    <id>https://karobben.github.io/2024/02/01/LearnNotes/ai-bayes/</id>
    <published>2024-02-02T05:20:50.000Z</published>
    <updated>2024-02-20T22:00:24.631Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Naive-Bayes">Naïve Bayes</h2><div class="admonition note"><p class="admonition-title">The problem with likelihood: Too many words</p><p>What does it mean to say that the words, x, have a particular probability?<br>Suppose our training corpus contains two sample emails:<br></p><ul><li>Email1: Y = spam, X =&quot;Hi there man – feel the vitality! Nice meeting you…&quot;<br></li><li>Email2: Y = ham, X =&quot;This needs to be in production by early afternoon…&quot;<br></li></ul><p>Our test corpus is just one email:<br></p><ul><li>Email1: X=&quot;Hi! You can receive within days an approved prescription for increased vitality and stamina&quot;<br>How can we estimate P(X=&quot;Hi! You can receive within days an approved prescription for increased vitality and stamina&quot;|Y = spam)?<br></li></ul></div><p>One thing we could do is:</p><ol><li>$P(W = \text{“hi”} | Y = \text{spam}), P(W = \text{“hi”} | Y = \text{ham})$</li><li>$P(W = \text{“vitality”} | Y = \text{spam}), P(W = \text{“vitality”} | Y = \text{ham})$</li><li>$P(W = \text{“production”} | Y = \text{spam}), P(W = \text{“production”} | Y = \text{ham})$</li></ol><p>Then the approximation formula for $P(X | Y)$ is given by:</p><p>$$ P(X = x | Y = y) \approx \prod_{i=1}^{n} P(W = w_i | Y = y) $$</p><p>In this context, $W$ represents a word in a document, $X$ represents the document itself, $Y$ represents the class (spam or ham), $w_i$ represents the $i$-th word in the document, and $n$ is the total number of words in the document. The product is taken over all words in the document, assuming that the words are conditionally independent of each other given the class label $Y$.</p><div class="admonition question"><p class="admonition-title">Why naïve Bayes is naïve?</p><p>We call this model &quot;naïve Bayes&quot; because the words aren't really conditionally independent given the label. For example, the sequence &quot;for you&quot; is more common in spam emails than it would be if the words &quot;for&quot; and &quot;you&quot; were conditionally independent.<br><strong>True Statement</strong>:<br></p><ul><li><em><strong>P(X = for you|Y= Spam &gt; P( W = for |Y = Spam)(P = you |= Spam)</strong></em><br>The naïve Bayes approximation simply says: estimating the likelihood of every word sequence is too hard, so for computational reasons, we'll pretend that sequence probability doesn't matter.<br></li></ul><p><strong>Naïve Bayes Approximation</strong>:<br></p><ul><li><em><strong>P(X = for you |Y = Spam) ≈ P(W = for |Y = Spam)P( W= you |Y= Spam)</strong></em><br>We use naïve Bayes a lot because, even though we know it's wrong, it gives us computationally efficient algorithms that work remarkably well in practice.</li></ul></div><h3 id="MPE-MAP-using-Bayes’-rule">MPE = MAP using Bayes’ rule</h3><p>$$<br>P(Y= y | X= x) = \frac{P(X =x| Y=y)P(Y = y)}{P(X =x)}<br>$$</p><p>Definition of conditional probability:</p><p>$$<br>P(Y|f(X), A) = \frac{P(f(X)|Y, A)P(Y|A)}{P(f(X)|A)}<br>$$</p><h3 id="Floating-point-underflow">Floating-point underflow</h3><p>That equation has a computational issue. Suppose that the probability of any given word is roughly <em><strong>P(W = W<sub>i</sub>|Y = y) ≈ 10<sup>-3</sup></strong></em>, and suppose that there are 103 words in an email. Then <em><strong>∏<sup>n</sup><sub>i=1</sub> P(W = W<sub>i</sub>|Y = y) = 10<sup>-309</sup></strong></em>,which gets rounded off to zero. This phenomenon is called “floating-point underflow”.</p><div class="admonition note"><p class="admonition-title">Solution</p><p>$$f(x) = \underset{y}{\mathrm{argmax}} \left( \ln P(Y = y) + \sum^n_{i=1} \ln P(W = w_i | Y = y) \right)$$</p></div><h3 id="Reducing-the-naivety-of-naive-Bayes">Reducing the naivety of naïve Bayes</h3><p>Remember that the bag-of-words model is unable to represent this fact:</p><ul><li><p><strong>True Statement</strong>:</p><ul><li><em><strong>P(X = for you|Y= Spam &gt; P( W = for |Y = Spam)(P = you |= Spam)</strong></em><br>Though the bag-of-words model can’t represent that fact, we can represent it using a slightly more sophisticated naïve Bayes model, called a “bigram” model.</li></ul></li><li><p>N-Grams:</p><ul><li>Unigram: a unigram (1-gram) is an isolated word, e.g., “you”</li><li>Bigram: a bigram (2-gram) is a pair of words, e.g., “for you”</li><li>Trigram: a trigram (3-gram) is a triplet of words, e.g., “prescription for you”</li><li>4-gram: a 4-gram is a 4-tuple of words, e.g., “approved prescription for you”</li></ul></li></ul><div class="admonition note"><p class="admonition-title">Bigram naïve Bayes</p><p>A bigram naïve Bayes model approximates the bigrams as conditionally independent, instead of the unigrams. For example,<br><em><strong>P(X = “approved prescription for you” | Y= Spam) ≈</strong></em><br><em><strong>P(B = “approved prescription” |Y = Spam) ×</strong></em><br><em><strong>P(B = “prescription for” | Y= Spam) ×</strong></em><br><em><strong>P(B = “for you” |Y = Spam)</strong></em></p></div><ul><li>The naïve Bayes model has two types of parameters:<ul><li>The a priori parameters: <em><strong>P(Y = y)</strong></em></li><li>The likelihood parameters: <em><strong>P(W = w<sub>i</sub>| Y = y)</strong></em></li></ul></li><li>In order to create a naïve Bayes classifiers, we must somehow estimate the numerical values of those parameters.</li><li>Model parameters: feature likelihoods <em><strong>P(Word | Class)</strong></em> and priors <em><strong>P(Class)</strong></em></li></ul><h3 id="Parameter-estimation-Prior">Parameter estimation: Prior</h3><p>The prior, <em><strong>P(x)</strong></em>, is usually estimated in one of two ways.</p><ul><li>If we believe that the test corpus is like the training corpus, then we just use frequencies in the training corpus:<br>$$<br>P(Y = Spam) = \frac{Docs(Y=Spam)}{Docs(Y=Spam) + Docs(Y \neq Spam)}<br>$$<br>where <em><strong>Docs(Y=Spam)</strong></em> means the number of documents in the training corpus that have the label Y=Spam.</li><li>If we believe that the test corpus is different from the training corpus, then we set <em><strong>P(Y = Spam)</strong></em> = the frequency with which we believe spam will occur in the test corpus.</li></ul><h3 id="Parameter-estimation-Likelihood">Parameter estimation: Likelihood</h3><p>The likelihood, ***P(W = w<sub>i</sub>|Y = y), is also estimated by counting. The “maximum likelihood estimate of the likelihood parameter” is the most intuitively obvious estimate:<br>$$<br>P(W=w_i| Y = Spam) = \frac{Count(W=w_i, Y = Spam)}{Count(Y = Spam)}<br>$$<br>where <em><strong>Count(W=w<sub>i</sub>, Y = Spam)</strong></em> means the number of times that the word <em><strong>w<sub>i</sub></strong></em> occurs in the Spam portion of the training corpus, and <em><strong>Count(Y = Spam)</strong></em> is the total number of words in the Spam portion.</p><h3 id="Laplace-Smoothing-for-Naive-Bayes">Laplace Smoothing for Naïve Bayes</h3><p>One of the biggest challenge for Bayes is it can’t handle unobserved situation.</p><ul><li><p>The basic idea: add $k$ “unobserved observations” to the count of every unigram</p><ul><li>If a word occurs 2000 times in the training data, Count = 2000+k</li><li>If a word occur once in training data, Count = 1+k</li><li>If a word never occurs in the training data, then it gets a pseudo-Count of $k$</li></ul></li><li><p>Estimated probability of a word that occurred Count(w) times in the training data:<br>$$ P(W = w) = \frac{k + \text{Count}(W = w)}{k + \sum_v (k + \text{Count}(W = v))} $$</p></li><li><p>Estimated probability of a word that never occurred in the training data (an “out of vocabulary” or OOV word):<br>$$ P(W = \text{OOV}) = \frac{k}{k + \sum_v (k + \text{Count}(W = v))} $$</p></li><li><p>Notice that<br>$$ P(W = \text{OOV}) + \sum_w P(W = w) = 1 $$</p></li></ul><h2 id="Bayesian-Networks">Bayesian Networks</h2><div class="admonition question"><p class="admonition-title">Why Network?</p><ul><li>Example: $Y$ is a scalar, but $X = [X_1, … , X_{100}]^T$ is a vector</li><li>Then, even if every variable is binary, $P(Y = y|X = x)$ is a table with $2^{101}$ numbers. Hard to learn from data; hard to use.</li></ul></div><p>A better way to represent knowledge: Bayesian network</p><ul><li>Each variable is a node.</li><li>An arrow between two nodes means that the child depends on the parent.</li><li>If the child has no direct dependence on the parent, then there is no arrow.</li></ul><h3 id="Space-Complexity-of-Bayesian-Network">Space Complexity of Bayesian Network</h3><pre class="mermaid">graph LR    B --> A    E --> A    A --> J    A --> M</pre><ul><li>Without the Bayes network: I have 5 variables, each is binary, so the probability distribution $P(B, E, A, M, J)$ is a table with $2^5 = 32$ entries.</li><li>With the Bayes network:<ul><li>Two of the variables, B and E, depend on nothing else, so I just need to know $P(B = ⊤)$ and $P(E = ⊤)$ — 1 number for each of them.</li><li>M and J depend on A, so I need to know $P(M = ⊤|A = ⊤)$ and $P(M = ⊤|A = ⊥)$ – 2 numbers for each of them.</li><li>A depends on both B and E, so I need to know $P(A = ⊤|B = b, E =e)$ for all 4 combinations of $(b, e)$</li><li>Total: 1+1+2+2+4 = 10 numbers to represent the whole distribution!</li></ul></li></ul><p>$$<br>P(B = T \mid J = T, M = T) = \frac{P(B = T, J = T, M = T)}{P(J = T, M = T)} \\<br>= \frac{P(B = T, J = T, M = T)}{P(J = T, M = T)} + \frac{P(B = L, J = T, M = T)}{P(J = T, M = T)} \\<br>= \sum_{e=T}^{L} \sum_{a=T}^{L} P(B = T, E = e, A = a, J = T, M = T) \\<br>= \sum_{e=T}^{L} \sum_{a=T}^{L} P(B = T) P(E = e) \ × \\<br>P(A = a \mid B = T, E = e) P(J = T \mid A = a) P(M = T \mid A = a)<br>$$</p><h3 id="Variables-are-independent-and-or-conditionally-independent">Variables are independent and/or conditionally independent</h3><h4 id="Independence">Independence</h4><pre class="mermaid">graph TDB --> AE --> A</pre><ul><li>Variables are independent if they have no common ancestors<br>$ P(B = \top, E = \top) = P(B = \top)P(E = \top)= P(B = \top) $</li></ul><p>!!! Independent variables may not be conditionally independent</p><ul><li>The variables B and E are not conditionally independent of one another given knowledge of A</li><li>If your alarm is ringing, then you probably have an earthquake OR a burglary. If there is an earthquake, then the conditional probability of a burglary goes down:<ul><li>$P(B = \top| E = \top, A = \top) \neq P(B = \top| E = \bot, A = \top)$</li></ul></li><li>This is called the “explaining away” effect. The earthquake “explains away” the alarm, so you become less worried about a burglary.</li></ul><h4 id="Conditional-Independence">Conditional Independence</h4><pre class="mermaid">graph TDA --> JA --> M</pre><ul><li>The variables J and M are conditionally independent of one another given knowledge of A</li><li>If you know that there was an alarm, then knowing that John texted gives no extra knowledge about whether Mary will text:<br>$P(M = \top | J = \top , A = \top)=P(M = \top | J = \bot, A = \top)= P(M = \top| A = \top)$</li></ul><div class="admonition note"><p class="admonition-title">Conditionally Independent variables may not be independent</p><ul><li>The variables J and M are not independent!</li><li>If you know that John texted, that tells you that there was probably an alarm. Knowing that there was an alarm tells you that Mary will probably text you too:</li><li>$P(M = \top| J = \top) \neq P(M= \top| J = \bot)$</li></ul></div><ul><li>Variables are conditionally independent of one another, given their common ancestors, if (1) they have no common descendants, and (2) none of the descendants of one are ancestors of the other<br>$ P(U = T, M = T \mid A = T) = P(U = T \mid A = T)P(M = T \mid A = T) $</li></ul><h4 id="How-to-tell-at-a-glance-if-variables-are-independent-and-or-conditionally-independent">How to tell at a glance if variables are independent and/or conditionally independent</h4><pre class="mermaid">graph LR    B --> A    E --> A    A --> J    A --> M</pre><ul><li>Variables are independent if they have no common ancestors<ul><li>$P(B = \top, E = \top) = P(B=\top)P(E =\top)$</li></ul></li><li>Variables are conditionally independent of one another, given their common ancestors, if:<ol><li>they have no common descendants, and</li><li>none of the descendants of one are ancestors of the other</li></ol><ul><li>$P(J = \top, M = \top| A = \top) = P(J = \top| A = \top) P (M = \top| A = \top)$</li></ul></li></ul><style>pre {  background-color:#EEFFEF ;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Naive Bayes and Bayes NetWork</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>OPTICAL SPECTROSCOPY – THE ABSORPTION PROCESS</title>
    <link href="https://karobben.github.io/2024/01/30/LearnNotes/absorption/"/>
    <id>https://karobben.github.io/2024/01/30/LearnNotes/absorption/</id>
    <published>2024-01-31T05:37:02.000Z</published>
    <updated>2024-02-05T16:04:10.670Z</updated>
    
    <content type="html"><![CDATA[<h2 id="OPTICAL-SPECTROSCOPY-–-THE-ABSORPTION-PROCESS">OPTICAL SPECTROSCOPY – THE ABSORPTION PROCESS</h2><table><thead><tr><th style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/1/14/EM_Spectrum_Properties_%28Amplitude_Corrected%2C_Bitmap%29.png" alt="Eelctromagenetic spectrum"></th></tr></thead><tbody><tr><td style="text-align:center">© wiki</td></tr></tbody></table><ul><li>Small wavelength; High frequency; Blue end of the visible spectrum</li><li>Long wavelength; Low frequency; Red end of the visible spectrum</li><li>Energy of visible light is about 100-500 kJ/mol</li></ul><h2 id="Beer’s-Law-and-Absorbance">Beer’s Law and Absorbance</h2><p>$$<br>\frac{I}{I_ 0} = 10^ {-\frac{kc(\Delta y)}{2.303}} = 10^ {- \epsilon c (\Delta y)} = 10^ {-A}<br>$$</p><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/66fOitE.png" alt="Beer’s Law"></th><th style="text-align:left"><mark>Define of absorbance</mark>: <li> $ A = \epsilon c (\Delta y)$ or $A=\epsilon c l $<li> <em><strong>ε</strong></em>: Molar extinction coefficient<li> <em><strong>c</strong></em>: concentration in <em><strong>M</strong></em><li> <em><strong>Δy</strong></em>: path length in <em><strong>cm</strong></em> (some place use <em><strong>l</strong></em> as <em><strong>Δy</strong></em>)</th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://imamagnets.com/en/blog/the-beer-lambert-law/">© imamagnets</a></td><td style="text-align:left"></td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Application of Absorbance</p><p>Use UV-Vis absorbance to calculate the concentration of the molecules like DNA, protein, etc.</p><p>$A=\epsilon (\lambda) c l$</p></div><table><thead><tr><th>Molecule</th><th>λ (nm)</th><th>ε (×10<sup>-3</sup>) (M<sup>-1</sup>cm<sup>-1</sup>)</th></tr></thead><tbody><tr><td>Adenine</td><td>260.5</td><td>13.4</td></tr><tr><td>Adenosine</td><td>259.5</td><td>14.9</td></tr><tr><td>NADH</td><td>340, 259</td><td>6.23, 14.4</td></tr><tr><td>NAD+</td><td>260</td><td>18</td></tr><tr><td>FAD</td><td>450</td><td>11.2</td></tr><tr><td>Tryptophan</td><td>280, 219</td><td>5.6, 47</td></tr><tr><td>Tyrosine</td><td>274,222,193</td><td>1.4, 8, 48</td></tr><tr><td>Phenylalanine</td><td>257, 206, 188</td><td>0.2, 9.3, 60</td></tr><tr><td>Histidine</td><td>211</td><td>5.9</td></tr><tr><td>Cysteine</td><td>250</td><td>0.3</td></tr></tbody></table><h2 id="Quantum-Mechanical-Transition-Probability">Quantum Mechanical Transition Probability</h2><p><img src="https://imgur.com/63nZqbL.png" alt="Transition Probability"></p><p>The probability per unit time that a molecule in state 1 will end up in state 2 in the presence of an oscillating electromagnetic field at the resonance frequency<br><mark>Energy of the light = Difference between energy levels</mark></p><p>$$<br>Rate_ {1 → 2} = B_ {12} \rho (\nu)[S_ 1]<br>$$</p><ul><li><em><strong>B<sub>12</sub></strong></em>: rate constant</li><li><em><strong>ρ(ν)</strong></em>: radiation field density</li><li><em><strong>S<sub>1</sub></strong></em>: Concentration of molecules in the ground state</li></ul><div class="admonition note"><p class="admonition-title">Rate constant dependents on the transition dipole moment </p><p>$B_{12} \propto \langle \mu \rangle^2$<br>$\langle \mu \rangle = \int \Psi_2 (q_e\vec{r})\Psi dV$<br>Transition dipole moment:<br>$\langle \mu \rangle \propto$ overlap between $\Psi_1$ and $\Psi_2$<br></p></div><h3 id="Dipole-approximation">Dipole approximation</h3><p>When a light wave hit hydrogen atom, the <em><strong>r</strong></em> from the atom into electron is far smaller than the <em><strong>λ</strong></em>.</p><ul><li>$\because r &lt;&lt; \lambda$</li><li>$\vec{\mu} = - q\vec{r}$</li><li>$Energy = -\vec{\mu}\cdot\vec{E}$<ul><li>$\vec{\mu}$: matter</li><li>$\vec{E}$: Electric Field (amplitude of light)</li></ul></li></ul><table><thead><tr><th style="text-align:center"><img src="https://i.stack.imgur.com/Dmk1Z.png" alt="Transition of the Dipole"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://chemistry.stackexchange.com/questions/90956/are-there-any-simple-molecules-with-very-different-absorption-and-emission-dip">© Sentry</a></td></tr><tr><td style="text-align:center">The transition dipole reflects the change of electron distribution by excitation</td></tr></tbody></table><table><thead><tr><th style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/e/e0/StationaryStatesAnimation.gif" alt="Transition of the Dipole"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://en.wikipedia.org/wiki/Transition_dipole_moment">© wikipedia</a></td></tr><tr><td style="text-align:center">Transition dipole moment</td></tr></tbody></table><h3 id="Overlap-of-wavefunctions-and-transition-probability">Overlap of wavefunctions and transition probability</h3><p>$$\mu_{mn} = \int_{-\infty}^{\infty} \Psi_n^* \left( \sum_{i=1}^{N} q_e \vec{r}_i \right) \Psi_m , dr $$</p><p>Where:</p><ul><li>$N$: Number of electrons in a molecule</li><li>$\vec{r}_i$: Position of each electron</li><li>$q_e$: electron charge</li><li>$\Psi_n$: Excited state (or final state) molecular wave function</li><li>$\Psi_m$: Ground state (or initial state) molecular wave function</li></ul><p>Note: each molecular wavefunction depends on the position of BOTH Nucleus AND electron but for now, let’s focus on electron wavefunction (i.e. no structural change of molecular structure or atom position).<br>Larger overlap of initial and final state wavefunctions means higher transition probability, which generates higher extinction coefficient (Fermi’s golden rule).</p><p><strong>Wavefunction overlap:</strong> Larger wavefunction overlap of initial and final state means higher transition probability, which generates higher extinction coefficient (Fermi’s golden rule).</p><p><strong>Orbital Symmetry:</strong></p><p>$\int_{-\infty}^{\infty} f(r ) , dr = 0 \quad \text{if} \quad f(r )$ is an odd function; i.e., $f(-x) = -f(x)$<br>$\int_{-\infty}^{\infty} f(r ) , dr \neq 0 \quad \text{if} \quad f(r )$ is an even function; i.e., $f(-x) = f(x)$</p><p>So: $\Psi_n^* \tilde{\Psi}_m$ must be an even function;<br>or: $\Psi_n^* \Psi_m$ must be an odd function (e.g., } $\pi$ and $\pi^ *$ state</p><h3 id="Dipole-strength-and-Oscillator-strength">Dipole strength and Oscillator strength</h3><p>Traditional ways to quantify the “strength of a transition”</p><p><strong>Dipole strength</strong>: $D_{mn} = |\mu_{mn}|^2 = 9.18 \times 10^{-3} \int \left( \frac{\varepsilon}{\nu} \right) d\nu $<br><strong>Oscillator strength</strong>: $f_{mn} = 4.315 \times 10^{-9} \int \varepsilon(\nu) d\nu $<br>Area under the spectrum associated with the <em><strong>m→n</strong></em> transition</p><p>$ f_{mn} \approx 0.1-1 $ Strong absorption (heme, chlorophyll, organic dyes)<br>$ f_{mn} \approx 10^{-5} $ Weak absorption</p><h3 id="Kuhn-Thomas-sum-rule-for-oscillator-strengths">Kuhn-Thomas sum rule for oscillator strengths</h3><p>In any molecule with N electrons the sum of the oscillator strengths from any one state to all of the other states is equal to the sum of the electrons<br>$$<br>\sum_j f_{ij} = N<br>$$</p><p>This means that <strong>the area underneath the absorption spectrum is a constant</strong> (ground state is the initial state).<br>If a molecule is perturbed (change environments) then if one transition goes down, another must go up.</p><div class="admonition note"><p class="admonition-title">Every transition is associated with a transition dipole</p><p>The transition dipole is a vector:</p><ul><li>Direction: point in the direction of the electron displacement</li><li>Amplitude: Strength of the absorption.</li></ul></div><h3 id="Possible-transitions-in-UV-Vis-light-range">Possible transitions in UV-Vis light range</h3><table><thead><tr><th style="text-align:center"><img src="http://www.chem.ucla.edu/~bacher/UV-vis/electronic_energy_diagram.jpg" alt=""></th><th style="text-align:center"><img src="http://www.chem.ucla.edu/~bacher/UV-vis/UV_vis_tetracyclone.jpg" alt=""></th></tr></thead><tbody></tbody></table><p><a href="http://www.chem.ucla.edu/~bacher/UV-vis/uv_vis_tetracyclone.html.html">© ucla.edu</a></p><p><em><strong>σ→σ<sup>*</sup></strong></em> often requires absorption of photons higher than the UV- vis range (200-700 nm).</p><h3 id="What-determines-the-probability-of-a-transition-strength-of-the-absorption-band">What determines the probability of a transition? (strength of the absorption band)</h3><ol><li>Orbital overlap (wavefunction)<br>π →π * Large overlap, more likely to happen, strong absorption<br>n →π * Small overlap, weak absorption</li><li>Spin multiplicity<br>Electrons prefer not to change its intrinsic spin direction after absorption.</li></ol><p><img src="https://imgur.com/zDYufIq.png" alt=""></p><h3 id="Effective-light-matter-interaction">Effective light-matter interaction</h3><p><img src="https://imgur.com/xIpNElQ.png" alt=""></p><ul><li><p><strong>Transition Rate:</strong><br>$$ Rate_{1 \rightarrow 2} = B_{12} \rho(\nu) [S_1]$$<br>Radiation field density</p></li><li><p><strong>Component of the Electric Field:</strong><br>$$ E_{\parallel} = |\vec{E}| \cos \theta$$</p></li><li><p><strong>Density of States:</strong><br>$$ \rho(\nu) \propto |E_{\parallel}|^2 = |\vec{E}|^2 \cos^2 \theta$$</p></li></ul><h3 id="Effective-light-matter-interaction-v2">Effective light-matter interaction</h3><p>The density of states $\rho(\nu)$ is proportional to the square of the parallel component of the electric field:</p><p>$$ \rho(\nu) \propto |E_{\parallel}|^2 = |\vec{E}|^2 \cos^2 \theta$$</p><p>This relationship is depicted through diagrams that illustrate the electric field vector $\vec{E}$ relative to the molecular transition dipole moment $\vec{\mu}$. The alignment of $\vec{E}$ with $\vec{\mu}$ affects the absorption, with maximum absorption when they are parallel and zero absorption when they are perpendicular. This is exemplified by the molecular orientations of adenine shown in the image.</p><h3 id="Absorption-emission-and-stimulated-emission">Absorption, emission, and stimulated emission</h3><p>The rates of absorption, emission, and stimulated emission can be described by the following equations:</p><table><thead><tr><th style="text-align:left">Rates</th><th style="text-align:left">Equetion</th></tr></thead><tbody><tr><td style="text-align:left"><strong>Absorption Rate</strong></td><td style="text-align:left">$ Rate_{abs} = B_{12} \rho(\nu) [S_1] $</td></tr><tr><td style="text-align:left"><strong>Emission Rate</strong></td><td style="text-align:left">$ Rate_{emi} = -A_{21} [S_2] $</td></tr><tr><td style="text-align:left"><strong>Stimulated Emission Rate</strong></td><td style="text-align:left">$ Rate_{se} = -B_{21} \rho(\nu) [S_2] $</td></tr></tbody></table><p>At steady state, the rate of upward transitions (absorption) equals the rate of downward transitions (emission and stimulated emission):<br>$$ B_{12} \rho(\nu) [S_1] = A_{21} [S_2] + B_{21} \rho(\nu) [S_2] $$</p><p><mark>A<sub>21</sub>, B<sub>12</sub>, and B<sub>21</sub> are called Einstein coefficients.</mark></p><p><img src="https://imgur.com/xLU05Eh.png" alt=""></p><p>It can be shown that:</p><ol><li>B<sub>12</sub>=B<sub>21</sub></li><li>$\frac{A_ {21}}{B_ {21}} = \frac{16\pi^ 2 \hbar \nu^ 3}{c^ 3}$</li></ol><p>Faster spontaneous emission at higher Frequency</p><p>In a typical UV-Vis spectroscopy (electronic transitions)</p><p><strong>Conditions:</strong> $ A \gg B \rho(\nu) $<br><strong>Einstein coefficients relationships:</strong> $ B_{12} \rho(\nu) [S_1] = A_{21} [S_2] + B_{21} \rho(\nu) [S_2] \approx A_{21} [S_2] $<br><strong>Approximations:</strong> $ \frac{B_{12} \rho(\nu)}{A_{21}} \frac{[S_2]}{[S_1]} \ll 1 $<br><strong>Population of states:</strong> $ [S_2] \ll [S_1] $<br>The population of the excited state never builds up to a significant amount.</p><p>laser requires stimulated emission rate constant, i.e. B21, to be much larger than the spontaneous emission rate constant, i.e. A21.<br>So, UV laser is harder to make than visible light laser</p><h2 id="Boltzmann-Distribution">Boltzmann Distribution</h2><p>The probability $P_i$ of a system being in a state i with energy $E_i$ at temperature T is given by:</p><p>$$ P_i = \frac{e^{-\frac{E_i}{k_B T}}}{\sum_{i=0}^{M} e^{-\frac{E_i}{k_B T}}} = \frac{e^{-\frac{E_i}{k_B T}}}{Q} $$</p><p>Where:</p><ul><li>$Q$: Partition function</li><li>$E_i$: Energy of the ith state</li><li>$k_B$: Boltzmann constant $= 1.38 \times 10^{-23} J/K$</li><li>$T$: Temperature (K)</li></ul><p>The ratio of probabilities between two states i and j is given by:</p><p>$$ \frac{P_i}{P_j} = e^{-\frac{(E_i - E_j)}{k_B T}} = e^{-\frac{\Delta E}{k_B T}} $$</p><h3 id="Quantum-Mechanical-Harmonic-Oscillator">Quantum Mechanical Harmonic Oscillator</h3><table><thead><tr><th style="text-align:center"><img src="https://www.researchgate.net/profile/Mauricio-Palafox/publication/277716088/figure/fig6/AS:651496681140224@1532340321204/Light-behaves-in-exactly-the-same-way-as-a-quantum-harmonic-oscillator-and-has-the-same.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.researchgate.net/publication/277716088_Spectra_and_structure_of_benzonitriles_and_some_of_its_simple_derivatives_Spectra_and_structure_of_benzonitriles_and_some_of_its_simple_derivatives?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Mauricio Alcolea Palafox</a></td></tr></tbody></table><table><thead><tr><th style="text-align:center"><img src="https://www.researchgate.net/profile/Anas-Al-Rabadi/publication/228529042/figure/fig3/AS:393722123571205@1470882077644/Harmonic-oscillator-HO-potential-and-wavefunctions-a-wavefunctions-for-various.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.researchgate.net/figure/Harmonic-oscillator-HO-potential-and-wavefunctions-a-wavefunctions-for-various_fig3_228529042">© Anas Al-Rabadi</a> The panel left: wave funciton, the panel right: porbability of finding a nuclei</td></tr></tbody></table><ul><li>Solve the time-independent Schrödinger Equation (for the nucleus) with<ul><li>$ V(x) = \frac{1}{2}kx^2 $</li></ul></li><li>The time-independent Schrödinger Equation is:<ul><li>$ -\frac{\hbar^2}{2m} \frac{d^ 2\Psi(x)}{dx^2} + V(x)\Psi(x) = E\Psi(x) $</li></ul></li><li>We get a set of wave functions and a set of energies:<ul><li>$ \Psi_n(x) \quad$ (set of wave functions)</li><li>$ E_n \quad $ (set of energies)</li></ul></li><li>For a <mark>harmonic oscillator potential</mark>, the energy levels are given by:<ul><li>$ E_n = \left(n + \frac{1}{2}\right)\hbar\omega $</li><li>$ \omega_0 = \sqrt{\frac{k}{m_r}} = 2\pi\nu_0 $<br>where $ n = 0,1,2,3,\ldots $</li></ul></li></ul><table><thead><tr><th style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Franck-Condon-diagram.png/800px-Franck-Condon-diagram.png" alt=""></th><th style="text-align:center"><img src="https://api.oe1.com/upload/2023/06/1_638205815647121689-20230606104546661.jpg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://en.wikipedia.org/wiki/Deslandres_table">© wikipedia</a> Vibrational energy of the nuclei on top of electronic energy</td><td style="text-align:center"><a href="https://www.oe1.com/article/7071678131478081536.html">© oe1.com</a> Jablonski energy diagram</td></tr></tbody></table><p><mark>VERTICAL TRANSITION</mark>: consider the nuclei to remain in the same place during an electronic transition.<br>At thermal equilibrium, most molecules will be in the lowest vibrational state</p><h3 id="Franck-Condon-factor-nuclear">Franck-Condon factor (nuclear)</h3><p>The total wavefunction $\Psi(r, R)$ is a product of the electronic $\Psi_{el}(r, R)$ and nuclear $\Psi_{nuc}®$ wavefunctions:</p><p>$$ \Psi(r, R) = \Psi_{el}(r, R)\Psi_{nuc}( R) $$</p><ul><li><code>electrons</code> refers to $\Psi_{el}(r, R)$</li><li><code>nuclei</code> refers to $\Psi_{nuc}(R )$</li></ul><p>Transition from vibrational level i of the ground electronic state to the vibrational level j of the exited electronic state is given by:</p><p>$$ \vec{\mu}_ {g \rightarrow ex,j} = \left( \vec{\mu}_ {g \rightarrow ex} \right) \int \Psi_ {nuc(j)}^* \Psi_ {nuc(i)} dR $$</p><ul><li>The electron transition dipole moment $\vec{\mu}_{g \rightarrow ex}$ represents the <code>Electron transition dipole moment</code>.</li><li>Rest of the integral represents the <code>Nuclear overlap Factor</code>, also known as the Franck-Condon factor.</li></ul><table><thead><tr><th style="text-align:center">Vibrational structure and the Franck- Condon principle: vertical transitions</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/tykLmAg.png" alt=""></td></tr></tbody></table><h3 id="Spectroscopic-broadening">Spectroscopic broadening</h3><h4 id="Intrinsic">Intrinsic</h4><table><thead><tr><th>Column1</th><th>Column2</th><th>Column3</th></tr></thead><tbody><tr><td>Vibrational Structure</td><td><img src="https://imgur.com/pmgO3tQ.png" alt=""></td><td><img src="https://imgur.com/eEq28Js.png" alt=""></td></tr><tr><td>Overlapping Electronic Bands</td><td><img src="https://imgur.com/g55mveQ.png" alt=""></td><td><img src="https://imgur.com/WxAS0gX.png" alt=""></td></tr></tbody></table><h4 id="Environment-solvent-effect">Environment: solvent effect</h4><p><img src="https://imgur.com/UzP9Z1j.png" alt=""></p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center"><li>25% λ1 (state 1)<li> 50% λ2 (state 2)<li> 25% λ3 (state 3)</td><td style="text-align:center"><img src="https://imgur.com/FIuq6xu.png" alt=""></td></tr></tbody></table><h3 id="Solvent-effects-on-the-absorption-spectrum-of-anisole">Solvent effects on the absorption spectrum of anisole</h3><table><thead><tr><th style="text-align:center"><img src="https://psiberg.com/wp-content/uploads/2021/09/chromophoric-shift-path.svg" alt="Hypochromic shift"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://psiberg.com/uv-vis-spectroscopy">© PSIBERG Team</a></td></tr></tbody></table><p><img src="https://imgur.com/eU4ljWs.png" alt=""></p><p>to left: Bathochromic or <mark>Red shift</mark><br>to right: Hypsochromic or <mark>Blue shift</mark><br>The nature of the changes are not always simple to predict.</p><div class="admonition note"><p class="admonition-title">How does the solvent influence the ground and excited states?</p><ol><li>SOLVENT POLARITY: permanent dipole of the solvent molecules measured by the static dielectric constant. <em><strong>ε~r~</strong></em></li><li>SOLVENT POLARIZABILITY: electron polarizability measured by index of refraction. <em><strong>n</strong></em></li><li>Hydrogen bonding (protic vs. aprotic solvent)</li></ol></div><p><img src="https://imgur.com/q6g0Azf.png" alt=""></p><div class="admonition note"><p class="admonition-title">More on the dielectric constant</p></div><p><strong>Material 1</strong>: High <em><strong>ε<sub>r</sub></strong></em>, therefore higher ability to cancel out (stabilize) the original source charge<br><strong>Material 2</strong>: Low <em><strong>ε<sub>r</sub></strong></em>, The ability to insulate charge or The ability to stabilize charges</p><p>The relative permittivity $\varepsilon_r$ as a function of frequency $\omega$ is given by:</p><p>$$ \varepsilon_r(\omega) = \frac{\varepsilon(\omega)}{\varepsilon_0} $$</p><p>Where:</p><ul><li>$\varepsilon_0$: vacuum permittivity (= 1.0)</li><li>$\varepsilon$: material’s absolute permittivity</li><li>$\varepsilon_r$: relative permittivity or dielectric constant</li></ul><p>Examples of relative permittivity for different materials:</p><ul><li>$\varepsilon_r$ (styrofoam) = 1.03</li><li>$\varepsilon_r$ (dry wood) = 1.4 - 2.9</li><li>$\varepsilon_r$ = 20</li></ul><p>Relative permittivity values for various solvents:</p><table><thead><tr><th>Solvent</th><th>Hexane</th><th>Ether</th><th>Ethanol</th><th>Methanol</th><th>Water</th></tr></thead><tbody><tr><td>$\varepsilon_r$</td><td>2</td><td>4.3</td><td>25.8</td><td>31</td><td>81</td></tr></tbody></table><p>Small value means it is non-polar solvent. Large value means it is a polar solvent.</p><h3 id="Polar-effects-on-transitions-between-molecular-orbitals">Polar effects on transitions between molecular orbitals</h3><table><thead><tr><th style="text-align:center"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0030402617306472-gr9_lrg.jpg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.sciencedirect.com/science/article/pii/S0030402617306472#fig0045">© Vaishali Gupta</a></td></tr></tbody></table><table><thead><tr><th>Compound</th><th>λ(nm)</th><th>Intensity/ε</th><th>Transition with lowest energy</th></tr></thead><tbody><tr><td>CH₄</td><td>122</td><td>intense</td><td>σ→σ* (C-H)</td></tr><tr><td>CH₃CH₃</td><td>130</td><td>intense</td><td>σ→σ* (C-C)</td></tr><tr><td>CH₃OH</td><td>183</td><td>200</td><td>n→σ* (C-O)</td></tr><tr><td>CH₃SH</td><td>235</td><td>180</td><td>n→σ* (C-S)</td></tr><tr><td>CH₃NH₂</td><td>210</td><td>800</td><td>n→σ* (C-N)</td></tr><tr><td>CH₃Cl</td><td>173</td><td>200</td><td>n→σ* (C-Cl)</td></tr><tr><td>CH₃I</td><td>258</td><td>380</td><td>n→σ* (C-I)</td></tr><tr><td>CH₂=CH₂</td><td>165</td><td>16000</td><td>π→π* (C=C)</td></tr><tr><td>CH₃COCH₃</td><td>187</td><td>950</td><td>π→π* (C=O)</td></tr><tr><td>CH₃COCH₃</td><td>273</td><td>14</td><td>n→π* (C=O)</td></tr><tr><td>CH₃CSCl₃</td><td>460</td><td>weak</td><td>n→π* (C=S)</td></tr><tr><td>CH₃N=NCCH₃</td><td>347</td><td>15</td><td>n→π* (N=N)</td></tr></tbody></table><h3 id="Polarity-effect-on-π-π-transitions">Polarity effect on π-π* transitions</h3><p><strong>Typical π-π<sup>*</sup> transitions</strong>: the dipole gets larger in the same direction<br>More stabilization, energy increases; <em><strong>high solvent polarity results in a RED SHIFT (of the absorption peak)</strong></em>.<br><img src="https://imgur.com/gvx01OG.png" alt=""><br>In a more polar state: More stabilization, energy increases; high solvent polarity results in a RED SHIFT (of the absorption peak).</p><p><strong>Typical n-π<sup>*</sup> transitions</strong>: the dipole of the chromophore <strong>gets smaller</strong> or shifts direction after excitation.<br><img src="https://imgur.com/OIis4vS.png" alt=""><br>Less stabilization, energy increases; <strong>high solvent polarity</strong> results in a <strong>BLUE SHIFT (of the absorption peak)</strong></p><h3 id="Example-spectral-shifts-of-mesityl-oxide">Example: spectral shifts of mesityl oxide</h3><table><thead><tr><th style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/d/d8/Mesityl_oxide.png" alt="Mesityl oxide"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://en.wikipedia.org/wiki/Mesityl_oxide">© wikipedia</a></td></tr></tbody></table><p>The solvent effects on the absorption maxima (λmax) for the π→π* and n→π* transitions in acetone:</p><table><thead><tr><th>solvent</th><th>Static dielectric constant</th><th>λmax (nm) π→π* (Red shift)</th><th>λmax (nm) n→π* (Blue shift)</th></tr></thead><tbody><tr><td>Hexane</td><td>2</td><td>229.5</td><td>327</td></tr><tr><td>Ether</td><td>4.3</td><td>230</td><td>326</td></tr><tr><td>Ethanol</td><td>25.8</td><td>237</td><td>315</td></tr><tr><td>Methanol</td><td>31</td><td>238</td><td>312</td></tr><tr><td>Water</td><td>81</td><td>244.5</td><td>305</td></tr></tbody></table><ul><li><mark>Red shift</mark> indicates a lower energy transition as the dielectric constant increases.</li><li><mark>Blue shift</mark> indicates a higher energy transition as the dielectric constant decreases.</li></ul><p>The transitions are characterized by their molar absorptivities (ε):</p><ul><li>n→π*: ε = 40M⁻¹cm⁻¹</li><li>π→π*: ε = 12,600M⁻¹cm⁻¹</li></ul><h3 id="Indol-Tryptophan-π-π-transitions">Indol (Tryptophan) π-π* transitions</h3><table><thead><tr><th style="text-align:center"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1386142506001314-gr1.jpg" alt=""></th><th style="text-align:center"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1386142506001314-gr3.gif" alt=""></th></tr></thead><tbody></tbody></table><p><a href="https://www.sciencedirect.com/science/article/pii/S1386142506001314?via%3Dihub">© Neera Sharma</a><br><em><strong>L<sub>a</sub></strong></em>: large excited state dipole (lower energy state)<br><em><strong>L<sub>b</sub></strong></em>: Smaller excited state dipole</p><h3 id="Solvent-polarizability-measured-by-the-index-of-refraction">Solvent polarizability measured by the index of refraction</h3><p>dipole is induced in the solvent by the dipole of the chromophore (ground state and excited state)<br>No nuclear movement involved. Purely due to electrons</p><p><img src="https://imgur.com/qK3vQLw.png" alt=""><br>ground state dipole ( ← )<br>excited state (↑)<br>induced dipoles in solvent ( ← )</p><h3 id="Some-values-of-solvent-index-of-refraction">Some values of solvent index of refraction</h3><table><thead><tr><th style="text-align:left">solvent</th><th style="text-align:center">Index of refraction</th></tr></thead><tbody><tr><td style="text-align:left">Perfluoropentane</td><td style="text-align:center">1.239</td></tr><tr><td style="text-align:left">Water</td><td style="text-align:center">1.333</td></tr><tr><td style="text-align:left">Ethanol</td><td style="text-align:center">1.362</td></tr><tr><td style="text-align:left">Iso-octane</td><td style="text-align:center">1.392</td></tr><tr><td style="text-align:left">Chloroform</td><td style="text-align:center">1.446</td></tr><tr><td style="text-align:left">Carbontetrachloride</td><td style="text-align:center">1.463</td></tr></tbody></table><p>Note that water is less polarizable than iso-octane although clearly water is a much more polar solvent (larger dielectric constant)</p><h3 id="Influence-on-the-energy-levels-of-π-n-π-orbitals-by-solvent-polarizability">Influence on the energy levels of π, n, π* orbitals by solvent polarizability</h3><p><img src="https://imgur.com/g0UTFzH.png" alt=""></p><ul><li>π - π* transitions: red shift in more polarizable solvent<br>π* interacts with solvent dipoles more strongly than π</li><li>n - π* transitions: blue shift in more polarizable solvent<br>n interacts with solvent dipoles more strongly than π*</li></ul><h3 id="Solvent-can-influence-the-energy-of-both-the-ground-and-the-excited-states">Solvent can influence the energy of both the ground and the excited states</h3><p><img src="https://imgur.com/WPrGRUi.png" alt=""></p><h3 id="Rhodopsin-a-protein-“solvent”-effect-on-the-absorption-spectrum-of-retinal">Rhodopsin: a protein “solvent” effect on the absorption spectrum of retinal</h3><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/fwb7LLC.png" alt=""></td><td style="text-align:left">vision is due to same pigment proteins in <strong><mark>rod</mark></strong> and <strong><mark>cone</mark></strong> cells:<li> λmax = 500 nm (rod cell)<li> λmax = 414 nm (blue cone)<li> λmax = 533 nm (green cone)<li> λmax = 560 nm (red cone) <br><br> Same chromophore: 11-cis retinal <br> “spectral tuning” by interaction with amino acid residues nearby</td></tr></tbody></table><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/zpUcJAg.png" alt=""></td><td style="text-align:left"><li>WT: 500 nm<li> G90S: 487 nm<li> T118A: 484 nm<li> E122D: 477 nm<li> A292S: 489 nm<li> A295S: 498 nm<li> T/E/A triple mutant: 453 nm</td></tr></tbody></table><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">OPTICAL SPECTROSCOPY – THE ABSORPTION PROCESS</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Artificial Intelligent 1</title>
    <link href="https://karobben.github.io/2024/01/26/LearnNotes/ai-probability/"/>
    <id>https://karobben.github.io/2024/01/26/LearnNotes/ai-probability/</id>
    <published>2024-01-26T07:22:21.000Z</published>
    <updated>2024-02-14T16:06:42.182Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Random-Variable">Random Variable</h2><p>Probability:</p><ul><li>Exp1: $Pr(A) &gt; 0$ which means that A has non-negative probability.</li><li>Exp2: $Pr(A) = 1$ means when event A occurs, the probability is 1.</li><li>Exp3: $Pr(A \cap B) = Pr(A) × Pr(B)$ when events A and B are independent.</li></ul><div class="admonition question"><p class="admonition-title">What is <b>Random Variable</b>?</p><ul><li>We use <b>capital letters</b> to denote the random variable</li><li>We use a small letter to denote a particular outcome of the experiment</li></ul></div><p>$P(X=x)$ means the probability of the occurs for value x. Here $P(X=x)$ is a <mark>number</mark>, the $P(X)$ is a distribution.</p><div class="admonition note"><p class="admonition-title">Example </p><p>Event = [Cloud, Cloud, Rain]In this Weather event P(X), it the probability of:</p><ul><li>Cloud: $P(X=Cloud) = \frac{2}{3}$.</li><li>Rain: $P(X=Rain) = \frac{1}{3}$</li><li>Sun:  $P(X=sun) = 0$</li></ul></div><p>The random variable we used in the example above is <mark>Discrete</mark> random variable, but sometimes we have to use continues random variable. For example: $X \in R$ (the set of all positive real numbers)</p><p>Because we have two types of random variable, the function for calculating the sun of all possible variables are different:</p><ul><li><strong>Probability Mass Function</strong> (pmf): For discrete random variable<ul><li>If <em>X</em> is a <strong>discrete random variable</strong>, then <em>P(X)</em> is its <strong>probability mass function (pmf)</strong>.</li><li>A probability mass is just a probability. <em>P(X=x) = Pr(X=x)</em> is the just the probability of the outcome <em>X = x</em> Thus:</li><li>$0 \leqslant P(X=x)$</li><li>$ 1 = \sum _x{P(X=x)}$</li></ul></li><li><strong>Probability Density Function</strong> (pdf):<ul><li>If <em>X</em> is a <strong>density random variable</strong>, then <em>P(x)</em> is its <strong>probability density function (pdf)</strong>.</li><li>A probability density is NOT a probability. Instead, we define it as a density $P(X=x) = \frac{d}{dx} Pr(X \leqslant x)$</li><li>$0 \leqslant P(X=x)$</li><li>$ 1 = \int_{-\infty}^\infty {P (X=x)dx}$</li></ul></li></ul><h3 id="Jointly-Random-Variables">Jointly Random Variables</h3><ul><li>Two or three random variables are “jointly random” if they are both outcomes of the same experiment.</li><li>For example, here are the temperature (Y, in °C), and precipitation (X, symbolic) for six days in Urbana:</li></ul><table><thead><tr><th>Date</th><th>X=Temperature (°C)</th><th>Y=Precipitation</th></tr></thead><tbody><tr><td>January 11</td><td>4</td><td>cloud</td></tr><tr><td>January 12</td><td>1</td><td>cloud</td></tr><tr><td>January 13</td><td>-2</td><td>snow</td></tr><tr><td>January 14</td><td>-3</td><td>cloud</td></tr><tr><td>January 15</td><td>-3</td><td>clear</td></tr><tr><td>January 16</td><td>4</td><td>rain</td></tr></tbody></table><p>For this table, we could have joint random variables <em>P(X=x, Y=y)</em>:</p><table><thead><tr><th>P(X=x,Y=y)</th><th>snow</th><th>rain</th><th>cloud</th><th>clear</th></tr></thead><tbody><tr><td>-3</td><td>0</td><td>0</td><td>1/6</td><td>1/6</td></tr><tr><td>-2</td><td>1/6</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1/6</td><td>0</td></tr><tr><td>4</td><td>0</td><td>1/6</td><td>1/6</td><td>0</td></tr></tbody></table><h4 id="Notation-Vectors-and-Matrices">Notation: Vectors and Matrices</h4><ul><li>A normal-font capital letter (<em>X</em>) is a random variable, which is a function mapping from the outcome of an experiment to a measurement</li><li>A normal-font small letter (<em>x</em>) is a scalar instance</li><li>A boldface small letter (<em><strong>x</strong></em>) is a vector instance</li><li>A boldface capital letter (<em><strong>X</strong></em>) is a matrix instance</li></ul><p><em>P(X=<strong>x</strong>)</em> is the probability that random variable <em>X</em> takes the value of the vector <em><strong>x</strong></em>. This is just a shorthand for the joint distribution of <em>x<sub>1</sub>, x<sub>2</sub>, …, x<sub>n</sub></em></p><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/FRHL0fI.png" alt="vector x"></th></tr></thead><tbody></tbody></table><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/ihgeuXc.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">When <em><strong>X</strong></em> is a random matrix</td></tr></tbody></table><h3 id="Marginal-Distributions">Marginal Distributions</h3><p>Suppose we know the joint distribution <em>P(X,Y)</em>. We want to find the two <strong>marginal distributions</strong> <em>P(X)</em>:</p><ul><li>If the unwanted variable is discrete, we marginalize by adding:<ul><li>$P(X) = \sum_ y P(X,Y=y)$</li></ul></li><li>If the unwanted variable is continuous, we marginalize by integrating:<ul><li>$P(X) = \int P(X,Y=y)$</li></ul></li></ul><p>Backing the table above, we could know that the marginal distributions of:</p><ul><li><em>P(X)</em> = 1/6 + 1/6 = 1/3; 1/6; 1/6; 1/6 + 1/6 = 1/3</li><li><em>P(Y)</em> = 1/6; 1/6; 1/6 + 1/6 + 1/6 = 1/2; 1/6</li></ul><p>PS: Some place also write <em>P(X)</em> as <em>P<sub>X</sub>(X)</em> or <em>P<sub>X</sub>(i)</em> and <em>P(Y)</em> as <em>P<sub>Y</sub>(Y)</em> or <em>P<sub>Y</sub>(j)</em>.</p><h3 id="Joint-and-Conditional-distributions">Joint and Conditional distributions</h3><p>With the joint possibility and marginal possibility, we could now calculating the Joint and Conditional distributions, which is <em>P(Y|X)</em></p><ul><li><em>P(Y|X)</em> is the probability (or pdf) that <em>Y = y</em> happens, given that <em>X = x</em> happens, over all <em>x</em> and <em>y</em>. This is called the <strong>conditional distribution</strong> of <em>Y</em> given <em>X</em>.</li><li><em>P(X|Y)</em> is the conditional probability distribution of outcomes <em>P(X=x|Y=y)</em></li><li>The conditional is the joint divided by the marginal:<ul><li>$P(X=x|Y=y) = \frac{P(X = x, Y = y)}{P(Y= y)}$</li></ul></li></ul><div class="admonition note"><p class="admonition-title">Exp of Joint and Conditional Distribution *P(X|Y = cloud)*</p><p>$P(X|Y=could) = \frac{P(X, Y = y)}{P(Y = cloud)}$</p><p>$=\frac{\frac{1}{6}\ \ 0\ \ \frac{1}{6}\ \ \frac{1}{6}}{\frac{1}{2}}$</p><p>So, the result is a vector = {1/3, 0, 1/3, 1/3}</p></div><p>According to the example, we could know that: <mark>Joint = Conditional×Marginal</mark>; which is:<br>$$<br>P(X,Y) = P(X|Y)P(Y)<br>$$</p><h3 id="Independent-Random-Variables">Independent Random Variables</h3><ul><li>Two random variables are said to be independent, which means <em>P(X|Y) = P(X)</em><br>In other words, knowing the value of <em>Y</em> tells you nothing about the value of <em>X</em>.</li><li>According to this, we can also know:<ul><li>$\because$ <em>P(X,Y) = P(X|Y)P(Y)</em></li><li>$\therefore$ <em>P(X,Y) = P(X)P(Y)</em></li></ul></li><li><em>Pr(A⋀B) = Pr(A)Pr(B)</em></li></ul><h3 id="Expectation">Expectation</h3><p>The expected value of a function is its weighted average, weighted by its pmf or pdf.</p><ul><li>For discrete X and Y:<br>$ E[f(X, Y)] = \sum_{x,y} f(x, y)P(X = x, Y = y) $</li><li>If X is continuous:<br>$ E[f(X, Y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y)P(X = x, Y = y) ,dx,dy $</li></ul><h3 id="Covariance">Covariance</h3><p>The covariance of two random variables is the expected product of their deviations:</p><p>$$<br>Covar(X,Y) = E[(X- E[X])(Y-E[Y])]<br>$$</p><ul><li>( E[X] ) is the expected value (or mean) of the random variable X.</li><li>( E[Y] ) is the expected value (or mean) of the random variable Y.</li><li>( X - E[X] ) is the deviation of X from its mean (how far X is from its mean).</li><li>( Y - E[Y] ) is the deviation of Y from its mean (how far Y is from its mean).</li><li>( E\left[(X - E[X])(Y - E[Y])\right] ) is the expected value of the product of these deviations.</li></ul><div class="admonition note"><p class="admonition-title">Example</p><p>Suppose we have two random variables, X and Y, with the following values:</p><ul><li>X: 1, 2, 3</li><li>Y: 2, 3, 4First, we calculate the $E[X]$ and $E(Y)$:</li><li>$E[X] = \sum_{i=i}^3 f(x_i)P(x_i) = 1×\frac{1}{3}+ 2×\frac{1}{3}+3×\frac{1}{3} = 2$</li><li>$E[Y] = \sum_{i=i}^3 f(y_i)P(y_i) = 2×\frac{1}{3}+ 3×\frac{1}{3}+4×\frac{1}{3} = 3$</li><li><strong>PS</strong>: $E[X] = mean(X)$ because when we calculating them through the whole list, we would count them one by one even though they are duplicated. In this case, if one element for example, the frequent of the x in X is 50% and the lenghth of the X is 10, we list x 5 times as the frequent of 1/10 equals doing one time by $x * \frac{1}{2}$Next, we calculate the deviations of each value from their means:</li><li>Deviations for $X-E[X] = [1-2, 2-2, 3-2] = [-1, 0, 1]$</li><li>Deviations for $Y-E[Y] = [2-3, 3-3, 4-3] = [-1, 0, 1]$</li></ul><p>Now we multiply these deviations pairwise and sum them up:</p><ul><li>$ (-1 \times -1) + (0 \times 0) + (1 \times 1) = 1 + 0 + 1 = 2 $</li></ul><p>Since we have three observations, we divide the sum by 3-1 (in the case of sample covariance) or simply by 3 (if we are dealing with a population).So, if we treat these as a population, the covariance is:</p><ul><li>Covar$(X, Y) = \frac{2}{3}$This positive value suggests that X and Y tend to increase together.</li></ul></div><p>for python code:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><br>X = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>])<br>Y = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br>(X - X.mean()) @ (Y - Y.mean()) / (<span class="hljs-built_in">len</span>(X))<br><br>X = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>])<br>Y = np.array([<span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])<br>(X - X.mean()) @ (Y - Y.mean()) / (<span class="hljs-built_in">len</span>(X))<br></code></pre></td></tr></table></figure></div><pre>2.5556-2.6665</pre><p>In this case, the covariance &gt; 0 means it is positively associated, covariance &lt; 0 means X and Y are negative-associated. Covariance = 0 means they are not associated at all.</p><p>Covariance Matrix:<br>Suppose <em>X = [X<sub>1</sub>, … , X<sub>n</sub>]</em> is a random vector. Its matrix of variances and covariances (a.k.a. covariance matrix) is</p><p>In other places, the covariance equation are mostly write as:<br>$$<br>Cov(X,Y) = \frac{\sum(X_i-\bar{X})(Y_j-\bar{Y})}{n}<br>$$</p><p>We can expected that they are the same because mostly, we expected the mean value is the expected value for a random variable.<br>\ tokens\ correctly\ classified}{n\ tokens\ total}$</p><ul><li>Error Rate<br>Equivalently, we could report error rate, which is just 1-accuracy:<br>$Error Rate = \frac{n\ tokens\ incorrectly\ classified}{n\ tokens\ total}$</li><li>Bayes Error Rate<br>The “Bayes Error Rate” is the smallest possible error rate of any classifier with labels “y” and features “x”:<br>$Error Rate = \sum_x P(X=x)\underset{y}{min} P(Y \neq y |x=x)$<br>It’s called the “Bayes error rate” because it’s the error rate of the Bayesian classifier</li></ul><h2 id="The-problem-with-accuracy">The problem with accuracy</h2><ul><li>In most real-world problems, there is one class label that is much more frequent than all others.<ul><li>Words: most words are nouns</li><li>Animals: most animals are insects</li></ul></li><li>Disease: most people are healthy</li><li>It is therefore easy to get a very high accuracy. All you need to do is write a program that completely ignores its input, and always guesses the majority class. The accuracy of this classifier is called the “chance accuracy.”</li><li>It is sometimes very hard to beat the chance accuracy. If chance=90%, and your classifier gets 89% accuracy, is that good, or bad?</li></ul><p>The solution: <mark>Confusion Matrix</mark>:<br>Confusion Matrix =<br>• <em><strong>(m, n)<sup>th</sup></strong></em> element is the number of tokens of the <em><strong>m<sup>th</sup></strong></em> class that were labeled, by the classifier, as belonging to the <em><strong>n<sup>th</sup></strong></em> class.</p><table><thead><tr><th style="text-align:center"><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/Basic-Confusion-matrix.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/">© Aniruddha Bhandari</a></td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Confusion matrix for a binary classifier</p><ul><li>Suppose that the correct label is either 0 or 1. Then the confusion matrix is just 2x2.<br></li><li>For example, in this box, you would write the # tokens of class 1 that were misclassified as class 0<br></li><li>Than, you got TP (True Positives), FN (False Negatives), FP (False Positives), and TN (True Negative)<br></li><li>The binary confusion matrix is standard in many fields, but different fields summarize its content in different ways.<br></li><li>In medicine, it is summarized using Sensitivity and Specificity.</li><li>In information retrieval (IR) and AI, we usually summarize it using Recall and Precision.</li></ul></div><h4 id="Specificity-and-Sensitivity">Specificity and Sensitivity</h4><ul><li>Specificity = True Negative Rate (TNR):<ul><li>$TNR = P(f(X) =0|Y=0) = \frac{TN}{TN+FP}$</li></ul></li><li>Sensitivity = True Positive Rate (TPR):<ul><li>$TRP = P(f(X) =1|Y=1) = \frac{TP}{TP+FN}$</li></ul></li><li>Precision:<ul><li>$P=P(Y =1|f(x)=1)=\frac{TP}{TP+FP}$</li></ul></li><li>Recall:<ul><li>Recall = Sensitivity = TPR:</li><li>$R = TRP = P(f(X) =1|Y=1) = \frac{TP}{TP+FN}$</li></ul></li></ul><h4 id="Training-Corpora">Training Corpora</h4><ul><li>Training vs. Test Corpora<br><strong>Training Corpus</strong>: a set of data that you use in order to optimize the parameters of your classifier (for example, optimize which features you measure, how you use those features to make decisions, and so on).<br><strong>Test Corpus</strong>: a set of data that is non-overlapping with the training set (none of the test tokens are also in the training dataset) that you can use to measure the accuracy.<ul><li>Measuring the training corpus accuracy is useful for debugging: if your training algorithm is working, then training corpus accuracy should always go up.</li><li>Measuring the test corpus accuracy is the only way to estimate how your classifier will work on new data (data that you’ve never yet seen).</li></ul></li></ul><div class="admonition note"><p class="admonition-title">Accuracy on which corpus?</p><ul><li>Large Scale Visual Recognition Challenge 2015: Each competing institution was allowed to test up to 2 different fully-trained classifiers per week.<br></li><li>One institution used 30 different e-mail addresses so that they could test a lot more classifiers (200, total). One of their systems achieved &lt;46% error rate – the competition’s best, at that time.<br></li><li>Is it correct to say that that institution’s algorithm was the best?</li></ul></div><ul><li>Training vs. development test vs. evaluation test corpora<ul><li><strong>Training Corpus</strong>: a set of data that you use in order to optimize the parameters of your classifier (for example, optimize which features you measure, what are the weights of those features, what are the thresholds, and so on).</li><li><strong>Development Test (DevTest or Validation) Corpus</strong>: a dataset, separate from the training dataset, on which you test 200 different fully-trained classifiers (trained, e.g., using different training algorithms, or different features) to find the best.</li><li><strong>Evaluation Test Corpus</strong>: a dataset that is used only to test the ONE classifier that does best on DevTest. From this corpus, you learn how well your classifier will perform in the real world.</li></ul></li></ul><h3 id="Summary">Summary</h3><ol><li><p>Bayes Error Rate:<br>$$<br>Error Rate = \sum_x P(X=x)\underset{y}{min} P(Y \neq y |x=x)<br>$$</p></li><li><p>Confusion Matrix, Precision &amp; Recall (Sensitivity)<br>$$P=P(Y =1|f(x)=1)=\frac{TP}{TP+FP}$$<br>$$R = TRP = P(f(X) =1|Y=1) = \frac{TP}{TP+FN}$$</p></li></ol><h4 id="Training-Corpora-v2">Training Corpora</h4><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Artificial Intelligent</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Schrodinger Function</title>
    <link href="https://karobben.github.io/2024/01/25/LearnNotes/schrodinger/"/>
    <id>https://karobben.github.io/2024/01/25/LearnNotes/schrodinger/</id>
    <published>2024-01-25T21:15:01.000Z</published>
    <updated>2024-01-31T05:30:45.767Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Energy-Momentum-Relation">Energy Momentum Relation</h2><p>$E^2 = (m_ 0 C ^2) ^2 + (pc)^ 2$</p><p>For Energy with rest mess: $E = m_0c^2$<br>For Energy with no rest mess: $E = pc$</p><p>Introducing Bacground:</p><ul><li>Light is a ruler; molecules are objects to be measured.</li><li>Visible light is a coarse ruler (300 nm resolution)</li><li>Molecules, on the other hand, are fine objects (0.1 nm)</li><li>How can we measure such a fine object with a coarse ruler?<ul><li>Use a finer ruler (electron microscopy)</li><li>Use indirect evidence to infer the information (today’s focus)</li></ul></li></ul><p>How to infer? What indirect evidence?</p><ul><li>Use a language to describe the matter (molecule).</li><li>Find information that is related to the size of a molecule.</li><li>Hopefully that information can be obtained by measurement (using light).</li><li>Infer the information about the molecule to be studied.</li><li>Here is one way to do this:<ul><li>Use the Schrödinger equation to describe the system (molecule, atoms, electrons).</li><li>Find out that energy of electrons is related to the size of the molecule.</li><li>Measure the energy of the system using light.</li><li>Infer the size of the molecule by interpreting the energy information.</li></ul></li></ul><h2 id="Use-the-Schrodinger-Equation-to-Describe-the-System">Use the Schrödinger Equation to Describe the System</h2><p>$$<br>-\frac{\hbar}{2m} \frac{d^2 \Psi (x)}{dx^ 2} + V(x)\Psi(x) = E\Psi(x)<br>$$</p><ul><li>V(x): Potential energy provides the constraints</li><li>E: Solve for Energy(also called eigenvalues)</li><li>$\Psi(x)$: Solve for wavefunctions (also called eigenvectors or eigenfunctions)</li></ul><p>We will end up with a series of wavefunctions with associated energies:<br>$$\Psi(x) \leftrightarrow E_ n $$</p><p>The Schrödinger Equation is a fundamental equation in quantum mechanics that describes how the quantum state of a physical system changes over time. It was formulated by Erwin Schrödinger in 1925. There are two forms of the Schrödinger Equation: the time-dependent and the time-independent forms.</p><h3 id="The-Meaning-of-Φ-x">The Meaning of Φ(x)</h3><p>$$<br>P(x_0, t_0)dx = \Psi^ * (x_0, t_0)\Psi(x_0, t_0)dx = |\Psi (x_0, t_0)|^2 dx<br>$$</p><p><em><strong>P(x<sub>0</sub>, t<sub>0</sub>)</strong></em>: <em>Probability</em>* of finding the particle (e.g. electron) within an interval<br>of <em>dx</em> of position <em>x<sub>0</sub></em> at time <em>t<sub>0</sub></em></p><h3 id="Properties-of-Φ-x">Properties of Φ(x);</h3><p>$$<br>\int_{-\infty}^{\infty} \Psi^* (x, t) \Psi (x, t)dx = 1<br>$$</p><p>Average value of f(x) over all space</p><p>$$<br>\left \langle  f(x) \right \rangle = \frac{ \int_{-\infty}^{\infty} \Psi^* (x) \Psi (x)f(x)dx }{\int_{-\infty}^{\infty} \Psi^* (x) \Psi (x)dx}<br>$$</p><h2 id="Find-Out-That-Energy-of-Electrons-Is-Related-to-the-Size-of-the-Molecule">Find Out That Energy of Electrons Is Related to the Size of the Molecule.</h2><h3 id="1-d-particle-in-a-box">1-d particle in a box</h3><p>Here <strong>Particle = electrons in a molecule</strong><br><strong>Layman language</strong>: Potential energy outside the box is infinite.<br><strong>Mathematical language</strong>: Boundary condition</p><p><img src="https://bouman.chem.georgetown.edu/S02/lect13/piab.gif" alt=""></p><p><em><strong>V(x) = 0 for L &gt; x &gt; 0</strong></em><br><em><strong>V(x) = ∞ for x ≥ L, x ≤ 0</strong></em></p><p>$$<br>\frac{d^ 2 \Psi(x)}{dx^ 2} = \frac{2m} {\hbar^ 2} [V(x)-E]\Psi(x)<br>$$</p><ol><li><p>Since <em><strong>V(x)</strong></em> is infinite outside the box, <em><strong>Ψ(x)</strong></em> must be zero outside the box<br>(otherwise $\frac{d^2 Ψ(x)}{dx^2}$ would be infinite: not allowed)</p></li><li><p>Since <em><strong>Ψ(x)</strong></em> must be continous, <em><strong>Ψ(x)</strong></em> inside the box must connect smoothly to <em><strong>Ψ(x)</strong></em> outside the box<br>Hence, $Ψ(0)= Ψ(L) = 0$</p></li></ol><p>$$ \Psi_ n (x) = \sqrt{\frac{2}{L}}  sin(\frac{n\pi x}{L}) $$</p><ul><li>$ E_ n= \frac{h^2 n ^2}{8ma ^ 2} $</li><li>$ where n = 1, 2, 3, 4… $</li><li>Energy of the electron (<em><strong>E</strong></em>) is related to the size of the molecule (<em><strong>L</strong></em>).</li></ul><p>With this theory, we could measure the energy of the system using light</p><ul><li>Pi-electrons behave as a “particle in a box: Molecules has different absorb peak when they as different number of conjugation bounds.</li></ul><div class="admonition note"><p class="admonition-title">The electrons and its orbitals</p><p>4 pi electrons =&gt; 2 orbitals8 pi electrons =&gt; 4 orbitals</p></div><h2 id="Estimate-bond-length-from-the-transition-energy">Estimate bond length from the transition energy</h2><p>$$<br>\Delta E = \frac{(n_ 3 ^ 2 - n_ 2^ 2) h^ 2 }{8mL^ 2}<br>$$</p><p>For this compound, There are <strong>4 pi electrons</strong>. Two each in the n=1 and n=2 orbitals. (This is due to electron spin, which we will see later).<br>The absorption is due to promoting an electron from the n=2 to the n=3 orbital.</p><ul><li>Infinite potential: infinite number of solutions</li><li>Finite potential:<ol><li>A finite number of solutions (5 in this example)</li><li>Wavefunctions are not zero at the boundary of the box</li><li>The wavefunctions have finite amplitude outside the box.<br>Finite chance the particle can be outside the box even though <em><strong>E&lt;V<sub>0</sub></strong></em></li></ol></li></ul><div class="admonition note"><p class="admonition-title">TUNNELING effect in Quantum Mechanics</p><p><img src="https://www.ntmdt-si.com/data/media/images/spm_basics/scanning_tunnel_microscopy_stm/stm_physical_backgrounds/tunneling_effect/img04.gif" alt="Quantum Tunneling" /> <a href="https://www.ntmdt-si.com/resources/spm-theory/theoretical-background-of-spm/1-scanning-tunnel-microscopy-%28stm%29/11-stm-physical-backgrounds/111-tunneling-effect">© ntmdt-si</a></p><p>a particle can go &quot;through&quot; an energy barrier instead of needing to have sufficient energy to go over the barrierWhen the wave go through the energy barrier, the exponential decay occurred inside of the energy barrier.</p></div><h2 id="Predict-the-Energy-and-Optical-Property">Predict the Energy and Optical Property</h2><p>If we know the structure of the molecule, can we predict the energy and optical property of the molecule?<br>Take the <strong>hydrogen atom</strong> as example.</p><ul><li>$V( r ) = \frac{e^ 2}{4\pi \epsilon_ 0 r}$</li><li><em><strong>e</strong></em> = electron charge</li><li><em><strong>ε<sub>0</sub></strong></em> = permittivity of free space</li></ul><h3 id="Predicted-the-Energy">Predicted the Energy</h3><p>Here we only need 3 quantum number: <em><strong>n</strong></em>, <em><strong>l</strong></em>, and <em><strong>m<sub>l</sub></strong></em></p><ul><li>the principal quantum number: <em><strong>n</strong></em>  = 1, 2, 3, 4…</li><li>the angular momentum quantum number: <em><strong>l</strong></em> = 0, 1, 2, … (n -1)</li><li>the magnetic quantum number: <em><strong>m<sub>l</sub></strong></em> = 0, ±1, ±2, … ± <em><strong>l</strong></em></li></ul><p>As you can see, the energy only depends on <em><strong>n</strong></em>:<br>$E_ n = - \frac{m_ e e^ 4}{8 \epsilon_ 0^ 2 h^2 n^ 2} = - \frac{2.179 × 10 ^ {-18}}{n^ 2}Joule = -\frac{13.6}{n^ 2}eV\ \ \ n = 1, 2, 3… $</p><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/Kwx6VEm.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">© Atkins, The Elements of Physical Chemistry</td></tr></tbody></table><h2 id="Cheat-Sheet">Cheat Sheet</h2><ul><li>Energy: kcal/mol, kJ/mol</li><li>Wavelength: nm</li><li>Frequency: Hz</li><li>$E = h\nu = \frac{hc}{\lambda}$</li><li>Planck constant (<em><strong>h</strong></em>) = 6.62607015 × 10<sup>-34</sup> J∙s</li><li>Speed of light (<em><strong>c</strong></em>) = 299 792 458 m/s ~ 3 × 10<sup>8</sup> m/s</li></ul><div class="admonition note"><p class="admonition-title">Calculate the energy of one photon of green light (532 nm)</p><p>$ E = h\nu = \frac{hc}{\lambda} = \frac{hc}{532 × 10^ {-9}m} = 3.823×10^ {-19}J$</p></div><div class="admonition note"><p class="admonition-title">Convert J to Hz</p><p>$ \nu = \frac{E}{h} = \frac{3.823×10^ {-19}J}{h} = 5.8 × 10^ {14} Hz$</p></div><div class="admonition note"><p class="admonition-title">Convert kJ/mol and kcal/mol</p><p>$E_ {total} = N_ A × E = 6.02 × 10^ {23} \frac{1}{mol} × 3.823 × 10^ {-19} J$</p><p>$= 230226 J/mol ~ 230 kJ/mol $</p><p>$= 55 kcal/mol$</p></div><br><br><br><br><br><br><br><h2 id="Extra-Reading">Extra Reading</h2><ol><li><p><strong>Time-Dependent Schrödinger Equation:</strong><br>$$<br>i\hbar\frac{\partial}{\partial t}\Psi(\mathbf{r}, t) = \hat{H}\Psi(\mathbf{r}, t)<br>$$<br>Here, $\Psi(\mathbf{r}, t)$ is the wave function of the system, $i$ is the imaginary unit, $\hbar$ is the reduced Planck constant, $t$ represents time, $\mathbf{r}$ is the position vector, and $\hat{H}$ is the Hamiltonian operator which represents the total energy of the system.</p></li><li><p><strong>Time-Independent Schrödinger Equation:</strong><br>$$<br>\hat{H}\psi(\mathbf{r}) = E\psi(\mathbf{r})<br>$$<br>In this form, $\psi(\mathbf{r})$ is the time-independent wave function, $E$ represents the energy of the system, and other symbols have the same meaning as in the time-dependent equation.</p></li></ol><p>The Schrödinger Equation is a cornerstone of quantum mechanics, providing a mathematical framework for understanding and predicting the behavior of quantum systems. It’s important to note that these equations are usually accompanied by specific boundary conditions or potentials, depending on the physical situation being modeled.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">To develop a quantitative understanding of macromolecules in biological systems.</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>PARTICLE-WAVE DUALITY</title>
    <link href="https://karobben.github.io/2024/01/23/LearnNotes/par-wave-dual/"/>
    <id>https://karobben.github.io/2024/01/23/LearnNotes/par-wave-dual/</id>
    <published>2024-01-23T15:57:26.000Z</published>
    <updated>2024-01-31T05:31:36.820Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Failure-of-Classical-Mechanics">Failure of Classical Mechanics</h2><h3 id="Black-Body-Radiation">Black Body Radiation</h3><ul><li><strong>Experiment</strong>: measure the radiation intensity as a function of frequency of the radiation emitted from a black body (a physical body that absorbs all incident electromagnetic radiation) at thermal equilibrium</li><li><strong>Model</strong>: Emitted radiation is classically due to oscillating electric dipoles within the material, acting like broadcast antennas.</li><li><strong>Classical expectation</strong>: higher temperature should result in a large increase in the radiation emitted at high frequencies (fast oscillation at high T)</li></ul><div class="admonition note"><p class="admonition-title">Gustav Kirchhoff</p><p>The blackbody is an idealized physical body that absorbs all incident electromagnetic radiation (such as light), regardless of frequency or angle of incidence.</p><p>A black body can also emit black-body radiation, which is solely determined by its temperature.</p><p><a href="https://en.wikipedia.org/wiki/Gustav_Kirchhoff"><img src="https://upload.wikimedia.org/wikipedia/commons/f/fe/Gustav_Robert_Kirchhoff.jpg" alt="Gustav Robert Kirchhoff" /></a></p></div><h3 id="Black-body-Radiation-Spectrum-Intensity-vs-Wavelength">Black-body Radiation Spectrum: Intensity vs. Wavelength</h3><table><thead><tr><th style="text-align:center"><img src="https://i.stack.imgur.com/f4xki.gif" alt="Relationship between radiational intensity vs. wavelength"></th></tr></thead><tbody></tbody></table><p>Notic: the “Cold” object also emit “light” as long as its above the absolute zero (-273.15 °C). And it would appear “blakc” because the peak wavelength is in infrared range, which human eye cannot recognize.</p><p>According to this theory, we could astimate the temperature of stars based on its color.</p><table><thead><tr><th style="text-align:center"><img src="https://stsci-opo.org/STScI-01G7RQ0CRCSNKZNX18HNNAQRV0.jpg" alt=""></th><th style="text-align:center"><img src="https://www.astronomy.com/wp-content/uploads/sites/2/2023/02/ScreenShot20210420at3.19.16PM.jpg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://webbtelescope.org/contents/media/images/01F8GF8WYBCQVKTGPX3MA58182?Type=Infographics&amp;Tag=Spectroscopy">webbtelescope</a></td><td style="text-align:center"><a href="https://www.astronomy.com/astronomy-for-beginners/color-coding-stars/">astronomy.com</a></td></tr></tbody></table><h3 id="Wien’s-Displacement-Law">Wien’s Displacement Law</h3><p>$ \lambda _{max} = \frac{W}{T} $</p><ul><li>$  \lambda _{max} $: Peak wavelength</li><li>W: Wien’s constant = 2.9 e<sup>-3</sup> m Kelvin</li><li>T: Surfave temperature</li></ul><p>Exp:</p><p>When λ<sub>max</sub> = 500nm:<br>$T = \frac{W}{\lambda_ {max}} = \frac{2.9 e^ {-3} mK}{500e^ {-9}m } = 5800K$</p><h3 id="Rayleigh-Jeans-law-Classical-description-of-light">Rayleigh-Jeans law (Classical description of light)</h3><ul><li>Predict the spectral irradiance (or spectral density) of a black body radiation as a function of wavelength or frequency for a fixed temperature.</li><li>Spectrum Density (ν, T) = $\frac{8\pi\nu^2}{ c^3 }k_BT$<ul><li>c: speed of light</li><li>k<sub>B</sub> = Boltzmann constant = 1.380649 × 10<sup>-23</sup> J/K</li><li>T: temperature</li><li>ν: frequency</li></ul></li></ul><h3 id="Spectral-density">Spectral density</h3><ul><li>Density = radiance<br>The power of radiation (W) passing a unit area (m<sup>2</sup>) within a unit solid angle (sr), within a unit time<br>(s). It has the unit of W/(m<sup>2</sup> × sr × s)</li><li>Spectral density = Spectral radiance<br>The radiance per unit wavelength (um) or per unit frequency (Hz), depending on if the power is measured over wavelength or frequency. Therefore, spectral density has the unit of W/(m<sup>2</sup> × sr × s × um) or W/(m<sup>2</sup> × sr × s × Hz).</li></ul><h3 id="Unexpected-Black-Body-Radiation-UV-Catastrophe">Unexpected Black Body Radiation: UV Catastrophe</h3><div class="admonition note"><p class="admonition-title">Conflict between observation and expectation</p><ul><li><strong>Observation</strong> :Spectral density does not monotonically increase as the frequency increases.</li><li><strong>Classical expectation</strong>:According to the Rayleigh-Jean law, spectral density should increase as the frequency increases</li></ul></div><p>Reason: <mark>The classic oscillating field from electromagnetic radiation drives the motion of a spring</mark><br>- faster oscillation → higher frequency → higher energy</p><p>For fix this conflict, a propportional model was given: E = nhν (<mark>Max Planck</mark>)</p><ul><li>n = 1, 2, 3…</li><li>Planck’s constant (h) = 6.626×10<sup>−34</sup> Js</li></ul><p>$$<br>\rho (\nu, T) = \frac{8\pi\nu^2 h \nu}{ c^3 } \frac{1}{e^{\frac{hv}{k_ BT}} - 1 }<br>$$</p><p>Explained:<br>- energy of photon (with frequent ν): hν<br>- weight if photon population (ν): g(ν) = $\frac{8\pi \nu ^2 }{c^ 3}$<br>- averagy number of photon (ν): n(#, T) = $\frac{1}{exp(\frac{h\nu}{k_BT})-1}$<br>- ρ (νT)dν = hν × g(ν) × n(#, T)<br>= $h\nu × \frac{8\pi \nu ^2 }{c^ 3} × \frac{1}{exp(\frac{h\nu}{k_BT})-1}$<br>= $\frac{8\pi h \nu^3}{ c^3 } \frac{1}{e^{\frac{hv}{k_ BT}} - 1 }$</p><p>In Planck Low, it show either high temperature or low frequency would case the “Ultralviolet catastrophe”:</p><p><img src="https://imgur.com/rWBTB6C.png" alt="planck Law"></p><ul><li>From the left to the right:<ul><li>$\frac{8\pi \nu ^2 }{c^ 3}$</li><li>$\frac{1}{exp(\frac{h\nu}{k_BT})-1}$</li><li>$\frac{8\pi h \nu^3}{ c^3 } \frac{1}{e^{\frac{hv}{k_ BT}} - 1 }$</li></ul></li></ul><details><summary>Plot Codes</summary><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Rayleigh_Jeans</span>(<span class="hljs-params">v, T</span>):</span><br>    kB = <span class="hljs-number">1.380649</span> * <span class="hljs-number">10</span>**(<span class="hljs-number">0</span>-<span class="hljs-number">23</span>)<br>    c = <span class="hljs-number">3</span> * <span class="hljs-number">10</span>**<span class="hljs-number">8</span><br>    <span class="hljs-keyword">return</span> (v**<span class="hljs-number">2</span>) * <span class="hljs-number">8</span> * np.pi * kB * T / (c**<span class="hljs-number">3</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Planck</span>(<span class="hljs-params">v, T</span>):</span><br>    h = <span class="hljs-number">6.626</span>*<span class="hljs-number">10</span>**(<span class="hljs-number">0</span>-<span class="hljs-number">34</span>)<br>    kB = <span class="hljs-number">1.380649</span> * <span class="hljs-number">10</span>**(<span class="hljs-number">0</span>-<span class="hljs-number">23</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> /( np.e ** (h*v / (kB * T) ) - <span class="hljs-number">1</span> )<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Merged</span>(<span class="hljs-params">v, T</span>):</span><br>    h = <span class="hljs-number">6.626e-34</span><br>    kB = <span class="hljs-number">1.380649e-23</span><br>    c = <span class="hljs-number">3</span> * <span class="hljs-number">10</span> **<span class="hljs-number">8</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">8</span> * np.pi * h * v**<span class="hljs-number">3</span> / c**<span class="hljs-number">3</span> / np.e**(h *v /(kB * T))<br><br><span class="hljs-comment"># Constants</span><br>h = <span class="hljs-number">6.62607015e-34</span>  <span class="hljs-comment"># Planck&#x27;s constant (m^2 kg / s)</span><br>c = <span class="hljs-number">299792458</span>       <span class="hljs-comment"># Speed of light in vacuum (m / s)</span><br>k_B = <span class="hljs-number">1.380649e-23</span>  <span class="hljs-comment"># Boltzmann constant (J / K)</span><br><br><span class="hljs-comment"># Planck&#x27;s law function</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">planck_law</span>(<span class="hljs-params">frequency, temperature</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Calculate the spectral radiance of a black body at thermal equilibrium.  </span><br><span class="hljs-string">    Parameters:</span><br><span class="hljs-string">    frequency (float): frequency of the electromagnetic radiation (Hz)</span><br><span class="hljs-string">    temperature (float): absolute temperature of the body (K)</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    float: spectral radiance (W / (m^2 * sr * Hz))</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    exponent = (h * frequency) / (k_B * temperature)<br>    spectral_radiance = (<span class="hljs-number">8</span> * np.pi * h * frequency**<span class="hljs-number">3</span>) / (c**<span class="hljs-number">3</span>) * (<span class="hljs-number">1</span> / (np.exp(exponent) - <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">return</span> spectral_radiance<br><br><span class="hljs-comment"># Example usage: Calculate the spectral radiance for a frequency of 1e14 Hz at 5000 K</span><br>frequency_example = <span class="hljs-number">1e14</span>  <span class="hljs-comment"># Hz</span><br>temperature_example = <span class="hljs-number">5000</span>  <span class="hljs-comment"># K</span><br>radiance_example = planck_law(frequency_example, temperature_example)<br><br><span class="hljs-comment"># Plot the graphs above</span><br>X = [i /<span class="hljs-number">10</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span>  <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">100</span>)]<br>fig, axs = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br>axs[<span class="hljs-number">0</span>].plot(X, [Planck(i * <span class="hljs-number">1e14</span>, <span class="hljs-number">3000</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> X])<br>axs[<span class="hljs-number">1</span>].plot(X, [Rayleigh_Jeans(i * <span class="hljs-number">1e14</span>, <span class="hljs-number">3000</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> X])<br>axs[<span class="hljs-number">2</span>].plot(X, [h *(i *<span class="hljs-number">1e14</span>)* Rayleigh_Jeans(i*<span class="hljs-number">1e14</span>, <span class="hljs-number">3000</span>) * Planck(i*<span class="hljs-number">1e14</span>, <span class="hljs-number">3000</span>)  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> X])<br>plt.tight_layout()<br>plt.show()<br><br><span class="hljs-comment"># plot the mergerd function</span><br>plt.plot(X, [Merged(i*<span class="hljs-number">1e14</span>/<span class="hljs-number">10</span>, <span class="hljs-number">3000</span>)  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> X])<br>plt.show()<br></code></pre></td></tr></table></figure></div></details><p>So, both Rayleigh-Jean law and Planck law agree with the observation very well in very low frequency.<br>But only Plank law could fit the decreasing of the intensity in high frequency situation.</p><h2 id="Light-is-Particle">Light is Particle</h2><h3 id="Einstein’s-Contribution-Photoelectric-Effect">Einstein’s Contribution: Photoelectric Effect</h3><p>$$<br>E = nhν<br>$$</p><p>By using the light to eject electrons from a copper, they found the <mark>kinetic energy of ejected electrons depends on light frequency</mark></p><p>$$<br>E_{light} = \beta\nu_{light}<br>$$</p><h3 id="The-Photoelectric-Effect">The Photoelectric Effect</h3><p>Conservation of energy</p><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/DSOpstj.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">…?unknow</td></tr></tbody></table><p>$$<br>E_{elec} + \Phi= \beta \nu<br>$$</p><ul><li>E<sub>elec</sub>: (Measured) Electron kinetic energy</li><li>&amp;\Phi; Work to remove electron from target (independently determined)</li><li>ν: Determine the value of &amp;\beta;</li></ul><div class="admonition note"><p class="admonition-title">Einstein concluded that light must be behaving like a particle in this experiment: PHOTON</p><p>E = <em>hν</em></p></div><h3 id="Duality-in-partical-with-mass">Duality in partical with mass</h3><ul><li><strong>Electron diffraction</strong>: This is a typical diffraction pattern of a beam of electrons diffracted by a crystalline solid</li></ul><p>from Planck’s equation to Einstein’s equestion:</p><ul><li><em>E = h&amp;nu</em> → <em>E = mc<sup>2</sup></em></li></ul><p>$v = h\nu$<br>$mv = P = m\lambda\nu$<br>$v = \frac{P}{m\lambda}$<br>$E = h\nu = \frac(hP){m\lambda} → E = \frac{p^2 }{m}$</p><p>De Broglie Equation:<br>$\lambda = \frac{h}{p} = \frac{h}{m\nu}$</p><h3 id="About-Weight-Terms-in-Spectral-Density">About Weight Terms in Spectral Density</h3><p>$g(\nu) = \frac{N(E)}{V} = \frac{Number of States (E)}{Volumne}$</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">PARTICLE-WAVE DUALITY</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Light in Physics</title>
    <link href="https://karobben.github.io/2024/01/18/LearnNotes/light/"/>
    <id>https://karobben.github.io/2024/01/18/LearnNotes/light/</id>
    <published>2024-01-19T03:30:37.000Z</published>
    <updated>2024-02-02T01:28:32.399Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Mathmatic-Description-of-the-Light">Mathmatic Description of the Light</h2><p>In the equation you’ve provided:</p><p>$$<br>E(x,t) = E^0 \sin\left[2\pi \left(\frac{x}{\lambda} - \frac{t}{T}\right)\right]<br>$$</p><p>This represents a sinusoidal wave function, where $E(x,t)$ is the electric field of the light wave at a position $x$ and at time $t$. Here’s what the terms mean:</p><ul><li>$E^0$ is the amplitude of the wave, which indicates the maximum strength of the electric field.</li><li>$\lambda$ (lambda) is the wavelength of the light, which is the distance over which the wave’s shape repeats.</li><li>$T$ is the period of the wave, which is the time it takes for one complete cycle of the wave to pass a point. ($\frac{1}{T} = \nu$)</li></ul><p>The reason both $\lambda$ and $T$ are present in the equation is because they describe different aspects of the wave:</p><ul><li>$\lambda$ describes the spatial repetition of the wave along the $x$-axis.</li><li>$T$ describes the temporal repetition of the wave along the $t$-axis (time).</li></ul><p>The term $\frac{x}{\lambda} - \frac{t}{T}$ is the phase of the wave, which determines the position of the peaks and troughs of the wave at any given time $t$ and position $x$. It’s not meant to equal zero; instead, it changes with time and position to represent the propagation of the wave through space and time.</p><p>The phase changes as time goes by, indicating that the peaks and troughs of the wave are moving. If $\frac{x}{\lambda} - \frac{t}{T}$ were always zero, it would imply a stationary wave, not a propagating one.</p><p>The product $2\pi$ times the phase gives you the argument of the sine function in radians, which is necessary because the sine function is periodic with a period of $2\pi$. This means that the wave repeats itself every $2\pi$ radians, which corresponds to one wavelength in space and one period in time.</p><p><img src="https://imgur.com/D0t8uGs.png" alt="Wave Function"></p><details><summary>Plot codes</summary><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">waveFun</span>(<span class="hljs-params">x, t = <span class="hljs-number">0</span>, lamb = <span class="hljs-number">1</span>, pi = np.pi, T = <span class="hljs-number">1</span>, E0 = <span class="hljs-number">1</span></span>):</span><br>    E = E0 * np.sin(<span class="hljs-number">2</span> * pi * (x/lamb - t/T))<br>    <span class="hljs-keyword">return</span> E     <br><br>Y = <span class="hljs-built_in">range</span>(-<span class="hljs-number">50</span>,<span class="hljs-number">50</span>)<br>X = <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">100</span>)<br>Points = []<br><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X:<br>    <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> Y:<br>        Dis = np.sqrt((x/<span class="hljs-number">10</span>)**<span class="hljs-number">2</span> + (y/<span class="hljs-number">10</span>)**<span class="hljs-number">2</span>)<br>        Points += [[x, y, waveFun(Dis)]]<br><br>Points = np.array(Points)<br><br>plt.figure(figsize=(<span class="hljs-number">7</span>, <span class="hljs-number">5</span>))<br>plt.scatter(Points[:, <span class="hljs-number">0</span>], Points[:, <span class="hljs-number">1</span>] + <span class="hljs-number">0.5</span>, c= Points[:, <span class="hljs-number">2</span>], marker=<span class="hljs-string">&#x27;o&#x27;</span>, cmap=plt.cm.coolwarm,)<br>plt.show()<br></code></pre></td></tr></table></figure></div></details><h3 id="Example-1-Single-Location-Position">Example 1: Single Location Position</h3><p>Let’s set the $\lambda$ as 1, T as 10, and x = 0. Then, the equation could be simplified as $E(0, t) = sin[2\pi(0 - \frac{t}{10})]$.<br>And the change of the E<sup>0</sup> with T could be show as the animation below.</p><table><thead><tr><th style="text-align:center">The E<sup>0</sup> corresponded with the t</th><th style="text-align:center">Static View by using x axis as t</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/WvAIY5p.gif" alt="Observing the wave in single position"></td><td style="text-align:center"><img src="https://imgur.com/Ct71NCk.png" alt="Spread the observed points"></td></tr></tbody></table><h3 id="Example-2-Multiple-Observation-Locations">Example 2: Multiple Observation Locations</h3><p>If we observe more positions, let’s say, 0 to 10, and using the full function, we could get an animation like below. This is the propagating wave we could observe in 10 different locations.</p><p><img src="https://imgur.com/KdWk5PU.gif" alt="The propagating wave"></p><table><thead><tr><th style="text-align:center"><img src="https://www.acs.psu.edu/drussell/Demos/wave-x-t/wave-x-t.gif" alt="Wave Motion in Time and Space"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.acs.psu.edu/drussell/Demos/wave-x-t/wave-x-t.html">© psu</a></td></tr></tbody></table><h2 id="Direction-of-the-Wave">Direction of the Wave</h2><p>When t increases, x increases =&gt; “+” direction on x;<br>When t increases, x decreases =&gt; “-” direction on x.</p><h2 id="Speed-of-the-Wave">Speed of the Wave</h2><p>$velocity = \frac{\lambda}{T} = \lambda \nu$</p><ul><li>$\nu$ id frequency ($\frac{1}{T}$) of the wave.</li></ul><p>So, the light of the wave is:</p><ul><li>$c = \lambda \nu$</li></ul><h2 id="Energy-of-the-Wave">Energy of the Wave</h2><p>Energy of a wave is proportional to the square of the amplitude (in classical mechanics)</p><p>$\rho(\nu)$: energy per unit volume</p><p>$$<br>\rho(\nu) = constant\ x (E^ o)^ 2<br>$$</p><ul><li><mark>Classic</mark>: Energy is dependent on amplitude</li><li><mark>QM</mark>: Energy is dependent on frequency</li></ul><h2 id="Commonly-used-notation">Commonly used notation</h2><ol><li><p><strong>Wave Vector ($k$)</strong>: The wave vector is defined as $\frac{2\pi}{\lambda}$, where $\lambda$ is the wavelength of the wave. The wave vector points in the direction of the wave’s propagation and has a magnitude equal to the number of wave cycles per unit distance. The notation $\hat{k}$ represents a unit vector in the direction of $k$, so the wave vector $k$ is sometimes written as $\frac{2\pi}{\lambda} \hat{k}$, emphasizing its direction.</p></li><li><p><strong>Angular Frequency ($\omega$)</strong>: This is defined as $2\pi\nu$, where $\nu$ is the frequency of the wave. It represents how many radians the wave cycles through per unit time.</p></li><li><p><strong>Phase ($\phi$)</strong>: The phase is a term that allows us to specify where in its cycle the wave is at $t = 0$ and $x = 0$. It lets us define <strong>the “zero point” or starting point</strong> of the wave at a place other than the origin of our coordinate system.</p></li></ol><p>$$E(x,t) = E^0 \sin(kx - \omega t + \phi)$$<br>$$H(x,t) = H^0 \sin(kx - \omega t + \phi)$$</p><div class="admonition question"><p class="admonition-title">Why Standard Form?</p><p>The first function form expresses the wave in terms of its wavelength λ and period T, which are perhaps more intuitive when you're first learning about waves.  It makes it very clear that the wave repeats itself every wavelength λ in space and every period T in time.</p><p>The second function is the standard form. It's particularly useful in more advanced topics like wave interference, diffraction, and quantum mechanics, where the concept of phase space and the relationship between position and momentum (or wavelength and frequency) are crucial.</p><p>For example, when you want to mimic the interference of the wave, the previous function could became extreamly complcated because the only difference between two wave is phase.</p></div><details> <summary>More descriptions</summary>These equations describe how the electric and magnetic fields oscillate as a function of space and time, which is characteristic of electromagnetic waves such as light. The quantities $E^0$ and $H^0$ are the maximum strengths of the electric and magnetic fields, respectively.<p>In an electromagnetic wave, the electric and magnetic fields are perpendicular to each other and to the direction of wave propagation. The equations show that both fields oscillate in sync (they have the same phase $\phi$) but are described by separate equations since they are perpendicular components.</p><p>The term $kx - \omega t$ indicates that the wave is moving in the positive $x$-direction. If the wave were moving in the negative $x$-direction, the sign in front of $\omega t$ would be positive.</p><p>The factor $\sin(kx - \omega t + \phi)$ varies between (-1) and (1), causing the electric and magnetic field strengths to oscillate between $-E^0$ to $E^0$ and $-H^0$ to $H^0$, respectively. The wave thus carries energy and, if it is light, can be observed as it interacts with matter.</p></details><h2 id="Huygen’s-Principle-1678">Huygen’s Principle (1678)</h2><p>Waves spread as if each region of space is behaving as a source of new waves of the <mark>same frequency and phase</mark>.<br>So, Huygen’s principle applied to light showing a wave front.</p><h2 id="Diffraction">Diffraction</h2><p><img src="https://imgur.com/vtuefhO.png" alt="Wave Diffraction"></p><p>When we konw d, D, X, and θ we could calcualate the λ (wave length):</p><ul><li>When the light patten shows the dark point, we know that the phase between two waves is $n\frac{\lambda}{2}$</li><li>So the $\Delta \phi$ of the first dark spot would be $\frac{\lambda}{2}$</li><li>According to the plot, the λ would be (path4 - path3) * 2 $ = \frac{d}{2}sin\theta$</li><li>When the angle is very small, we have $sin\theta \approx tan\theta = \frac{X}{2}\frac{1}{D}$</li><li>So finally, we could get: $\frac{d}{2}\frac{X}{2D} = \frac{\lambda}{2}$<ul><li>$\lambda = \frac{dX}{2D}$</li></ul></li></ul><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/qiLpQ7c.png" alt="Two wave interference"></th><th style="text-align:center"><img src="https://imgur.com/1Cqo89a.png" alt="Three wave interference"></th></tr></thead><tbody><tr><td style="text-align:center">Two wave interference (center: y<sub>1</sub> = -15, y<sub>2</sub> = 15)</td><td style="text-align:center">Three wave interference (y<sub>1</sub> = 10, y<sub>2</sub> = 0, y<sub>3</sub> = -10)</td></tr></tbody></table><table><thead><tr><th style="text-align:center"><img src="http://hyperphysics.phy-astr.gsu.edu/hbase/phyopt/imgpho/muls2.png" alt="Double Slit"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="http://hyperphysics.phy-astr.gsu.edu/hbase/phyopt/mulslid.html">© gsu</a></td></tr><tr><td style="text-align:center"><img src="https://myslu.stlawu.edu/~jmil/physics/labs/152_lab/setup_manual/blackboard/img/double_slit.gif" alt="Double Slit"></td></tr><tr><td style="text-align:center"><a href="http://stlawu.edu">stlawu.edu</a></td></tr></tbody></table><p>$$<br>n\lambda = d sin\theta_n \approx d tan\theta_n = d \frac{x_ n }{D}<br>$$</p><ul><li>Each photon is represented as a plane wave at the slits.</li><li>The square of the amplitude of the recombined wave is proportional to the probability of finding the photon at this point</li></ul><details><summary>Plot code</summary><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>Y = <span class="hljs-built_in">range</span>(-<span class="hljs-number">50</span>,<span class="hljs-number">50</span>)<br>X = <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">100</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">waveFun</span>(<span class="hljs-params">x, t = <span class="hljs-number">0</span>, lamb = <span class="hljs-number">1</span>, pi = np.pi, T = <span class="hljs-number">1</span>, E0 = <span class="hljs-number">1</span></span>):</span><br>    E = E0 * np.sin(<span class="hljs-number">2</span> * pi * (x/lamb - t/T))<br>    <span class="hljs-keyword">return</span> E     <br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">WaveE</span>(<span class="hljs-params">X, Y, dy = <span class="hljs-number">0</span></span>):</span><br>    Points = []<br>    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X:<br>        <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> Y:<br>            Dis = np.sqrt((x/<span class="hljs-number">10</span>)**<span class="hljs-number">2</span> + (y/<span class="hljs-number">10</span>)**<span class="hljs-number">2</span>)<br>            Points += [[x, y + dy, waveFun(Dis)]]<br>    Points = np.array(Points)<br>    <span class="hljs-keyword">return</span> Points<br><br>P1 = WaveE(X, Y, <span class="hljs-number">0</span>)<br>P2 = WaveE(X, Y, <span class="hljs-number">10</span>)<br>P3 = WaveE(X, Y, -<span class="hljs-number">10</span>)<br><br>P1 = pd.DataFrame(P1, columns = [<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&quot;E1&quot;</span>])<br>P2 = pd.DataFrame(P2, columns = [<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&quot;E2&quot;</span>])<br>P3 = pd.DataFrame(P3, columns = [<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&quot;E3&quot;</span>])<br><br>TB = pd.merge(P1, P2)<br>TB = pd.merge(TB, P3)<br>TB[<span class="hljs-string">&#x27;E&#x27;</span>] = TB.E1 + TB.E2  + TB.E3<br><br>fig, ax = plt.subplots(subplot_kw=&#123;<span class="hljs-string">&quot;projection&quot;</span>: <span class="hljs-string">&quot;3d&quot;</span>&#125;, figsize=(<span class="hljs-number">6</span>,<span class="hljs-number">6</span>))<br>ax.plot_trisurf(TB.x, TB.y, TB.E, vmin=TB.E.<span class="hljs-built_in">min</span>() * <span class="hljs-number">2</span>, cmap=cm.coolwarm)<br>plt.show()<br><br><span class="hljs-comment">#plt.figure(figsize=(5, 5))</span><br><span class="hljs-comment">#plt.scatter(TB.x, TB.y, c= TB.E, marker=&#x27;o&#x27;, cmap=plt.cm.coolwarm,)</span><br><span class="hljs-comment">#plt.show()</span><br></code></pre></td></tr></table></figure></div></details><h2 id="Principle-of-Superposition">Principle of Superposition</h2><p>At the beginning of the story, let’s say there are two same waves with different $\phi$ which: $\phi_2 - \phi_1 = \pi$</p><p>In this case, the Intensity of this two wave is:</p><ul><li>$E^2_{sum} = (E_1 + E_2 )^2 = 0$</li><li>ps: <mark>Not</mark> $E^2_{sum} = E_1^2 + E_2^2 \neq 0$</li></ul><p>(Destructive interference)</p><h3 id="For-Two-Traveling-Waves">For Two Traveling Waves</h3><ul><li>Two sine waves with the same amplitude but slightly different frequencies traveling at the same velocity in the same direction</li></ul><p>$ E(x,t) = E^o sin(k_1 x - \omega _1 t) +E^o sin(k_2 x - \omega _2 t) $<br>$ = 2 E^o cos [\frac{[k_1 - k_2]}{2}x - \frac{\omega _1 - \omega _2}{2}t] sin[\frac{[k_1 - k_2]}{2}x - \frac{\omega _1 - \omega _2}{2}t] $</p><h2 id="Direction">Direction</h2><p>$$ 𝑛_1 \cdot sin 𝜃_1 = 𝑛_2 \cdot sin 𝜃_2 $$</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style><h2 id="Extra-Explore">Extra Explore</h2><p>Becuase we konw:</p><ul><li>$E(x,t) = E^0 \sin\left[2\pi \left(\frac{x}{\lambda} - \frac{t}{T}\right)\right]$</li></ul><p>So, when we move to the 2D wave, we could have function:</p><ul><li>$E(x, y, t) = E^0 \sin\left[2\pi \left(\frac{\sqrt{x^2 - y^2}}{\lambda} - \frac{t}{T}\right)\right]$</li></ul><p>In order to calcualte the 2D wave from the different emission location, we need to introduce the initial point b=(b<sub>x</sub>, b<sub>y</sub>)</p><ul><li>$E(x, y, b_x, b_y, t) = E^0 \sin\left[2\pi \left(\frac{\sqrt{(x-b_x)^2 - (y-b_y)^2}}{\lambda} - \frac{t}{T}\right)\right]$</li></ul><details><summary>Plot codes</details><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> NearestNeighbors<br><span class="hljs-keyword">import</span> pyvista <span class="hljs-keyword">as</span> pv<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">waveFun</span>(<span class="hljs-params">x, y, bx =<span class="hljs-number">0</span>, by = <span class="hljs-number">0</span>, t = <span class="hljs-number">0</span>, lamb = <span class="hljs-number">1</span>, pi = np.pi, T = <span class="hljs-number">1</span>, E0 = <span class="hljs-number">1</span></span>):</span><br>    E = E0 * np.sin(<span class="hljs-number">2</span> * pi * ((np.sqrt((x-bx)**<span class="hljs-number">2</span>+(y-by)**<span class="hljs-number">2</span>))/lamb - t/T))<br>    <span class="hljs-keyword">return</span> E     <br><br><br>X = np.arange(<span class="hljs-number">100</span>, step = <span class="hljs-number">.1</span>)<br>Y = np.arange(-<span class="hljs-number">50</span>,<span class="hljs-number">50</span>, step = <span class="hljs-number">.1</span>)<br>X, Y = np.meshgrid(X, Y)<br>Points = waveFun(X,X)<br><br>X = np.arange(-<span class="hljs-number">50</span>, <span class="hljs-number">50</span>, step = <span class="hljs-number">.1</span>)<br>Y = np.arange(-<span class="hljs-number">50</span>, <span class="hljs-number">50</span>, step = <span class="hljs-number">.1</span>)<br>X, Y = np.meshgrid(X, Y)<br>Points += waveFun(X,Y, lamb = <span class="hljs-number">.7</span>, T = <span class="hljs-number">.7</span>)<br><br>plt.imshow(Points)<br>plt.show()<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">200</span>):<br>    i = (i-<span class="hljs-number">100</span>)/<span class="hljs-number">20</span>  <br>    X = np.arange(<span class="hljs-number">100</span>, step = <span class="hljs-number">.1</span>)<br>    Y = np.arange(-<span class="hljs-number">50</span> + i,<span class="hljs-number">50</span> + i, step = <span class="hljs-number">.1</span>)<br>    X, Y = np.meshgrid(X, Y)<br>    Points += waveFun(X,Y)<br><br><br><span class="hljs-comment">#plt.scatter(points_array[:, 0], points_array[:, 1] + 0.5, c= points_array[:, 2], marker=&#x27;o&#x27;, cmap=plt.cm.coolwarm,)</span><br><span class="hljs-comment">#plt.show()</span><br><span class="hljs-comment"># Create a PyVista point cloud</span><br>point_cloud = pv.PolyData(points_array)<br>point_cloud[<span class="hljs-string">&#x27;point_color&#x27;</span>] = point_cloud.points[:, <span class="hljs-number">2</span>]<br>point_cloud.plot(point_size=<span class="hljs-number">5</span>,scalars=<span class="hljs-string">&#x27;point_color&#x27;</span>, cmap=<span class="hljs-string">&quot;jet&quot;</span>, show_bounds=<span class="hljs-literal">True</span>)<br><br><br><br><br><br><br>Y = <span class="hljs-built_in">range</span>(-<span class="hljs-number">100</span>,<span class="hljs-number">100</span>)<br>X = <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">200</span>)<br>Points = []<br><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X:<br>    x /=<span class="hljs-number">10</span><br>    <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> Y:<br>        y /=<span class="hljs-number">10</span><br>        E = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> by <span class="hljs-keyword">in</span> np.arange(-<span class="hljs-number">10</span>,<span class="hljs-number">11</span>, <span class="hljs-number">.1</span>):<br>            E += waveFun(x, y, <span class="hljs-number">0</span>, by)<br>        Points += [[x, y, E] ]<br><br>points_array = np.array(Points)<br><span class="hljs-comment">#plt.scatter(points_array[:, 0], points_array[:, 1] + 0.5, c= points_array[:, 2], marker=&#x27;o&#x27;, cmap=plt.cm.coolwarm,)</span><br><span class="hljs-comment">#plt.show()</span><br><span class="hljs-comment"># Create a PyVista point cloud</span><br>point_cloud = pv.PolyData(points_array)<br>point_cloud[<span class="hljs-string">&#x27;point_color&#x27;</span>] = point_cloud.points[:, <span class="hljs-number">2</span>]<br>point_cloud.plot(point_size=<span class="hljs-number">5</span>,scalars=<span class="hljs-string">&#x27;point_color&#x27;</span>, cmap=<span class="hljs-string">&quot;jet&quot;</span>, show_bounds=<span class="hljs-literal">True</span>)<br><br><br><br>point_cloud = pv.PolyData(Points)<br>volume = point_cloud.delaunay_3d(alpha = <span class="hljs-number">5</span>)<br>shell = volume.extract_geometry()<br>shell.plot(show_edges=<span class="hljs-literal">True</span>)<br><br>TB = pd.DataFrame(Points, columns = [<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&#x27;E&#x27;</span>])<br><br>fig, ax = plt.subplots(subplot_kw=&#123;<span class="hljs-string">&quot;projection&quot;</span>: <span class="hljs-string">&quot;3d&quot;</span>&#125;, figsize=(<span class="hljs-number">6</span>,<span class="hljs-number">6</span>))<br>P = ax.plot_trisurf(TB.x, TB.y, TB.E, vmin=TB.E.<span class="hljs-built_in">min</span>() * <span class="hljs-number">2</span>, cmap=plt.cm.coolwarm)<br>plt.show()<br><br><br><span class="hljs-keyword">from</span> scipy.interpolate <span class="hljs-keyword">import</span> griddata<br><br>df =TB<br>x1 = np.linspace(df[<span class="hljs-string">&#x27;x&#x27;</span>].<span class="hljs-built_in">min</span>(), df[<span class="hljs-string">&#x27;x&#x27;</span>].<span class="hljs-built_in">max</span>(), <span class="hljs-built_in">len</span>(df[<span class="hljs-string">&#x27;x&#x27;</span>].unique()))<br>y1 = np.linspace(df[<span class="hljs-string">&#x27;y&#x27;</span>].<span class="hljs-built_in">min</span>(), df[<span class="hljs-string">&#x27;y&#x27;</span>].<span class="hljs-built_in">max</span>(), <span class="hljs-built_in">len</span>(df[<span class="hljs-string">&#x27;y&#x27;</span>].unique()))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">x, y via meshgrid for vectorized evaluation of</span><br><span class="hljs-string">2 scalar/vector fields over 2-D grids, given</span><br><span class="hljs-string">one-dimensional coordinate arrays x1, x2,..., xn.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>x2, y2 = np.meshgrid(x1, y1)<br><span class="hljs-comment"># Interpolate unstructured D-dimensional data.</span><br>z2 = griddata((df[<span class="hljs-string">&#x27;x&#x27;</span>], df[<span class="hljs-string">&#x27;y&#x27;</span>]), df[<span class="hljs-string">&#x27;E&#x27;</span>], (x2, y2), method=<span class="hljs-string">&#x27;cubic&#x27;</span>)<br><br><br><br><span class="hljs-comment"># Ready to plot</span><br>fig = plt.figure()<br>ax = fig.subplots(subplot_kw = &#123;<span class="hljs-string">&quot;projection&quot;</span>:<span class="hljs-string">&#x27;3d&#x27;</span>&#125;)<br><br>surf = ax.plot_surface(x2, y2, z2, rstride=<span class="hljs-number">1</span>, cstride=<span class="hljs-number">1</span>, cmap=plt.cm.coolwarm,<br>                       linewidth=<span class="hljs-number">0</span>, antialiased=<span class="hljs-literal">False</span>)<br>ax.set_zlim(-<span class="hljs-number">1.01</span>, <span class="hljs-number">1.01</span>)<br><br>ax.zaxis.set_major_locator(LinearLocator(<span class="hljs-number">10</span>))<br>ax.zaxis.set_major_formatter(FormatStrFormatter(<span class="hljs-string">&#x27;%.02f&#x27;</span>))<br>fig.colorbar(surf, shrink=<span class="hljs-number">0.5</span>, aspect=<span class="hljs-number">5</span>)<br>plt.title(<span class="hljs-string">&#x27;Meshgrid Created from 3 1D Arrays&#x27;</span>)<br><br>plt.show()<br><br></code></pre></td></tr></table></figure></div></details>]]></content>
    
    
    <summary type="html">Light in Physics</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Using Vim as Python and R IDE</title>
    <link href="https://karobben.github.io/2024/01/03/Linux/nvimr/"/>
    <id>https://karobben.github.io/2024/01/03/Linux/nvimr/</id>
    <published>2024-01-03T20:42:34.000Z</published>
    <updated>2024-01-20T23:06:45.609Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Using-Vim-as-IDE">Using Vim as IDE</h2><p>Vim is a classic text editor known for efficiency, while NeoVim is its modernized fork with improvements like better plugin support. LunarVim, built on NeoVim, offers a pre-configured setup, making it easier for users to get a powerful, feature-rich environment without the hassle of individual configurations. Ideal for those new to Vim/NeoVim or seeking a ready-to-use development setup, LunarVim combines ease of setup with customizability. It’s particularly appealing for its integrated toolset, active community support, and a balance between functionality and performance, making it a great choice for a streamlined coding experience.</p><p>Plug: <a href="https://github.com/jamespeapen/Nvim-R/wiki">Nvim-R</a><br>Video Tutorial: <a href="https://www.youtube.com/watch?v=nm45WagtV3w">Rohit Farmer</a><br>Instruction following the video: <a href="https://gist.github.com/rohitfarmer/68cdadeaeeb196e8a6ecdebdee6e76a5">rohitfarmer</a></p><p>Final work:</p><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/Pdzomq1.png" alt="Using vim as R IDE"></th></tr></thead><tbody><tr><td style="text-align:center">© Karobben</td></tr></tbody></table><h2 id="Installation">Installation</h2><p>Please install the latest NeoVim by following the <a href="https://github.com/neovim/neovim/blob/master/INSTALL.md">Neovim document</a> and <a href="https://www.lunarvim.org/docs/installation">LunarVim Document</a></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># install neovim</span><br><span class="hljs-comment"># sudo apt install neovim</span><br><span class="hljs-comment"># install vim-plug</span><br>curl -fLo ~/.<span class="hljs-built_in">local</span>/share/nvim/site/<span class="hljs-built_in">autoload</span>/plug.vim --create-dirs \<br>    https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim<br><br><span class="hljs-comment"># for storing the config file</span><br>mkdir ~/.config/nvim<br>touch ~/.config/nvim/init.vim<br><span class="hljs-comment"># for storing plug</span><br><span class="hljs-comment"># because all plug would be installed by here, you could just delete the directories here to delete plugs.</span><br>mkdir ~/.vim/plugged<br><span class="hljs-comment"># open and edit the config file</span><br>vim ~/.config/nvim/init.vim<br></code></pre></td></tr></table></figure></div><p>Save the lines below in the <code>~/.config/nvim/init.vim</code> file to install the pluges.</p><pre>" Specify a directory for plugins" - Avoid using standard Vim directory names like 'plugin'call plug#begin('~/.vim/plugged')" List of plugins." Make sure you use single quotes" Shorthand notationPlug 'jalvesaq/Nvim-R', { 'branch' : 'stable' }Plug 'ncm2/ncm2'Plug 'roxma/nvim-yarp'Plug 'gaalcaras/ncm-R'Plug 'preservim/nerdtree'Plug 'Raimondi/delimitMate'Plug 'patstockwell/vim-monokai-tasty'Plug 'itchyny/lightline.vim'" Initialize plugin systemcall plug#end()</pre><div class="admonition note"><p class="admonition-title">How to instsall the plunges</p><p>After stored the change, you need to open it again by using <code>nvim ~/.config/nvim/init.vim</code>. And then, under the command model (which is triggered by <code>:</code>), input <code>PlugInstall</code> (<code>PlugUpdate</code> if you want to update them). After you see the picture below which means you installed it successfully:<img src="https://imgur.com/SMB1YM3.png" alt="" /></p></div><p>By following the instruction from <a href="https://gist.github.com/rohitfarmer/68cdadeaeeb196e8a6ecdebdee6e76a5">rohitfarmer</a>’s post, we could add more things at the end of the <code>init.vim</code> file:</p><div class="admonition note"><p class="admonition-title">folding behavior</p><pre>" Set foldbehavior<p>set tabstop=2        &quot; Number of spaces that a <Tab> in the file counts for<br>set shiftwidth=2     &quot; Number of spaces to use for each step of (auto)indent<br>set softtabstop=2    &quot; Number of spaces that a <Tab> counts for while performing editing operations<br>set expandtab        &quot; Use spaces instead of tabs</p><p>set foldmethod=indent<br>set foldlevelstart=2    &quot; Start folding at an indent level greater than 2<br></pre></p><p>For quick unfold all codes:</p><pre>:set nofoldenable</pre></div><pre>" Set a Local Leader" With a map leader it's possible to do extra key combinations" like <leader>w saves the current filelet mapleader = ","let g:mapleader = ","" Plugin Related Settings" NCM2autocmd BufEnter * call ncm2#enable_for_buffer()    " To enable ncm2 for all buffers.set completeopt=noinsert,menuone,noselect           " :help Ncm2PopupOpen for more                                                    " information." NERD Treemap <leader>nn :NERDTreeToggle<CR>                  " Toggle NERD tree." Monokai-tastylet g:vim_monokai_tasty_italic = 1                  " Allow italics.colorscheme vim-monokai-tasty                       " Enable monokai theme." LightLine.vim set laststatus=2              " To tell Vim we want to see the statusline.let g:lightline = {   \ 'colorscheme':'monokai_tasty',   \ }" General NVIM/VIM Settings" Mouse Integrationset mouse=i                   " Enable mouse support in insert mode." Tabs & Navigationmap <leader>nt :tabnew<cr>    " To create a new tab.map <leader>to :tabonly<cr>     " To close all other tabs (show only the current tab).map <leader>tc :tabclose<cr>    " To close the current tab.map <leader>tm :tabmove<cr>     " To move the current tab to next position.map <leader>tn :tabn<cr>        " To swtich to next tab.map <leader>tp :tabp<cr>        " To switch to previous tab." Line Numbers & Indentationset backspace=indent,eol,start  " To make backscape work in all conditions.set ma                          " To set mark a at current cursor location.set number                      " To switch the line numbers on.set expandtab                   " To enter spaces when tab is pressed.set smarttab                    " To use smart tabs.set autoindent                  " To copy indentation from current line                                 " when starting a new line.set si                          " To switch on smart indentation." Searchset ignorecase                  " To ignore case when searching.set smartcase                   " When searching try to be smart about cases.set hlsearch                    " To highlight search results.set incsearch                   " To make search act like search in modern browsers.set magic                       " For regular expressions turn magic on." Bracketsset showmatch                   " To show matching brackets when text indicator                                 " is over them.set mat=2                       " How many tenths of a second to blink                                 " when matching brackets." Errorsset noerrorbells                " No annoying sound on errors." Color & Fontssyntax enable                   " Enable syntax highlighting.set encoding=utf8                " Set utf8 as standard encoding and                                  " en_US as the standard language." Enable 256 colors palette in Gnome Terminal.if $COLORTERM == 'gnome-terminal'    set t_Co=256endiftry    colorscheme desertcatchendtry" Files & Backupset nobackup                     " Turn off backup.set nowb                         " Don't backup before overwriting a file.set noswapfile                   " Don't create a swap file.set ffs=unix,dos,mac             " Use Unix as the standard file type." Return to last edit position when opening filesau BufReadPost * if line("'\"") > 1 && line("'\"") <= line("$") | exe "normal! g'\"" | endif</pre><h2 id="Basic-Usage-of-Nvim-R">Basic Usage of Nvim-R</h2><pre>Ctrl + W + HJKL   " Remove the cursor from window to window,nt               " Open a new tab,tn               " Move to the next tab,tp               " Back to the previous tab# code fold behaviorzc - Close (fold) the current fold under the cursor.zo - Open (unfold) the current fold under the cursor.za - Toggle between closing and opening the fold under the cursor.zR - Open all folds in the current buffer.zM - Close all folds in the current buffer.# Nvim-RCtrl + x + o      " Access the help information (auto fill)\rf               " Connect to R console.\rq               " Quit R console.\ro               " Open object bowser.\d                " Execute current line of code and move to the next line.\ss               " Execute a block of selected code.\aa               " Execute the entire script. This is equivalent to source().\xx               " Toggle comment in an R script.# NERDTree,nn               " Toggle NERDTree.</pre><h2 id="Basic-codes-for-nvim">Basic codes for nvim</h2><p>You could also included them into the <code>vim.init</code> file</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># wrap the words in it is too long</span><br>:<span class="hljs-built_in">set</span> wrap <br>:<span class="hljs-built_in">set</span> nowrap<br><span class="hljs-comment"># set the wrap behavior</span><br>:<span class="hljs-built_in">set</span> showbreak=↪\ <br></code></pre></td></tr></table></figure></div><h2 id="Bugs">Bugs</h2><p>After installed the Nvim-R (them master branch), you’ll have the error code below whenever you open nvim. Just ignore it and it would be fine.</p><pre>Error detected while processing function ROnJobStdout[40]..UpdateSynRhlist[11]..FunHiOtherBf:line   10:E117: Unknown function: nvim_set_option_valuePress ENTER or type command to continue</pre><h2 id="Configure-for-LuanrVim">Configure for LuanrVim</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">curl --proto <span class="hljs-string">&#x27;=https&#x27;</span> --tlsv1.2 -sSf https://sh.rustup.rs | sh<br>cat <span class="hljs-string">&quot;<span class="hljs-variable">$HOME</span>/.cargo/env&quot;</span> &gt;&gt; ~/.zshrc<br><span class="hljs-built_in">source</span> ~/.zshrc<br><br><span class="hljs-comment"># install LunarVim</span><br>LV_BRANCH=<span class="hljs-string">&#x27;release-1.3/neovim-0.9&#x27;</span> bash &lt;(curl -s https://raw.githubusercontent.com/LunarVim/LunarVim/release-1.3/neovim-0.9/utils/installer/install.sh)<br><br>rm -rf ~/.config/lvim/<br>git <span class="hljs-built_in">clone</span> https://github.com/Karobben/kickstart.nvim.git ~/.config/lvim<br></code></pre></td></tr></table></figure></div><h3 id="Nerd-font">Nerd font</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># curl -fsSL https://raw.githubusercontent.com/ronniedroid/getnf/master/install.sh | bash</span><br>mkdir -p ~/.<span class="hljs-built_in">local</span>/share/fonts<br><span class="hljs-built_in">cd</span> ~/.<span class="hljs-built_in">local</span>/share/fonts &amp;&amp; curl -fLO https://github.com/ryanoasis/nerd-fonts/raw/HEAD/patched-fonts/DroidSansMono/DroidSansMNerdFont-Regular.otf<br>fc-cache -f -v<br></code></pre></td></tr></table></figure></div><h3 id="Words-Editing">Words Editing</h3><p>For editing the word, we need to switch the model of read, visual, and editing. Press <code>i</code> enable the editing mode. Type <code>Esc</code> or <code>Ctrl + c</code> exist the editing mode and back to the reading mode. <code>v</code> enable selection model so you could select words.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="LIVECODESERVER"><figure class="iseeu highlight /livecodeserver"><table><tr><td class="code"><pre><code class="hljs livecodeserver">i                   enter editing mode<br>dd                  cut <span class="hljs-keyword">the</span> selected <span class="hljs-built_in">line</span> <span class="hljs-keyword">into</span> paste board<br>p                   paste <span class="hljs-keyword">the</span> coppied contents<br>v                   selecte mode <span class="hljs-built_in">to</span> selecte multiple <span class="hljs-keyword">words</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">lines</span><br>    wd              <span class="hljs-built_in">delete</span> <span class="hljs-keyword">the</span> selected <span class="hljs-keyword">words</span>/<span class="hljs-keyword">lines</span><br>    y               copy <span class="hljs-keyword">the</span> selected <span class="hljs-keyword">words</span>/<span class="hljs-keyword">lines</span><br>    p               repalce <span class="hljs-keyword">the</span> selected workds/<span class="hljs-keyword">lines</span> <span class="hljs-keyword">with</span> coppied contents<br>o                   Start <span class="hljs-keyword">a</span> <span class="hljs-built_in">new</span> <span class="hljs-built_in">line</span><br>:&gt;                  Intend <span class="hljs-keyword">the</span> selected <span class="hljs-built_in">line</span> <br>:&gt;&gt;                 Intend tiwce<br>:&lt;                  Undo <span class="hljs-keyword">the</span> intend<br>: m <span class="hljs-number">10</span>              Move <span class="hljs-keyword">the</span> selected <span class="hljs-keyword">words</span>/<span class="hljs-built_in">line</span> <span class="hljs-keyword">into</span> <span class="hljs-built_in">line</span> <span class="hljs-number">10</span><br>Alt+ j/k        Move selected <span class="hljs-keyword">words</span>/<span class="hljs-keyword">lines</span> up/donw<br></code></pre></td></tr></table></figure></div><h3 id="Cursor-Related">Cursor Related</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="LIVECODESERVER"><figure class="iseeu highlight /livecodeserver"><table><tr><td class="code"><pre><code class="hljs livecodeserver">h                   moving left<br>j                   moving down<br>k                   moving up<br>l                   moving <span class="hljs-literal">right</span><br>:<span class="hljs-number">10</span>                 moving <span class="hljs-built_in">to</span> <span class="hljs-built_in">line</span> <span class="hljs-number">10</span><br>w                   moving <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> head <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> next <span class="hljs-built_in">word</span><br>e                   moving <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> <span class="hljs-function"><span class="hljs-keyword">end</span> <span class="hljs-title">of</span> <span class="hljs-title">the</span> <span class="hljs-title">word</span></span><br>b                   moving <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> head <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> previous <span class="hljs-built_in">word</span><br>ge                  moving back <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> <span class="hljs-function"><span class="hljs-keyword">end</span> <span class="hljs-title">of</span> <span class="hljs-title">the</span> <span class="hljs-title">word</span></span><br><br>gg                  moving <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> top <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> <span class="hljs-built_in">file</span><br>G                   moving <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> <span class="hljs-function"><span class="hljs-keyword">end</span> <span class="hljs-title">of</span> <span class="hljs-title">the</span> <span class="hljs-title">file</span></span><br>Ctrl + f            page foward (donw)<br>Ctrl + b            page back (up)<br></code></pre></td></tr></table></figure></div><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="VIM"><figure class="iseeu highlight /vim"><table><tr><td class="code"><pre><code class="hljs vim"><span class="hljs-symbol">&lt;leader&gt;</span> <span class="hljs-keyword">e</span>          <span class="hljs-keyword">open</span> directory exploer<br><span class="hljs-symbol">&lt;leader&gt;</span> <span class="hljs-keyword">w</span>          save the <span class="hljs-keyword">edit</span> <span class="hljs-keyword">file</span> (:<span class="hljs-keyword">w</span>)<br><br># about <span class="hljs-keyword">tab</span><br><span class="hljs-symbol">&lt;leader&gt;</span> bb         back <span class="hljs-keyword">to</span> the <span class="hljs-keyword">tab</span> <span class="hljs-keyword">left</span><br><span class="hljs-symbol">&lt;leader&gt;</span> <span class="hljs-keyword">bn</span>         <span class="hljs-keyword">go</span> <span class="hljs-keyword">to</span> the <span class="hljs-keyword">next</span> <span class="hljs-keyword">tab</span> (<span class="hljs-keyword">right</span>)<br>:<span class="hljs-keyword">bd</span>                 <span class="hljs-keyword">close</span> the current <span class="hljs-keyword">tab</span><br><br><span class="hljs-symbol">&lt;Alt&gt;</span> <span class="hljs-number">1</span>/<span class="hljs-number">2</span>/<span class="hljs-number">3</span>         <span class="hljs-keyword">open</span> <span class="hljs-keyword">a</span> terminal (<span class="hljs-keyword">only</span> when you installed the pluges)<br><span class="hljs-symbol">&lt;Ctrl&gt;</span> <span class="hljs-keyword">w</span>            swtich between windows<br><br>:<span class="hljs-keyword">set</span> mouse-=<span class="hljs-keyword">a</span>       Disable the mouse selection<br>:<span class="hljs-keyword">set</span> mouse=<span class="hljs-keyword">a</span>        Inable the mouse <br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Using Vim as Python and R IDE, easy configure</summary>
    
    
    
    <category term="Linux" scheme="https://karobben.github.io/categories/Linux/"/>
    
    
    <category term="IDE" scheme="https://karobben.github.io/tags/IDE/"/>
    
    <category term="vim" scheme="https://karobben.github.io/tags/vim/"/>
    
  </entry>
  
  <entry>
    <title>Structure of the Immunoglobulin</title>
    <link href="https://karobben.github.io/2024/01/02/LearnNotes/igstructure/"/>
    <id>https://karobben.github.io/2024/01/02/LearnNotes/igstructure/</id>
    <published>2024-01-02T16:53:01.000Z</published>
    <updated>2024-01-12T00:29:13.787Z</updated>
    
    <content type="html"><![CDATA[<h2 id="The-Structure-of-the-Immunoglobulin">The Structure of the Immunoglobulin</h2><table><thead><tr><th style="text-align:center"><img src="https://www.researchgate.net/publication/337733518/figure/fig2/AS:832370903097347@1575464096611/Basic-structure-of-an-IgG-antibody-The-IgG-antibody-is-made-out-of-variable-V-and.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.researchgate.net/publication/337733518_Quantitative_Mass_Spectrometric_Analysis_of_Autoantibodies_as_a_Paradigm_Shift_in_Autoimmune_Serology">© Adrian Y. S. Lee</a></td></tr></tbody></table><p>Immunoglobulins, commonly known as antibodies, are crucial proteins in the immune system that recognize and bind to specific antigens, such as bacteria and viruses, to help protect the body. Their structure is both unique and complex, consisting of several key components:</p><ol><li><p><strong>Basic Structure</strong>: Immunoglobulins are Y-shaped molecules made up of four polypeptide chains - two identical heavy (H) chains and two identical light (L) chains. These chains are held together by disulfide bonds.</p></li><li><p><strong>Variable (V) and Constant © Regions</strong>:</p><ul><li><strong>Variable Regions</strong>: The tips of the ‘Y’ shape consist of the variable regions of the light and heavy chains. These regions are highly diverse and are responsible for the antigen-binding specificity of the antibody.</li><li><strong>Constant Regions</strong>: The rest of the molecule forms the constant region, which is relatively conserved across different antibodies. The constant region of the heavy chains determines the class or isotype (e.g., IgG, IgM, IgA, IgE, IgD) of the antibody and mediates effector functions.</li></ul></li><li><p><strong>Isotypes</strong>: Mammals have several classes of immunoglobulins (IgG, IgA, IgM, IgE, IgD), each with different roles in the immune response. These isotypes differ mainly in their heavy chain constant regions.</p></li><li><p><strong>Glycosylation</strong>: Many antibodies are glycosylated, meaning they have carbohydrate groups attached. This glycosylation can affect the antibody’s stability, distribution, and activity.</p></li><li><p><strong>Light Chain Types</strong>: There are two types of light chains in antibodies - kappa (<mark>κ</mark>) and lambda (<mark>λ</mark>). An individual antibody will have two identical light chains of one type.</p></li></ol><h2 id="V-D-J">V(D)J</h2><table><thead><tr><th style="text-align:center"><img src="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5089068/bin/nihms-673138-f0001.jpg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5089068/">© David B. Roth</a></td></tr></tbody></table><p>V(D)J recombination is a mechanism in the immune system that generates the immense diversity of antibodies (immunoglobulins) and T cell receptors necessary for the adaptive immune response. This process is named for the three gene segments involved in the recombination: Variable (V), Diversity (D), and Joining (J). So, V(D)J is the recombination unit.</p><h3 id="V-D-J-in-3D-Structure">V(D)J in 3D Structure</h3><p>We take <code>5wl2</code> as example:</p><table><thead><tr><th style="text-align:center"><img src="https://cdn.rcsb.org/images/structures/5wl2_assembly-1.jpeg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.rcsb.org/structure/5wl2">©PDB 5wl2</a></td></tr></tbody></table><h2 id="Kabat-Number">Kabat Number</h2><table><thead><tr><th style="text-align:center"><img src="https://assets-global.website-files.com/628cfd01406f3f5bb9c8477d/63821c6283c0cdf36d5289da_Antibody-numbering-IMGT-Kabat-Chothia.png" alt="Kabat number"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://pipebio.com/blog/antibody-numbering">© pipebio</a></td></tr></tbody></table><p>How to make it in python:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> abnumber <span class="hljs-keyword">import</span> Chain<br><br>Fasta = <span class="hljs-string">&quot;xxx.fa&quot;</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(Fasta, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> F:<br>    Seq = F.read()<br><br>KABAT = pd.DataFrame([<span class="hljs-built_in">dict</span>(Chain(i, scheme=<span class="hljs-string">&#x27;kabat&#x27;</span>)) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span>  Seq.split(<span class="hljs-string">&#x27;\n&#x27;</span>)[:-<span class="hljs-number">2</span>] <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;&gt;&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> i])<br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Structure of the Immunoglobulin</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>ImageMagick: convert</title>
    <link href="https://karobben.github.io/2023/12/25/Linux/convert/"/>
    <id>https://karobben.github.io/2023/12/25/Linux/convert/</id>
    <published>2023-12-25T23:16:09.000Z</published>
    <updated>2023-12-26T01:02:50.382Z</updated>
    
    <content type="html"><![CDATA[<p>Documentation: <a href="https://imagemagick.org/script/command-line-tools.php">imagemagick.org</a></p><p>The <code>convert</code> command in Linux is a part of the ImageMagick suite, a powerful toolset for image manipulation. This command allows you to convert between different image formats, resize images, change image quality, and perform a wide variety of other image transformations.</p><p>Here are a few basic examples of what you can do with the <code>convert</code> command:</p><ol><li><strong>Converting an Image Format:</strong><br>To convert a JPEG image to a PNG, you would use:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert image.jpg image.png<br></code></pre></td></tr></table></figure></div><ol start="2"><li><strong>Resizing an Image:</strong><br>To resize an image to a specific width and height (e.g., 100x100 pixels), you can use:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># based on the pixel of x*y </span><br>convert original.jpg -resize 100x100 resized.jpg<br><span class="hljs-comment"># based on the ratio </span><br>convert original.jpg -resize 30% reduced.jpg<br><span class="hljs-comment"># based on the ratio of x*y</span><br>convert original.jpg -resize 50%x30% reduced.jpg<br></code></pre></td></tr></table></figure></div><ol start="3"><li><strong>Changing Image Quality:</strong><br>To change the quality of a JPEG image, useful for reducing file size, use:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -quality 85 compressed.jpg<br></code></pre></td></tr></table></figure></div><ol start="4"><li><strong>Combining Multiple Images:</strong><br>To combine multiple images into one, for instance, side by side:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert image1.jpg image2.jpg +append combined.jpg<br></code></pre></td></tr></table></figure></div><ol start="5"><li><strong>Converting a PDF to an Image:</strong><br>To convert a PDF file to a series of images, you can use:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert document.pdf document.png<br></code></pre></td></tr></table></figure></div><ol start="6"><li><strong>Creating an Animated GIF:</strong><br>To create an animated GIF from a series of images:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert -delay 20 -loop 0 frame1.png frame2.png frame3.png animated.gif<br></code></pre></td></tr></table></figure></div><p>These examples are just the tip of the iceberg in terms of what ImageMagick’s <code>convert</code> command can do. It’s a very powerful tool with a wide array of options and capabilities. For more detailed information, you can check the manual page (<code>man convert</code>) or the official ImageMagick documentation.</p><h2 id="Other-Functions-You-May-Want-to-Know">Other Functions You May Want to Know</h2><p>Certainly! Here are examples demonstrating various capabilities of ImageMagick:</p><ol><li><strong>Image Composition:</strong><br>Overlay one image on top of another (watermark):</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert background.jpg watermark.png -gravity center -composite output.jpg<br></code></pre></td></tr></table></figure></div><ol start="2"><li><strong>Color Manipulation:</strong><br>Convert an image to grayscale:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -colorspace Gray grayscale.jpg<br></code></pre></td></tr></table></figure></div><ol start="3"><li><strong>Cropping:</strong><br>Crop an image to a 100x100 pixel square starting at (50,50):</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -crop 100x100+50+50 cropped.jpg<br></code></pre></td></tr></table></figure></div><ol start="4"><li><strong>Rotating and Flipping:</strong><br>Rotate an image by 90 degrees:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -rotate 90 rotated.jpg<br></code></pre></td></tr></table></figure></div><ol start="5"><li><strong>Blurring and Sharpening:</strong><br>Apply a Gaussian blur:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -blur 0x8 blurred.jpg<br></code></pre></td></tr></table></figure></div><ol start="6"><li><strong>Drawing:</strong><br>Draw a red rectangle:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -fill none -stroke red -draw <span class="hljs-string">&quot;rectangle 10,10 50,50&quot;</span> output.jpg<br></code></pre></td></tr></table></figure></div><ol start="7"><li><strong>Format Conversion:</strong><br>Convert an image to a different format (e.g., PNG to GIF):</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert image.png image.gif<br></code></pre></td></tr></table></figure></div><ol start="8"><li><strong>Handling Transparency:</strong><br>Make the white background of an image transparent:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -transparent white output.png<br></code></pre></td></tr></table></figure></div><ol start="9"><li><strong>Image Annotation:</strong><br>Add text to an image:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -pointsize 24 -fill black -annotate +50+50 <span class="hljs-string">&#x27;Sample Text&#x27;</span> output.jpg<br></code></pre></td></tr></table></figure></div><ol start="10"><li><strong>Special Effects:</strong><br>Add a shadow to an image:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg \( +<span class="hljs-built_in">clone</span> -background black -shadow 60x5+10+10 \) +swap -background none -layers merge +repage shadow.jpg<br></code></pre></td></tr></table></figure></div><ol start="11"><li><strong>Image Optimization:</strong><br>Optimize an image for the web (reduce file size):</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -strip -interlace Plane -gaussian-blur 0.05 -quality 85% optimized.jpg<br></code></pre></td></tr></table></figure></div><ol start="12"><li><strong>Batch Processing:</strong><br>Resize all PNG images in a directory (example in a bash loop):</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> *.png; <span class="hljs-keyword">do</span> convert <span class="hljs-string">&quot;<span class="hljs-variable">$img</span>&quot;</span> -resize 50% <span class="hljs-string">&quot;resized_<span class="hljs-variable">$img</span>&quot;</span>; <span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure></div><ol start="13"><li><strong>Image Analysis:</strong><br>Get image information (format, dimensions, etc.):</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">identify -verbose image.jpg<br></code></pre></td></tr></table></figure></div><ol start="14"><li><strong>Creating Thumbnails:</strong><br>Generate a thumbnail:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -thumbnail 100x100 thumbnail.jpg<br></code></pre></td></tr></table></figure></div><ol start="15"><li><strong>Morphing and Transforming:</strong><br>Morph between two images:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert image1.jpg image2.jpg -morph 10 morph_output.jpg<br></code></pre></td></tr></table></figure></div><p>These commands showcase the versatility of ImageMagick. Remember to adjust the file names and parameters according to your specific needs. The ImageMagick documentation provides more detailed information and examples for these and other features.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">ImageMagick: convert</summary>
    
    
    
    <category term="Linux" scheme="https://karobben.github.io/categories/Linux/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Linux/Software/"/>
    
    
    <category term="Image" scheme="https://karobben.github.io/tags/Image/"/>
    
    <category term="Linux" scheme="https://karobben.github.io/tags/Linux/"/>
    
    <category term="bash" scheme="https://karobben.github.io/tags/bash/"/>
    
    <category term="Scripting" scheme="https://karobben.github.io/tags/Scripting/"/>
    
    <category term="CLI Tools" scheme="https://karobben.github.io/tags/CLI-Tools/"/>
    
  </entry>
  
  <entry>
    <title>IMGT®: the international ImMunoGeneTics information system®</title>
    <link href="https://karobben.github.io/2023/12/25/Bioinfor/imgt/"/>
    <id>https://karobben.github.io/2023/12/25/Bioinfor/imgt/</id>
    <published>2023-12-25T17:06:14.000Z</published>
    <updated>2023-12-25T19:08:52.156Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-IMGT">What is IMGT</h2><p>IMGT® provides a range of databases, tools, and web resources focused on the immune system, particularly on the genetic and structural aspects of immunoglobulins (IG), T cell receptors (TCR), major histocompatibility complex (MHC) of all vertebrate species, and related proteins of the immune system (RPI) of any species. These resources are crucial for research in various fields, including immunology, genetics, bioinformatics, drug design, and personalized medicine.</p><p>Key features and offerings of IMGT® include:</p><ol><li><p><strong>Databases:</strong> IMGT® offers several databases containing detailed information on IG, TCR, and MHC sequences and structures, along with RPI. These databases are meticulously curated and regularly updated.</p></li><li><p><strong>Analysis Tools:</strong> IMGT® provides tools for sequence analysis, gene identification, and 3D structure determination. IMGT/V-QUEST, for instance, is a widely used tool for the analysis of IG and TCR sequences.</p></li><li><p><strong>Standardized Nomenclature:</strong> IMGT® has established a standardized nomenclature for the description of IG and TCR genetic components, which is essential for consistent communication and research in the field.</p></li><li><p><strong>Educational Resources:</strong> The system also offers educational resources for those new to the field of immunogenetics, including tutorials, glossaries, and comprehensive descriptions of the molecular components of the immune system.</p></li><li><p><strong>Research and Clinical Applications:</strong> The information and tools provided by IMGT® are invaluable for various applications, including research in immunology, genetics, and autoimmunity, as well as in clinical settings for antibody engineering, diagnosis, and understanding of immune disorders.</p></li></ol><h2 id="Main-Service-of-the-IMGT">Main Service of the IMGT</h2><table><thead><tr><th><strong>Category</strong></th><th><strong>Name</strong></th><th><strong>Description/Focus Area</strong></th></tr></thead><tbody><tr><td><strong>Databases</strong></td><td>IMGT/GENE-DB</td><td>Database for immunoglobulin (IG) and T cell receptor (TCR) genes of all vertebrate species.</td></tr><tr><td></td><td>IMGT/3Dstructure-DB</td><td>Database for 3D structures of IG, TCR, MHC, and RPI (related proteins of the immune system).</td></tr><tr><td></td><td>IMGT/LIGM-DB</td><td>A comprehensive database of IG and TCR nucleotide sequences from various species.</td></tr><tr><td></td><td>IMGT/PRIMER-DB</td><td>Database of primers and probes for IG and TCR gene sequences.</td></tr><tr><td></td><td>IMGT/PROTEIN-DB</td><td>Database for IG, TCR, MHC, and RPI protein sequences and structures.</td></tr><tr><td><strong>Analysis Tools</strong></td><td><mark>IMGT/V-QUEST</mark></td><td>Tool for the analysis of IG and TCR nucleotide sequences. Identifies V, D, and J gene segments and alleles.</td></tr><tr><td></td><td><mark>IMGT/JunctionAnalysis</mark></td><td>Tool focused on detailed analysis of the V-J and V-D-J junctions of IG and TCR sequences.</td></tr><tr><td></td><td>IMGT/HighV-QUEST</td><td>High-throughput version of IMGT/V-QUEST for next-generation sequencing (NGS) data.</td></tr><tr><td></td><td>IMGT/DomainGapAlign</td><td>Tool for the analysis of IG, TCR, and RPI domain sequences and comparison with IMGT reference directory.</td></tr><tr><td></td><td>IMGT/Collier-de-Perles</td><td>Tool for two-dimensional (2D) graphical representation of IG and TCR variable domains.</td></tr><tr><td><strong>Resources</strong></td><td>IMGT Education</td><td>Educational resources, including tutorials, glossaries, and comprehensive descriptions of immunogenetics.</td></tr><tr><td></td><td>IMGT Scientific chart</td><td>Standardized nomenclature and classification for IG, TCR, and MHC of humans and other vertebrates.</td></tr><tr><td></td><td>IMGT Repertoire</td><td>Compilation of allelic polymorphisms and protein displays for variable (V), diversity (D), and joining (J) genes.</td></tr><tr><td><strong>Other Services</strong></td><td>IMGT/Therapeutic</td><td>Information on therapeutic antibodies and fusion proteins for immune applications.</td></tr><tr><td></td><td>IMGT/mAb-DB</td><td>Specific database for monoclonal antibodies (mAbs).</td></tr></tbody></table><h2 id="What-is-IMGT-V-QUEST-database">What is IMGT/V-QUEST database?</h2><p>IMGT/V-QUEST is a specialized database and analysis tool that is part of the broader IMGT®, the international ImMunoGeneTics information system®. This system is a high-quality integrated knowledge resource specializing in immunoglobulins (IG), T cell receptors (TCR), major histocompatibility complex (MHC) of all vertebrate species, and related proteins of the immune system (RPI) of any species.</p><p>IMGT/V-QUEST specifically provides detailed analysis of nucleotide sequences for immunoglobulins (IG) and T cell receptors (TCR). It is widely used in immunology and related research fields for tasks such as:</p><ol><li><p><strong>Sequence Analysis:</strong> It allows for the identification and delimitation of V, D, and J genes and alleles in the input sequences. This is crucial for understanding the genetic basis of the immune response.</p></li><li><p><strong>Clonotype Characterization:</strong> Researchers use it to characterize the clonotypes (unique T cell or B cell receptor sequences) in an individual’s immune repertoire, which is important in studies of immune system diversity and response.</p></li><li><p><strong>Somatic Hypermutation Studies:</strong> It helps in the analysis of somatic hypermutations, which are critical for understanding adaptive immunity and processes like affinity maturation.</p></li><li><p><strong>Comparative Immunology:</strong> By providing a comprehensive and curated database of IG and TCR sequences across different species, it aids in comparative immunology studies.</p></li></ol><p>As of my last update in April 2023, IMGT/V-QUEST continues to be a valuable resource for immunologists, molecular biologists, and other researchers studying the adaptive immune system. The database is regularly updated to include new findings and sequences, ensuring its relevance and usefulness in the field.</p><div class="admonition note"><p class="admonition-title">Something You May Want to Know</p><p>c16 &gt; g, Q6 &gt; E (++−) means that the nt mutation (c &gt; g) leads to an AA change at codon 6 with the same hydropathy (+) and volume (+) but with different physicochemical properties (−) classes ( 12 )[^Pommié_04].</p></div><h2 id="What-is-IMGT-Ontology">What is IMGT-Ontology?</h2><p>IMGT/Ontology is a key component of IMGT®, the international ImMunoGeneTics information system®. It represents the first ontology for immunogenetics and immunoinformatics and is a foundational aspect of the IMGT® information system. Developed by Marie-Paule Lefranc and her team, IMGT/Ontology provides a standardized vocabulary and a set of concepts that are essential for the consistent annotation, description, and comparison of the immune system’s components across different species. So, <strong>IMGT/Ontology is more of a set of criteria or a framework rather than a specific tool or software.</strong></p><p>Key features and aspects of IMGT/Ontology include:</p><details><summary>1. Standardized Vocabulary</summary>IMGT/Ontology provides a controlled vocabulary for the description of immunogenetic sequences. This includes terms for genes, alleles, and other genetic elements relevant to immunogenetics.</details><details><summary>2. Classification Criteria</summary>It establishes clear and standardized criteria for the classification of immunoglobulins (IG), T cell receptors (TCR), and major histocompatibility complex (MHC) molecules. This classification is crucial for the accurate comparison and analysis of these molecules.</details><details><summary>3. Identification of Key Concepts</summary>The ontology identifies and defines key concepts in immunogenetics, such as gene segment functionality, recombination features, and junctional diversity. These concepts are critical for understanding the adaptive immune response.</details><details><summary>4. IMGT-ONTOLOGY Axioms</summary>These are the rules that define the relations between the concepts and are used for the annotation and description of the IG, TCR, and MHC sequences in the IMGT® databases and tools.</details><details><summary>5. Facilitating Data Integration and Analysis</summary>By providing a common framework and language, IMGT/Ontology enables the integration, analysis, and sharing of immunogenetic data across different research projects and databases.</details><details><summary>6. Support for Research and Clinical Applications</summary>The standardized approach of IMGT/Ontology supports various research and clinical applications, including antibody engineering, vaccine development, and the study of immune responses.</details><br><div class="admonition note"><p class="admonition-title">IMGT-ONTOLOGY axioms and concepts</p></div><blockquote><p>Seven IMGT-ONTOLOGY axioms have been defined: ‘IDENTIFICATION’ <sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>, ‘DESCRIPTION’ <sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>, ‘CLASSIFICATION’ <sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>, ‘NUMEROTATION’ <sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup><sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>, ‘LOCALIZATION’, ‘ORIENTATION’, and ‘OBTENTION’. They constitute the Formal IMGT-ONTOLOGY or IMGT-Kaleidoscope <sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>.</p></blockquote><table><thead><tr><th style="text-align:center"><img src="https://www.imgt.org/IMGTeducation/Tutorials/Ontology/UK/Figure3.png" alt="IMGT-ONTOLOGY"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.imgt.org/IMGTeducation/Tutorials/index.php?article=Ontology&amp;lang=UK&amp;nbr=all">© IMGT Education</a></td></tr></tbody></table><p>IMGT/Ontology is a critical resource for researchers and professionals in immunology, genetics, bioinformatics, and related fields. It ensures that data and analyses are consistent, reproducible, and interoperable, which is vital in advancing our understanding of the immune system and in developing immunotherapy and other medical applications.</p><p>The oldest paper about ontology:</p><ul><li>1999: <a href="https://academic.oup.com/bioinformatics/article/15/12/1047/248534">Ontology for immunogenetics: the IMGT-ONTOLOGY</a> <sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup><ul><li>The first ontology to be developed for immunogenetics. ‘An ontology is a specification of conceptualization’ (Gruber, 1993).</li><li>Covers four main concepts: ‘IDENTIFICATION’, ‘DESCRIPTION’, ‘CLASSIFICATION’ and ‘OBTENTION’.</li></ul></li><li>2008: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0300908407002453">IMGT-Kaleidoscope, the formal IMGT-ONTOLOGY paradigm</a><sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup><ul><li>seven axioms, ‘IDENTIFICATION’, ‘DESCRIPTION’, ‘CLASSIFICATION’, ‘NUMEROTATION’, ‘LOCALIZATION’, ‘ORIENTATION’ and ‘OBTENTION’</li></ul></li></ul><h3 id="References">References</h3><ol><li><a href="https://www.imgt.org/IMGTindex/ontology.php">IMGT®: Ontology (IMGT-ONTOLOGY)</a></li><li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0300908407002453">Duroux P, Kaas Q, Brochet X, et al. IMGT-Kaleidoscope, the formal IMGT-ONTOLOGY paradigm[J]. Biochimie, 2008, 90(4): 570-583.</a></li><li><a href="https://www.frontiersin.org/articles/10.3389/fgene.2012.00079/full">Giudicelli V, Lefranc M P. Imgt-ontology 2012[J]. Frontiers in genetics, 2012, 3: 79.</a></li><li><a href="https://www.frontiersin.org/articles/10.3389/fimmu.2014.00022/full">Lefranc M P. Immunoglobulin and T cell receptor genes: IMGT® and the birth and rise of immunoinformatics[J]. Frontiers in immunology, 2014, 5: 22.</a></li></ol><h2 id="Other-References">Other References</h2><ol><li><a href="https://academic.oup.com/nar/article/36/suppl_2/W503/2506795?login=false">Brochet X, Lefranc M P, Giudicelli V. IMGT/V-QUEST: the highly customized and integrated system for IG and TR standardized VJ and VDJ sequence analysis[J]. Nucleic acids research, 2008, 36(suppl_2): W503-W508.</a></li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Lefranc, M.-P. From IMGT-ONTOLOGY IDENTIFICATION Axiom to IMGT Standardized Keywords: For Immunoglobulins (IG), T Cell Receptors (TR), and Conventional Genes. Cold Spring Harb Protoc., 1;2011(6): 604-613. pii: pdb.ip82. doi: 10.1101/pdb.ip82(2011) PMID:21632792. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Lefranc, M.-P. From IMGT-ONTOLOGY DESCRIPTION Axiom to IMGT Standardized Labels: For Immunoglobulin (IG) and T Cell Receptor (TR) Sequences and Structures. Cold Spring Harb Protoc., 1;2011(6): 614-626. pii: pdb.ip83. doi: 10.1101/pdb.ip83 (2011) PMID:21632791. <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Lefranc, M.-P. From IMGT-ONTOLOGY CLASSIFICATION Axiom to IMGT Standardized Gene and Allele Nomenclature: For Immunoglobulins (IG) and T Cell Receptors (TR). Cold Spring Harb Protoc., 1;2011(6): 627-632. pii: pdb.ip84. doi: 10.1101/pdb.ip84 (2011) PMID:21632790. <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>Lefranc, M.-P. “IMGT Collier de Perles for the Variable (V), Constant ©, and Groove (G) Domains of IG, TR, MH, IgSF, and MhSF” Cold Spring Harb Protoc. 2011 Jun 1;2011(6). pii: pdb.ip86. doi: 10.1101/pdb.ip86. PMID: 21632788 <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>Lefranc, M.-P. IMGT Unique Numbering for the Variable (V), Constant ©, and Groove (G) Domains of IG, TR, MH, IgSF, and MhSF. Cold Spring Harb Protoc., 1;2011(6). pii: pdb.ip85. doi: 10.1101/pdb.ip85 (2011) PMID: 21632789 <a href="#fnref5" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p>Duroux P et al., IMGT-Kaleidoscope, the Formal IMGT-ONTOLOGY paradigm. Biochimie, 90:570-83. Epub 2007 Sep 11 (2008) PMID:17949886 <a href="#fnref6" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p>Giudicelli V, Lefranc M P. Ontology for immunogenetics: the IMGT-ONTOLOGY[J]. Bioinformatics, 1999, 15(12): 1047-1054. <a href="#fnref7" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p>Duroux P, Kaas Q, Brochet X, et al. IMGT-Kaleidoscope, the formal IMGT-ONTOLOGY paradigm[J]. Biochimie, 2008, 90(4): 570-583. <a href="#fnref8" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">IMGT®: the international ImMunoGeneTics information system®</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Database" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Database/"/>
    
    
    <category term="Database" scheme="https://karobben.github.io/tags/Database/"/>
    
    <category term="Bioinformatic" scheme="https://karobben.github.io/tags/Bioinformatic/"/>
    
    <category term="Immunology" scheme="https://karobben.github.io/tags/Immunology/"/>
    
    <category term="Protein" scheme="https://karobben.github.io/tags/Protein/"/>
    
  </entry>
  
</feed>
