<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Karobben</title>
  
  <subtitle>Engjoy~</subtitle>
  <link href="https://karobben.github.io/atom.xml" rel="self"/>
  
  <link href="https://karobben.github.io/"/>
  <updated>2025-01-06T08:28:46.787Z</updated>
  <id>https://karobben.github.io/</id>
  
  <author>
    <name>Karobben</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>High Dimension Data</title>
    <link href="https://karobben.github.io/2025/01/01/AI/highdimension/"/>
    <id>https://karobben.github.io/2025/01/01/AI/highdimension/</id>
    <published>2025-01-01T21:23:33.000Z</published>
    <updated>2025-01-06T08:28:46.787Z</updated>
    
    <content type="html"><![CDATA[<h2 id="High-Dimensional-Data">High Dimensional Data</h2><p>$$<br>\begin{bmatrix}<br>\mathbf{x}_ 1 \\<br>\mathbf{x}_ 2 \\<br>\vdots \\<br>\mathbf{x}_ N<br>\end{bmatrix} =<br>\begin{bmatrix}<br>x_1^{(1)} &amp; x_1^{(2)} &amp; \cdots &amp; x_1^{(d)} \\<br>x_2^{(1)} &amp; x_2^{(2)} &amp; \cdots &amp; x_2^{(d)} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>x_N^{(1)} &amp; x_N^{(2)} &amp; \cdots &amp; x_N^{(d)}<br>\end{bmatrix}<br>$$</p><h3 id="Mean-and-Covariance-of-High-Dimensional-Data">Mean and Covariance of High-Dimensional Data</h3><p>When working with high-dimensional data, it is important to understand the <strong>mean</strong> and <strong>covariance matrix</strong>, which are essential statistical measures that summarize the data’s location and spread.</p><hr><h3 id="1-Mean-Vector">1. <strong>Mean Vector</strong></h3><p>For a dataset with $ n $ samples and $ d $ features (dimensions):</p><ul><li>Let $ \mathbf{X} \in \mathbb{R}^{n \times d} $ be the dataset, where each row $ \mathbf{x}_i \in \mathbb{R}^d $ represents a data point and each column corresponds to a feature.</li></ul><h4 id="Definition">Definition:</h4><p>The mean vector $ \mathbf{\mu} \in \mathbb{R}^d $ is defined as:<br>$$<br>\mathbf{\mu} = \frac{1}{n} \sum_{i=1}^n \mathbf{x}_i<br>$$</p><h4 id="Element-Wise">Element-Wise:</h4><p>The $ j $-th element of the mean vector is:<br>$$<br>\mu_j = \frac{1}{n} \sum_{i=1}^n x_{ij}, \quad j = 1, 2, \dots, d<br>$$<br>Where $ x_{ij} $ is the $ j $-th feature of the $ i $-th sample.</p><h3 id="2-Covariance-Matrix">2. <strong>Covariance Matrix</strong></h3><p>The covariance matrix $ \mathbf{\Sigma} \in \mathbb{R}^{d \times d} $ captures the pairwise relationships between features.</p><h4 id="Definition-v2">Definition:</h4><p>$$<br>\mathbf{\Sigma} = \frac{1}{n-1} \sum_{i=1}^n (\mathbf{x}_i - \mathbf{\mu})(\mathbf{x}_i - \mathbf{\mu})^T<br>$$</p><h4 id="Element-Wise-v2">Element-Wise:</h4><p>The $ (j, k) $-th entry of the covariance matrix is:<br>$$<br>\Sigma_{jk} = \frac{1}{n-1} \sum_{i=1}^n (x_{ij} - \mu_j)(x_{ik} - \mu_k)<br>$$<br>Where:</p><ul><li>$ \Sigma_{jk} $: Covariance between feature $ j $ and feature $ k $.</li><li>$ x_{ij} $: Value of the $ j $-th feature for the $ i $-th sample.</li></ul><h4 id="Properties">Properties:</h4><ul><li>$ \Sigma_{jj} $: Variance of feature $ j $.</li><li>$ \Sigma_{jk} $: Correlation between features $ j $ and $ k $ if scaled by their standard deviations.</li><li>The matrix $ \mathbf{\Sigma} $ is symmetric: $ \Sigma_{jk} = \Sigma_{kj} $.</li></ul><h3 id="3-Matrix-Representation">3. <strong>Matrix Representation</strong></h3><p>Using matrix notation, the mean vector $ \mathbf{\mu} $ and covariance matrix $ \mathbf{\Sigma} $ can be computed efficiently:</p><h4 id="Mean-Vector">Mean Vector:</h4><p>$$<br>\mathbf{\mu} = \frac{1}{n} \mathbf{X}^T \mathbf{1}<br>$$<br>Where:</p><ul><li>$ \mathbf{X}^T $: Transpose of the data matrix.</li><li>$ \mathbf{1} $: A column vector of ones with size $ n $.</li></ul><h4 id="Covariance-Matrix">Covariance Matrix:</h4><p>$$<br>\mathbf{\Sigma} = \frac{1}{n-1} (\mathbf{X} - \mathbf{1} \mathbf{\mu}^T )^T (\mathbf{X} - \mathbf{1} \mathbf{\mu}^T)<br>$$</p><h3 id="Summary-of-Notation">Summary of Notation</h3><table><thead><tr><th><strong>Symbol</strong></th><th><strong>Meaning</strong></th></tr></thead><tbody><tr><td>$ \mathbf{\mu} $</td><td>Mean vector of the dataset ($ d $-dimensional).</td></tr><tr><td>$ \mathbf{\Sigma} $</td><td>Covariance matrix ($ d \times d $).</td></tr><tr><td>$ \mu_j $</td><td>Mean of the $ j $-th feature.</td></tr><tr><td>$ \Sigma_{jk} $</td><td>Covariance between feature $ j $ and $ k $.</td></tr></tbody></table><h3 id="Example-of-the-Covariance">Example of the Covariance</h3><p>The <strong>Iris dataset</strong> has been successfully loaded. Here’s a brief look at the dataset:</p><table><thead><tr><th><strong>Sepal Length (cm)</strong></th><th><strong>Sepal Width (cm)</strong></th><th><strong>Petal Length (cm)</strong></th><th><strong>Petal Width (cm)</strong></th></tr></thead><tbody><tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td></tr><tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td></tr><tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td></tr><tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td></tr><tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td></tr></tbody></table><p>Let’s compute the <strong>mean</strong> and <strong>covariance matrix</strong> for this dataset and visualize their insights.</p><h3 id="Results-from-the-Iris-Dataset">Results from the Iris Dataset:</h3><h4 id="Mean-Vector-v2"><strong>Mean Vector</strong>:</h4><p>The mean of each feature (measured in centimeters) is:</p><ul><li><strong>Sepal Length</strong>: $ 5.843 $</li><li><strong>Sepal Width</strong>: $ 3.057 $</li><li><strong>Petal Length</strong>: $ 3.758 $</li><li><strong>Petal Width</strong>: $ 1.199 $</li></ul><h4 id="Covariance-Matrix-v2"><strong>Covariance Matrix</strong>:</h4><p>The covariance matrix is:<br>$$<br>\mathbf{\Sigma} =<br>\begin{bmatrix}<br>0.6857 &amp; -0.0424 &amp; 1.2743 &amp; 0.5163 \\<br>-0.0424 &amp; 0.1900 &amp; -0.3297 &amp; -0.1216 \\<br>1.2743 &amp; -0.3297 &amp; 3.1163 &amp; 1.2956 \\<br>0.5163 &amp; -0.1216 &amp; 1.2956 &amp; 0.5810<br>\end{bmatrix}<br>$$</p><h4 id="Interpretation">Interpretation:</h4><ol><li><p><strong>Diagonal Entries</strong>:</p><ul><li>These are the variances of the features:<ul><li>Variance of <strong>Sepal Length</strong>: $ 0.6857 $</li><li>Variance of <strong>Sepal Width</strong>: $ 0.1900 $</li><li>Variance of <strong>Petal Length</strong>: $ 3.1163 $</li><li>Variance of <strong>Petal Width</strong>: $ 0.5810 $</li></ul></li></ul></li><li><p><strong>Off-Diagonal Entries</strong>:</p><ul><li>These represent covariances between pairs of features:<ul><li><strong>Positive covariance</strong> (e.g., $ 1.2743 $ between Sepal Length and Petal Length) suggests a positive relationship.</li><li><strong>Negative covariance</strong> (e.g., $ -0.3297 $ between Sepal Width and Petal Length) suggests a negative relationship.</li></ul></li></ul></li></ol><p>The <strong>Covariance Matrix with Species Encoding</strong> is as follows:</p><table><thead><tr><th>Feature</th><th>Sepal Length (cm)</th><th>Sepal Width (cm)</th><th>Petal Length (cm)</th><th>Petal Width (cm)</th><th>Species Encoded</th></tr></thead><tbody><tr><td><strong>Sepal Length (cm)</strong></td><td>0.6857</td><td>-0.0424</td><td>1.2743</td><td>0.5163</td><td>0.5309</td></tr><tr><td><strong>Sepal Width (cm)</strong></td><td>-0.0424</td><td>0.1900</td><td>-0.3297</td><td>-0.1216</td><td>-0.1523</td></tr><tr><td><strong>Petal Length (cm)</strong></td><td>1.2743</td><td>-0.3297</td><td>3.1163</td><td>1.2956</td><td>1.3725</td></tr><tr><td><strong>Petal Width (cm)</strong></td><td>0.5163</td><td>-0.1216</td><td>1.2956</td><td>0.5810</td><td>0.5973</td></tr><tr><td><strong>Species Encoded</strong></td><td>0.5309</td><td>-0.1523</td><td>1.3725</td><td>0.5973</td><td>0.6711</td></tr></tbody></table><h3 id="Key-Observations">Key Observations:</h3><ol><li><p><strong>Species Encoded Relationships</strong>:</p><ul><li>Positive covariance with features like petal length ($1.3725$) and petal width ($0.5973$).</li><li>Indicates that these features strongly vary with the species.</li></ul></li><li><p><strong>Feature Variability</strong>:</p><ul><li>Variance (diagonal values) is high for petal length ($3.1163$), meaning it varies most across the dataset.</li><li>Sepal width ($0.1900$) has the least variance.</li></ul></li></ol><p><img src="https://imgur.com/gTRpILR.png" alt=""></p><div id="chart_bar" style="width: 100%; height: 400px;"></div><div class="admonition note"><p class="admonition-title">What can we get from this results?</p><p>As you can see, <strong>Petal Length</strong> has the largest variance (var = 3.1163). Meanwhile, it also has the highest covariance with species (1.37). This indicates that much of its variance is explained by the species, making Petal Length a potentially good feature for species classification.</p></div><h2 id="Transformations">Transformations</h2><p>High-dimensional data transformation refers to the process of modifying or converting data that exists in a high-dimensional space (i.e., data with a large number of features or variables) into a more manageable or meaningful representation. This transformation can involve reducing dimensions, re-organizing data, or mapping it to a different space while preserving important information or relationships.</p><ol><li><p><strong>Source Dataset (${x}$)</strong> and <strong>Target Dataset (${m}$)</strong>:</p><ul><li>The target dataset ${m_i}$ is generated by applying a rotation and translation to the source dataset:<br>$$<br>m_i = A x_i + b<br>$$</li><li>Here, $A$ is the rotation matrix, and $b$ is the translation vector.</li></ul></li><li><p><strong>Mean Transformation</strong>:</p><ul><li>The mean of the transformed dataset (${m}$) can be expressed as:<br>$$<br>\text{mean}({m}) = A \cdot \text{mean}({x}) + b<br>$$</li></ul></li><li><p><strong>Covariance Transformation</strong>:</p><ul><li>The covariance matrix of the transformed dataset (${m}$) is derived as:<br>$$<br>\text{Covmat}({m}) = A \cdot \text{Covmat}({x}) \cdot A^\top<br>$$</li><li>This shows how the covariance matrix of the source dataset transforms under a linear transformation.</li><li>The covariance matrix of ${x}$ is defined as:<br>$$<br>\text{Covmat}({x}) = \frac{1}{N} \sum_i (x_i - \text{mean}({x}))(x_i - \text{mean}({x}))^\top<br>$$</li></ul></li></ol><h2 id="Eigenvector-and-Eigenvalue">Eigenvector and Eigenvalue</h2><p>Imagine you’re analyzing data (like in machine learning or physics). Eigenvalues and eigenvectors can:</p><ul><li>Find patterns: In large data (like PCA), eigenvectors show the “directions” of most variation, and eigenvalues tell how important each direction is.</li><li>Simplify problems: Diagonalization makes hard matrix computations easier.</li></ul><h3 id="1-Eigenvector-u-and-Eigenvalue-lambda"><strong>1. Eigenvector ($u$) and Eigenvalue ($\lambda$)</strong></h3><ul><li>An <strong>eigenvector</strong> $u$ of a matrix $S$ is a vector that does not change direction when $S$ is applied to it. Instead, it is scaled by a factor $\lambda$, the <strong>eigenvalue</strong>:<br>$$<br>S u = \lambda u<br>$$<ul><li>$S$: A square matrix.</li><li>$u$: An eigenvector (non-zero vector).</li><li>$\lambda$: The corresponding eigenvalue.</li></ul></li></ul><h3 id="2-Symmetric-Matrices-S"><strong>2. Symmetric Matrices ($S$)</strong></h3><ul><li>If $S$ is symmetric ($S = S^\top$), it has special properties:<ul><li>All eigenvalues are <strong>real</strong>.</li><li>Eigenvectors corresponding to distinct eigenvalues are <strong>orthogonal</strong>:<br>$$<br>u_i \perp u_j \quad \text{if} \quad i \neq j<br>$$</li><li>Eigenvectors can also be <strong>normalized</strong> to form an orthonormal set ($|u| = 1$).</li></ul></li></ul><h3 id="3-Orthonormal-Matrix-U"><strong>3. Orthonormal Matrix ($U$)</strong></h3><ul><li>By stacking all the eigenvectors of $S$ as columns into a matrix $U$:<br>$$<br>U = [u_1, u_2, \dots, u_d]<br>$$<ul><li>$U$ is an <strong>orthonormal matrix</strong>, meaning:<br>$$<br>U^\top U = I \quad \text{(identity matrix)}<br>$$</li></ul></li></ul><h3 id="4-Eigenvalues-as-a-Diagonal-Matrix-Lambda"><strong>4. Eigenvalues as a Diagonal Matrix ($\Lambda$)</strong></h3><ul><li>Arrange eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_d$ into a diagonal matrix:<br>$$<br>\Lambda =<br>\begin{bmatrix}<br>\lambda_1 &amp; 0 &amp; \dots &amp; 0 \\<br>0 &amp; \lambda_2 &amp; \dots &amp; 0 \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>0 &amp; 0 &amp; \dots &amp; \lambda_d<br>\end{bmatrix}<br>$$</li></ul><h3 id="5-Diagonalization"><strong>5. Diagonalization</strong></h3><ul><li>If $S$ is symmetric, it can be <strong>diagonalized</strong> using its eigenvectors and eigenvalues:<br>$$<br>S = U \Lambda U^\top<br>$$<ul><li>$U$: Matrix of eigenvectors.</li><li>$\Lambda$: Diagonal matrix of eigenvalues.</li></ul></li></ul><h3 id="6-Key-Properties-of-Diagonalization"><strong>6. Key Properties of Diagonalization</strong></h3><ul><li>Simplifies computations, e.g., powers of $S$:<br>$$<br>S^k = U \Lambda^k U^\top<br>$$<ul><li>$\Lambda^k$ is simply the diagonal matrix with each eigenvalue raised to the power $k$.</li></ul></li><li>Used in many fields such as:<ul><li>Principal Component Analysis (PCA).</li><li>Solving differential equations.</li><li>Modal analysis in engineering.</li></ul></li></ul><h2 id="Principal-Component-Analysis-PCA">Principal Component Analysis (PCA)</h2><p>Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of data while retaining as much variance as possible. It identifies new axes, called principal components, which are uncorrelated and align with the directions of maximum variance. PCA transforms the data to these new axes, ranks the components by their variance (eigenvalues), and allows dimensionality reduction by selecting the top components.</p><h3 id="PCA-in-3-Steps-More-Accurate-Breakdown">PCA in 3 Steps (More Accurate Breakdown):</h3><ol><li><p><strong>Transformation (Centering the Data)</strong>:</p><ul><li>Before applying PCA, you need to <strong>center</strong> the data by subtracting the mean of each feature. This step ensures that the principal components (axes of maximum variance) pass through the origin.</li><li>Mathematically:<br>$$<br>x_{\text{centered}} = x - \text{mean}(x)<br>$$</li></ul></li><li><p><strong>Rotation (Find Eigenvalues and Eigenvectors)</strong>:</p><ul><li>The goal of PCA is to find the directions (principal components) where the data has the most variance.</li><li>This involves computing the <strong>eigenvectors</strong> and <strong>eigenvalues</strong> of the covariance matrix:<br>$$<br>S = \frac{1}{n} X^\top X<br>$$<ul><li>The eigenvectors represent the new axes (principal components).</li><li>The eigenvalues indicate how much variance is captured by each axis.</li></ul></li><li><strong>Rotation</strong> refers to aligning the data along the directions of these principal components.</li></ul></li><li><p><strong>Dimensional Reduction (Keep Principal Components)</strong>:</p><ul><li>After identifying the principal components, you can choose the top $k$ components with the highest eigenvalues (the directions of the most variance) and ignore the rest.</li><li>This step reduces the dimensionality while retaining as much information as possible.</li></ul></li></ol><h3 id="1-Original-Dataset">1. <strong>Original Dataset</strong>:</h3><ul><li>The dataset ${x}$:<ul><li>It has $d$ features (dimensions).</li><li>Each data point is a vector in a $d$-dimensional space.</li></ul></li></ul><h3 id="2-Step-1-Covariance-Matrix">2. <strong>Step 1: Covariance Matrix</strong>:</h3><ul><li>The covariance matrix captures how features are correlated. It is computed as:<br>$$<br>\text{Covmat}({x}) = \frac{1}{N} \sum_{i=1}^N \left( x_i - \text{mean}({x}) \right) \left( x_i - \text{mean}({x}) \right)^\top<br>$$</li><li>PCA works by <strong>diagonalizing</strong> this covariance matrix.</li></ul><h3 id="3-Step-2-Eigen-Decomposition">3. <strong>Step 2: Eigen Decomposition</strong>:</h3><ul><li>Decompose the covariance matrix into <strong>eigenvalues ($\lambda$)</strong> and <strong>eigenvectors ($u$)</strong>:<br>$$<br>U^\top \text{Covmat}({x}) U = \Lambda<br>$$<ul><li>$U$: Matrix of eigenvectors (principal components).</li><li>$\Lambda$: Diagonal matrix of eigenvalues (variance explained by each principal component).</li></ul></li></ul><h3 id="4-Step-3-Choose-s-Principal-Components">4. <strong>Step 3: Choose $s$ Principal Components</strong>:</h3><ul><li>Eigenvalues represent the <strong>variance</strong> explained by each principal component. They are sorted in descending order.</li><li>To reduce dimensions:<ul><li>Choose the top $s$ eigenvalues that explain the most variance.</li><li>Often, the ratio is calculated:<br>$$<br>\frac{\sum_{j=s+1}^d \lambda_j}{\sum_{j=1}^d \lambda_j}<br>$$<ul><li>This ratio helps decide $s$ by ensuring the <strong>remaining variance (error)</strong> is small.</li></ul></li><li>Plotting $\lambda_i$ vs. $i$ (as shown in the slide) can help visualize where most variance is captured (the “elbow” point).</li></ul></li></ul><h3 id="5-Step-4-Project-Data-to-Lower-Dimensions">5. <strong>Step 4: Project Data to Lower Dimensions</strong>:</h3><ul><li>Once you have selected $s$ principal components, project the original data onto this lower-dimensional space:<br>$$<br>\hat{x}_ i = \sum_ {j=1}^s \left[ u_ j^\top (x_ i - \text{mean}({x})) \right] u_j + \text{mean}({x})<br>$$<ul><li>Here:<ul><li>$u_j$: The eigenvectors corresponding to the top $s$ eigenvalues.</li><li>$\hat{x}_i$: The low-dimensional representation of $x_i$.</li></ul></li></ul></li></ul><h3 id="6-Visualization-from-the-Slide">6. <strong>Visualization from the Slide</strong>:</h3><ul><li>The graph shows the eigenvalues ($\lambda_i$) vs. their indices ($i$):<ul><li>The blue curve represents the sorted eigenvalues.</li><li>The orange circle highlights the “elbow” point, which suggests the optimal number of principal components to retain.</li></ul></li></ul><h3 id="Summary-of-PCA-Calculation">Summary of PCA Calculation:</h3><ol><li>Center the data (subtract the mean).</li><li>Compute the covariance matrix.</li><li>Find eigenvalues and eigenvectors of the covariance matrix.</li><li>Choose the top $s$ eigenvalues to decide the number of principal components.</li><li>Project the data onto the top $s$ eigenvectors to get a reduced representation.</li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style><script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script><script type="text/javascript">  google.charts.load('current', { packages: ['corechart'] });  google.charts.setOnLoadCallback(drawChart);  function drawChart() {    // Data for the bar chart with error bars    var data = google.visualization.arrayToDataTable([      ['Feature', 'Setosa Mean', { role: 'interval' }, { role: 'interval' },                 'Versicolor Mean', { role: 'interval' }, { role: 'interval' },                 'Virginica Mean', { role: 'interval' }, { role: 'interval' }],      ['Sepal Length',       5.006, 4.8, 5.2,  // Setosa       5.936, 5.7, 6.2,  // Versicolor       6.588, 6.4, 6.8], // Virginica      ['Sepal Width',       3.428, 3.2, 3.6,  // Setosa       2.770, 2.6, 2.9,  // Versicolor       2.974, 2.8, 3.1], // Virginica      ['Petal Length',       1.462, 1.3, 1.6,  // Setosa       4.260, 4.0, 4.5,  // Versicolor       5.552, 5.3, 5.8], // Virginica      ['Petal Width',       0.246, 0.2, 0.3,  // Setosa       1.326, 1.2, 1.5,  // Versicolor       2.026, 1.9, 2.2]  // Virginica    ]);    // Chart options    var options = {      title: 'Mean Values with Error Bars by Species',      hAxis: { title: 'Feature' },      vAxis: { title: 'Mean Value', minValue: 0 },      legend: { position: 'top' },      bar: { groupWidth: '75%' },    };    // Draw the chart    var chart = new google.visualization.ColumnChart(document.getElementById('chart_bar'));    chart.draw(data, options);  }</script>]]></content>
    
    
    <summary type="html">High Dimension Data</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>AI: Logistic Regression</title>
    <link href="https://karobben.github.io/2024/12/30/AI/ai-logistic-reg/"/>
    <id>https://karobben.github.io/2024/12/30/AI/ai-logistic-reg/</id>
    <published>2024-12-31T02:03:29.000Z</published>
    <updated>2024-12-31T04:01:44.345Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Logistic-Regression">Logistic Regression</h2><p>Logistic regression is a <strong>supervised machine learning algorithm</strong> used for <strong>binary classification</strong> tasks. Unlike linear regression, which predicts continuous values, logistic regression predicts the <strong>probability</strong> that a given input belongs to a certain class.</p><h3 id="Key-Concepts-in-Logistic-Regression">Key Concepts in Logistic Regression</h3><ol><li><p><strong>Logistic Function (Sigmoid Function)</strong>:</p><ul><li>Logistic regression uses the <strong>sigmoid function</strong> to map predicted values to probabilities:<br>$$<br>\sigma(z) = \frac{1}{1 + e^{-z}}<br>$$<ul><li>$ z = X \beta $: Linear combination of features.</li><li>The output of $ \sigma(z) $ is always between 0 and 1, representing the probability.</li></ul></li></ul></li><li><p><strong>Logit Link Function</strong>:</p><ul><li>The logit function is the natural logarithm of the odds (log-odds) of the binary outcome:<br>$$<br>g(\theta) = \log\left(\frac{P(y=1|X)}{P(y=0|X)}\right)<br>$$</li><li>It transforms probabilities into log-odds:<br>$$<br>g(\theta) = X^T\beta<br>$$</li></ul></li><li><p><strong>Inverse Link Function</strong>:</p><ul><li>To map the log-odds ($ X^T\beta $) back to probabilities, we use the <strong>inverse of the logit function</strong>:<br>$$<br>P(y=1|X, \beta) = \frac{e<sup>{X</sup>T\beta}}{1 + e<sup>{X</sup>T\beta}}<br>$$<ul><li>This is the <strong>sigmoid function</strong>, which outputs probabilities between 0 and 1.</li></ul></li></ul></li><li><p><strong>Decision Boundary</strong>:</p><ul><li>For binary classification:<ul><li>If $ \sigma(z) \geq 0.5 $, classify the input as Class 1.</li><li>If $ \sigma(z) &lt; 0.5 $, classify the input as Class 0.</li></ul></li></ul></li><li><p><strong>Log-Likelihood</strong>:</p><ul><li>Logistic regression optimizes the <strong>log-likelihood</strong> instead of minimizing residuals (like in linear regression):<br>$$<br>\ell(\beta) = \sum_{i=1}^n \left[ y_i \ln(\hat{y}_i) + (1 - y_i) \ln(1 - \hat{y}_i) \right]<br>$$<br>Where:<ul><li>$ \hat{y}_i = \sigma(z_i) $: Predicted probability.</li><li>$ y_i $: Actual class (0 or 1).</li></ul></li></ul></li><li><p><strong>Negative Log-Likelihood</strong>:</p><ul><li>The optimization process in machine learning (and statistics) often involves <strong>minimizing</strong> a cost function. Since the log-likelihood is a measure of fit (higher is better), we take its <strong>negative</strong> to convert the maximization problem into a <strong>minimization problem</strong>:</li><li>$$ -\ln L(\beta) = -\sum_{i=1}^n \left[ y_i X_i^T \beta - \ln(1 + e^ {X_i^ T \beta}) \right] $$</li></ul></li><li><p><strong>Optimization</strong>:</p><ul><li>The goal is to find the coefficients $ \beta $ that maximize the log-likelihood using algorithms like <strong>Gradient Descent</strong> or <strong>Newton’s Method</strong>.</li></ul></li></ol><div class="admonition note"><p class="admonition-title">What is a Link Function?</p><p>A <strong>link function</strong> connects the <strong>linear predictor</strong> ($ X\beta $) to the <strong>mean of the response variable</strong> in a generalized linear model (GLM). It provides a transformation that ensures the predicted values from the model stay within the valid range for the response variable.</p></div><div class="admonition note"><p class="admonition-title">Why negative Log-likelihood function?</p><ul><li>The negative log-likelihood is used to simplify optimization by turning a maximization problem into a minimization one.</li><li>The formula on the slide and in my explanation are equivalent, just written in slightly different forms.</li></ul></div><h3 id="Applications-of-Logistic-Regression">Applications of Logistic Regression</h3><ul><li>Binary classification problems such as:<ul><li>Email spam detection (Spam/Not Spam).</li><li>Disease diagnosis (Positive/Negative).</li><li>Customer churn prediction (Churn/No Churn).</li></ul></li></ul><h3 id="Practical-Example-Binary-Classification-with-Logistic-Regression">Practical Example: Binary Classification with Logistic Regression</h3><p>Below is a Python example using scikit-learn:</p><h4 id="Problem-Predict-whether-a-person-has-heart-disease-based-on-two-features">Problem: Predict whether a person has heart disease based on two features.</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_classification<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score, confusion_matrix, classification_report<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># Generate synthetic binary classification dataset</span><br>X, y = make_classification(n_samples=<span class="hljs-number">200</span>, n_features=<span class="hljs-number">2</span>, n_informative=<span class="hljs-number">2</span>, n_redundant=<span class="hljs-number">0</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># Split the dataset into training and testing sets</span><br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># Fit logistic regression model</span><br>model = LogisticRegression()<br>model.fit(X_train, y_train)<br><br><span class="hljs-comment"># Make predictions</span><br>y_pred = model.predict(X_test)<br><br><span class="hljs-comment"># Evaluate the model</span><br>accuracy = accuracy_score(y_test, y_pred)<br>print(<span class="hljs-string">&quot;Accuracy:&quot;</span>, accuracy)<br>print(<span class="hljs-string">&quot;Confusion Matrix:\n&quot;</span>, confusion_matrix(y_test, y_pred))<br>print(<span class="hljs-string">&quot;Classification Report:\n&quot;</span>, classification_report(y_test, y_pred))<br><br><span class="hljs-comment"># Plot decision boundary</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>x_min, x_max = X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br>y_min, y_max = X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br>xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="hljs-number">0.1</span>),<br>                     np.arange(y_min, y_max, <span class="hljs-number">0.1</span>))<br>Z = model.predict(np.c_[xx.ravel(), yy.ravel()])<br>Z = Z.reshape(xx.shape)<br><br>plt.contourf(xx, yy, Z, alpha=<span class="hljs-number">0.8</span>)<br>plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y, edgecolor=<span class="hljs-string">&#x27;k&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br>plt.title(<span class="hljs-string">&quot;Logistic Regression Decision Boundary&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Feature 1&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Feature 2&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/DvGwCmw.png" alt=""></p><h2 id="Logistic-Regression-for-Multiclass-Classification">Logistic Regression for Multiclass Classification</h2><p>Logistic regression can be extended to handle <strong>multiclass classification problems</strong> where the target variable has more than two classes. The two common approaches are <strong>One-vs-Rest (OvR)</strong> and <strong>Softmax (Multinomial)</strong> logistic regression.</p><h3 id="One-vs-Rest-OvR">One-vs-Rest (OvR)</h3><h4 id="Overview">Overview:</h4><ul><li>In OvR, a separate binary classifier is trained for each class.</li><li>For a class $ k $, the classifier treats:<ul><li>$ y = k $ as <strong>positive (1)</strong>.</li><li>$ y \neq k $ as <strong>negative (0)</strong>.</li></ul></li><li>Each classifier predicts the probability of the input belonging to its class.</li></ul><h4 id="Prediction">Prediction:</h4><ul><li>For a new data point, the class with the <strong>highest probability</strong> is chosen:<br>$$<br>\hat{y} = \arg\max_{k} P(y = k | x)<br>$$</li></ul><h3 id="Softmax-Multinomial-Logistic-Regression">Softmax (Multinomial) Logistic Regression</h3><p>Softmax logistic regression generalizes binary logistic regression to multiple classes. Instead of fitting separate binary classifiers, it predicts the probability for all classes simultaneously using the <strong>softmax function</strong>.</p><h4 id="Softmax-Function">Softmax Function:</h4><p>$$<br>P(y = k | x) = \frac{e^{X \beta_k}}{\sum_{j=1}^K e^{X \beta_j}}<br>$$<br>Where:</p><ul><li>$ K $: Total number of classes.</li><li>$ \beta_k $: Coefficients for class $ k $.</li><li>$ P(y = k | x) $: Probability of class $ k $ given the input $ x $.</li></ul><h4 id="Prediction-v2">Prediction:</h4><ul><li>For a new data point, the class with the highest softmax probability is chosen:<br>$$<br>\hat{y} = \arg\max_{k} P(y = k | x)<br>$$</li></ul><h3 id="Summary-of-Methods">Summary of Methods:</h3><table><thead><tr><th><strong>Method</strong></th><th><strong>When to Use</strong></th><th><strong>Advantages</strong></th><th><strong>Disadvantages</strong></th></tr></thead><tbody><tr><td><strong>One-vs-Rest</strong></td><td>Small datasets with a limited number of classes.</td><td>Easy to implement, interpretable.</td><td>Can struggle with overlapping classes.</td></tr><tr><td><strong>Softmax</strong></td><td>When normalized probabilities across classes are needed.</td><td>Probabilities are calibrated.</td><td>Computationally expensive.</td></tr></tbody></table><p><strong>Softmax approach</strong> (also called multinomial logistic regression)</p><ol><li><p><strong>C-Class Classification</strong>:</p><ul><li>The goal is to classify the target variable $ y $ into one of $ C $ classes:<br>$$<br>y \in {0, 1, \dots, C-1}<br>$$</li></ul></li><li><p><strong>Discrete Probability Distribution</strong>:</p><ul><li>The probabilities $ \theta_0, \theta_1, \dots, \theta_{C-1} $ represent the likelihood of a data point belonging to each class.</li><li>These probabilities satisfy:<br>$$<br>\theta_i \in [0, 1] \quad \text{and} \quad \sum_{i=0}^{C-1} \theta_i = 1<br>$$</li></ul></li><li><p><strong>Link Function</strong>:</p><ul><li>The relationship between the linear model ($ X\beta $) and the class probabilities is established using the <strong>Softmax function</strong>:<br>$$<br>g(\theta) = \log \left( \frac{\theta_i}{1 - \sum_{u=0}^{C-1} \theta_u} \right) = X^T \beta<br>$$</li></ul></li><li><p><strong>Class Probabilities</strong>:</p><ul><li>For each class $ i $, the probability is computed as:<br>$$<br>P(y = i | X, \beta) = \frac{e^ {X^ T \beta_i}}{1 + \sum_{j=0}^ {C-1} e^ {X^ T \beta_j}}<br>$$</li><li>For the last class $ C-1 $, the probability is:<br>$$<br>P(y = C-1 | X, \beta) = \frac{1}{1 + \sum_{i=0}^{C-2} e^ {X^T \beta_i}}<br>$$</li></ul></li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">AI: Logistic Regression</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Linear Model Optimization</title>
    <link href="https://karobben.github.io/2024/12/30/AI/linearoptimal/"/>
    <id>https://karobben.github.io/2024/12/30/AI/linearoptimal/</id>
    <published>2024-12-30T20:59:30.000Z</published>
    <updated>2024-12-31T03:33:25.115Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Measure-Information">Measure Information</h2><p>Both Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are measures used to evaluate the quality of statistical models, particularly in the context of selecting the best model size or complexity.</p><ol><li><p><strong>Why Use AIC and BIC?</strong></p><ul><li>When building statistical or machine learning models, we often face the challenge of balancing <strong>model fit</strong> (how well the model explains the data) with <strong>model simplicity</strong> (avoiding overfitting).</li><li>AIC and BIC are metrics that help in selecting the best model by incorporating penalties for the number of parameters used in the model.</li></ul></li><li><p><strong>Akaike Information Criterion (AIC):</strong></p><ul><li>AIC estimates the relative quality of a model for a given dataset.</li><li>Formula:<br>$$<br>\text{AIC} = 2k - 2 \ln(L)<br>$$<ul><li>$ k $: Number of parameters in the model.</li><li>$ L $: Likelihood of the model (how well it fits the data).</li></ul></li><li><strong>Objective</strong>: Choose the model with the <strong>lowest AIC value</strong>, which balances fit and complexity.</li></ul></li><li><p><strong>Bayesian Information Criterion (BIC):</strong></p><ul><li>Similar to AIC, BIC adds a stronger penalty for model complexity to account for overfitting.</li><li>Formula:<br>$$<br>\text{BIC} = k \ln(n) - 2 \ln(L)<br>$$<ul><li>$ n $: Number of observations in the dataset.</li><li>$ k $: Number of parameters in the model.</li></ul></li><li><strong>Objective</strong>: Choose the model with the <strong>lowest BIC value</strong> for a balance between fit and simplicity, especially when sample size $ n $ is large.</li></ul></li><li><p><strong>Key Difference Between AIC and BIC:</strong></p><ul><li>AIC focuses on model quality and is less strict about model size.</li><li>BIC penalizes complexity more heavily, making it more conservative in selecting simpler models.</li></ul></li></ol><p><strong>Applications</strong>:</p><ul><li>Model selection in regression, time-series analysis, and machine learning.</li><li>Comparing models with different numbers of features or parameters.</li><li>Evaluating trade-offs between underfitting and overfitting.</li></ul><p>Would you like a detailed example or visual demonstration of how AIC and BIC are used?</p><table><thead><tr><th><strong>Criterion</strong></th><th><strong>Formula</strong></th><th><strong>Focus</strong></th><th><strong>Penalty for Complexity</strong></th><th><strong>Use Case</strong></th><th><strong>Objective</strong></th></tr></thead><tbody><tr><td><strong>Akaike Information Criterion (AIC)</strong></td><td>$ 2k - 2\ln(L) $</td><td>Model fit vs. simplicity</td><td>Proportional to $ k $</td><td>Choosing models that balance goodness-of-fit and simplicity</td><td>Minimize AIC</td></tr><tr><td><strong>Bayesian Information Criterion (BIC)</strong></td><td>$ k\ln(n) - 2\ln(L) $</td><td>Model fit vs. parsimony</td><td>Stronger penalty with $ \ln(n) $</td><td>Suitable for large datasets and emphasizing simpler models</td><td>Minimize BIC</td></tr><tr><td><strong>Penalty Strength</strong></td><td>Moderate</td><td>High</td><td><strong>Depends on Sample Size ($ n $)</strong></td><td>Larger datasets lead to stricter penalties in BIC</td><td></td></tr><tr><td><strong>Common Application</strong></td><td>Time-series, regression, machine learning</td><td>Model selection across varying complexity</td><td>Multi-model comparison</td><td>Best when balancing underfitting and overfitting</td><td></td></tr></tbody></table><ol><li><p><strong>AIC</strong>:</p><ul><li>Prefers models with a better balance between complexity and fit.</li><li>Less conservative than BIC, suitable for small datasets or exploratory analysis.</li></ul></li><li><p><strong>BIC</strong>:</p><ul><li>Stronger emphasis on simplicity.</li><li>More appropriate for larger datasets or when avoiding overfitting is crucial.</li></ul></li><li><p><strong>Choosing Between AIC and BIC</strong>:</p><ul><li>Use <strong>AIC</strong> if you prioritize model quality over strict simplicity.</li><li>Use <strong>BIC</strong> if simplicity and generalization are more important.</li></ul></li></ol><h3 id="Likelihood">Likelihood</h3><p>When calculating AIC or BIC, the likelihood refers to <strong>how well the model trained on the training data</strong> explains the same training data. The likelihood is not calculated on the test data, as AIC and BIC are measures of model quality on the training dataset itself.</p><h3 id="Likelihood-in-AIC-BIC-Context">Likelihood in AIC/BIC Context:</h3><ol><li><strong>Training Data</strong>:<ul><li>We use the model parameters (e.g., coefficients in regression) estimated from the training data to calculate the likelihood of the training data.</li></ul></li><li><strong>Likelihood Calculation</strong>:<ul><li>For a model trained on the training data, the likelihood is the probability (or density) of the observed training data under the model:<br>$$<br>L(\theta | \text{Training Data}) = \prod_{i=1}^n f(y_i | \theta)<br>$$<br>Where:<ul><li>$ y_i $: Observed target value.</li><li>$ \theta $: Model parameters estimated during training.</li><li>$ f(y_i | \theta) $: Probability density of $ y_i $ under the model.</li></ul></li></ul></li><li><strong>Log-Likelihood for AIC/BIC</strong>:<ul><li>Instead of working with $ L $, we calculate the <strong>log-likelihood</strong> to simplify computations:<br>$$<br>\ln L(\theta | \text{Training Data}) = \sum_{i=1}^n \ln f(y_i | \theta)<br>$$</li></ul></li></ol><h3 id="Steps-to-Calculate-Likelihood-for-AIC-BIC">Steps to Calculate Likelihood for AIC/BIC:</h3><ol><li>Train the Model:<ul><li>Use the training data to estimate the model parameters ($ \theta $).</li></ul></li><li>Calculate Predictions ($ \hat{y}_i $):<ul><li>Predict the mean or central tendency of the model for each training data point.</li></ul></li><li>Calculate Residuals and Likelihood:<ul><li>Assume a distribution for the residuals (e.g., normal distribution).</li><li>For a normal distribution:<br>$$<br>f(y_i | \hat{y}_i, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \hat{y}_i)^ 2}{2\sigma^ 2}\right)<br>$$</li><li>The log-likelihood becomes:<br>$$<br>\ln L = \sum_{i=1}^n \left[ -\frac{1}{2} \ln(2\pi\sigma^2) - \frac{(y_i - \hat{y}_i)^ 2}{2\sigma^ 2} \right]<br>$$</li></ul></li></ol><p>$\sigma$ represents the <strong>standard deviation of the residuals</strong></p><h3 id="Example-Using-Training-Data-to-Calculate-Likelihood">Example: Using Training Data to Calculate Likelihood</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Simulated training data</span><br>X_train = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>y_train = np.array([<span class="hljs-number">1.2</span>, <span class="hljs-number">2.3</span>, <span class="hljs-number">2.8</span>, <span class="hljs-number">4.1</span>, <span class="hljs-number">5.3</span>])<br><br><span class="hljs-comment"># Train a linear regression model</span><br>model = LinearRegression()<br>model.fit(X_train, y_train)<br>y_pred_train = model.predict(X_train)<br><br><span class="hljs-comment"># Calculate residuals and variance</span><br>residuals = y_train - y_pred_train<br>sigma_squared = np.var(residuals, ddof=<span class="hljs-number">1</span>)  <span class="hljs-comment"># Variance of residuals</span><br><br><span class="hljs-comment"># Calculate log-likelihood</span><br>n = <span class="hljs-built_in">len</span>(y_train)<br>log_likelihood = -<span class="hljs-number">0.5</span> * n * np.log(<span class="hljs-number">2</span> * np.pi * sigma_squared) - np.<span class="hljs-built_in">sum</span>((residuals**<span class="hljs-number">2</span>) / (<span class="hljs-number">2</span> * sigma_squared))<br><br><span class="hljs-comment"># AIC and BIC</span><br>k = <span class="hljs-number">2</span>  <span class="hljs-comment"># Number of parameters (intercept + slope)</span><br>aic = <span class="hljs-number">2</span> * k - <span class="hljs-number">2</span> * log_likelihood<br>bic = k * np.log(n) - <span class="hljs-number">2</span> * log_likelihood<br><br>&#123;<span class="hljs-string">&quot;Log-Likelihood&quot;</span>: log_likelihood, <span class="hljs-string">&quot;AIC&quot;</span>: aic, <span class="hljs-string">&quot;BIC&quot;</span>: bic&#125;<br></code></pre></td></tr></table></figure></div><h3 id="Example">Example</h3><p>Source: <a href="https://www.youtube.com/watch?v=HOqHI53x9Go">Model selection with AIC and AICc</a><br><img src="https://imgur.com/K4j8QGS.png" alt=""></p><h2 id="Forward-stagewise-regression-and-Backward-stagewise-regression">Forward stagewise regression and Backward stagewise regression</h2><p><strong>Backward stagewise regression</strong> and <strong>Forward stagewise regression</strong> are methods for variable selection and model fitting, primarily used in regression contexts. They are stepwise procedures for adding or removing predictors in a systematic way to improve model performance or interpretability.</p><h3 id="Backward-Stagewise-Regression"><strong>Backward Stagewise Regression</strong></h3><h4 id="Overview">Overview:</h4><ul><li>Starts with a <strong>full model</strong> (all predictors included).</li><li>Gradually <strong>removes predictors</strong> one by one, based on a criterion (e.g., p-value, AIC, or adjusted $ R^2 $).</li><li>The goal is to find a smaller, simpler model without significantly compromising the fit.</li></ul><h4 id="Procedure">Procedure:</h4><ol><li>Begin with a model containing all predictors.</li><li>Evaluate the significance of each predictor (e.g., using p-values).</li><li>Remove the <strong>least significant predictor</strong> (highest p-value) that exceeds a predefined</li></ol><p>threshold (e.g., $p &gt; 0.05$).</p><ol start="4"><li>Refit the model and repeat the process until all remaining predictors are statistically significant or meet the stopping criteria.</li></ol><h4 id="Advantages">Advantages:</h4><ul><li>Simple and interpretable.</li><li>Useful for removing irrelevant predictors in high-dimensional datasets.</li></ul><h4 id="Disadvantages">Disadvantages:</h4><ul><li>Can miss optimal combinations of predictors.</li><li>Sensitive to multicollinearity among predictors.</li></ul><h3 id="2-Forward-Stagewise-Regression">2. <strong>Forward Stagewise Regression</strong></h3><h4 id="Overview-v2">Overview:</h4><ul><li>Starts with an <strong>empty model</strong> (no predictors included).</li><li>Gradually <strong>adds predictors</strong> one at a time, based on a criterion (e.g., reducing residual sum of squares or improving AIC/BIC).</li><li>The goal is to build a model step-by-step, adding only significant predictors.</li></ul><h4 id="Procedure-v2">Procedure:</h4><ol><li>Begin with an empty model.</li><li>Evaluate all predictors not yet in the model, adding the one that most improves the model fit (e.g., the one with the smallest p-value or largest improvement in $ R^2 $).</li><li>Refit the model and repeat the process until no additional predictors meet the inclusion criteria.</li></ol><h4 id="Advantages-v2">Advantages:</h4><ul><li>Can handle datasets with a large number of predictors.</li><li>Less likely to overfit compared to starting with a full model.</li></ul><h4 id="Disadvantages-v2">Disadvantages:</h4><ul><li>Ignores potential joint effects of predictors (e.g., interactions).</li><li>May miss the best subset of predictors.</li></ul><h3 id="Key-Differences-Between-Backward-and-Forward-Stagewise-Regression">Key Differences Between Backward and Forward Stagewise Regression</h3><table><thead><tr><th>Feature</th><th>Backward Stagewise</th><th>Forward Stagewise</th></tr></thead><tbody><tr><td><strong>Starting Point</strong></td><td>Full model (all predictors).</td><td>Empty model (no predictors).</td></tr><tr><td><strong>Procedure</strong></td><td>Removes predictors iteratively.</td><td>Adds predictors iteratively.</td></tr><tr><td><strong>Use Case</strong></td><td>Small datasets with fewer predictors.</td><td>Large datasets with many predictors.</td></tr><tr><td><strong>Limitations</strong></td><td>May retain redundant predictors.</td><td>May miss joint effects of predictors.</td></tr></tbody></table><h3 id="When-to-Use-Each-Method">When to Use Each Method?</h3><ul><li><p><strong>Backward Stagewise</strong>:</p><ul><li>When you suspect many predictors are irrelevant.</li><li>When computational resources are not a concern (since fitting starts with a large model).</li></ul></li><li><p><strong>Forward Stagewise</strong>:</p><ul><li>When you have a large number of predictors and computational efficiency is critical.</li><li>When you want a simpler starting point and add complexity gradually.</li></ul></li></ul><h3 id="A-Quick-Example">A Quick Example</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Re-import necessary libraries after environment reset</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_regression<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br><br><span class="hljs-comment"># Generate a dataset with 100 samples and 10 features</span><br>np.random.seed(<span class="hljs-number">42</span>)<br>X, y = make_regression(n_samples=<span class="hljs-number">100</span>, n_features=<span class="hljs-number">10</span>, noise=<span class="hljs-number">10</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># Initialize model and variables for Forward Stagewise Regression</span><br>selected_features = []<br>remaining_features = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]))<br>forward_scores = []<br><br><span class="hljs-comment"># Forward Stagewise Regression</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(remaining_features)):<br>    scores = []<br>    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> remaining_features:<br>        <span class="hljs-comment"># Fit a model with the current feature added</span><br>        features_to_test = selected_features + [feature]<br>        model = LinearRegression().fit(X[:, features_to_test], y)<br>        score = model.score(X[:, features_to_test], y)  <span class="hljs-comment"># R^2 score</span><br>        scores.append((score, feature))<br>    <br>    <span class="hljs-comment"># Select the feature with the highest R^2 score</span><br>    scores.sort(reverse=<span class="hljs-literal">True</span>)<br>    best_score, best_feature = scores[<span class="hljs-number">0</span>]<br>    forward_scores.append(best_score)<br>    selected_features.append(best_feature)<br>    remaining_features.remove(best_feature)<br><br><span class="hljs-comment"># Results of Forward Stagewise Regression</span><br>selected_features_forward = selected_features  <span class="hljs-comment"># Save selected features for clarity</span><br><br><span class="hljs-comment"># Backward Stagewise Regression</span><br>selected_features_backward = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]))<br>backward_scores = []<br><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(selected_features_backward) - <span class="hljs-number">1</span>):<br>    scores = []<br>    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> selected_features_backward:<br>        <span class="hljs-comment"># Fit a model with the current feature removed</span><br>        features_to_test = [f <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> selected_features_backward <span class="hljs-keyword">if</span> f != feature]<br>        model = LinearRegression().fit(X[:, features_to_test], y)<br>        score = model.score(X[:, features_to_test], y)  <span class="hljs-comment"># R^2 score</span><br>        scores.append((score, feature))<br>    <br>    <span class="hljs-comment"># Remove the feature with the smallest impact on R^2 score</span><br>    scores.sort(reverse=<span class="hljs-literal">True</span>)<br>    best_score, worst_feature = scores[-<span class="hljs-number">1</span>]<br>    backward_scores.append(best_score)<br>    selected_features_backward.remove(worst_feature)<br><br><span class="hljs-comment"># Plot R^2 scores for Forward and Backward Stagewise Regression</span><br>plt.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))<br><br><span class="hljs-comment"># Forward Stagewise Regression</span><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(forward_scores) + <span class="hljs-number">1</span>), forward_scores, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">&#x27;Forward Stagewise&#x27;</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>)<br><br><span class="hljs-comment"># Backward Stagewise Regression</span><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(backward_scores), <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>), backward_scores, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">&#x27;Backward Stagewise&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br><br><span class="hljs-comment"># Formatting the plot</span><br>plt.title(<span class="hljs-string">&quot;R² Scores During Forward and Backward Stagewise Regression&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Number of Features&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;R² Score&quot;</span>)<br>plt.xticks(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(forward_scores) + <span class="hljs-number">1</span>))<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.tight_layout()<br>plt.show()<br></code></pre></td></tr></table></figure></div><script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script><script type="text/javascript">  google.charts.load('current', { packages: ['corechart'] });  google.charts.setOnLoadCallback(drawChart);  function drawChart() {    var data = google.visualization.arrayToDataTable([          ['Number of Features', 'Forward Stagewise R²', 'Backward Stagewise R²'],          [1, 0.287, null],          [2, 0.511, null],          [3, 0.698, null],          [4, 0.801, null],          [5, 0.918, null],          [6, 0.988, null],          [7, 0.995, null],          [8, 0.997, null],          [9, 0.997, null],          [9, 0.997, 0.787],          [8, null, 0.643],          [7, null, 0.441],          [6, null, 0.285],          [5, null, 0.176],          [4, null, 0.076],          [3, null, 0.045],          [2, null, 0.011],          [1, null, 0.002],        ]);    var options = {      title: 'R² Scores During Forward and Backward Stagewise Regression',      hAxis: { title: 'Number of Features' },      vAxis: { title: 'R² Score', minValue: 0, maxValue: 1 },      legend: { position: 'top' },      colors: ['blue', 'red'],    };    var chart = new google.visualization.LineChart(document.getElementById('chart_div'));    chart.draw(data, options);  }</script><div id="chart_div" style="width: 100%  ; height: 300px"></div><h3 id="Limitations">Limitations</h3><p><strong>Forward and Backward Stagewise Regression</strong> can become computationally expensive and impractical when dealing with a <strong>large number of features (e.g., 1000+ features)</strong> because:</p><ol><li><strong>High Computational Cost</strong>:<ul><li>Both methods involve iteratively adding or removing features, which requires fitting a model at each step. For large datasets, this becomes infeasible.</li></ul></li><li><strong>Potential Overfitting</strong>:<ul><li>With a large number of features, stepwise methods might select features that fit noise in the data rather than actual patterns.</li></ul></li><li><strong>Ignoring Interactions</strong>:<ul><li>These methods do not account for interactions between features, which can lead to suboptimal feature selection.</li></ul></li></ol><p><strong>Alternative Methods for Large Feature Spaces</strong></p><table><thead><tr><th><strong>Method</strong></th><th><strong>Description</strong></th><th><strong>Advantages</strong></th><th><strong>Disadvantages</strong></th><th><strong>Best Use Case</strong></th></tr></thead><tbody><tr><td><strong>Lasso Regression (L1)</strong></td><td>Shrinks coefficients and sets some to exactly zero for feature selection.</td><td>- Efficient for high-dimensional data.<br>- Automatically selects features.<br>- Prevents overfitting.</td><td>- May ignore correlated features.<br>- Requires hyperparameter tuning ($ \lambda $).</td><td>When many features are irrelevant, and sparse solutions are desired.</td></tr><tr><td><strong>Elastic Net</strong></td><td>Combines L1 (Lasso) and L2 (Ridge) regularization.</td><td>- Balances feature selection and handling multicollinearity.<br>- Suitable for correlated features.</td><td>- More complex than Lasso.<br>- Requires tuning of both $ \lambda $ and $ \alpha $.</td><td>When predictors are highly correlated, and feature selection is needed.</td></tr><tr><td><strong>Recursive Feature Elimination (RFE)</strong></td><td>Iteratively removes the least important features based on a chosen model.</td><td>- Works with any estimator (e.g., linear, tree-based).<br>- Provides a rank of feature importance.</td><td>- Computationally expensive.<br>- Sensitive to model choice and training data.</td><td>When model-specific feature ranking is required.</td></tr><tr><td><strong>Principal Component Analysis (PCA)</strong></td><td>Reduces dimensionality by transforming features into uncorrelated components that capture most variance.</td><td>- Handles high-dimensional data well.<br>- Removes multicollinearity.<br>- No need for target variable.</td><td>- Components are linear combinations of features, losing interpretability.<br>- Not ideal for feature selection.</td><td>When reducing dimensionality is more important than interpretability.</td></tr><tr><td><strong>Tree-Based Feature Importance</strong></td><td>Uses models like Random Forest or Gradient Boosting to rank feature importance.</td><td>- Naturally handles non-linearity.<br>- Accounts for feature interactions.<br>- Fast for large datasets.</td><td>- Can be biased toward high-cardinality features.<br>- Does not directly reduce feature count.</td><td>When using tree-based models or ranking feature importance is a priority.</td></tr><tr><td><strong>Mutual Information</strong></td><td>Measures the statistical dependency between features and the target variable.</td><td>- Non-parametric.<br>- Detects non-linear relationships.</td><td>- Computationally expensive for many features.<br>- Does not handle feature interactions.</td><td>When quantifying feature relevance to the target variable without assumptions is needed.</td></tr><tr><td><strong>Feature Clustering</strong></td><td>Groups similar features into clusters and uses cluster representatives for modeling.</td><td>- Reduces redundancy in correlated features.<br>- Scales well with high-dimensional data.</td><td>- May lose specific feature contributions.<br>- Requires a meaningful distance metric.</td><td>When dealing with highly correlated features or datasets with groups of similar features.</td></tr><tr><td><strong>Embedding-Based Methods</strong></td><td>Uses deep learning or models like word2vec to transform features into a lower-dimensional space.</td><td>- Captures complex relationships between features.<br>- Flexible for large feature spaces.</td><td>- Requires advanced techniques and computational resources.<br>- May lose interpretability.</td><td>When handling very high-dimensional data (e.g., text, genomic data) with complex dependencies.</td></tr></tbody></table><h4 id="Recommendations">Recommendations:</h4><ul><li><strong>Lasso Regression</strong>: If feature selection is the goal and the data has many irrelevant features.</li><li><strong>Elastic Net</strong>: If features are highly correlated and Lasso alone may struggle.</li><li><strong>PCA</strong>: When interpretability is less important, and you want to reduce dimensionality.</li><li><strong>Tree-Based Importance</strong>: For datasets where feature importance ranking is needed, especially with tree-based models.</li><li><strong>Feature Clustering</strong>: For correlated features where redundancy needs to be reduced.</li></ul><h2 id="M-Estimators">M-Estimators</h2><p><strong>M-Estimators</strong> (Maximum Likelihood-type Estimators) are a general class of estimators in statistics used for robust parameter estimation. They extend the principle of Maximum Likelihood Estimation (MLE) to allow for more flexibility and robustness, especially in the presence of outliers or non-normal errors.</p><h3 id="What-Are-M-Estimators">What Are M-Estimators?</h3><ol><li><p><strong>Definition</strong>:</p><ul><li>M-Estimators generalize Maximum Likelihood Estimators by minimizing a <strong>loss function</strong> (also called the objective function) over the parameters of interest.</li></ul></li><li><p><strong>Loss Function</strong>:</p><ul><li>The core idea is to minimize a function of residuals:<br>$$<br>\hat{\theta} = \arg\min_{\theta} \sum_{i=1}^n \rho\left(\frac{r_i}{\sigma}\right)<br>$$<br>Where:<ul><li>$ r_i = y_i - f(x_i, \theta) $: Residual (difference between observed and predicted values).</li><li>$ \rho(\cdot) $: A loss function that determines the contribution of residuals.</li><li>$ \sigma $: Scale parameter (controls the spread).</li></ul></li></ul></li><li><p><strong>Goal</strong>:</p><ul><li>Instead of focusing purely on minimizing squared residuals (like in Ordinary Least Squares), M-Estimators allow for more flexible functions to make the estimator <strong>less sensitive to outliers</strong>.</li></ul></li></ol><h3 id="Examples-of-M-Estimators">Examples of M-Estimators</h3><table><thead><tr><th><strong>Type</strong></th><th><strong>Loss Function ($ \rho $)</strong></th><th><strong>Characteristics</strong></th></tr></thead><tbody><tr><td><strong>Ordinary Least Squares (OLS)</strong></td><td>$ \rho( r ) = r^2 $</td><td>Highly sensitive to outliers. Minimizes sum of squared errors.</td></tr><tr><td><strong>Huber Loss</strong></td><td>$\rho( r) = \begin{cases} r^2 &amp; \text{if } |r| \leq c \\ 2c|r| - c^2 &amp; \text{if } |r| &gt; c\end{cases}$</td><td>Combines squared loss (for small residuals) and absolute loss (for large residuals).</td></tr><tr><td><strong>Tukey’s Biweight</strong></td><td>$\rho( r ) = \begin{cases}  c^2\left(1 - \left[1 - \left(\frac{r}{c}\right)^ 2\right]^ 3\right) &amp; \text{if } |r| \leq c \\ c^2 &amp; \text{if } |r| &gt; c \end{cases}$</td><td>Completely ignores residuals larger than a threshold $ c $.</td></tr><tr><td><strong>Huberized Absolute Loss</strong></td><td>$\rho( r) = |r|$</td><td>Linear penalty, robust but less efficient.</td></tr></tbody></table><h3 id="Advantages-of-M-Estimators">Advantages of M-Estimators</h3><ol><li><strong>Robustness to Outliers</strong></li><li><strong>Flexibility</strong></li><li><strong>Generalization of MLE</strong>:<ul><li>MLE is a special case of M-Estimators, making them widely applicable in parametric settings.</li></ul></li></ol><h3 id="When-to-Use-M-Estimators">When to Use M-Estimators?</h3><ol><li><strong>Presence of Outliers</strong>:</li><li><strong>Non-Normal Errors</strong>:</li><li><strong>Heavy-Tailed Distributions</strong>:</li></ol><h3 id="Practical-Example-Using-Huber-Loss">Practical Example: Using Huber Loss</h3><p>Below is an example of applying <strong>Huber Loss</strong> to regression in Python:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> HuberRegressor<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># Simulate data with outliers</span><br>np.random.seed(<span class="hljs-number">42</span>)<br>X = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>y = <span class="hljs-number">3</span> * X.flatten() + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size=X.shape[<span class="hljs-number">0</span>])<br>y[::<span class="hljs-number">10</span>] += <span class="hljs-number">20</span>  <span class="hljs-comment"># Add outliers every 10th point</span><br><br><span class="hljs-comment"># Fit Ordinary Least Squares (OLS) Regression</span><br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br>ols = LinearRegression().fit(X, y)<br><br><span class="hljs-comment"># Fit Huber Regression</span><br>huber = HuberRegressor(epsilon=<span class="hljs-number">1.35</span>).fit(X, y)<br><br><span class="hljs-comment"># Plot the results</span><br>plt.scatter(X, y, color=<span class="hljs-string">&quot;blue&quot;</span>, label=<span class="hljs-string">&quot;Data with Outliers&quot;</span>)<br>plt.plot(X, ols.predict(X), color=<span class="hljs-string">&quot;red&quot;</span>, label=<span class="hljs-string">&quot;OLS Regression Line&quot;</span>)<br>plt.plot(X, huber.predict(X), color=<span class="hljs-string">&quot;green&quot;</span>, label=<span class="hljs-string">&quot;Huber Regression Line&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;Comparison of OLS and Huber Regression&quot;</span>)<br>plt.legend()<br>plt.xlabel(<span class="hljs-string">&quot;X&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;y&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div><script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script><script type="text/javascript">  google.charts.load('current', { packages: ['corechart'] });  google.charts.setOnLoadCallback(drawChart);  function drawChart() {    // Data for OLS and Huber Regression    var data = google.visualization.arrayToDataTable([['X', 'Observed Data', 'OLS Regression', 'Huber Regression'], [0.0, 20.496714153011233, 2.3618406112691552, -0.05895399071098967], [0.10101010101010101, 0.16476600185911838, 2.6554630935183825, 0.24599617373579663], [0.20202020202020202, 1.2537491441612985, 2.9490855757676098, 0.550946338182583], [0.30303030303030304, 2.4321207654989347, 3.242708058016837, 0.8558965026293694], [0.40404040404040403, 0.9779678373978762, 3.5363305402660643, 1.1608466670761555], [0.5050505050505051, 1.2810145582023347, 3.8299530225152916, 1.465796831522942], [0.6060606060606061, 3.39739463368921, 4.123575504764519, 1.7707469959697284], [0.7070707070707071, 2.88864685036503, 4.417197987013745, 2.0756971604165146], [0.8080808080808081, 1.9547680383074721, 4.710820469262973, 2.380647324863301], [0.9090909090909091, 3.2698327708586916, 5.0044429515122, 2.6855974893100876], [1.0101010101010102, 22.566885337490568, 5.298065433761428, 2.990547653756874], [1.1111111111111112, 2.8676035797630766, 5.591687916010654, 3.29549781820366], [1.2121212121212122, 3.878325907929671, 5.8853103982598824, 3.6004479826504467], [1.3131313131313131, 2.0261136947361416, 6.178932880509109, 3.905398147097233], [1.4141414141414141, 2.5175064099112094, 6.472555362758336, 4.210348311544019], [1.5151515151515151, 3.983167016213572, 6.766177845007563, 4.5152984759908055], [1.6161616161616161, 3.835653728150425, 7.059800327256791, 4.820248640437591], [1.7171717171717171, 5.465762484110425, 7.353422809506018, 5.125198804884378], [1.8181818181818181, 4.546521379024243, 7.647045291755245, 5.430148969331165], [1.9191919191919191, 4.345272056240466, 7.940667774004472, 5.73509913377795], [2.0202020202020203, 27.526254829527616, 8.2342902562537, 6.040049298224737], [2.121212121212121, 6.1378600631498275, 8.527912738502927, 6.344999462671523], [2.2222222222222223, 6.734194871354591, 8.821535220752153, 6.64994962711831], [2.323232323232323, 5.5449487834835125, 9.11515770300138, 6.954899791565095], [2.4242424242424243, 6.728344548202091, 9.40878018525061, 7.259849956011883], [2.525252525252525, 7.6866801654674415, 9.702402667499836, 7.564800120458668], [2.6262626262626263, 6.727794301365576, 9.996025149749062, 7.8697502849054555], [2.727272727272727, 8.557516200163853, 10.289647631998289, 8.17470044935224], [2.8282828282828283, 7.88420979492968, 10.583270114247517, 8.479650613799027], [2.929292929292929, 8.49618503808551, 10.876892596496743, 8.784600778245814], [3.0303030303030303, 28.489202478679694, 11.170515078745971, 9.0895509426926], [3.131313131313131, 11.24621757844833, 11.464137560995198, 9.394501107139385], [3.2323232323232323, 9.683472472231763, 11.757760043244426, 9.699451271586172], [3.3333333333333335, 8.9422890710441, 12.051382525493654, 10.00440143603296], [3.4343434343434343, 11.125575215133491, 12.34500500774288, 10.309351600479745], [3.5353535353535355, 9.385216956089582, 12.638627489992109, 10.614301764926532], [3.6363636363636362, 11.117954504095664, 12.932249972241335, 10.919251929373319], [3.7373737373737375, 9.252451088241438, 13.225872454490563, 11.224202093820105], [3.8383838383838382, 10.186965466253085, 13.51949493673979, 11.52915225826689], [3.9393939393939394, 12.015043054050942, 13.813117418989018, 11.834102422713677], [4.040404040404041, 32.85967870120753, 14.106739901238244, 12.139052587160464], [4.141414141414141, 12.595610705432392, 14.40036238348747, 12.444002751607249], [4.242424242424242, 12.611624444884486, 14.693984865736699, 12.748952916054035], [4.343434343434343, 12.729199334713742, 14.987607347985925, 13.053903080500822], [4.444444444444445, 11.854811342965906, 15.281229830235153, 13.358853244947609], [4.545454545454545, 12.916519427968927, 15.57485231248438, 13.663803409394394], [4.646464646464646, 13.47875516843415, 15.868474794733606, 13.96875357384118], [4.747474747474747, 15.299546468643157, 16.162097276982834, 14.273703738287967], [4.848484848484849, 14.889072835023008, 16.45571975923206, 14.578653902734755], [4.94949494949495, 13.085444693122115, 16.74934224148129, 14.883604067181542], [5.05050505050505, 35.47559912090995, 17.042964723730513, 15.188554231628325], [5.151515151515151, 15.069463174129137, 17.336587205979743, 15.493504396075112], [5.252525252525253, 15.080653757269799, 17.630209688228973, 15.7984545605219], [5.353535353535354, 16.67228234944693, 17.9238321704782, 16.103404724968687], [5.454545454545454, 17.394635886132313, 18.217454652727426, 16.40835488941547], [5.555555555555555, 17.597946785782863, 18.511077134976652, 16.713305053862257], [5.656565656565657, 16.13047944647433, 18.80469961722588, 17.018255218309044], [5.757575757575758, 16.96351489687606, 19.09832209947511, 17.32320538275583], [5.858585858585858, 17.907021007161138, 19.39194458172433, 17.628155547202617], [5.959595959595959, 18.854333005910238, 19.68556706397356, 17.933105711649404], [6.0606060606060606, 37.70264394397289, 19.979189546222788, 18.23805587609619], [6.161616161616162, 18.299189508184668, 20.272812028472018, 18.543006040542977], [6.262626262626262, 17.681543813872757, 20.56643451072124, 18.84795620498976], [6.363636363636363, 17.89470246682842, 20.86005699297047, 19.152906369436547], [6.4646464646464645, 20.20646521633359, 21.153679475219697, 19.457856533883334], [6.565656565656566, 21.05320972554052, 21.447301957468927, 19.762806698330124], [6.666666666666667, 19.927989878419666, 21.740924439718153, 20.06775686277691], [6.767676767676767, 21.306563200922326, 22.03454692196738, 20.372707027223694], [6.8686868686868685, 20.96769663110824, 22.328169404216606, 20.67765719167048], [6.96969696969697, 20.263971154485787, 22.621791886465832, 20.982607356117267], [7.070707070707071, 41.573516817629624, 22.915414368715062, 21.287557520564054], [7.171717171717171, 23.053188081617485, 23.209036850964285, 21.592507685010837], [7.2727272727272725, 21.782355779071864, 23.502659333213515, 21.897457849457627], [7.373737373737374, 23.685855777026127, 23.79628181546274, 22.202408013904414], [7.474747474747475, 19.80449732015268, 24.08990429771197, 22.5073581783512], [7.575757575757575, 23.54917523164795, 24.383526779961194, 22.812308342797984], [7.6767676767676765, 23.117350098541202, 24.677149262210424, 23.11725850724477], [7.777777777777778, 23.034325982867465, 24.97077174445965, 23.422208671691557], [7.878787878787879, 23.728124412899138, 25.26439422670888, 23.727158836138344], [7.979797979797979, 21.951825024793045, 25.558016708958103, 24.03210900058513], [8.080808080808081, 44.02275235458673, 25.851639191207333, 24.337059165031917], [8.181818181818182, 24.902567116966292, 26.14526167345656, 24.642009329478704], [8.282828282828282, 26.32637889322636, 26.438884155705786, 24.946959493925487], [8.383838383838384, 24.633244933241507, 26.732506637955016, 25.251909658372277], [8.484848484848484, 24.646051851652267, 27.026129120204242, 25.55685982281906], [8.585858585858587, 25.25581871399122, 27.319751602453472, 25.86180998726585], [8.686868686868687, 26.976008178308135, 27.613374084702695, 26.166760151712634], [8.787878787878787, 26.692387473296044, 27.90699656695192, 26.47171031615942], [8.88888888888889, 26.136906462899628, 28.20061904920115, 26.776660480606207], [8.98989898989899, 27.482964402810325, 28.494241531450378, 27.081610645052994], [9.09090909090909, 47.36980482207531, 28.787864013699604, 27.386560809499777], [9.191919191919192, 28.54440256629047, 29.081486495948834, 27.691510973946567], [9.292929292929292, 27.176734784910522, 29.375108978198057, 27.99646113839335], [9.393939393939394, 27.854156035220416, 29.66873146044729, 28.30141130284014], [9.494949494949495, 28.09274033171633, 29.962353942696513, 28.606361467286924], [9.595959595959595, 27.324363839746667, 30.25597642494574, 28.91131163173371], [9.696969696969697, 29.38702936797367, 30.54959890719497, 29.2162617961805], [9.797979797979798, 29.65499466611928, 30.843221389444196, 29.521211960627284], [9.8989898989899, 29.70208315361216, 31.136843871693426, 29.826162125074074], [10.0, 29.765412866624853, 31.430466353942652, 30.131112289520857]]);    // Chart options    var options = {      title: 'OLS vs Huber Regression',      hAxis: { title: 'X', minValue: 0 },      vAxis: { title: 'y', minValue: 0 },      pointSize: 2,      legend: { position: 'right' },      series: {        0: { color: 'black', pointShape: 'circle' }, // Observed data points        1: { color: 'red', lineWidth: 2, pointSize: 0 },          // OLS Regression line        2: { color: 'green', lineWidth: 2, pointSize: 0 },        // Huber Regression line      },    };    // Render the chart    var chart = new google.visualization.ScatterChart(document.getElementById('chart_div2'));    chart.draw(data, options);  }</script><div id="chart_div2" style="width: 100%; height: 400px;"></div><hr><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Linear Model Optimization</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Regularization</title>
    <link href="https://karobben.github.io/2024/12/30/AI/regularization/"/>
    <id>https://karobben.github.io/2024/12/30/AI/regularization/</id>
    <published>2024-12-30T16:50:04.000Z</published>
    <updated>2024-12-30T18:18:52.803Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Quick-View">Quick View</h2><p><strong>Video Tutorial</strong>:</p><ul><li><a href="https://www.youtube.com/watch?v=Q81RR3yKn30">StatQuest with Josh Starmer: Regularization Part 1: Ridge (L2) Regression</a></li><li><a href="https://www.youtube.com/watch?v=NGf0voTMlcs">StatQuest with Josh Starmer: Regularization Part 2: Lasso (L1) Regression</a></li><li><a href="https://www.youtube.com/watch?v=1dKRdX9bfIo">StatQuest with Josh Starmer: Regularization Part 3: Elastic Net Regression</a></li></ul><h3 id="What-is-Regularization">What is Regularization?</h3><p><strong>Regularization</strong> is a technique used in machine learning and regression to prevent <strong>overfitting</strong> by adding a penalty to the loss function. The penalty discourages overly complex models and large coefficients, helping the model generalize better to unseen data.</p><h3 id="Why-Do-We-Need-Regularization">Why Do We Need Regularization?</h3><ol><li><p><strong>Overfitting</strong>:</p><ul><li>When a model becomes too complex, it memorizes the training data, leading to poor performance on test data.</li><li>Example: In polynomial regression, high-degree polynomials might perfectly fit the training data but fail to generalize.</li></ul></li><li><p><strong>Ill-Conditioned Data</strong>:</p><ul><li>When predictors are highly correlated or there are many predictors relative to observations, the regression model can become unstable.</li></ul></li><li><p><strong>Bias-Variance Tradeoff</strong>:</p><ul><li>Regularization introduces some bias but reduces variance, improving the model’s robustness.</li></ul></li></ol><h3 id="Types-of-Regularization-Why-Ridge-Lasso-and-Elastic-Net">Types of Regularization: Why Ridge, Lasso, and Elastic Net?</h3><p>These are three popular regularization methods used for linear regression:</p><h4 id="1-Ridge-Regression-L2-Regularization">1. <strong>Ridge Regression (L2 Regularization)</strong>:</h4><ul><li><p><strong>Penalty</strong>: Adds the squared magnitude of coefficients to the loss function.<br>$$<br>\text{Loss Function: } \sum_{i=1}^n (y_i - \hat{y}_ i)^2 + \lambda \sum_ {j=1}^p \beta_ j^2<br>$$</p><ul><li>$ \lambda $: Regularization parameter (controls penalty strength).</li><li>$ \beta_j $: Coefficients of predictors.</li></ul></li><li><p><strong>Effect</strong>:</p><ul><li>Shrinks coefficients towards zero, but never makes them exactly zero.</li><li>Reduces the impact of less important predictors without removing them entirely.</li></ul></li><li><p><strong>Use Case</strong>:</p><ul><li>Works well when many predictors are correlated.</li></ul></li></ul><h4 id="2-Lasso-Regression-L1-Regularization">2. <strong>Lasso Regression (L1 Regularization)</strong>:</h4><ul><li><p><strong>Penalty</strong>: Adds the absolute value of coefficients to the loss function.<br>$$<br>\text{Loss Function: } \sum_ {i=1}^n (y_ i - \hat{y}_ i)^2 + \lambda \sum_ {j=1}^p |\beta_ j|<br>$$</p></li><li><p><strong>Effect</strong>:</p><ul><li>Can shrink some coefficients to exactly zero, effectively performing <strong>feature selection</strong>.</li><li>Helps in creating sparse models by keeping only the most relevant predictors.</li></ul></li><li><p><strong>Use Case</strong>:</p><ul><li>Useful when you expect only a subset of predictors to be important.</li></ul></li></ul><h4 id="3-Elastic-Net-Regression">3. <strong>Elastic Net Regression</strong>:</h4><ul><li><p><strong>Penalty</strong>: Combines both L1 (lasso) and L2 (ridge) penalties.<br>$$<br>\text{Loss Function: } \sum_{i=1}^n (y_ i - \hat{y}_ i)^2 + \lambda_ 1 \sum_{j=1}^p |\beta_ j| + \lambda_ 2 \sum_{j=1}^p \beta_ j^2<br>$$</p></li><li><p><strong>Effect</strong>:</p><ul><li>Balances the strengths of Ridge and Lasso regression.</li><li>Retains the ability to perform feature selection (like Lasso) while handling multicollinearity (like Ridge).</li></ul></li><li><p><strong>Use Case</strong>:</p><ul><li>Best when there are many predictors and some are correlated, but feature selection is also desired.</li></ul></li></ul><h3 id="Comparison-of-Regularization-Methods">Comparison of Regularization Methods:</h3><table><thead><tr><th><strong>Method</strong></th><th><strong>Penalty</strong></th><th><strong>Effect on Coefficients</strong></th><th><strong>Use Case</strong></th></tr></thead><tbody><tr><td><strong>Ridge</strong></td><td>$ \beta_j^2 $</td><td>Shrinks coefficients, no zeros.</td><td>Multicollinearity or many predictors.</td></tr><tr><td><strong>Lasso</strong></td><td>$|\beta_j|$</td><td>Shrinks coefficients to zero.</td><td>Feature selection with fewer predictors.</td></tr><tr><td><strong>Elastic Net</strong></td><td>$|\beta_j| + \beta_j^2 $</td><td>Combination of Ridge and Lasso.</td><td>Multicollinearity with feature selection.</td></tr></tbody></table><h3 id="Why-Are-They-Discussed-Together">Why Are They Discussed Together?</h3><ul><li>All three are <strong>extensions of linear regression</strong>.</li><li>They <strong>regularize the model</strong> to prevent overfitting, but they differ in the type of penalty they impose on the coefficients.</li></ul><h2 id="Ridge-Regression">Ridge Regression</h2><h3 id="Ridge-Regression-Loss-Function">Ridge Regression Loss Function</h3><p>The Ridge regression modifies the Ordinary Least Squares (OLS) cost function by adding a penalty (regularization term) to the sum of squared coefficients:</p><p>$$<br>\text{Loss} = \sum_ {i=1}^n \left( y_ i - \hat{y}_ i \right)^2 + \lambda \sum_{j=1}^p \beta_ j^2<br>$$</p><p>Where:</p><ul><li>$ y_i $: Observed target value.</li><li>$ \hat{y}_i $: Predicted value ($ \hat{y}_i = X_i \cdot \beta $).</li><li>$ \beta_j $: Coefficients of the regression model.</li><li>$ \lambda $: Regularization parameter (also called penalty parameter).</li></ul><h3 id="Ridge-Coefficient-Solution">Ridge Coefficient Solution</h3><p>The Ridge regression coefficients are obtained by solving the following optimization problem:</p><p>$$<br>\min_{\beta} \{ \|y - X\beta\|^2 + \lambda \|\beta\|^2  \}<br>$$</p><ol><li><p><strong>Matrix Form</strong>:</p><ul><li>Rewrite the problem in matrix notation:<br>$$<br>\min_{\beta} \{ (y - X\beta)^T (y - X\beta) + \lambda \beta^T \beta \}<br>$$</li></ul></li><li><p><strong>Solution for $ \beta $</strong>:</p><ul><li>Differentiating the loss function with respect to $ \beta $, we get:<br>$$<br>\beta = \left( X^T X + \lambda I \right)^{-1} X^T y<br>$$</li><li>Here:<ul><li>$ X^T X $: Correlation matrix of predictors.</li><li>$ \lambda I $: Regularization term, where $ I $ is the identity matrix.</li><li>$ \lambda $: Controls the trade-off between minimizing the squared error and penalizing large coefficients.</li></ul></li></ul></li></ol><h3 id="Why-Add-lambda-I">Why Add $ \lambda I $?</h3><ul><li>Inverse of $ X^T X $ might not exist if the predictors are highly correlated or there are fewer observations than predictors (multicollinearity).</li><li>Adding $ \lambda I $ ensures that $ X^T X + \lambda I $ is always invertible.</li></ul><h3 id="Finding-the-Optimal-lambda">Finding the Optimal $ \lambda $</h3><ol><li><p><strong>Grid Search with Cross-Validation</strong>:</p><ul><li>Evaluate the model’s performance (e.g., Mean Squared Error) for different values of $ \lambda $.</li><li>Use k-fold cross-validation to select the $ \lambda $ that minimizes validation error.</li></ul></li><li><p><strong>Mathematical Insight</strong>:</p><ul><li>When $ \lambda = 0 $: Ridge reduces to Ordinary Least Squares (OLS).</li><li>As $ \lambda \to \infty $: Coefficients $ \beta \to 0 $ (model becomes very simple).</li></ul></li><li><p><strong>Validation-Based Optimization</strong>:</p><ul><li>Define a range of $ \lambda $ values (e.g., $ \lambda = [0.001, 0.01, 0.1, 1, 10, 100] $).</li><li>For each $ \lambda $, perform cross-validation and select the value with the lowest error.</li></ul></li></ol><h3 id="Example-Finding-lambda-with-Cross-Validation">Example: Finding $ \lambda $ with Cross-Validation</h3><p>Here’s Python code to find the optimal $ \lambda $ using grid search:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score<br><br><span class="hljs-comment"># Define a range of lambda (alpha) values</span><br>alphas = np.logspace(-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">50</span>)  <span class="hljs-comment"># Lambda values from 0.001 to 1000</span><br><span class="hljs-comment"># Compute cross-validated MSE and standard deviation for each alpha</span><br>mse_values = []<br>std_errors = []<br><br><span class="hljs-keyword">for</span> alpha <span class="hljs-keyword">in</span> alphas:<br>    ridge = Ridge(alpha=alpha)<br>    scores = -cross_val_score(ridge, X_train, y_train, scoring=<span class="hljs-string">&quot;neg_mean_squared_error&quot;</span>, cv=<span class="hljs-number">5</span>)<br>    mse_values.append(scores.mean())<br>    std_errors.append(scores.std())<br><br><span class="hljs-comment"># Plot the results with error bars</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.errorbar(alphas, mse_values, yerr=std_errors, fmt=<span class="hljs-string">&#x27;o&#x27;</span>, linestyle=<span class="hljs-string">&#x27;-&#x27;</span>, label=<span class="hljs-string">&#x27;Cross-validated MSE&#x27;</span>, capsize=<span class="hljs-number">3</span>)<br>plt.xscale(<span class="hljs-string">&#x27;log&#x27;</span>)  <span class="hljs-comment"># Log scale for alpha</span><br>plt.xlabel(<span class="hljs-string">&quot;Lambda (α)&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Mean Squared Error (MSE)&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;Finding Optimal Lambda (α) for Ridge Regression with Error Bars&quot;</span>)<br>plt.axvline(alphas[np.argmin(mse_values)], color=<span class="hljs-string">&#x27;red&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">f&quot;Optimal λ = <span class="hljs-subst">&#123;alphas[np.argmin(mse_values)]:<span class="hljs-number">.3</span>f&#125;</span>&quot;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br><br><span class="hljs-comment"># Output optimal lambda</span><br>optimal_lambda_error_bar = alphas[np.argmin(mse_values)]<br>optimal_lambda_error_bar<br></code></pre></td></tr></table></figure></div><pre>0.21209508879201902</pre><p><img src="https://imgur.com/ir98xgb.png" alt=""></p><h3 id="Key-Insights">Key Insights</h3><ol><li><p><strong>Ridge Regression Purpose</strong>:</p><ul><li>Penalizes large coefficients to reduce model complexity and improve generalization.</li></ul></li><li><p><strong>Finding $ \lambda $</strong>:</p><ul><li>Perform grid search with cross-validation to select $ \lambda $ that minimizes validation error.</li></ul></li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Simulate noisier data</span><br>np.random.seed(<span class="hljs-number">42</span>)<br>n_samples = <span class="hljs-number">100</span><br>X = np.random.rand(n_samples, <span class="hljs-number">2</span>)  <span class="hljs-comment"># Two predictors</span><br>X[:, <span class="hljs-number">1</span>] = X[:, <span class="hljs-number">0</span>] + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, size=n_samples)  <span class="hljs-comment"># Add stronger multicollinearity</span><br>y = <span class="hljs-number">4</span> * X[:, <span class="hljs-number">0</span>] + <span class="hljs-number">3</span> * X[:, <span class="hljs-number">1</span>] + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, size=n_samples)  <span class="hljs-comment"># More noise in the data</span><br><br><span class="hljs-comment"># Randomly sample 20% for training</span><br>train_indices = np.random.choice(<span class="hljs-built_in">range</span>(n_samples), size=<span class="hljs-built_in">int</span>(<span class="hljs-number">0.2</span> * n_samples), replace=<span class="hljs-literal">False</span>)<br>test_indices = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples) <span class="hljs-keyword">if</span> i <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> train_indices]<br><br>X_train, y_train = X[train_indices], y[train_indices]<br>X_test, y_test = X[test_indices], y[test_indices]<br><br><span class="hljs-comment"># Ordinary Least Squares Regression</span><br>ols_model = LinearRegression()<br>ols_model.fit(X_train, y_train)<br>y_pred_ols = ols_model.predict(X_test)<br><br><span class="hljs-comment"># Ridge Regression</span><br>ridge_model = Ridge(alpha=<span class="hljs-number">1.0</span>)  <span class="hljs-comment"># Alpha is equivalent to λ</span><br>ridge_model.fit(X_train, y_train)<br>y_pred_ridge = ridge_model.predict(X_test)<br><br><span class="hljs-comment"># Evaluate models</span><br>mse_ols = mean_squared_error(y_test, y_pred_ols)<br>mse_ridge = mean_squared_error(y_test, y_pred_ridge)<br><br><span class="hljs-comment"># Return updated MSE and coefficients</span><br>mse_results_updated = &#123;<br>    <span class="hljs-string">&quot;Mean Squared Error (OLS)&quot;</span>: mse_ols,<br>    <span class="hljs-string">&quot;Mean Squared Error (Ridge)&quot;</span>: mse_ridge,<br>    <span class="hljs-string">&quot;OLS Coefficients&quot;</span>: ols_model.coef_,<br>    <span class="hljs-string">&quot;Ridge Coefficients&quot;</span>: ridge_model.coef_,<br>&#125;<br><br>mse_results_updated<br><br><br><span class="hljs-comment"># Correct the regression line plotting using predicted results</span><br><br><span class="hljs-comment"># Generate predictions for the entire feature range for consistent straight lines</span><br>X_range = np.linspace(X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>(), X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>(), <span class="hljs-number">100</span>).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>X_range_full = np.hstack([X_range, X_range + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, size=X_range.shape)])<br><br><span class="hljs-comment"># Predict the regression lines for OLS and Ridge models</span><br>y_ols_line = ols_model.predict(X_range_full)<br>y_ridge_line = ridge_model.predict(X_range_full)<br><br><span class="hljs-comment"># Plot regression results</span><br>plt.figure(figsize=(<span class="hljs-number">14</span>, <span class="hljs-number">6</span>))<br><br><span class="hljs-comment"># OLS Regression</span><br>plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>plt.scatter(X_test[:, <span class="hljs-number">0</span>], y_test, color=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&quot;Test Set&quot;</span>, alpha=<span class="hljs-number">0.7</span>)<br>plt.scatter(X_train[:, <span class="hljs-number">0</span>], y_train, color=<span class="hljs-string">&#x27;orange&#x27;</span>, label=<span class="hljs-string">&quot;Training Set&quot;</span>, alpha=<span class="hljs-number">0.7</span>)<br>plt.plot(X_range[:, <span class="hljs-number">0</span>], y_ols_line, color=<span class="hljs-string">&#x27;red&#x27;</span>, label=<span class="hljs-string">&quot;OLS Regression Line&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;OLS Regression&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Feature 1&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Target (y)&quot;</span>)<br>plt.legend()<br><br><span class="hljs-comment"># Ridge Regression</span><br>plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>plt.scatter(X_test[:, <span class="hljs-number">0</span>], y_test, color=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&quot;Test Set&quot;</span>, alpha=<span class="hljs-number">0.7</span>)<br>plt.scatter(X_train[:, <span class="hljs-number">0</span>], y_train, color=<span class="hljs-string">&#x27;orange&#x27;</span>, label=<span class="hljs-string">&quot;Training Set&quot;</span>, alpha=<span class="hljs-number">0.7</span>)<br>plt.plot(X_range[:, <span class="hljs-number">0</span>], y_ridge_line, color=<span class="hljs-string">&#x27;purple&#x27;</span>, label=<span class="hljs-string">&quot;Ridge Regression Line&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;Ridge Regression&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Feature 1&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Target (y)&quot;</span>)<br>plt.legend()<br><br>plt.tight_layout()<br>plt.show()<br><br><span class="hljs-comment"># Plot feature contributions (coefficients) as bar plots</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>x_labels = [<span class="hljs-string">&#x27;Feature 1&#x27;</span>, <span class="hljs-string">&#x27;Feature 2&#x27;</span>]<br><br><span class="hljs-comment"># OLS Coefficients</span><br>plt.bar(x_labels, ols_model.coef_, label=<span class="hljs-string">&#x27;OLS Coefficients&#x27;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br><br><span class="hljs-comment"># Ridge Coefficients</span><br>plt.bar(x_labels, ridge_model.coef_, label=<span class="hljs-string">&#x27;Ridge Coefficients&#x27;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>)<br><br><span class="hljs-comment"># Add title and labels</span><br>plt.title(<span class="hljs-string">&quot;Comparison of Feature Contributions (OLS vs Ridge)&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Coefficient Value&quot;</span>)<br>plt.legend()<br>plt.show()<br></code></pre></td></tr></table></figure></div><pre>{'Mean Squared Error (OLS)': 3.747535481239866, 'Mean Squared Error (Ridge)': 3.7344119726941427, 'OLS Coefficients': array([4.0641917 , 3.31246222]), 'Ridge Coefficients': array([2.93270253, 2.91932805])}</pre><p><img src="https://imgur.com/tUDhjah.png" alt=""><br><img src="https://imgur.com/dJzQZyh.png" alt=""></p><p>In this specific example, ridge regression slight reduced the mean squared error by reducing the contribution of <strong>feature 1</strong>. Contribution of the <strong>feature 1</strong> and <strong>feature 2</strong> are almost the same (Blue color in barplot). The linear regression plot was updated by removing the effects of <strong>feature 2</strong>.</p><h2 id="Compare-3-Methods">Compare 3 Methods</h2><p>Code continue from above</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> ElasticNet<br><br><span class="hljs-comment"># Perform Elastic Net Regression</span><br>elastic_net_model = ElasticNet(alpha=<span class="hljs-number">0.1</span>, l1_ratio=<span class="hljs-number">0.5</span>)  <span class="hljs-comment"># Alpha controls regularization strength, l1_ratio balances Lasso and Ridge</span><br>elastic_net_model.fit(X_train, y_train)<br>y_pred_elastic_net = elastic_net_model.predict(X_test)<br><br><span class="hljs-comment"># Evaluate Elastic Net model</span><br>mse_elastic_net = mean_squared_error(y_test, y_pred_elastic_net)<br><br><span class="hljs-comment"># Plot feature contributions (coefficients) for Elastic Net</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.bar(x_labels, ols_model.coef_, label=<span class="hljs-string">&quot;OLS Coefficients&quot;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&quot;red&quot;</span>)<br>plt.bar(x_labels, ridge_model.coef_, label=<span class="hljs-string">&quot;Ridge Coefficients&quot;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&quot;blue&quot;</span>)<br>plt.bar(x_labels, lasso_model.coef_, label=<span class="hljs-string">&quot;Lasso Coefficients&quot;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&quot;green&quot;</span>)<br>plt.bar(x_labels, elastic_net_model.coef_, label=<span class="hljs-string">&quot;Elastic Net Coefficients&quot;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&quot;purple&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;Comparison of Feature Contributions (OLS, Ridge, Lasso, Elastic Net)&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Coefficient Value&quot;</span>)<br>plt.legend()<br>plt.show()<br><br><span class="hljs-comment"># Return MSE and coefficients for Elastic Net</span><br>mse_results_elastic_net = &#123;<br>    <span class="hljs-string">&quot;Mean Squared Error (OLS)&quot;</span>: mse_ols,<br>    <span class="hljs-string">&quot;Mean Squared Error (Ridge)&quot;</span>: mse_ridge,<br>    <span class="hljs-string">&quot;Mean Squared Error (Lasso)&quot;</span>: mse_lasso,<br>    <span class="hljs-string">&quot;Mean Squared Error (Elastic Net)&quot;</span>: mse_elastic_net,<br>    <span class="hljs-string">&quot;OLS Coefficients&quot;</span>: ols_model.coef_,<br>    <span class="hljs-string">&quot;Ridge Coefficients&quot;</span>: ridge_model.coef_,<br>    <span class="hljs-string">&quot;Lasso Coefficients&quot;</span>: lasso_model.coef_,<br>    <span class="hljs-string">&quot;Elastic Net Coefficients&quot;</span>: elastic_net_model.coef_,<br>&#125;<br><br>mse_results_elastic_net<br></code></pre></td></tr></table></figure></div><pre>{'Mean Squared Error (OLS)': 3.747535481239866, 'Mean Squared Error (Ridge)': 3.7344119726941427, 'Mean Squared Error (Lasso)': 3.681549054209485, 'Mean Squared Error (Elastic Net)': 3.810867636824328, 'OLS Coefficients': array([4.0641917 , 3.31246222]), 'Ridge Coefficients': array([2.93270253, 2.91932805]), 'Lasso Coefficients': array([3.3579283 , 2.97769008]), 'Elastic Net Coefficients': array([2.72149455, 2.71825096])}</pre><p><img src="https://imgur.com/ExhrW4P.png" alt=""></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Regularization is a way to make sure our model doesn&#39;t become too complicated. It ensures the model doesn’t overfit the training data while still making good predictions on new data. Think of it as adding a &#39;&lt;b&gt;rule&lt;/b&gt;&#39; or &#39;&lt;b&gt;constraint&lt;/b&gt;&#39; that prevents the model from relying too much on any specific feature or predictor.</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>pyrosetta</title>
    <link href="https://karobben.github.io/2024/12/20/Bioinfor/pyrosetta/"/>
    <id>https://karobben.github.io/2024/12/20/Bioinfor/pyrosetta/</id>
    <published>2024-12-21T00:06:54.000Z</published>
    <updated>2024-12-23T21:35:44.191Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Loop-Regenerate-Codes-From-ChatGPT">Loop Regenerate (Codes From ChatGPT)</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> init, Pose, get_fa_scorefxn<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.loops <span class="hljs-keyword">import</span> Loops, Loop<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.loops.loop_mover.perturb <span class="hljs-keyword">import</span> LoopMover_Perturb_KIC<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.loops.loop_mover.refine <span class="hljs-keyword">import</span> LoopMover_Refine_KIC, LoopMover_Refine_CCD<br><br><span class="hljs-comment">#from pyrosetta.rosetta.core.import_pose import pose_from_pdbstring as pose_from_pdb</span><br><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span>  pose_from_pdb<br><br><span class="hljs-comment"># 1. Initialize PyRosetta</span><br>init()<br><br><span class="hljs-comment"># 2. Load Your Protein Pose</span><br>pose = pose_from_pdb( <span class="hljs-string">&quot;test.pdb&quot;</span>)<br><br>n_chains = pose.num_chains()<br><span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>    start_res = pose.chain_begin(chain_index)<br>    end_res = pose.chain_end(chain_index)<br>    print(<span class="hljs-string">f&quot;Chain <span class="hljs-subst">&#123;chain_index&#125;</span>: residues <span class="hljs-subst">&#123;start_res&#125;</span> to <span class="hljs-subst">&#123;end_res&#125;</span>&quot;</span>)<br><br>loops_by_chain = &#123;&#125;<br><br><span class="hljs-comment"># Iterate over chains</span><br>n_chains = pose.num_chains()<br><span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>    start_res = pose.chain_begin(chain_index)<br>    end_res = pose.chain_end(chain_index)<br><br>    <span class="hljs-comment"># Extract secondary structure substring for this chain</span><br>    chain_secstruct = secstruct[start_res-<span class="hljs-number">1</span>:end_res]<br><br>    loop_regions = []<br>    current_loop_start = <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># Identify loop regions as stretches of &#x27;L&#x27;</span><br>    <span class="hljs-keyword">for</span> i, s <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(chain_secstruct, start=start_res):<br>        <span class="hljs-keyword">if</span> s == <span class="hljs-string">&#x27;L&#x27;</span>:<br>            <span class="hljs-keyword">if</span> current_loop_start <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                current_loop_start = i<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> current_loop_start <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                loop_regions.append((current_loop_start, i-<span class="hljs-number">1</span>))<br>                current_loop_start = <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># Check if a loop extends to the end of the chain</span><br>    <span class="hljs-keyword">if</span> current_loop_start <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        loop_regions.append((current_loop_start, end_res))<br><br>    <span class="hljs-comment"># Extract sequences for each loop region</span><br>    <span class="hljs-comment"># Store them in a dictionary keyed by chain index</span><br>    chain_loops = []<br>    <span class="hljs-keyword">for</span> (loop_start, loop_end) <span class="hljs-keyword">in</span> loop_regions:<br>        <span class="hljs-comment"># Extract the sequence of the loop</span><br>        loop_seq = <span class="hljs-string">&quot;&quot;</span>.join([pose.residue(r).name1() <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(loop_start, loop_end+<span class="hljs-number">1</span>)])<br>        chain_loops.append(&#123;<br>            <span class="hljs-string">&quot;start&quot;</span>: loop_start,<br>            <span class="hljs-string">&quot;end&quot;</span>: loop_end,<br>            <span class="hljs-string">&quot;sequence&quot;</span>: loop_seq<br>        &#125;)<br><br>    loops_by_chain[chain_index] = chain_loops<br><br>n_chains = pose.num_chains()<br><span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>    start_res = pose.chain_begin(chain_index)<br>    end_res = pose.chain_end(chain_index)<br>    print(<span class="hljs-string">f&quot;Chain <span class="hljs-subst">&#123;chain_index&#125;</span>: residues <span class="hljs-subst">&#123;start_res&#125;</span> to <span class="hljs-subst">&#123;end_res&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 3. Define the Loop(s) You Want to Remodel</span><br><span class="hljs-comment"># Suppose you want to remodel the loop from residues 45 to 55.</span><br><span class="hljs-comment"># Choose a cut point (ideally inside the loop), typically near the middle.</span><br>loop_start = <span class="hljs-number">593</span><br>loop_end = <span class="hljs-number">608</span><br>cutpoint = <span class="hljs-number">601</span><br><br>loops = Loops()<br>loops.add_loop( Loop(loop_start, loop_end, cutpoint) )<br><br><span class="hljs-comment"># 4. Set Up a Scorefunction</span><br>scorefxn = get_fa_scorefxn()<br><br><span class="hljs-comment"># 5. Set Up the Loop Remodeling Protocol</span><br><span class="hljs-comment"># You have multiple options: </span><br><span class="hljs-comment"># Example: Use KIC Perturb and then Refine</span><br>perturb_mover = LoopMover_Perturb_KIC(loops)<br>perturb_mover.set_scorefxn(scorefxn)<br><br>refine_mover = LoopMover_Refine_KIC(loops)<br>refine_mover.set_scorefxn(scorefxn)<br><br><span class="hljs-comment"># Alternatively, you might use CCD refinement:</span><br><span class="hljs-comment"># refine_mover = LoopMover_Refine_CCD(loops)</span><br><span class="hljs-comment"># refine_mover.set_scorefxn(scorefxn)</span><br><br><span class="hljs-comment"># 6. Optionally: Set Up Monte Carlo or Repeats</span><br><span class="hljs-comment"># Often you do multiple trials and pick the best model.</span><br><br><span class="hljs-comment"># 7. Apply the Movers</span><br><span class="hljs-comment"># First do perturbation</span><br>perturb_mover.apply(pose)<br><br><span class="hljs-comment"># Then refine</span><br>refine_mover.apply(pose)<br><br><span class="hljs-comment"># After this, you should have a remodeled loop region.</span><br><span class="hljs-comment"># You can save the resulting structure to a PDB file:</span><br>pose.dump_pdb(<span class="hljs-string">&quot;remodeled_loop.pdb&quot;</span>)<br></code></pre></td></tr></table></figure></div><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/eJG1S0x.png" alt="Raw loop"></td><td style="text-align:left">Raw loop</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/Lu1BdXo.png" alt="Predicted loop"></td><td style="text-align:left">Predicted loop by ussing ImmuneBuilder. The Predicted results has some trouble in the CDRH3 region. And if we place it in the corrected position and it has crush.</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/l41Fw8X.png" alt="Reconstructed loop"></td><td style="text-align:left">Rosetta reconstructed loop by using the code above. Rosetta takes lots of time to reconstruct the loop and the result is terrible. The loop inseted into a very wired and unlikly position</td></tr></tbody></table><h2 id="Loop-Regenerate-Codes-From-Tutorial">Loop Regenerate (Codes From Tutorial)</h2><p><img src="https://imgur.com/1Op8NKe.png" alt=""></p><p>In the Tutorial 9.01, it use 2 structure: 1) the complete structure and 2) the structure has gap. The missing parts is range from 29~31. It not only deleted 5 residues, but also split it into 2 chains.</p><p>Because it was in a separate chain, the index 28 and 29 is the C terminal and N terminal in the chain with gap. The selected residues is 28 and 32 when they are in the original structure</p><h3 id="How-it-works-in-antibody-CDRH3">How it works in antibody CDRH3</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Notebook setup</span><br><span class="hljs-keyword">import</span> pyrosettacolabsetup; pyrosettacolabsetup.install_pyrosetta()<br><span class="hljs-keyword">import</span> pyrosetta; pyrosetta.init()<br><br><span class="hljs-comment"># py3Dmol setup (if there&#x27;s an error, make sure you have &#x27;py3Dmol&#x27; and &#x27;ipywidgets&#x27; pip installed)</span><br><span class="hljs-keyword">import</span> glob<br><span class="hljs-keyword">import</span> logging<br>logging.basicConfig(level=logging.INFO)<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pyrosetta.distributed<br><span class="hljs-keyword">import</span> pyrosetta.distributed.io <span class="hljs-keyword">as</span> io<br><span class="hljs-keyword">import</span> pyrosetta.distributed.viewer <span class="hljs-keyword">as</span> viewer<br><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> pose_from_pdb<br><br>input_pose = pose_from_pdb(<span class="hljs-string">&#x27;/mnt/Data/PopOS/Data_Ana/Wu/PigAntiBodies/AB_regine/data/14-1_ImmuneCorrect_partial.pdb&#x27;</span>)<br>input_pose_no_loop = pose_from_pdb(<span class="hljs-string">&#x27;/mnt/Data/PopOS/Data_Ana/Wu/PigAntiBodies/AB_regine/data/14-1_ImmuneCorrect_partial_Noloop.pdb&#x27;</span>)<br><br><br>helix_selector = pyrosetta.rosetta.core.select.residue_selector.SecondaryStructureSelector(<span class="hljs-string">&quot;H&quot;</span>)<br>loop_selector = pyrosetta.rosetta.core.select.residue_selector.SecondaryStructureSelector(<span class="hljs-string">&quot;L&quot;</span>)<br><br>modules = [<br>    viewer.setBackgroundColor(color=<span class="hljs-string">&quot;black&quot;</span>),<br>    viewer.setStyle(residue_selector=helix_selector, cartoon_color=<span class="hljs-string">&quot;blue&quot;</span>, label=<span class="hljs-literal">False</span>, radius=<span class="hljs-number">0</span>),<br>    viewer.setStyle(residue_selector=loop_selector, cartoon_color=<span class="hljs-string">&quot;yellow&quot;</span>, label=<span class="hljs-literal">False</span>, radius=<span class="hljs-number">0</span>),<br>    viewer.setZoomTo(residue_selector=loop_selector)<br>]<br><br><span class="hljs-comment">#view = viewer.init(input_pose, window_size=(800, 600), modules=modules).show()</span><br><span class="hljs-comment">#view = viewer.init(input_pose_no_loop, window_size=(800, 600), modules=modules).show()</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Chain_num</span>(<span class="hljs-params">pose</span>):</span><br>    n_chains = pose.num_chains()<br>    <span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>        start_res = pose.chain_begin(chain_index)<br>        end_res = pose.chain_end(chain_index)<br>        print(<span class="hljs-string">f&quot;Chain <span class="hljs-subst">&#123;chain_index&#125;</span>: residues <span class="hljs-subst">&#123;start_res&#125;</span> to <span class="hljs-subst">&#123;end_res&#125;</span>&quot;</span>)<br><br>Chain_num(input_pose)<br>Chain_num(input_pose_no_loop)<br><br>Start = input_pose_no_loop.chain_end(<span class="hljs-number">1</span>)<br>Len = input_pose.chain_end(<span class="hljs-number">1</span>) - input_pose_no_loop.chain_end(<span class="hljs-number">2</span>)<br>End = Start + Len<br>Miss_Seq = <span class="hljs-string">&quot;&quot;</span>.join([input_pose.residue(i).name1() <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Start, End)])<br><br><span class="hljs-comment">##The c terminus of one helix</span><br>print(input_pose_no_loop.residue(Start).name())<br><span class="hljs-comment">#The N terminus of the other helix</span><br>print(input_pose_no_loop.residue(Start+<span class="hljs-number">1</span>).name())<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mutate_position</span>(<span class="hljs-params">pose,position,mutate</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;A simple function to mutate an amino acid given a pose number&#x27;&#x27;&#x27;</span><br>    mr = pyrosetta.rosetta.protocols.simple_moves.MutateResidue()<br>    mr.set_target(position)<br>    mr.set_res_name(mutate)<br>    mr.apply(pose)<br><br><br><span class="hljs-comment">##Mutate both 28 and 29 to ALA</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">In tutorial, they mutated the residues into ALA. It says is make a pose that can be applied by GenKic. I am not sure it is because of the GenKic algorithem or GenKic function. I possible could be that GenKic function doesn&#x27;t accept the resideus from either side do the termianl. So, we need to remove them and add them back. Because my goal is get a better conformation of the loop, I just relpace it with itself to do the lateral test.</span><br><span class="hljs-string">Also, I tried to use ALA at the begining, too. The folding resutls are not promissing.</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>Resi1 = input_pose_no_loop.residue(Start).name3()<br>Resi2 = input_pose_no_loop.residue(Start+<span class="hljs-number">1</span>).name3()<br><br>mutate_position(input_pose_no_loop,Start,Resi1)<br>mutate_position(input_pose_no_loop,Start+<span class="hljs-number">1</span>,Resi2)<br><span class="hljs-keyword">assert</span>(input_pose_no_loop.residue(Start).name() == Resi1)<br><span class="hljs-keyword">assert</span>(input_pose_no_loop.residue(Start+<span class="hljs-number">1</span>).name() == Resi2)<br><br><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> Pose<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">slice_pose</span>(<span class="hljs-params">p,start,end</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Take a pose object and return from start, end</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    sliced = Pose()<br>    <span class="hljs-keyword">if</span> end &gt; p.size() <span class="hljs-keyword">or</span> start &gt; p.size():<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;end/start slice is longer than total lenght of pose &#123;&#125; &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(start,end)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start,end+<span class="hljs-number">1</span>):<br>        sliced.append_residue_by_bond(p.residue(i))<br>    <span class="hljs-keyword">return</span> sliced<br><br><span class="hljs-comment">##Pose object 1 - helix_AB all the way up to residue 28</span><br>helix_ab_pose = slice_pose(input_pose_no_loop,<span class="hljs-number">1</span>,Start)<br><span class="hljs-comment">##Pose object 2 - helix C and the reaminder of the pose</span><br><span class="hljs-comment">#helix_c_pose = slice_pose(input_pose_no_loop,Start+1,input_pose_no_loop.size())</span><br>helix_c_pose = slice_pose(input_pose_no_loop,Start+<span class="hljs-number">1</span>,input_pose_no_loop.chain_end(<span class="hljs-number">2</span>))<br><br><span class="hljs-comment"># We&#x27;re just going to quicky add in pdb info so that our viewing commands work</span><br>add_pdb_info_mover = pyrosetta.rosetta.protocols.simple_moves.AddPDBInfoMover()<br>add_pdb_info_mover.apply(helix_ab_pose)<br>add_pdb_info_mover.apply(helix_c_pose)<br><span class="hljs-comment"># Here&#x27;s the second part</span><br><span class="hljs-comment">#view = viewer.init(helix_c_pose, window_size=(800, 600), modules=modules).show()</span><br><span class="hljs-comment"># Here&#x27;s the first object</span><br><span class="hljs-comment">#view = viewer.init(helix_ab_pose, window_size=(800, 600), modules=modules).show()</span><br><br><span class="hljs-comment"># Here&#x27;s the second object</span><br><span class="hljs-comment">#view = viewer.init(helix_c_pose, window_size=(800, 600), modules=modules).show()</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">crudely_connect_w_loop</span>(<span class="hljs-params">n_term_pose,c_term_pose,connect_with</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    The function will take two poses and join them with a loop</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Keep in mind this is just joined as far as the pose is concerned. The bond angles and lenghts will be sub-optimal</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    one_to_three = &#123;<br>    <span class="hljs-string">&#x27;A&#x27;</span>: <span class="hljs-string">&#x27;ALA&#x27;</span>,<br>    <span class="hljs-string">&#x27;C&#x27;</span>: <span class="hljs-string">&#x27;CYS&#x27;</span>,<br>    <span class="hljs-string">&#x27;D&#x27;</span>: <span class="hljs-string">&#x27;ASP&#x27;</span>,<br>    <span class="hljs-string">&#x27;E&#x27;</span>: <span class="hljs-string">&#x27;GLU&#x27;</span>,<br>    <span class="hljs-string">&#x27;F&#x27;</span>: <span class="hljs-string">&#x27;PHE&#x27;</span>,<br>    <span class="hljs-string">&#x27;G&#x27;</span>: <span class="hljs-string">&#x27;GLY&#x27;</span>,<br>    <span class="hljs-string">&#x27;H&#x27;</span>: <span class="hljs-string">&#x27;HIS&#x27;</span>,<br>    <span class="hljs-string">&#x27;I&#x27;</span>: <span class="hljs-string">&#x27;ILE&#x27;</span>,<br>    <span class="hljs-string">&#x27;K&#x27;</span>: <span class="hljs-string">&#x27;LYS&#x27;</span>,<br>    <span class="hljs-string">&#x27;L&#x27;</span>: <span class="hljs-string">&#x27;LEU&#x27;</span>,<br>    <span class="hljs-string">&#x27;M&#x27;</span>: <span class="hljs-string">&#x27;MET&#x27;</span>,<br>    <span class="hljs-string">&#x27;N&#x27;</span>: <span class="hljs-string">&#x27;ASN&#x27;</span>,<br>    <span class="hljs-string">&#x27;P&#x27;</span>: <span class="hljs-string">&#x27;PRO&#x27;</span>,<br>    <span class="hljs-string">&#x27;Q&#x27;</span>: <span class="hljs-string">&#x27;GLN&#x27;</span>,<br>    <span class="hljs-string">&#x27;R&#x27;</span>: <span class="hljs-string">&#x27;ARG&#x27;</span>,<br>    <span class="hljs-string">&#x27;S&#x27;</span>: <span class="hljs-string">&#x27;SER&#x27;</span>,<br>    <span class="hljs-string">&#x27;T&#x27;</span>: <span class="hljs-string">&#x27;THR&#x27;</span>,<br>    <span class="hljs-string">&#x27;V&#x27;</span>: <span class="hljs-string">&#x27;VAL&#x27;</span>,<br>    <span class="hljs-string">&#x27;Y&#x27;</span>: <span class="hljs-string">&#x27;TYR&#x27;</span>,<br>    <span class="hljs-string">&#x27;W&#x27;</span>: <span class="hljs-string">&#x27;TRP&#x27;</span>&#125;<br><br>    pose_a = Pose()<br>    pose_a.assign(n_term_pose)<br><br>    pose_b = Pose()<br>    pose_b.assign(c_term_pose)<br><br>    <span class="hljs-comment"># Setup CHEMICAL MANAGER TO MAKE NEW RESIDUES</span><br>    chm = pyrosetta.rosetta.core.chemical.ChemicalManager.get_instance()<br>    rts = chm.residue_type_set(<span class="hljs-string">&#x27;fa_standard&#x27;</span>)<br>    get_residue_object = <span class="hljs-keyword">lambda</span> x: pyrosetta.rosetta.core.conformation.ResidueFactory.create_residue(<br>        rts.name_map(x))<br><br>    <span class="hljs-comment"># Will keep track of indexing of rebuilt loop</span><br>    rebuilt_loop = []<br><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Iterate through string turning each letter into a residue object and then</span><br><span class="hljs-string">    appending it to the N term pose&#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">for</span> one_letter <span class="hljs-keyword">in</span> connect_with:<br>        resi = get_residue_object(one_to_three[one_letter])<br>        pose_a.append_residue_by_bond(resi, <span class="hljs-literal">True</span>)<br>        pose_a.set_omega(pose_a.total_residue(), <span class="hljs-number">180.</span>)<br>        rebuilt_loop.append(pose_a.total_residue())<br><br>    <span class="hljs-comment">##ADD the C term pose to the end of the loop we just appended</span><br>    <span class="hljs-keyword">for</span> residue_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, pose_b.total_residue()+<span class="hljs-number">1</span>):<br>        pose_a.append_residue_by_bond(<br>            pose_b.residue(residue_index))<br><br>    print(<span class="hljs-string">&quot;Joined NTerm and CTerm pose with loop &#123;&#125; at residues &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(connect_with,rebuilt_loop))<br>    <span class="hljs-keyword">return</span> pose_a<br><br><span class="hljs-comment">#Returns a pose that is connected, but sub-optimal geometry</span><br>gk_input_pose = crudely_connect_w_loop(helix_ab_pose,helix_c_pose,Miss_Seq)<br><span class="hljs-comment">#gk_input_pose = crudely_connect_w_loop(helix_ab_pose,helix_c_pose,Miss_Seq)</span><br><br>print(Miss_Seq)<br><span class="hljs-keyword">for</span> chain <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>,input_pose_no_loop.num_chains()+<span class="hljs-number">1</span>):<br>    print(chain)<br>    gk_input_pose.append_pose_by_jump(input_pose_no_loop.split_by_chain(chain), gk_input_pose.total_residue())<br><br>Chain_num(helix_ab_pose)<br>Chain_num(gk_input_pose)<br>Chain_num(input_pose)<br><br><br><br><span class="hljs-keyword">from</span> additional_scripts.GenKic <span class="hljs-keyword">import</span> GenKic<br><br><span class="hljs-comment">##All that GenKic needs is the loop residue list</span><br>loop_residues = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Start,End+<span class="hljs-number">2</span>)]<br>gk_object = GenKic(loop_residues)<br><br><span class="hljs-comment">##Let&#x27;s set the closure attempt to 500000</span><br>gk_object.set_closure_attempts(<span class="hljs-number">500000</span>)<br>gk_object.set_min_solutions(<span class="hljs-number">10</span>)<br><br><br><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> ScoreFunction<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_bb_only_sfxn</span>():</span><br>    scorefxn = ScoreFunction()<br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.fa_atr, <span class="hljs-number">1</span>)    <span class="hljs-comment"># full-atom attractive score</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.fa_rep, <span class="hljs-number">0.55</span>)    <span class="hljs-comment"># full-atom repulsive score</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.hbond_sr_bb, <span class="hljs-number">1</span>)    <span class="hljs-comment"># short-range hbonding</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.hbond_lr_bb, <span class="hljs-number">1</span>)    <span class="hljs-comment"># long-range hbonding</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.rama_prepro, <span class="hljs-number">0.45</span>)    <span class="hljs-comment"># ramachandran score</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.omega, <span class="hljs-number">0.4</span>)    <span class="hljs-comment"># omega torsion score</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.p_aa_pp, <span class="hljs-number">0.625</span>)<br>    <span class="hljs-keyword">return</span> scorefxn<br><br><span class="hljs-comment">##Grab BB Only SFXN</span><br>bb_only_sfxn = get_bb_only_sfxn()<br><br><span class="hljs-comment">##Pass it to GK</span><br>gk_object.set_scorefxn(bb_only_sfxn)<br><br>gk_object.set_selector_type(<span class="hljs-string">&#x27;lowest_energy_selector&#x27;</span>)<br><span class="hljs-comment">#First lets set alll mainchain omega values to 180 degrees in our loop. We don&#x27;t want to include residue after the last anchor residue as that could potentially not exist.</span><br><span class="hljs-keyword">for</span> res_num <span class="hljs-keyword">in</span> loop_residues[:-<span class="hljs-number">1</span>]:<br>    gk_object.set_dihedral(res_num, res_num + <span class="hljs-number">1</span>, <span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;N&quot;</span>, <span class="hljs-number">180.1</span>)<br><br><span class="hljs-comment">###Or there is a convienience function within the class that does the same thing</span><br>gk_object.set_omega_angles()<br><br><span class="hljs-keyword">for</span> res_num <span class="hljs-keyword">in</span> loop_residues:<br>    gk_object.randomize_backbone_by_rama_prepro(res_num)<br><span class="hljs-comment">##This will grab the GK instance and apply everything we have set to our pose</span><br>gk_object.get_instance().apply(gk_input_pose)<br><br><span class="hljs-comment">##You can see, we perturbed the loop, but we did not tell GK to close the bond</span><br><span class="hljs-comment">#view = viewer.init(gk_input_pose, window_size=(800, 600), modules=modules).show()</span><br><br>gk_object.close_normal_bond(End,End+<span class="hljs-number">1</span>) <span class="hljs-comment">#or gk_object.close_normal_bond(loop_residues[-2],loop_residues[-1])</span><br>gk_object.get_instance().apply(gk_input_pose)<br><br><span class="hljs-comment">#view = viewer.init(gk_input_pose, window_size=(800, 600), modules=modules).show()</span><br><br><span class="hljs-comment">##The first residue in our loop definition will be confiend to alpha-helical rama space</span><br>gk_object.set_filter_backbone_bin(loop_residues[<span class="hljs-number">0</span>],<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-built_in">bin</span>=<span class="hljs-string">&#x27;ABBA&#x27;</span>)<br><span class="hljs-comment">##The last residue in our loop definition will be confiend to alpha-helical rama space</span><br>gk_object.set_filter_backbone_bin(loop_residues[-<span class="hljs-number">1</span>],<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-built_in">bin</span>=<span class="hljs-string">&#x27;ABBA&#x27;</span>)<br><br>gk_object.set_filter_loop_bump_check()<br><br><br><span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> gk_object.pivot_residues:<br>    gk_object.set_filter_rama_prepro(r,cutoff=<span class="hljs-number">0.5</span>)<br><br><span class="hljs-comment">##Grab GK instance</span><br>gk_instance = gk_object.get_instance()<br><span class="hljs-comment">##apply it to the pose</span><br>gk_instance.apply(gk_input_pose)<br><span class="hljs-comment">##The Input pose with no loop, the reference pose we are trying to recreate and the GK pose</span><br>poses = [input_pose_no_loop, input_pose, gk_input_pose]<br><span class="hljs-comment"># view = viewer.init(poses) + viewer.setStyle()</span><br><span class="hljs-comment"># view()</span><br><br><span class="hljs-comment">#gk_input_pose.dump_pdb(&quot;final_pose.pdb&quot;)</span><br><br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.relax <span class="hljs-keyword">import</span> FastRelax<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.core.kinematics <span class="hljs-keyword">import</span> MoveMap<br><br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.relax <span class="hljs-keyword">import</span> FastRelax<br><br><br><br>gk_input_pose.dump_pdb(<span class="hljs-string">&quot;loop.pdb&quot;</span>)<br><br><span class="hljs-comment"># loop alone</span><br>relax = FastRelax()<br>relax.set_scorefxn(bb_only_sfxn)<br>movemap = MoveMap()<br><span class="hljs-comment">#for res in loop_residues:</span><br><span class="hljs-comment">#  movemap.set_bb(res, True)</span><br>relax.set_movemap(movemap)<br>relax.apply(gk_input_pose)<br><br><br>gk_input_pose.dump_pdb(<span class="hljs-string">&quot;loop_relaxe.pdb&quot;</span>)<br><br><span class="hljs-comment"># relax: Full</span><br>relax = FastRelax()<br>relax.set_scorefxn(bb_only_sfxn)<br>relax.apply(gk_input_pose)<br><br><br>gk_input_pose.dump_pdb(<span class="hljs-string">&quot;loop_full_relaxe.pdb&quot;</span>)<br></code></pre></td></tr></table></figure></div><p>In this script, I am not only test the loop reconstruction, but also add relaxation steps. Here is the results from different methods. It seems like no matter how you try, it is hard to reconstruct this loop in Rosetta.</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/iyjWPH6.png" alt="Reconstructed Loop"></td><td style="text-align:center"><img src="https://imgur.com/ZX450qV.png" alt="Loop-reconstruction and Relaxation"></td></tr><tr><td style="text-align:center"><img src="https://imgur.com/wTwR6P5.png" alt="Relaxation only"></td><td style="text-align:center"><img src="https://imgur.com/BRtUTD1.png" alt="Realxation and then loop rexonstruction"></td></tr></tbody></table><h2 id="How-to-check-the-Chain-and-the-number-of-residues">How to check the Chain and the number of residues</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> init, pose_from_pdb<br><br><span class="hljs-comment"># 1. Initialize PyRosetta</span><br>init()<br><span class="hljs-comment"># 2. Load Your Protein Pose</span><br>pose = pose_from_pdb( <span class="hljs-string">&quot;data/14-1_ImmuneCorrect.pdb&quot;</span>)<br><br><span class="hljs-comment"># 3. Count and print the result</span><br><br>n_chains = pose.num_chains()<br><span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>    start_res = pose.chain_begin(chain_index)<br>    end_res = pose.chain_end(chain_index)<br>    print(<span class="hljs-string">f&quot;Chain <span class="hljs-subst">&#123;chain_index&#125;</span>: residues <span class="hljs-subst">&#123;start_res&#125;</span> to <span class="hljs-subst">&#123;end_res&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure></div><pre>Chain 1: residues 1 to 322Chain 2: residues 323 to 493Chain 3: residues 494 to 620Chain 4: residues 621 to 729</pre><h2 id="Get-the-Second-Structure">Get the Second Structure</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> init, pose_from_pdb<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.core.scoring.dssp <span class="hljs-keyword">import</span> Dssp<br><br><span class="hljs-comment"># 1. Initialize PyRosetta</span><br>init()<br><span class="hljs-comment"># 2. Load Your Protein Pose</span><br>pose = pose_from_pdb( <span class="hljs-string">&quot;data/14-1_ImmuneCorrect.pdb&quot;</span>)<br><br><span class="hljs-comment"># Run DSSP to get secondary structure</span><br>dssp = Dssp(pose)<br>secstruct = dssp.get_dssp_reduced_IG_as_L_secstruct()<br></code></pre></td></tr></table></figure></div><pre>LLEEEEELELLLLLLEEEELLEEEEEELLEEELEELLLLLLEEEELLELLEELLLELHHHHHHLLLLLLLLLLLLLLLLEEELLLLLELLLLLLLELLHHHHHHHLLLELLLEEEELLLLLLLLLLEELLLLEHLHLLLLLLELLLLEEEEEELLLLLLLEEEEEELLLLLLEEEEEEEEELLLHHHHHHHHLLLLLLEEEEELLLEEEELLLLLLLLLLLLLLLEEEEEEEEELLLLEEEEEELLLEEEELEEEELLELLLLLEEELLLLEEEEEELEELLLLLEL...</pre><div class="admonition note"><p class="admonition-title">What does it mean?</p><ul><li>H: Alpha-Helix</li><li>E: Beta-Strand</li><li>L: Loop or Irregular Region</li></ul></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">pyrosetta</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Heatmap with GGplot</title>
    <link href="https://karobben.github.io/2024/12/15/R/ggplot-heatmap/"/>
    <id>https://karobben.github.io/2024/12/15/R/ggplot-heatmap/</id>
    <published>2024-12-15T06:23:10.000Z</published>
    <updated>2024-12-15T07:20:26.023Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Data-Ready">Data Ready</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r"><span class="hljs-comment"># Library</span><br>library(ggplot2)<br><br><span class="hljs-comment"># Dummy data</span><br>x &lt;- <span class="hljs-built_in">LETTERS</span>[<span class="hljs-number">1</span>:<span class="hljs-number">20</span>]<br>y &lt;- paste0(<span class="hljs-string">&quot;var&quot;</span>, seq(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>))<br>data &lt;- expand.grid(X=x, Y=y)<br>data$Z &lt;- runif(<span class="hljs-number">400</span>, <span class="hljs-number">0</span>, <span class="hljs-number">5</span>)<br> <br><span class="hljs-comment"># Heatmap </span><br>p &lt;- ggplot(data, aes(X, Y, fill= Z)) + <br>  geom_tile()<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/lxVWCkQ.png" alt="heatmap in ggplot"></p><pre>  X    Y         Z1 A var1 2.56291662 B var1 4.91312173 C var1 0.12522194 D var1 2.66059005 E var1 1.23435786 F var1 4.7347760</pre><h2 id="Cluster-column-and-rows">Cluster column and rows</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(stringr)<br><br>data2 &lt;- reshape(data, idvar=<span class="hljs-string">&#x27;X&#x27;</span>, timevar= <span class="hljs-string">&#x27;Y&#x27;</span>, direction= <span class="hljs-string">&#x27;wide&#x27;</span>)<br>row.names(data2) &lt;- data2$X<br>colnames(data2) &lt;- str_remove(colnames(data2), <span class="hljs-string">&#x27;Z.&#x27;</span>)<br>data2 &lt;- data2[-<span class="hljs-number">1</span>]<br><br>ClusLevel &lt;- <span class="hljs-keyword">function</span>(data2)&#123;<br>    tmp &lt;- hclust(dist(data2))<br>    <span class="hljs-built_in">return</span>(tmp$labels[tmp$order])<br>&#125;<br><br>data$X &lt;- factor(data$X, level = ClusLevel(data2))<br>data$Y &lt;- factor(data$Y, level = ClusLevel(t(data2)))<br><br>p &lt;- ggplot(data, aes(X, Y, fill= Z)) + <br>  geom_tile()<br><br><br></code></pre></td></tr></table></figure></div><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/4Lq2Nhf.png" alt="Heatmap"></th></tr></thead><tbody><tr><td style="text-align:center">The cluster resutls are not very clear. It is expectable because the generated dataset doesn’t has any kind of relationship at all</td></tr></tbody></table><h2 id="Classic-RdYlBu-Palette">Classic RdYlBu Palette</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r"><span class="hljs-comment">## Best color for heatmap</span><br>library(RColorBrewer)<br>colorRampPalette(rev(brewer.pal(n = 7,name = &quot;RdYlBu&quot;))) -&gt; cc<br><br>p + scale_fill_gradientn(colors=cc(<span class="hljs-number">100</span>))<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/PYhCSUc.png" alt=""></p><h2 id="Set-Limits-and-Change-Color">Set Limits and Change Color</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">Heatmape &lt;- <span class="hljs-keyword">function</span>(TB, x, y, fill, minpoint = <span class="hljs-literal">FALSE</span>, midpoint = <span class="hljs-literal">FALSE</span>, maxpoint = <span class="hljs-literal">FALSE</span>,<br>                    legend.name = <span class="hljs-built_in">expression</span>(paste(<span class="hljs-string">&quot;Δ&quot;</span>, <span class="hljs-built_in">log</span>[<span class="hljs-number">10</span>], <span class="hljs-string">&quot;(&quot;</span>, K[D], <span class="hljs-string">&quot;)&quot;</span>, sep = <span class="hljs-string">&#x27;&#x27;</span>)),<br>                    axis.title.x = <span class="hljs-string">&#x27;X&#x27;</span>,<br>                    axis.title.y = <span class="hljs-string">&#x27;Y&#x27;</span>,<br>                    colors = <span class="hljs-built_in">c</span>(<span class="hljs-string">&quot;Firebrick4&quot;</span>, <span class="hljs-string">&quot;white&quot;</span>, <span class="hljs-string">&quot;royalblue4&quot;</span>) <br>                    )&#123;<br>    <span class="hljs-keyword">if</span>(minpoint == <span class="hljs-literal">FALSE</span>)&#123;<br>        minpoint = <span class="hljs-built_in">min</span>(TB[[fill]], na.rm = <span class="hljs-literal">TRUE</span>)<br>    &#125;<br>    <span class="hljs-keyword">if</span>(midpoint == <span class="hljs-literal">FALSE</span>)&#123;<br>        midpoint = mean(<span class="hljs-built_in">c</span>(<span class="hljs-built_in">min</span>(TB[[fill]], na.rm = <span class="hljs-literal">TRUE</span>), <span class="hljs-built_in">min</span>(TB[[fill]], na.rm = <span class="hljs-literal">TRUE</span>)))<br>    &#125;<br>    <span class="hljs-keyword">if</span>(maxpoint == <span class="hljs-literal">FALSE</span>)&#123;<br>        maxpoint = <span class="hljs-built_in">max</span>(TB[[fill]], na.rm = <span class="hljs-literal">TRUE</span>)<br>    &#125;<br>    P &lt;- ggplot(TB, aes(TB[[x]], TB[[y]], fill = TB[[fill]])) + geom_tile() +<br>      scale_fill_gradientn(<br>          colors = colors, <br>          values = scales::rescale(<span class="hljs-built_in">c</span>(minpoint, midpoint, maxpoint)),  <span class="hljs-comment"># Set the key values</span><br>          oob = scales::squish,<br>          na.value = <span class="hljs-string">&quot;gray&quot;</span>,<br>          limit = <span class="hljs-built_in">c</span>(minpoint, maxpoint)) +<br>      labs(x = axis.title.x, y = axis.title.y, fill = legend.name) +<br>      theme_linedraw() + <br>      coord_trans(expand = <span class="hljs-number">0</span>) + <br>      theme(panel.background= element_rect (<span class="hljs-string">&#x27;gray&#x27;</span>,), panel.grid = element_blank())<br>    <span class="hljs-built_in">return</span>(P)<br>&#125;<br><br>Heatmape(data, <span class="hljs-string">&quot;X&quot;</span>, <span class="hljs-string">&quot;Y&quot;</span>, <span class="hljs-string">&quot;Z&quot;</span>) + ggtitle(<span class="hljs-string">&quot;With default scale&quot;</span>)<br>Heatmape(data, <span class="hljs-string">&quot;X&quot;</span>, <span class="hljs-string">&quot;Y&quot;</span>, <span class="hljs-string">&quot;Z&quot;</span>, minpoint=<span class="hljs-number">1.5</span>, midpoint =<span class="hljs-number">2</span>, maxpoint=<span class="hljs-number">2.5</span>) + ggtitle(<span class="hljs-string">&quot;given value range&quot;</span>)<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/kgMkccx.png" alt=""></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Heatmap with GGplot</summary>
    
    
    
    <category term="R" scheme="https://karobben.github.io/categories/R/"/>
    
    <category term="Plot" scheme="https://karobben.github.io/categories/R/Plot/"/>
    
    <category term="GGPLOT" scheme="https://karobben.github.io/categories/R/Plot/GGPLOT/"/>
    
    
    <category term="R" scheme="https://karobben.github.io/tags/R/"/>
    
    <category term="ggplot" scheme="https://karobben.github.io/tags/ggplot/"/>
    
  </entry>
  
  <entry>
    <title>GGplot: Prism style</title>
    <link href="https://karobben.github.io/2024/12/15/R/ggplot-prismStyle/"/>
    <id>https://karobben.github.io/2024/12/15/R/ggplot-prismStyle/</id>
    <published>2024-12-15T06:07:28.000Z</published>
    <updated>2024-12-19T00:47:27.075Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Barplot">Barplot</h2><h3 id="Data-Prepare">Data Prepare</h3><p>This code would use the inner data set <code>chickwts</code> as example to calculate the mean value and sd value to use as bar height and error bar</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(dplyr)<br><br>result &lt;- chickwts %&gt;%<br>  group_by(feed) %&gt;%<br>  summarise(mean = mean(weight), sd = sd(weight))<br></code></pre></td></tr></table></figure></div><p>Data <code>chickwts</code>:</p><pre>  weight      feed1    179 horsebean2    160 horsebean3    136 horsebean4    227 horsebean</pre><p>Data frame after converted:</p><pre># A tibble: 6 × 3  feed       mean    sd  <fct>     <dbl> <dbl>1 casein     324.  64.42 horsebean  160.  38.63 linseed    219.  52.24 meatmeal   277.  64.95 soybean    246.  54.16 sunflower  329.  48.8</pre><h3 id="Plot-the-Plot-and-Define-the-Theme">Plot the Plot and Define the Theme</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(ggplot2)<br><br>Prim_bar &lt;- <span class="hljs-keyword">function</span>(p)&#123;<br>  P &lt;- p + theme(panel.background = element_blank(),<br>          axis.line = element_line(size = <span class="hljs-number">1</span>),<br>          axis.ticks = element_line(colour = <span class="hljs-string">&quot;black&quot;</span>, size = <span class="hljs-number">1</span>),<br>          axis.ticks.length = unit(<span class="hljs-number">.25</span>, <span class="hljs-string">&#x27;cm&#x27;</span>),<br>          axis.text = element_text(size = <span class="hljs-number">15</span>),<br>          axis.text.x = element_text(angle = <span class="hljs-number">45</span>, vjust = <span class="hljs-number">1</span>, hjust = <span class="hljs-number">1</span>),<br>          axis.title = element_text(size = <span class="hljs-number">20</span>),<br>          plot.title = element_text(hjust = <span class="hljs-number">.5</span>, size = <span class="hljs-number">25</span>)) +<br>  scale_y_continuous(expand = <span class="hljs-built_in">c</span>(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0</span>)) +<br>  ggtitle(<span class="hljs-string">&#x27;plot&#x27;</span>)<br>  <span class="hljs-built_in">return</span>(P)<br>&#125;<br><br>p &lt;- ggplot(result, aes(feed, mean)) +<br>    geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = <span class="hljs-number">.3</span>, size = <span class="hljs-number">1</span>) +<br>geom_bar(stat = <span class="hljs-string">&#x27;identity&#x27;</span>, color = <span class="hljs-string">&#x27;black&#x27;</span>, size = <span class="hljs-number">1</span>, width = <span class="hljs-number">.6</span>, fill = <span class="hljs-string">&#x27;Gainsboro&#x27;</span>)<br><br>Prim_bar(p)<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/omoKjMU.png" alt="Apply the Prism Themes to ggplot"></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">GGplot: Prism style</summary>
    
    
    
    <category term="R" scheme="https://karobben.github.io/categories/R/"/>
    
    <category term="Plot" scheme="https://karobben.github.io/categories/R/Plot/"/>
    
    <category term="GGPLOT" scheme="https://karobben.github.io/categories/R/Plot/GGPLOT/"/>
    
    
    <category term="R" scheme="https://karobben.github.io/tags/R/"/>
    
    <category term="Plot" scheme="https://karobben.github.io/tags/Plot/"/>
    
    <category term="ggplot" scheme="https://karobben.github.io/tags/ggplot/"/>
    
  </entry>
  
  <entry>
    <title>OpenMM, Molecular Dynamic Simulation</title>
    <link href="https://karobben.github.io/2024/12/08/Bioinfor/openMM/"/>
    <id>https://karobben.github.io/2024/12/08/Bioinfor/openMM/</id>
    <published>2024-12-08T17:48:03.000Z</published>
    <updated>2024-12-19T02:29:45.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Install">Install</h2><p>More detailed installations: <a href="http://docs.openmm.org/latest/userguide/application/01_getting_started.html">OpenMM User Guide</a></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">conda create -n openmm python=3.9 -y<br>conda activate openmm<br>conda install -c conda-forge openmm<br></code></pre></td></tr></table></figure></div><p><strong>Test the installation</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">python -m openmm.testInstallation<br></code></pre></td></tr></table></figure></div><pre>OpenMM Version: 8.2Git Revision: 53770948682c40bd460b39830d4e0f0fd3a4b868There are 4 Platforms available:1 Reference - Successfully computed forces2 CPU - Successfully computed forces3 CUDA - Successfully computed forces1 warning generated.1 warning generated.4 OpenCL - Successfully computed forcesMedian difference in forces between platforms:Reference vs. CPU: 6.29538e-06Reference vs. CUDA: 6.75176e-06CPU vs. CUDA: 7.49106e-07Reference vs. OpenCL: 6.75018e-06CPU vs. OpenCL: 7.64529e-07CUDA vs. OpenCL: 1.757e-07All differences are within tolerance.</pre><h2 id="Use-the-GPU-in-Simulation">Use the GPU in Simulation</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> openmm <span class="hljs-keyword">import</span> Platform<br><br><span class="hljs-comment"># define the platform</span><br>platform = Platform.getPlatformByName(<span class="hljs-string">&#x27;CUDA&#x27;</span>)<br>properties = &#123;<span class="hljs-string">&#x27;DeviceIndex&#x27;</span>: <span class="hljs-string">&#x27;1&#x27;</span>, <span class="hljs-string">&#x27;Precision&#x27;</span>: <span class="hljs-string">&#x27;mixed&#x27;</span>&#125; <br><br><span class="hljs-comment"># add the parameter in simulation</span><br>simulation = app.Simulation(pdb.topology, system, integrator, platform, properties)<br></code></pre></td></tr></table></figure></div><p>To be notice: Evene though, you implied that the GPU parameter in the code, it still heavily relies on the CPU.</p><h2 id="PDB-fix">PDB fix</h2><p>Before you run the simulation, you may need to fix the PDB first.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> openmm <span class="hljs-keyword">import</span> app<br><span class="hljs-keyword">from</span> openmm.unit <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> pdbfixer <span class="hljs-keyword">import</span> PDBFixer<br><span class="hljs-keyword">from</span> openmm.app <span class="hljs-keyword">import</span> PDBFile<br><br><br><span class="hljs-comment"># repair the PDB</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">PDB_fix</span>(<span class="hljs-params">INPUT, OUPUT</span>):</span><br>    fixer = PDBFixer(filename=INPUT)<br>    fixer.findMissingResidues()<br>    fixer.findNonstandardResidues()<br>    fixer.replaceNonstandardResidues()<br>    fixer.findMissingAtoms()<br>    fixer.addMissingAtoms()<br>    fixer.addMissingHydrogens(<span class="hljs-number">7.0</span>)<br>    <span class="hljs-comment">#fixer.addSolvent(fixer.topology.getUnitCellDimensions())</span><br>    <span class="hljs-comment"># Remove problematic water molecules and add correct TIP3P water</span><br>    fixer.removeHeterogens(keepWater=<span class="hljs-literal">True</span>)<br>    PDBFile.writeFile(fixer.topology, fixer.positions, <span class="hljs-built_in">open</span>(OUPUT, <span class="hljs-string">&#x27;w&#x27;</span>))<br><br>PDB_fix(<span class="hljs-string">&#x27;best.pdb&#x27;</span>, <span class="hljs-string">&#x27;best_fix.pdb&#x27;</span>)<br></code></pre></td></tr></table></figure></div><h2 id="Anchor-a-protein">Anchor a protein</h2><p><mark>Code was not tested because my protein has hydrophobic surface and it would crush in the water environment</mark></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Anchor protein 1 by restraining its atoms</span><br>anchor_force = mm.CustomExternalForce(<span class="hljs-string">&#x27;0.5 * k * (x^2 + y^2 + z^2)&#x27;</span>)<br>anchor_force.addPerParticleParameter(<span class="hljs-string">&#x27;k&#x27;</span>)<br><br><span class="hljs-comment"># Add position restraints to protein 1</span><br><span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> pdb.topology.atoms():<br>    <span class="hljs-keyword">if</span> atom.residue.chain.<span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>]:  <span class="hljs-comment"># Assume protein 1 is in chain A</span><br>        anchor_force.addParticle(atom.index, [<span class="hljs-number">1000</span>])  <span class="hljs-comment"># High force constant</span><br><br>system.addForce(anchor_force)<br></code></pre></td></tr></table></figure></div><h2 id="Pull-Protein-Force-Apply">Pull Protein Force Apply</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Apply a pulling force to protein 2</span><br>pulling_force = mm.CustomExternalForce(<span class="hljs-string">&#x27;k_pull * (x - x0)^2&#x27;</span>)<br>pulling_force.addPerParticleParameter(<span class="hljs-string">&#x27;k_pull&#x27;</span>)<br>pulling_force.addPerParticleParameter(<span class="hljs-string">&#x27;x0&#x27;</span>)<br><br><span class="hljs-comment"># Add pulling force to atoms of protein 2 (e.g., chain B)</span><br><span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> pdb.topology.atoms():<br>    <span class="hljs-keyword">if</span> atom.residue.chain.<span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>]:  <span class="hljs-comment"># Assume protein 2 is in chain B</span><br>        pulling_force.addParticle(atom.index, [-<span class="hljs-number">1</span>, <span class="hljs-number">1.0</span>])  <span class="hljs-comment"># Adjust constants</span><br><br>system.addForce(pulling_force)<br></code></pre></td></tr></table></figure></div><h2 id="Change-Record">Change Record</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">simulation.reporters.append(app.StateDataReporter(<br>    <span class="hljs-string">&#x27;best_fix_sld_tr.csv&#x27;</span>, <span class="hljs-number">10</span>, step=<span class="hljs-literal">True</span>, potentialEnergy=<span class="hljs-literal">True</span>, temperature=<span class="hljs-literal">True</span>))<br>simulation.reporters.append(app.PDBReporter(<span class="hljs-string">&#x27;best_fix_sld_tr.pdb&#x27;</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure></div><h2 id="Save-the-structure-as-cif">Save the structure as cif</h2><p>During the simulation, you may wants to add lots of wather molecular. When the number of molecular over than 100,000, pdb format can’t handle it anymore. So you want to save it as <code>cif</code> format.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;best_fix_sld.cif&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    app.PDBxFile.writeFile(modeller.topology, modeller.positions, f)<br></code></pre></td></tr></table></figure></div><h2 id="Save-the-structure-from-the-simulation">Save the structure from the simulation</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># After finishing your MD steps:</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;final_structure.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br></code></pre></td></tr></table></figure></div><h2 id="In-Action-Protein-in-Water">In Action: Protein in Water</h2><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/JnRSG6l.gif" alt=""></td><td style="text-align:left">With the help of openMM toolkit, you can simulate the protein in water easily. Here is the code from the <a href="https://openmm.github.io/openmm-cookbook/latest/notebooks/tutorials/protein_in_water.html">document</a>.</td></tr></tbody></table><p>In the simulation, the main codes is explained:</p><ol><li>read the pdb file with <code>PDBFile</code></li><li>Specify the force filed.</li><li>Clean water and add water as a period box. In this step, you can add the water in the force filed based on the size of the filed. Our you can test small filed. The filed is a period box (replicated infinitely in all directions) which means when the object moves to the end of one side, it will not run out of the box, but coming back from <strong>against face</strong>.</li><li>Setup the integrator:<ul><li><code>forcefield.createSystem</code>:<ul><li><code>modeller.topology</code>: you’ll add you molecular (protein)</li><li><code>nonbondedMethod=PME</code>: <font title='ChatGPT o1' color=gray>specifies how long-range electrostatic interactions. Simply cutting them off at a certain radius can introduce errors. <strong>PME</strong> (Particle Mesh Ewald) uses a combination of direct space calculations (for short distances) and reciprocal space calculations (using fast Fourier transforms) to accurately handle these interactions.</font></li><li><code>nonbondedCutoff=1.0*nanometer</code>: <font title='ChatGPT o1' color=gray>When using a cutoff-based approach (like nonbondedCutoff=1.0*nanometer), the simulation engine directly calculates the vdW (Lennard-Jones) interactions only between pairs of atoms that are within that 1 nm cutoff distance. If two atoms are farther apart than 1 nm, their vdW interactions are not explicitly computed.</font></li></ul></li><li><code>integrator</code>: This is the place to given the value of the Tm and time scale.</li></ul></li><li><code>simulation.minimizeEnergy()</code>: Start to minimize local Energy.</li><li>Setup report: set up the report to record the energy change and save the trajectory.</li><li>Simulate in the <mark>NVT equillibration</mark> and <mark>NPT production MD</mark> condition.<ul><li><code>1*bar</code>: bar is a standard measure of pressure, and 1 bar is approximately equal to atmospheric pressure at sea level.</li></ul></li></ol><div class="admonition note"><p class="admonition-title">What is NVT and NPT?</p><p><font title='ChatGPT o1' color=gray>In the NVT (constant Number of particles, Volume, and Temperature) ensemble, the system is thermally equilibrated at a fixed volume to achieve a stable temperature distribution. This step ensures that any initial structural distortions and non-equilibrium distributions of kinetic energy dissipate, providing a well-relaxed starting point. Following NVT equilibration, the system is often subjected to an NPT (constant Number of particles, Pressure, and Temperature) ensemble, where both temperature and pressure are maintained constant. This allows the simulation box volume to fluctuate to the pressure target, enabling the system’s density and structure to equilibrate under more experimentally relevant conditions. The transition from NVT to NPT thus facilitates a smooth pathway from initial equilibration to realistic production conditions, offering a balanced and physically representative environment for subsequent analyses of structural, thermodynamic, and dynamic properties.</font></p></div><table><thead><tr><th>Feature</th><th>NVT Equilibration</th><th>NPT Production MD</th></tr></thead><tbody><tr><td>Ensemble</td><td>Canonical (NVT)</td><td>Isothermal–Isobaric (NPT)</td></tr><tr><td>Variables Held Fixed</td><td>Number of particles (N), Volume (V), Temperature (T)</td><td>Number of particles (N), Pressure §, Temperature (T)</td></tr><tr><td>Volume Adjustment</td><td>Fixed volume</td><td>Volume fluctuates to maintain target pressure</td></tr><tr><td>Pressure Control</td><td>Not controlled, can fluctuate</td><td>Actively controlled via a barostat</td></tr><tr><td>Typical Use</td><td>Initial temperature equilibration after energy minimization</td><td>Production runs to simulate conditions resembling experimental environments</td></tr><tr><td>Realism</td><td>Less physically representative of ambient conditions (volume fixed)</td><td>More realistic: system adapts to pressure, resulting in stable density</td></tr><tr><td>Common Duration</td><td>Shorter (tens to hundreds of picoseconds)</td><td>Longer (nanoseconds to microseconds) for data collection</td></tr><tr><td>Outcome</td><td>Thermally equilibrated structure at given T</td><td>Equilibrium structure and dynamics at given P and T, suitable for analysis</td></tr></tbody></table><h2 id="Protein-Relaxation-Test">Protein Relaxation Test</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sys <span class="hljs-keyword">import</span> stdout<br><span class="hljs-keyword">from</span> openmm.app <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> openmm <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> openmm.unit <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># Input files</span><br>pdb_filename = <span class="hljs-string">&#x27;best_fix.pdb&#x27;</span>  <span class="hljs-comment"># Your starting protein structure (from cryo-EM)</span><br>forcefield_files = [<span class="hljs-string">&#x27;amber14-all.xml&#x27;</span>, <span class="hljs-string">&#x27;amber14/tip3pfb.xml&#x27;</span>]  <span class="hljs-comment"># Force fields</span><br>ionic_strength = <span class="hljs-number">0.15</span>*molar<br><br><span class="hljs-comment"># Load the PDB</span><br>pdb = PDBFile(pdb_filename)<br><br><span class="hljs-comment"># Create a forcefield object</span><br>forcefield = ForceField(*forcefield_files)<br><br><span class="hljs-comment"># Create a model of the system with solvent</span><br><span class="hljs-comment"># Add a water box around the protein (10 Å padding)</span><br>modeller = Modeller(pdb.topology, pdb.positions)<br><span class="hljs-comment">#modeller.addSolvent(forcefield, model=&#x27;tip3p&#x27;, boxSize=Vec3(10,10,20)*nanometer, ionicStrength=ionic_strength)</span><br>modeller.addSolvent(forcefield, padding=<span class="hljs-number">1.0</span>*nanometer, ionicStrength=ionic_strength)<br><br><span class="hljs-comment"># Create the system</span><br>system = forcefield.createSystem(<br>    modeller.topology,<br>    nonbondedMethod=PME,<br>    nonbondedCutoff=<span class="hljs-number">1.0</span>*nanometer,<br>    constraints=HBonds,<br>    hydrogenMass=<span class="hljs-number">4</span>*amu<br>)<br><br><span class="hljs-comment"># Add a thermostat and barostat for later (NPT)</span><br>temperature = <span class="hljs-number">300</span>*kelvin<br>pressure = <span class="hljs-number">1</span>*bar<br>friction = <span class="hljs-number">1</span>/picosecond<br>timestep = <span class="hljs-number">0.002</span>*picoseconds<br><br>system.addForce(MonteCarloBarostat(pressure, temperature))<br><br><span class="hljs-comment"># Create integrator (for equilibration and production)</span><br>integrator = LangevinIntegrator(temperature, friction, timestep)<br><br><span class="hljs-comment"># Create simulation object</span><br>platform = Platform.getPlatformByName(<span class="hljs-string">&#x27;CUDA&#x27;</span>)  <span class="hljs-comment"># or &#x27;CUDA&#x27;/&#x27;OpenCL&#x27; if available</span><br>simulation = Simulation(modeller.topology, system, integrator, platform)<br>simulation.context.setPositions(modeller.positions)<br><br><span class="hljs-comment"># Minimization</span><br>print(<span class="hljs-string">&quot;Minimizing...&quot;</span>)<br>simulation.minimizeEnergy(maxIterations=<span class="hljs-number">1000</span>)<br><br><span class="hljs-comment"># save the last structure from simulation</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_mini.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br>    <br><span class="hljs-comment"># NVT Equilibration: Remove barostat and fix volume for initial temp equilibration</span><br><span class="hljs-comment"># (Optional step: you can also start directly with NPT if you prefer)</span><br>forces = &#123; force.__class__.__name__: force <span class="hljs-keyword">for</span> force <span class="hljs-keyword">in</span> system.getForces() &#125;<br>system.removeForce(<span class="hljs-built_in">list</span>(forces.keys()).index(<span class="hljs-string">&#x27;MonteCarloBarostat&#x27;</span>))<br>simulation.context.reinitialize(preserveState=<span class="hljs-literal">True</span>)<br><br>simulation.context.setVelocitiesToTemperature(temperature)<br>print(<span class="hljs-string">&quot;Equilibrating under NVT conditions...&quot;</span>)<br>simulation.reporters.append(StateDataReporter(stdout, <span class="hljs-number">1000</span>, step=<span class="hljs-literal">True</span>, potentialEnergy=<span class="hljs-literal">True</span>, temperature=<span class="hljs-literal">True</span>))<br>simulation.reporters.append(DCDReporter(<span class="hljs-string">&#x27;relax.dcd&#x27;</span>, <span class="hljs-number">1000</span>))  <span class="hljs-comment"># Save a frame every 1000 steps</span><br><br>print(<span class="hljs-string">&#x27;start simulation&#x27;</span>)<br>simulation.step(<span class="hljs-number">50000</span>)  <span class="hljs-comment"># ~100 ps of NVT equilibration (adjust as needed)</span><br><br><span class="hljs-comment"># save the last structure from simulation</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_NVT.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br><br><span class="hljs-comment"># Re-introduce NPT conditions (barostat)</span><br>system.addForce(MonteCarloBarostat(pressure, temperature))<br>simulation.context.reinitialize(preserveState=<span class="hljs-literal">True</span>)<br><br>print(<span class="hljs-string">&quot;Equilibrating under NPT conditions...&quot;</span>)<br><span class="hljs-comment"># Remove old reporters and add a new one</span><br>simulation.step(<span class="hljs-number">50000</span>)  <span class="hljs-comment"># Another ~100 ps (adjust as needed)</span><br><br><span class="hljs-comment"># save the last structure from simulation</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_NPT.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br><br><span class="hljs-comment"># Now we have an equilibrated system at NPT.</span><br><span class="hljs-comment"># This is where you might start your production run.</span><br><br>production_steps = <span class="hljs-number">250000</span>  <span class="hljs-comment"># ~500 ps of production (adjust as needed)</span><br><span class="hljs-comment">#simulation.reporters.append(PDBReporter(&#x27;output_production.pdb&#x27;, 5000)) # Save frames every 10 ps</span><br>simulation.reporters.append(StateDataReporter(<span class="hljs-string">&#x27;production_log.csv&#x27;</span>, <span class="hljs-number">1000</span>, step=<span class="hljs-literal">True</span>, time=<span class="hljs-literal">True</span>, potentialEnergy=<span class="hljs-literal">True</span>,<br>                                               kineticEnergy=<span class="hljs-literal">True</span>, totalEnergy=<span class="hljs-literal">True</span>, temperature=<span class="hljs-literal">True</span>,<br>                                               volume=<span class="hljs-literal">True</span>, density=<span class="hljs-literal">True</span>))<br><br>print(<span class="hljs-string">&quot;Running Production MD...&quot;</span>)<br>simulation.step(production_steps)<br>print(<span class="hljs-string">&quot;Done!&quot;</span>)<br><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_final.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br></code></pre></td></tr></table></figure></div><p>In the final simulation, your protein may on the “edge” of the box. So, we need to adjust the relative position of the protein</p><table><thead><tr><th style="text-align:center">Before Recenter</th><th style="text-align:center">After Recenter</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/1obBq2B.png" alt=""></td><td style="text-align:center"><img src="https://imgur.com/kyKbB8I.png" alt=""></td></tr></tbody></table><h2 id="Recenter-the-Protein">Recenter the Protein</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mdtraj <span class="hljs-keyword">as</span> md<br><br><span class="hljs-comment"># Load the trajectory and topology</span><br>traj = md.load(<span class="hljs-string">&#x27;relax.dcd&#x27;</span>, top=<span class="hljs-string">&#x27;relax_test_mini.cif&#x27;</span>)<br>traj = traj.image_molecules()<br><br><span class="hljs-comment"># Re-center coordinates so the protein is centered in the box</span><br>centered_traj = traj.center_coordinates()<br><span class="hljs-comment"># Save the re-centered trajectory as a multi-model PDB (one MODEL per frame)</span><br><span class="hljs-comment">#centered_traj.save_pdb(&#x27;centered_system.pdb&#x27;)</span><br>centered_traj.save_dcd(<span class="hljs-string">&#x27;centered_system.dcd&#x27;</span>)<br></code></pre></td></tr></table></figure></div><h2 id="Trouble-Shot">Trouble Shot</h2><h3 id="No-template-found-for-residue-30730-HOH">No template found for residue 30730 (HOH)</h3><p>Citation: <a href=""></a><br>Error Code:</p><pre>ValueError: No template found for residue 30730 (HOH).  The set of atoms matches HOH, but the bonds are different.</pre><p>Bug reason: <a href="https://github.com/openmm/openmm/issues/3393">The PDB format doesn’t support models with more than 100,000 atoms.</a></p><p>How to solve: save the output as <code>cif</code> format by using <code>PDBxFile</code></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="DIFF"><figure class="iseeu highlight /diff"><table><tr><td class="code"><pre><code class="hljs diff"><span class="hljs-deletion">- with open(&#x27;best_fix_sld.pdb&#x27;, &#x27;w&#x27;) as f:</span><br><span class="hljs-addition">+ with open(&#x27;best_fix_sld.cif&#x27;, &#x27;w&#x27;) as f:</span><br><span class="hljs-deletion">-    app.PDBFile.writeFile(modeller.topology, modeller.positions, f)</span><br><span class="hljs-addition">+    app.PDBxFile.writeFile(modeller.topology, modeller.positions, f)</span><br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">OpenMM, Molecular Dynamic Simulation</summary>
    
    
    
    <category term="Python" scheme="https://karobben.github.io/categories/Python/"/>
    
    <category term="Bio" scheme="https://karobben.github.io/categories/Python/Bio/"/>
    
    
    <category term="openMM" scheme="https://karobben.github.io/tags/openMM/"/>
    
    <category term="Molecular Dynamic Simulation" scheme="https://karobben.github.io/tags/Molecular-Dynamic-Simulation/"/>
    
  </entry>
  
  <entry>
    <title>HDF5 Data Format Introduction</title>
    <link href="https://karobben.github.io/2024/10/23/AI/hdf5/"/>
    <id>https://karobben.github.io/2024/10/23/AI/hdf5/</id>
    <published>2024-10-24T02:35:09.000Z</published>
    <updated>2024-10-30T17:49:23.621Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Structure-of-hdf5">Structure of hdf5</h2><p><mark>Key Features of HDF5</mark>:</p><ol><li>Hierarchical Structure: HDF5 files are organized like a file system, with “groups” that act like directories and “datasets” that act like files. This allows for complex, hierarchical data storage.</li><li>Efficient Storage: HDF5 is optimized for storing and retrieving large datasets. It uses compression techniques (like GZIP or SZIP) to reduce file size without losing data.</li><li>Cross-platform Compatibility: The format is portable across different platforms and operating systems, meaning that HDF5 files can be used on Windows, macOS, Linux, etc.</li><li>Self-describing Format: HDF5 files include metadata that describe the contents of the file. This makes it easy to understand the data structure without additional documentation.</li><li>Multidimensional Data: HDF5 supports storing complex, multidimensional data (such as arrays, tables, images, etc.).</li><li>Supports Many Data Types: It can store data in various types, such as integers, floats, strings, and more.</li></ol><pre>/root                (Group)    /experiment1     (Group)        /data        (Dataset)        /info        (Dataset)    /experiment2     (Group)        /data        (Dataset)        /info        (Dataset)</pre><p>Use Cases:</p><ul><li><strong>Scientific Data</strong>: For example, storing results from simulations, satellite data, or genome sequences.</li><li><strong>Machine Learning</strong>: Large training datasets can be stored in HDF5 format for efficient access during training.</li><li><strong>Image Storage</strong>: Storing large collections of images or medical imaging data (e.g., MRI scans).</li></ul><h2 id="Show-all-Names-of-Groups-and-Data">Show all Names of Groups and Data</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> h5py<br><br><span class="hljs-comment"># Open the file in read mode</span><br><span class="hljs-keyword">with</span> h5py.File(<span class="hljs-string">&#x27;file1.h5&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">print_hdf5_structure</span>(<span class="hljs-params">group, indent=<span class="hljs-number">0</span></span>):</span><br>        <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> group.keys():<br>            item = group[key]<br>            print(<span class="hljs-string">&quot;  &quot;</span> * indent + <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;key&#125;</span>: <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(item)&#125;</span>&quot;</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(item, h5py.Group):<br>                print_hdf5_structure(item, indent + <span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># Print structure from the root</span><br>    print_hdf5_structure(f)<br></code></pre></td></tr></table></figure></div><h2 id="How-to-Merge-Multiple-hdf5-Files">How to Merge Multiple hdf5 Files</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> h5py<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Function to recursively copy/merge the structure and data from source_group to target_group</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">copy_and_merge</span>(<span class="hljs-params">source_group, target_group</span>):</span><br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> source_group.keys():<br>        item = source_group[key]<br>        <span class="hljs-comment"># If the item is a group, we create the same group in the target and copy its contents</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(item, h5py.Group):<br>            <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> target_group:<br>                target_group.create_group(key)<br>            copy_and_merge(item, target_group[key])  <span class="hljs-comment"># Recursive call to merge the group&#x27;s contents</span><br>        <span class="hljs-comment"># If the item is a dataset, we merge it</span><br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(item, h5py.Dataset):<br>            <span class="hljs-comment"># If the dataset doesn&#x27;t exist in the target file, copy it</span><br>            <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> target_group:<br>                target_group.create_dataset(key, data=item[:])<br>            <span class="hljs-comment"># If the dataset exists, concatenate the data along the first axis</span><br>            <span class="hljs-keyword">else</span>:<br>                existing_data = target_group[key][:]<br>                new_data = item[:]<br>                <span class="hljs-comment"># Concatenate datasets along the first axis</span><br>                merged_data = np.concatenate((existing_data, new_data), axis=<span class="hljs-number">0</span>)<br>                <span class="hljs-comment"># Delete the old dataset and replace it with the merged one</span><br>                <span class="hljs-keyword">del</span> target_group[key]<br>                target_group.create_dataset(key, data=merged_data)<br><br><span class="hljs-comment"># Function to merge multiple HDF5 files and save the result to a new file</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge_multiple_hdf5</span>(<span class="hljs-params">files, output_file</span>):</span><br>    <span class="hljs-comment"># Create a new HDF5 file to store the merged result</span><br>    <span class="hljs-keyword">with</span> h5py.File(output_file, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> target_file:<br>        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:<br>            <span class="hljs-keyword">with</span> h5py.File(file, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> source_file:<br>                <span class="hljs-comment"># Merge the contents of each source file into the target file</span><br>                copy_and_merge(source_file, target_file)<br>        print(<span class="hljs-string">f&quot;All files have been merged into <span class="hljs-subst">&#123;output_file&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># List of HDF5 files to be merged</span><br>files_to_merge = [<span class="hljs-string">&#x27;file1.h5&#x27;</span>, <span class="hljs-string">&#x27;file2.h5&#x27;</span>, <span class="hljs-string">&#x27;file3.h5&#x27;</span>]  <span class="hljs-comment"># Add as many files as needed</span><br><span class="hljs-comment"># Specify the output file where the merged data will be saved</span><br>output_file = <span class="hljs-string">&#x27;merged_output.h5&#x27;</span><br><span class="hljs-comment"># Merge the files and save the result</span><br>merge_multiple_hdf5(files_to_merge, output_file)<br></code></pre></td></tr></table></figure></div><p>Explanation of the Code:</p><ol><li><strong><code>copy_and_merge</code> function</strong> remains the same, recursively merging groups and datasets from the source to the target.</li><li><strong><code>merge_multiple_hdf5</code> function</strong>:<ul><li>Accepts a list of HDF5 files (<code>files</code>) and an <code>output_file</code> name.</li><li>It creates a new HDF5 file (<code>output_file</code>) in <strong>write mode</strong> (<code>'w'</code>).</li><li>It loops through each file in the list, opens it in <strong>read mode</strong> (<code>'r'</code>), and calls the <code>copy_and_merge</code> function to copy the contents into the newly created file.</li><li>After all files are merged, it saves the result as <code>output_file</code>.</li></ul></li></ol><p>!!! note Key Points:<br>- Each dataset is merged by <strong>concatenating along the first axis</strong>. If you need to merge along a different axis or have more complex merging rules, we can adjust the code.<br>- Make sure the datasets you’re merging are compatible (same dimensionality along non-concatenated axes).</p><h2 id="Change-the-Group-Names">Change the Group Names</h2><p>To rename a group in an HDF5 file using <code>h5py</code>, you can’t directly change the group’s name. Instead, you can <strong>copy the group to a new group with the desired name</strong>, and then <strong>delete the original group</strong>.</p><p>Here’s how you can rename the group “4skj” to “4skj_10086”:</p><h2 id="Step-by-Step-Code">Step-by-Step Code:</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> h5py<br><span class="hljs-keyword">import</span> shutil<br><br><span class="hljs-comment"># Function to rename a group in an HDF5 file</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rename_group</span>(<span class="hljs-params">hdf5_file, old_group_name, new_group_name</span>):</span><br>    <span class="hljs-comment"># Open the file in read/write mode</span><br>    <span class="hljs-keyword">with</span> h5py.File(hdf5_file, <span class="hljs-string">&#x27;r+&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-comment"># Check if the group exists</span><br>        <span class="hljs-keyword">if</span> old_group_name <span class="hljs-keyword">in</span> f:<br>            <span class="hljs-comment"># Copy the old group to the new group</span><br>            f.copy(old_group_name, new_group_name)<br>            <span class="hljs-comment"># Delete the old group</span><br>            <span class="hljs-keyword">del</span> f[old_group_name]<br>            print(<span class="hljs-string">f&quot;Group &#x27;<span class="hljs-subst">&#123;old_group_name&#125;</span>&#x27; has been renamed to &#x27;<span class="hljs-subst">&#123;new_group_name&#125;</span>&#x27;&quot;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            print(<span class="hljs-string">f&quot;Group &#x27;<span class="hljs-subst">&#123;old_group_name&#125;</span>&#x27; does not exist in the file.&quot;</span>)<br><br><span class="hljs-comment"># Rename the group in the HDF5 file</span><br>hdf5_file = <span class="hljs-string">&#x27;file1.h5&#x27;</span>  <span class="hljs-comment"># Replace with your actual file path</span><br>old_group_name = <span class="hljs-string">&#x27;4skj&#x27;</span>  <span class="hljs-comment"># Original group name</span><br>new_group_name = <span class="hljs-string">&#x27;4skj_10086&#x27;</span>  <span class="hljs-comment"># New group name</span><br><br>rename_group(hdf5_file, old_group_name, new_group_name)<br></code></pre></td></tr></table></figure></div><ol><li><strong>Check if the group exists</strong>: The script checks if the group <code>&quot;4skj&quot;</code> exists in the HDF5 file.</li><li><strong>Copy the group</strong>: It uses the <code>f.copy()</code> function to copy the group and its contents to a new group with the desired name (<code>&quot;4skj_10086&quot;</code>).</li><li><strong>Delete the old group</strong>: After copying, the original group is deleted with <code>del f[old_group_name]</code>.</li><li><strong>Save changes</strong>: Since the file is opened in <code>'r+'</code> mode (read/write), all changes are saved automatically.</li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">HDF5 (Hierarchical Data Format version 5) is a file format designed for efficiently storing and organizing large, complex datasets. It uses a hierarchical structure of **groups** (like directories) and **datasets** (like files) to store data, supporting multidimensional arrays, metadata, and a wide variety of data types. Key advantages include **compression**, **cross-platform compatibility**, and the ability to handle large datasets that don’t fit in memory. It’s widely used in fields like scientific computing, machine learning, and bioinformatics due to its efficiency and flexibility.</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="Data Format" scheme="https://karobben.github.io/categories/Machine-Learning/Data-Format/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data" scheme="https://karobben.github.io/tags/Data/"/>
    
    <category term="AI" scheme="https://karobben.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Render Your Protein in Blender with Molecular Nodes</title>
    <link href="https://karobben.github.io/2024/10/19/Bioinfor/blender-molecular-nodes/"/>
    <id>https://karobben.github.io/2024/10/19/Bioinfor/blender-molecular-nodes/</id>
    <published>2024-10-19T14:53:38.000Z</published>
    <updated>2024-11-02T22:56:43.703Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Who-to-Install-Molecular-Nodes-for-Blender">Who to Install Molecular Nodes for Blender</h2><p>First, you should download Blender yourself. Instead of the latest version, opt for a stable version because the newest release may have bugs or be incompatible with Molecular Nodes. I tried version 4.40, but when I changed the style of the molecule to Ribbon or another style, Blender crashed and closed itself. Then, I switched to version 4.2.2, and it worked fine.</p><p>As following the figure “Install the Extension”, you can find this plugin and install it. Once you down installation, you can find there is some thing new pops up like its show in figure “Update in Scene”. In this new module, you could download the pdb online or when you have pdb in the “Cache Downloads” directory, you could also load it with “Molecular Nodes”. When you load the molecular, it looks terrible. You need to follow the figure “Render By Cycles” and “Start Render” to get a normal view of molecular(“Atoms View”).</p><p><img src="https://imgur.com/uCbxiP9.png" alt="Install the Extension"></p><p><img src="https://imgur.com/9fEK7wf.png" alt="Update in Scene"></p><p><img src="https://imgur.com/ehHyKKP.png" alt="Render By Cycles"><br><img src="https://imgur.com/1DMegwb.png" alt="Start Render"><br><img src="https://imgur.com/LoFewhU.png" alt="Atoms View"></p><h2 id="Add-a-Pure-Perfect-Background">Add a Pure Perfect Background</h2><p>Source: <a href="https://www.youtube.com/watch?v=aegiN7XeLow">YouTube: EMPossible</a></p><p><img src="https://imgur.com/Wl0ea71.png" alt="Pure White Background"></p><p>How to set:</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/UndFMUw.png" alt="Set pure background"></td><td style="text-align:left">select <li>“Render” → “Film” → “Transparent”<li>“Render” → “Color Management” → “View Transform” → “Starndard”</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/jxy0bGJ.png" alt="Set Compositing"></td><td style="text-align:left">Set the Compositing. And that’s it. Go to rendering and it woud add an Perfectwhite at the background</td></tr></tbody></table><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># codes for set a pure white background</span><br><span class="hljs-comment"># Those codes only working on the first few steps.</span><br><span class="hljs-comment"># I didn&#x27;t figure how to use script to make the Geometry Nodes </span><br><span class="hljs-comment"># So, After you run those 3 commands, you still need to started from step 3 in the second pictures to manually finish the Geometry Nodes setting.</span><br>bpy.context.scene.render.engine = <span class="hljs-string">&#x27;CYCLES&#x27;</span><br>bpy.context.scene.cycles.device = <span class="hljs-string">&#x27;GPU&#x27;</span><br>bpy.context.space_data.shading.<span class="hljs-built_in">type</span> = <span class="hljs-string">&#x27;RENDERED&#x27;</span><br><br>bpy.context.<span class="hljs-built_in">object</span>.scale[<span class="hljs-number">0</span>] = <span class="hljs-number">3.5</span><br>bpy.context.<span class="hljs-built_in">object</span>.scale[<span class="hljs-number">1</span>] = <span class="hljs-number">3.5</span><br>bpy.context.<span class="hljs-built_in">object</span>.scale[<span class="hljs-number">2</span>] = <span class="hljs-number">3.5</span><br><br><br>bpy.context.scene.render.film_transparent = <span class="hljs-literal">True</span><br>bpy.context.scene.view_settings.view_transform = <span class="hljs-string">&#x27;Standard&#x27;</span><br>bpy.context.scene.use_nodes = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure></div><h2 id="Different-Colors-in-a-Surface">Different Colors in a Surface</h2><table><thead><tr><th style="text-align:left"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left"><img src="https://imgur.com/tTOeKDl.png" alt="Settings in Geomitry"></td><td style="text-align:left">The key idea for given different color is by rendern multiple layers of color on the surface. By reverse select residues , we could delete the colors from selected layer and expose the color from inner layer.</td></tr><tr><td style="text-align:left"><img src="https://imgur.com/MGA9Mqk.png" alt="Results"></td><td style="text-align:left">Final resutls show</td></tr></tbody></table><h2 id="Multiple-Style-in-One-Object">Multiple Style in One Object</h2><table><thead><tr><th style="text-align:left"><img src="https://imgur.com/TVEMkDe.png" alt="Blender Join Geometry"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left"><img src="https://imgur.com/AOUUkWK.png" alt=""></td><td style="text-align:left">Like the example in the picture, it rendered both surface model and the stick model in one object. This is achieved by <code>Join Geometry</code></td></tr></tbody></table><h2 id="Customize-the-Color-From-The-Surface">Customize the Color From The Surface</h2><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/QU1xa7K.png" alt=""></th><th style="text-align:center"><img src="https://imgur.com/7rdCxfl.png" alt=""></th></tr></thead><tbody></tbody></table><p>For Customizing the surface color, there are 2 ways to do it.</p><ol><li>using <code>pLDDT</code> nodes from <code>Color</code></li><li>using the <code>Color Attribute Map</code> nodes from <code>Color</code>.</li></ol><p>In both case, they are actually using the same set of value stored in <code>pdb</code> or <code>cif</code> file.<br>In the pdb format show below, the 11th column marked as white is the value for <code>pLDDT</code>. If you want to manage it with <code>Color Attribute Map</code>, the name of it is <code>b_factor</code></p><pre><font size=1>ATOM   4365  C   ASP C 150      17.854  27.766  83.090  1.00 <font color=white>99.42</font>      C    C  ATOM   4366  O   ASP C 150      17.369  28.239  82.038  1.00 <font color=white>95.32</font>      C    O  ATOM   4367  CB  ASP C 150      19.712  26.091  82.521  1.00 <font color=white>98.18</font>      C    C  ATOM   4368  CG  ASP C 150      20.447  24.817  82.987  1.00 <font color=white>96.59</font>      C    C  ATOM   4369  OD1 ASP C 150      20.121  24.255  84.056  1.00 <font color=white>96.78</font>      C    O  ATOM   4370  OD2 ASP C 150      21.402  24.406  82.277  1.00 <font color=white>96.06</font>      C    O1-ATOM   4371  OXT ASP C 150      18.041  28.393  84.184  1.00 <font color=white>95.18</font>      C    O1-</font></pre><h2 id="Watching-List">Watching List</h2><ul class="task-list"><li class="task-list-item"><input type="checkbox" id="cbx_0" disabled="true"><label for="cbx_0"> <a href="https://www.youtube.com/watch?v=sIblmWV0NuM">Select color pallet</a></label></li></ul><h2 id="Trouble-Shoot">Trouble Shoot</h2><h3 id="Dead-Black-in-Transparent">Dead Black in Transparent</h3><table><thead><tr><th>Dead Black</th><th>Change Setting</th><th>After Change</th></tr></thead><tbody><tr><td><img src="https://imgur.com/jCo5bO3.png" alt="Befor Change"></td><td><img src="https://imgur.com/IaT6UlB.png" alt="Change Setting"></td><td><img src="https://imgur.com/Zb3XWZz.png" alt="After Correction"></td></tr></tbody></table><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Render Your Protein in Blender with Molecular Nodes</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Protein Structure" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Protein-Structure/"/>
    
    
    <category term="3D" scheme="https://karobben.github.io/tags/3D/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Biochmistry" scheme="https://karobben.github.io/tags/Biochmistry/"/>
    
    <category term="Plot" scheme="https://karobben.github.io/tags/Plot/"/>
    
  </entry>
  
  <entry>
    <title>NCBI Data Submit with FTP/ASCP</title>
    <link href="https://karobben.github.io/2024/10/17/Bioinfor/ncbisubmit/"/>
    <id>https://karobben.github.io/2024/10/17/Bioinfor/ncbisubmit/</id>
    <published>2024-10-18T04:36:10.000Z</published>
    <updated>2024-10-19T21:44:30.634Z</updated>
    
    <content type="html"><![CDATA[<p>This post only talks about how to use the <code>ascp</code> to upload your sequencing data into NCBI.</p><h2 id="3-Different-Ways-of-Submit-Your-Data">3 Different Ways of Submit Your Data</h2><ul><li>Cloud from <strong>Amazon S3</strong> or <strong>Google Cloud</strong><br>If your data was stored in Amazon/Google Cloud at the beginning, you can easily and safely transfer them into NCBI. (I think so though I’ve never tried).</li><li><strong>FTP</strong> or <strong>ASCP</strong><br>I would recommend the second approach since our data was mostly stored in a Linux server. FTP and ASCP are very reliable. Especially for <code>ftp</code>, it is a very popular protocol. You could find a bunch of software like ‘<a href="https://filezilla-project.org/">FileZilla</a>’ to upload through ftp. The best feature of ‘FileZilla’ is it supports <strong>resume interrupted transfer</strong> and lists the fail-up loaded files so you can upload them again with one click. So, no matter how many and how big the files are, it can help you upload them safely. The main <strong>limitation</strong> for <strong>FileZilla</strong> is that it can’t used in command form and so, is not suitable for the server.</li><li><strong>Web Browser</strong><br>Unless your data are very small, you would never want to try uploading them online.</li></ul><h2 id="When-to-Upload-Your-Data">When to Upload Your Data</h2><p>You can upload your data whenever you want. It is better to upload your file before you start to fill the submission tables. In step 7, you could find the data/director you submitted and include them in the submission. But it seems like the NCBI would delete an inactivated file within 30 days. It is long enough to finish the submission.</p><p>I prefer to use <code>FileZilla</code> to upload my data. But since now, all of my data are on the server and I don’t want to download them again, I give the <code>ftp</code> and use <code>ascp</code>.</p><h2 id="How-to-use-the-ASCP">How to use the ASCP</h2><p><img src="https://imgur.com/Wik8zeG.png" alt=""></p><p>ASCP is very easy and works similarly to <code>scp</code>. In the Submission home page, select <mark>My submissions</mark> → <mark>Upload via Aspera command line or FTP</mark> → <mark>Aspera command line instructions</mark> to find the instructions. It would give you the download link and key for connecting to the NCBI server. After that, use it just like the <code>scp</code>.</p><p>Shortcomings or suggestions for <code>ascp</code></p><ol><li><strong>Keep everything in 1 director</strong>: You’d like to upload all files into one directory. Because during the submission step <strong>7 FILES</strong>, you could only select 1 directory.</li><li><strong>No sub-directories</strong>: You may want to upload the directory without subdirectories. Because in the submission portal, you can’t check files in subdirectories. So, it is hard to track back which filed upload failed.</li><li><strong>Keep ascp log</strong>: In the submission portal, you got only first 5 lines of uploaded files. So, remember to keep the <code>ascp</code> log to record the fail uploaded data.</li><li><mark><strong>upload the data one by one</strong></mark>: Strongly recommend to upload each <code>fq</code> or compressed file one by one with scripts. When you try to upload the entire directory, it may fail (I never get it done when upload a directory)</li><li>After you upload your data, you can’t see them until you go to step <strong>7 FILES</strong> in the submission portal.</li><li>You can’t check them immediately even from the submission portal. It takes time for them to show in the <strong>Step 7</strong></li></ol><p><mark>The good thing is it is easy to write a script to upload your data automatically.</mark></p><div class="admonition note"><p class="admonition-title">In the instructions, it suggest you to use those parameters: `-QT -l100m -k1`</p><ol><li><strong><code>-Q</code></strong>: This option disables the real-time display of progress and transfer statistics during the transfer. Normally, ASCP displays ongoing statistics, such as speed and percentage of completion, but using <code>-Q</code> will suppress this output.</li><li><strong><code>-T</code></strong>: This option disables encryption of the data stream during transfer. ASCP by default uses encryption for data security, but <code>-T</code> turns this off, which might improve transfer speed but at the cost of security.</li><li><strong><code>-l100m</code></strong>: This sets the transfer speed limit to <strong>100 megabits per second</strong>. You can adjust the value (e.g., <code>100m</code>) to control how fast the transfer is allowed to go, helping to prevent network congestion or manage bandwidth usage.</li><li><strong><code>-k1</code></strong>: This option controls file resume behavior. The value <code>1</code> means that if a transfer is interrupted, ASCP will resume from the point where it left off (resumable transfer). The other possible values for <code>-k</code> are:</li></ol><ul><li><code>0</code>: No resume. The transfer restarts from the beginning.</li><li><code>2</code>: Sparse resume. ASCP resumes only the missing parts of the file.</li></ul></div><h2 id="Personal-Experience">Personal Experience</h2><h3 id="Upload-your-data-into-a-specific-directory">Upload your data into a specific directory</h3><p>Though we gave the argument <code>-k1</code>, it could still fail. In the log, it says:</p><pre>Partial Completion: 19711732K bytes transferred in 3695 seconds(43691K bits/sec), in 6 files, 4 directories; 3 files failed.Session Stop  (Error: Disk write failed (server))</pre><p>After you went to the step 7, you could see:<br><img src="https://imgur.com/xxbYXVE.png" alt=""></p><p>Which means 2 directories are empty. In this case, you don’t need to worry too much. You can change the code a little bit and continue to upload could solve this problem.<br>For example, you uploaded a directory named <code>ALL_RNA</code> with code <code>ascp -i $key_file -QT -l100m -k1 -d ALL_RNA $AddressFromInstruction</code>, the data in the directory <code>ALL_RNA/SAMPLEX</code> was failed to upload, you can use the code <code>ascp -i $key_file -QT -l100m -k1 -d ALL_RNA/SAMPLEX $AddressFromInstruction/ALL_RNA</code> to continue upload the directory <code>SAMPLEX</code> into the <code>ALL_RNA</code> in NCBI server</p><pre> ascp -i $key_file -QT -l100m -k1 -d ALL_RNA $AddressFromInstructionascp -i $key_file -QT -l100m -k1 -d ALL_RNA<font color=red>/SAMPLEX</font> $AddressFromInstruction<font color=red>/ALL_RNA</font></pre><h3 id="Upload-your-date-in-the-script">Upload your date in the script</h3><p>When you have lots of data, one of the convenient ways I found is we could <code>ascp</code> each data independently. With a for loop, we could generate all codes into a script. When there are failed uploads, we just need to copy and paste the failed codes and run them again. Or we could also delete the code for the successfully uploaded one and run the entire script again.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> $(find YourDirectories -name <span class="hljs-string">&quot;.fastq.gz&quot;</span>); <span class="hljs-keyword">do</span><br>    <span class="hljs-built_in">echo</span> ascp -i <span class="hljs-variable">$key_file</span> -QT -l100m -k1 -d <span class="hljs-variable">$i</span> <span class="hljs-variable">$AddressFromInstruction</span><br><span class="hljs-keyword">done</span> &gt;&gt; ascp.sh<br><br>bash ascp.sh<br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">NCBI Data Submit</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Database" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Database/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Database" scheme="https://karobben.github.io/tags/Database/"/>
    
  </entry>
  
  <entry>
    <title>Protein Dock Overview</title>
    <link href="https://karobben.github.io/2024/10/15/AI/proteindock/"/>
    <id>https://karobben.github.io/2024/10/15/AI/proteindock/</id>
    <published>2024-10-15T20:51:32.000Z</published>
    <updated>2024-11-04T23:32:14.035Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Physical-Based-Docking">Physical Based Docking</h2><h3 id="1982-Dock-Kuntz-Irwin-D-et-al-Rigid-body-shape-based">1982: Dock; Kuntz, Irwin D., et al.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> (Rigid body-shape based)</h3><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/lsob6Ob.png" alt="Dock; Kuntz, Irwin D., et al. 1982"></th></tr></thead><tbody><tr><td style="text-align:center">© Kuntz, Irwin D., et al. 1982<sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1:1]</a></sup></td></tr></tbody></table><p>In this paper, Kuntz present a way of docking prediction by searching the steric overlap based on the knowing surface structure of 2 proteins. It originally developed by Irwin “Tack” Kuntz and colleagues at the University of California, San Francisco (<strong>UCSF</strong>), DOCK was initially used for small-molecule docking. However, it laid the foundation for the development of more advanced docking algorithms and software that could handle macromolecular docking.</p><p>In the first generation of the Dock, it focus on 2 rigid bodies. It treat 2 proteins as one object. The goal of this program is to <mark>fix the 6 degree of freedom (3 transitions and 3 orientations) that determine the best relative position</mark>. For achieving this goal, three rules are followed:</p><ol><li>No overlap between 2 proteins</li><li>all hydrogen are pared with N or O within 3.5 Å.</li><li>all ligand atoms within the receptor binding cite.</li></ol><p><strong>Dock families:</strong></p><ol><li>1994: Firstly extend the DOCK into DNA-protein Docking and by screening the Cambridge Crystallographic Database, they find that the protein CC-1065 has high score.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup><ul><li>1999: DREAM++<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>: It is a extent package for Dock. It use Dock to predict binding and evaluated the interaction and predicts the product, finally search to find the prohibits.</li></ul></li><li>2001: <strong>DOCK 4.0</strong><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>: It added incremental construction (to sample the internal degrees of freedom of the ligand) and random search. In the Dock4, the ligand is not rigid anymore. Ligands with rotatable-bonds generated multiple conformation by other model.</li><li>2006: <strong>DOCK 5.0</strong><sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>:<ul><li>anchoring: new scoring functions, sampling methods and analysis tools; energy minimizing was mentioned during the.</li><li>scoring: energy scoring function based on the AMBERL: only <strong>intermolecular</strong> van der Waals (VDW) and electrostatic components in the function.</li><li>main limitation: Ligands has lots of rotatable-bonds would cause lots of resource. During the test set, ligands with &gt; 7 rotatable bonds were removed.</li><li>Some test data correction: using “Compute” and “Biopolymer” from <strong>Sybyl</strong><sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> to calculate the Gasteiger–Hückel partial electrostatic charges and add hydrogen for residues.</li></ul></li><li>2009: <strong>DOCK 6</strong><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>: In this version, it extents it’s abilities in RNA-ligands. But the rotatable-bonds from the ligands are still limited into 7~13. With the increasing of the RNA, the accuracy are decreased.<ul><li>update scoring in <strong>solvation energy</strong>:<ul><li>Hawkins–Cramer–Truhlar (HCT) generalized Born with solvent-accessible surface area (GB/SA) solvation scoring with optional salt screening</li><li>Poisson–Boltzmann with solvent-accessible surface area (PB/SA) solvation scoring</li><li>AMBER molecular mechanics with GB/SA solvation scoring and optional receptor flexibility</li></ul></li><li>other scoring:<ul><li>VDW: grid-based form of the Lennard-Jones potential</li><li>electrostatic: Zap Tool Kit from OpenEye</li></ul></li></ul></li><li>2013: <strong>DOCK3.7</strong><sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>:</li></ol><table><thead><tr><th style="text-align:center">DOCK4</th><th style="text-align:center">DOCK5</th><th style="text-align:center">DOCK6</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/SJJfgKt.png" alt="DOCK4"></td><td style="text-align:center"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10822-006-9060-4/MediaObjects/10822_2006_9060_Fig1_HTML.gif" alt="DOCK5"></td><td style="text-align:center"><img src="https://rnajournal.cshlp.org/content/15/6/1219/F1.large.jpg" alt="DOCK6"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center">incremental: anchor-and-grow</td><td style="text-align:center">The number of<br>rotatable-bonds hashuge<br>effects on success rate</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">anchor-and-grow</p><p>The “anchor-and-grow” conformational search algorithm. The algorithm performs the following steps: (1) DOCK perceives the molecule’s rotatable bonds, which it uses to identify an anchor segment and overlapping rigid layer segments. (2) Rigid docking is used to generate multiple poses of the anchor within the receptor. (3) The first layer atoms are added to each anchor pose, and multiple conformations of the layer 1 atoms are generated. An energy score within the context of the receptor is computed for each conformation. (4) The partially grown conformations are ranked by their score and are spatially clustered. The least energetically favorable and spatially diverse conformations are discarded. (5) The next rigid layer is added to each remaining conformation, generating a new set of conformations. (6) Once all layers have been added, the set of completely grown conformations and orientations is returned</p></div><h4 id="Compare-to-Other-Related-Tools">Compare to Other Related Tools</h4><div class="c-article-table-container"><div class="c-article-table-border c-table-scroll-wrapper"><div class="c-table-scroll-wrapper__content c-table-scroll-wrapper__fade--transparent" data-component-scroll-wrapper=""><table class="data last-table"><thead class="c-article-table-head"><tr><th class="u-text-left "><p>Method</p></th><th class="u-text-left "><p>Ligand sampling method<sup>a</sup>                                          </p></th><th class="u-text-left "><p>Receptor sampling method<sup>a</sup>                                          </p></th><th class="u-text-left "><p>Scoring function<sup>b</sup>                                          </p></th><th class="u-text-left "><p>Solvation scoring<sup>c,d</sup>                                          </p></th></tr></thead><tbody><tr><td class="u-text-left "><p>DOCK 4/5 </p></td><td class="u-text-left "><p>IC</p></td><td class="u-text-left "><p>SE</p></td><td class="u-text-left "><p>MM</p></td><td class="u-text-left "><p>DDD, GB, PB</p></td></tr><tr><td class="u-text-left "><p>FlexX/FlexE </p></td><td class="u-text-left "><p>IC</p></td><td class="u-text-left "><p>SE</p></td><td class="u-text-left "><p>ED</p></td><td class="u-text-left "><p>NA</p></td></tr><tr><td class="u-text-left "><p>Glide</p></td><td class="u-text-left "><p>CE&nbsp;+&nbsp;MC</p></td><td class="u-text-left "><p>TS</p></td><td class="u-text-left "><p>MM&nbsp;+&nbsp;ED</p></td><td class="u-text-left "><p>DS</p></td></tr><tr><td class="u-text-left "><p>GOLD </p></td><td class="u-text-left "><p>GA</p></td><td class="u-text-left "><p>GA</p></td><td class="u-text-left "><p>MM&nbsp;+&nbsp;ED</p></td><td class="u-text-left "><p>NA</p></td></tr></tbody></table></div></div><div class="c-article-table-footer"><ol>                      <li>                                    <sup>a</sup>Sampling methods are defined as Genetic Algorithm (GA), Conformational Expansion (CE), Monte Carlo (MC), incremental construction (IC), merged target structure ensemble (SE), torsional search (TS)</li>                      <li>                                    <sup>b</sup>Scoring functions are defined as either empirically derived (ED) or based on molecule mechanics (MM)</li>                      <li>                                    <sup>c</sup>If the package does not accommodate this option, the symbol NA (Not Available) is used</li>                      <li>                                    <sup>d</sup>Additional accuracy can be added to the scoring function using implicit solvent models. The most commonly used options are distance dependent dielectric (DDD), a parameterized desolvation term (DS), generalized Born (GB) and linearized Poisson Boltzmann (PB)</li>                    </ol></div></div><hr><h3 id="2003-ZDock">2003: ZDock</h3><p>Version iteration:</p><ul><li>ZDOCK 2.3/2.3.2 Scoring Function: Chen R, Li L, Weng Z. (2003) ZDOCK<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup></li><li>ZDOCK 3.0/3.0.2 Scoring Function: Mintseris J, Pierce B, Wiehe K, Anderson R, Chen R, Weng Z. (2007)<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup></li><li>M-ZDOCK: Pierce B, Tong W, Weng Z. (2005) M-ZDOCK<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup></li><li>ZDOCK 3.0.2/2.3.2: Pierce BG, Hourai Y, Weng Z. (2011)<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup></li><li>Online Server: Pierce BG, Wiehe K, Hwang H, Kim BH, Vreven T, Weng Z. (2014) ZDOCK Server<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup></li></ul><h4 id="Abstract">Abstract</h4><p>ZDock was developed for ubbound docking. It is based on pairwise shape complementarity (Docking) with desolvation and electrostatics (Scoring). In there test, it shows high success rate in the <strong>antibody-antigen</strong> docking test case. It is especially helpful in <strong>“large concave binding pocket”</strong>.</p><p>Before the ZDock, there are:</p><ul><li><strong>FTDOck</strong>: gird-based shape complementarity (GSC) and electrostatic using a Fast Fourier Transform (FFT)</li><li><strong>DOT</strong>: FFT-based computes Poission-Bolzmann electrostatics.</li><li><strong>HEX</strong>: evaluates overlapping surface skins and electrostatic complementarity with Fourier coorelation.</li><li><strong>GRAMM</strong>: low-resolutoin docking with the similar scoring as FTDOck;</li><li><strong>PPD</strong>: matches critial poitns by using geometric hashing.</li><li><strong>GIGGER</strong>: maximal surface mapping and favorable amino acid contacts by bit-mapping.</li><li><strong>DARWIN</strong>: molecular mechanics energy defined according to CHARMM.</li></ul><p>For <strong>ZDock</strong>:</p><ul><li>Optimizes desolvation (<strong>GSC</strong>), <mark>key scoring function</mark>.<ul><li>GSC = grid points surrounding the receptor corresponding to ligand atoms - clash penalty</li></ul></li><li>*<em>FFT</em> for electrostatics</li><li>Novel pairwise shape complementarity function (<strong>PSC</strong>) by distance cut-off of receptor-ligand atom minus clash penalty.<ul><li>Favorable: Number of pair within cutoff</li><li>Penalty: The clash penalty for core-core, surface-core, and surface-surface (9<sup>9</sup>, 9<sup>3</sup>, 9)</li></ul></li><li><strong>DE</strong>: desolvation, estimated by atomic contact energy (<strong>ACE</strong>), which is a free energy change of breaking two protein atom-water contacts and forming a protein atom-protein atom contact and water-water contact. The sum of <strong>ACE</strong> is <strong>DE</strong></li></ul><p>Version for scoring functions:</p><ul><li><strong>ZDOCK1.3</strong><sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup>: GSC+DE+ELEC</li><li><strong>ZDOCK2.1</strong><sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup>: PSC</li><li><strong>ZDOCK2.2</strong><sup class="footnote-ref"><a href="#fn9" id="fnref9:1">[9:1]</a></sup>: PSC+DE</li><li><strong>ZDOCK2.3</strong><sup class="footnote-ref"><a href="#fn9" id="fnref9:2">[9:2]</a></sup>: PSC+DE+ELEC</li></ul><h3 id="2004-ClusPro">2004: ClusPro</h3><p><a href="https://academic.oup.com/nar/article/32/suppl_2/W96/1040440">ClusPro: a fully automated algorithm for protein–protein docking</a></p><h3 id="2010-Hex">2010: Hex</h3><p><a href="https://academic.oup.com/bioinformatics/article/26/19/2398/229220">Ultra-fast FFT protein docking on graphics processors</a></p><p><a href="https://hex.loria.fr/">Home page</a>, <a href="https://hex.loria.fr/manual800/hex_manual.html">Documentation</a></p><p>Hex is extremely fast but lack of accuracy. I tried to sampling over 100,1000 but results even close to native structure.<br>On the other hand, I didn’t find a way to mark the surface residues so we could focus on specific area. Although, GhatGPT said it could do constrained docking, but it seems we could only constrain the range angles of the receptor and the ligand.</p><table><thead><tr><th style="text-align:center"><img src="https://documentation.samson-connect.net/tutorials/hex/images/hex-results-animation.gif" alt="Hex Dock in SAMSON"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://documentation.samson-connect.net/tutorials/hex/protein-docking-with-hex/">© SAMSON</a></td></tr></tbody></table><h3 id="2014-rDock">2014: rDock</h3><p><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003571">rDock: a fast, versatile and open source program for docking ligands to proteins and nucleic acids</a></p><h3 id="2018-InterEvDock">2018: InterEvDock</h3><p><a href="https://link.springer.com/protocol/10.1007/978-1-4939-7759-8_28">Protein-Protein Docking Using Evolutionary Information</a></p><h2 id="Machine-Learning-Based-Docking">Machine Learning Based Docking</h2><h3 id="2021-DeepRank">2021: DeepRank</h3><table><thead><tr><th style="text-align:center">Model Grpah Abstract</th><th style="text-align:left">Model Name</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-021-27396-0/MediaObjects/41467_2021_27396_Fig1_HTML.png" alt="DeepRank"><br>© Chen, M., &amp; Zhou, X</td><td style="text-align:left">DeepRank</td></tr><tr><td style="text-align:center"><img src="https://raw.githubusercontent.com/DeepRank/Deeprank-GNN/master/deeprank_gnn.png" alt="DeepRank-GNN"><br>© Réau, M.</td><td style="text-align:left">DeepRank-GNN</td></tr><tr><td style="text-align:center"><img src="https://github.com/DeepRank/deeprank2/raw/main/deeprank2.png" alt="DeepRank2"><br>© Crocioni, G.</td><td style="text-align:left">Deeprank2</td></tr></tbody></table><p><strong>DeepRank</strong><sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup> is a <a href="https://github.com/DeepRank/deeprank">open source</a> framework designed to analyze 3D protein-protein interfaces by using deep learning to capture spatial and biochemical features. The paper presents DeepRank’s approach to transforming 3D structural data into 3D grids that a neural network can process. This setup allows DeepRank to identify interaction patterns, rank docking models, and predict binding affinities with high accuracy. It’s especially useful for discovering patterns in protein interfaces that might be overlooked with traditional scoring functions.</p><p>In this model, it turn the <strong>pdb into sql</strong> for efficient processing. The interfacing residues <strong>cut-off is 5.5 Å</strong>. When find all interfacing atoms, they would be mapped into **3D grid using a <strong>Gaussian mapping</strong>. The target value is very flexible, too. You can using any kind of values, iRMSD, FNAT, or DockQ score for instance, as the target values (Predicted value). The data was stored as <strong>hdf5</strong> format which keep the efficiency and small storage size.</p><p>DeepRank family:</p><ul><li><strong>DeepRank</strong><sup class="footnote-ref"><a href="#fn16" id="fnref16:1">[16:1]</a></sup>: 2021, Chen, M., et al.; It mapped the protein interfacing into a 3D grid and using CNN to train the regression model. It established the foundation of the architectural of DeepRank.<ul><li>In the DeepRank, it use information both from atom-level and residue-level. From the atom level, it calculates the atom density, charges, electrostatic energy, and VDW contacts. In residue-level, it included number of residue-residue contacts, buried surface area, and Position specific scoring matrix (PSSM)</li></ul></li><li><strong>DeepRank-GNN</strong><sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup>: 2023, Réau, M. et al.; from the same team replace the 3D grid based CNN into GNN which could avoid rotation challenge in 3D grid.<ul><li>The input information is very similar to the DeepRank. Instead of 3D grid, it relies on the adjacent matrix to build the network. In this time, the cut-off became 8.5 Å.</li><li>It has more rich features like Distance, residue half sphere exposure, Residue depth (from biopython, MSMS)</li></ul></li><li><strong>Deeprank_GNN_ESM</strong><sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup>: 2024, Xu, X., et al.; The <strong>PSSM</strong> calculating requires sequence alignment which consumes lots of time. For generate the graph efficiently, they replaced the <strong>PSSM</strong> with <strong>ESM</strong> embedding vectors.</li><li><strong>DeepRank2</strong><sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup>: 2024, Crocioni, G., et al.; In the DeepRank2., it supports both 3D grid and graph network as inputs. It also integrated the <a href="https://github.com/DeepRank/DeepRank-Mut">Deep-Mut</a> to do in silicon mutation screening.</li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p><a href="https://www.sciencedirect.com/science/article/pii/002228368290153X">Kuntz I D, Blaney J M, Oatley S J, et al. A geometric approach to macromolecule-ligand interactions[J]. Journal of molecular biology, 1982, 161(2): 269-288.</a> <a href="#fnref1" class="footnote-backref">↩︎</a> <a href="#fnref1:1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p><a href="https://link.springer.com/article/10.1007/BF00124018">Grootenhuis P D J, Roe D C, Kollman P A, et al. Finding potential DNA-binding compounds by using molecular shape[J]. Journal of Computer-Aided Molecular Design, 1994, 8: 731-750.</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p><a href="https://link.springer.com/article/10.1023/A:1008066310669">Makino S, Ewing T J A, Kuntz I D. DREAM++: flexible docking program for virtual combinatorial libraries[J]. Journal of computer-aided molecular design, 1999, 13: 513-532.</a> <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p><a href="https://link.springer.com/article/10.1023/a:1011115820450">Ewing T J A, Makino S, Skillman A G, et al. DOCK 4.0: search strategies for automated molecular docking of flexible molecule databases[J]. Journal of computer-aided molecular design, 2001, 15: 411-428.</a> <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p><a href="https://link.springer.com/article/10.1007/s10822-006-9060-4">Moustakas D T, Lang P T, Pegg S, et al. Development and validation of a modular, extensible docking program: DOCK 5[J]. Journal of computer-aided molecular design, 2006, 20: 601-619.</a> <a href="#fnref5" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p><a href="https://bcrf.biochem.wisc.edu/all-tutorials/tutorial-materials-guests/185-2/">S. Pérez, C. Meyer, A. Imberty. “Practical tools for accurate modeling of complex carbohydrates and their interactions with proteins” A. Pullman, J. Jortner, B. Pullman (Eds.), Modelling of Biomolecular Structures and Mechanisms, Kluwer Academic Publishers, Dordrecht (1996), pp. 425-454.</a> <a href="#fnref6" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p><a href="https://rnajournal.cshlp.org/content/15/6/1219.short">Lang P T, Brozell S R, Mukherjee S, et al. DOCK 6: Combining techniques to model RNA–small molecule complexes[J]. Rna, 2009, 15(6): 1219-1230.</a> <a href="#fnref7" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0075992">Coleman R G, Carchia M, Sterling T, et al. Ligand pose and orientational sampling in molecular docking[J]. PloS one, 2013, 8(10): e75992.</a> <a href="#fnref8" class="footnote-backref">↩︎</a></p></li><li id="fn9" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/12784371">Chen, R., Li, L., &amp; Weng, Z. (2003). ZDOCK: an initial‐stage protein‐docking algorithm. Proteins: Structure, Function, and Bioinformatics, 52(1), 80-87.</a> <a href="#fnref9" class="footnote-backref">↩︎</a> <a href="#fnref9:1" class="footnote-backref">↩︎</a> <a href="#fnref9:2" class="footnote-backref">↩︎</a></p></li><li id="fn10" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/17623839">Mintseris, J., Pierce, B., Wiehe, K., Anderson, R., Chen, R., &amp; Weng, Z. (2007). Integrating statistical pair potentials into protein complex prediction. Proteins: Structure, Function, and Bioinformatics, 69(3), 511-520.</a> <a href="#fnref10" class="footnote-backref">↩︎</a></p></li><li id="fn11" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/15613396">Pierce, B., Tong, W., &amp; Weng, Z. (2005). M-ZDOCK: a grid-based approach for C n symmetric multimer docking. Bioinformatics, 21(8), 1472-1478.</a> <a href="#fnref11" class="footnote-backref">↩︎</a></p></li><li id="fn12" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/21949741">Pierce, B. G., Hourai, Y., &amp; Weng, Z. (2011). Accelerating protein docking in ZDOCK using an advanced 3D convolution library. PloS one, 6(9), e24657.</a> <a href="#fnref12" class="footnote-backref">↩︎</a></p></li><li id="fn13" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/24532726">Pierce, B. G., Wiehe, K., Hwang, H., Kim, B. H., Vreven, T., &amp; Weng, Z. (2014). ZDOCK server: interactive docking prediction of protein–protein complexes and symmetric multimers. Bioinformatics, 30(12), 1771-1773.</a> <a href="#fnref13" class="footnote-backref">↩︎</a></p></li><li id="fn14" class="footnote-item"><p>Chen R, Weng Z. Docking unbound proteins using shape complementarity, desolvation, and electrostatics. Proteins 2002; 47: 281–294. <a href="#fnref14" class="footnote-backref">↩︎</a></p></li><li id="fn15" class="footnote-item"><p>Chen R, Weng Z. A novel shape complementarity scoring function for protein-protein docking. Proteins 2003; 51: 397–408. <a href="#fnref15" class="footnote-backref">↩︎</a></p></li><li id="fn16" class="footnote-item"><p><a href="https://www.nature.com/articles/s41467-021-27396-0">Renaud, N., Geng, C., Georgievska, S., Ambrosetti, F., Ridder, L., Marzella, D. F., … &amp; Xue, L. C. (2021). DeepRank: a deep learning framework for data mining 3D protein-protein interfaces. Nature communications, 12(1), 7068.</a> <a href="#fnref16" class="footnote-backref">↩︎</a> <a href="#fnref16:1" class="footnote-backref">↩︎</a></p></li><li id="fn17" class="footnote-item"><p><a href="https://academic.oup.com/bioinformatics/article/39/1/btac759/6845451">Réau, M., Renaud, N., Xue, L. C., &amp; Bonvin, A. M. (2023). DeepRank-GNN: a graph neural network framework to learn patterns in protein–protein interfaces. Bioinformatics, 39(1), btac759.</a> <a href="#fnref17" class="footnote-backref">↩︎</a></p></li><li id="fn18" class="footnote-item"><p>Xu, X., &amp; Bonvin, A. M. (2024). DeepRank-GNN-esm: a graph neural network for scoring protein–protein models using protein language model. Bioinformatics advances, 4(1), vbad191. <a href="#fnref18" class="footnote-backref">↩︎</a></p></li><li id="fn19" class="footnote-item"><p><a href="https://joss.theoj.org/papers/10.21105/joss.05983.pdf">Crocioni, G., Bodor, D. L., Baakman, C., Parizi, F. M., Rademaker, D. T., Ramakrishnan, G., … &amp; Xue, L. C. (2024). DeepRank2: Mining 3D Protein Structures with Geometric Deep Learning. Journal of Open Source Software, 9(94), 5983.</a> <a href="#fnref19" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">Protein Dock Tools and algorithm Overview</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Protein Structure" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Protein-Structure/"/>
    
    
    <category term="3D" scheme="https://karobben.github.io/tags/3D/"/>
    
    <category term="protein" scheme="https://karobben.github.io/tags/protein/"/>
    
    <category term="dock" scheme="https://karobben.github.io/tags/dock/"/>
    
  </entry>
  
  <entry>
    <title>Softmax</title>
    <link href="https://karobben.github.io/2024/10/09/AI/softmax/"/>
    <id>https://karobben.github.io/2024/10/09/AI/softmax/</id>
    <published>2024-10-09T22:46:15.000Z</published>
    <updated>2024-10-09T22:52:46.766Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Softmax">Softmax</h2><p>Key idea: $f_c(x) =$ posterior probability of cass $c$</p><ul><li>A perceptron has a one-hot output vector, in which $f_c(x) = 1$ if the<br>neural net thinks $c$ is the most likely value of $y$, and 0 otherwise</li><li>A softmax computes $f_c(x) \approx Pr(Y =c |x)$. The conditions for this to be true are:<ul><li>It needs to satisfy the axioms of probability:<br>$$ 0 \leq f_c(x) \leq 1, \quad \sum_{c=1}^{v} f_c(x) = 1$$</li><li>The weight matrix, $W$, is trained using a loss function that encourages $f(x)$ to approximate posterior probability of the labels on some training dataset:<br>$$f_c(x) \approx \Pr(Y = c|x)$$</li></ul></li></ul><h3 id="Softmax-satisfies-the-axioms-of-probability">Softmax satisfies the axioms of probability</h3><ul><li><p>Axiom #1, probabilities are non-negative $(f_k(x) \geq 0)$. There are many ways to do this, but one way that works is to choose:</p><p>$$<br>f_c(x) \propto \exp(w_c^T x + b_c)<br>$$</p></li><li><p>Axiom #2, probabilities should sum to one $(\sum_{k=1}^{v} f_k(x) = 1)$. This can be done by normalizing:</p></li></ul><p>$$<br>f(x) = [f_1(x), …, f_v(x)]^T<br>$$<br>$$<br>f_c(x) = \frac{\exp(w_c^T x + b_c)}{\sum_{k=0}^{v-1} \exp(w_k^T x + b_k)}<br>$$</p><p>where $w_k^T$ is the $k^{th}$ row of the matrix $W$.</p><h3 id="The-logistic-sigmoid-function">The logistic sigmoid function</h3><p>For a two-class classifier, we don’t really need the vector label. If we define $w = w_2 - w_1$ and $b = b_2 - b_1$, then the softmax simplifies to:</p><p>$$<br>f(Wx + b) =<br>\begin{bmatrix}<br>\text{Pr}(Y = 1|x) \\<br>\text{Pr}(Y = 2|x)<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\frac{1}{1+e^ {-(w^ Tx+b)}} \\<br>\frac{e^ {-(w^ Tx+b)}}{1+e^ {-(w^ Tx+b)}}<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\sigma(w^Tx + b) \\<br>1 - \sigma(w^Tx + b)<br>\end{bmatrix}<br>$$</p><p>… so instead of the softmax, we use a scalar function called the logistic sigmoid function:</p><p>$$<br>\sigma(z) = \frac{1}{1+e^{-z}}<br>$$</p><p>This function is called sigmoid because it is S-shaped.</p><p>For $z \to -\infty$, $\sigma(z) \to 0$</p><p>For $z \to +\infty$, $\sigma(z) \to 1$</p><h3 id="Gradient-descent">Gradient descent</h3><p>Suppose we have training tokens $(x_i, y_i)$, and we have some initial class vectors $w_1$ and $w_2$. We want to update them as</p><p>$$<br>w_1 \leftarrow w_1 - \eta \frac{\partial \mathcal{L}}{\partial w_1}<br>$$</p><p>$$<br>w_2 \leftarrow w_2 - \eta \frac{\partial \mathcal{L}}{\partial w_2}<br>$$</p><p>…where $\mathcal{L}$ is some loss function. What loss function makes sense?</p><h3 id="Zero-one-loss-function">Zero-one loss function</h3><p>The most obvious loss function for a classifier is its classification error rate,</p><p>$$<br>\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \ell(\hat{f}(x_i), y_i)<br>$$</p><p>Where $\ell(\hat{y}, y)$ is the zero-one loss function,</p><p>$$<br>\ell(f(x), y) =<br>\begin{cases}<br>0 &amp; \text{if } f(x) = y \\<br>1 &amp; \text{if } f(x) \neq y<br>\end{cases}<br>$$</p><p>The problem with zero-one loss is that it’s not differentiable.</p><h3 id="A-loss-function-that-learns-probabilities">A loss function that learns probabilities</h3><p>Suppose we have a softmax output, so we want $f_c(x) \approx \Pr(Y = c|x)$. We can train this by learning $W$ and $b$ to maximize the probability of the training corpus. If we assume all training tokens are independent, we get:</p><p>$$<br>W, b = \underset{W,b}{\text{argmax}} \prod_{i=1}^{n} \Pr(Y = y_i|x_i) = \underset{W,b}{\text{argmax}} \sum_{i=1}^{n} \ln \Pr(Y = y_i|x_i)<br>$$</p><p>But remember that $f_c(x) \approx \Pr(Y = c|x)$! Therefore, maximizing the log probability of training data is the same as minimizing the cross entropy between the neural net and the ground truth:</p><p>$$<br>W, b = \underset{W,b}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} \mathcal{L}_ i, \quad \mathcal{L}_ i = - \log f_ {y_ i}(x_ i)<br>$$</p><h3 id="Cross-entropy">Cross-entropy</h3><p>This loss function:</p><p>$$<br>\mathcal{L} = - \ln f_{y}(x)<br>$$</p><p>is called cross-entropy. It measures the difference in randomness between:</p><ul><li>Truth: $Y = y$ with probability 1.0, $\ln(1.0) = 0$, minus the</li><li>Neural net estimate: $Y = y$ with probability $f_{y}(x)$.</li></ul><p>Thus</p><p>$$<br>\mathcal{L} = 0 - \ln f_{y}(x)<br>$$</p><h3 id="Gradient-of-the-cross-entropy-of-the-softmax">Gradient of the cross-entropy of the softmax</h3><p>Since we have these definitions:</p><p>$$<br>\mathcal{L} = - \ln f_{y}(x), \quad f_{y}(x) = \frac{\exp(z_{y})}{\sum_{k=1}^{v} \exp(z_{k})}, \quad z_{c} = w_c^T x + b_c<br>$$</p><p>Then:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w_c} = \left( \frac{\partial \mathcal{L}}{\partial z_c} \right) \left( \frac{\partial z_c}{\partial w_c} \right) = \left( \frac{\partial \mathcal{L}}{\partial z_c} \right) x<br>$$</p><p>…where:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial z_c} =<br>\begin{cases}<br>f_{c}(x_i) - 1 &amp; c = y \\<br>f_{c}(x_i) &amp; c \neq y<br>\end{cases}<br>$$</p><h3 id="Similarity-to-linear-regression">Similarity to linear regression</h3><p>For linear regression, we had:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w} = \epsilon x, \quad \epsilon = f(x) - y<br>$$</p><p>For the softmax classifier with cross-entropy loss, we have</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w_c} = \epsilon_c x<br>$$</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y \text{ (output should be 1)} \\<br>f_c(x_i) &amp; \text{otherwise (output should be 0)}<br>\end{cases}<br>$$</p><h3 id="Similarity-to-perceptron">Similarity to perceptron</h3><p>Suppose we have a training token $(x, y)$, and we have some initial class vectors $w_c$. Using softmax and cross-entropy loss, we can update the weight vectors as</p><p>$$<br>w_c \leftarrow w_c - \eta \epsilon_c x<br>$$</p><p>…where</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y_i \\<br>f_c(x_i) &amp; \text{otherwise}<br>\end{cases}<br>$$</p><p>In other words, like a perceptron,</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>\epsilon_c &lt; 0 &amp; c = y_i \\<br>\epsilon_c &gt; 0 &amp; \text{otherwise}<br>\end{cases}<br>$$</p><h3 id="Outline">Outline</h3><ul><li><p>Softmax:<br>$$ f_c(x) = \frac{\exp(w_c^T x + b_c)}{\sum_{k=1}^{v} \exp(w_k^T x + b_k)} \approx \Pr(Y = c|x) $$</p></li><li><p>Cross-entropy:<br>$$ \mathcal{L} = - \ln f_{y}(x) $$</p></li><li><p>Derivative of the cross-entropy of a softmax:<br>$$ \frac{\partial \mathcal{L}}{\partial w_c} = \epsilon_c x, \quad \epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y \text{ (output should be 1)} \\<br>f_c(x_i) &amp; \text{otherwise (output should be 0)}<br>\end{cases} $$</p></li><li><p>Gradient descent:<br>$$ w_c \leftarrow w_c - \eta \epsilon_c x $$</p></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Softmax</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Support Vector Machine</title>
    <link href="https://karobben.github.io/2024/09/29/AI/supportvectormachine/"/>
    <id>https://karobben.github.io/2024/09/29/AI/supportvectormachine/</id>
    <published>2024-09-30T02:41:26.000Z</published>
    <updated>2024-10-09T22:55:34.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-Support-Vector-Machine">What is Support Vector Machine</h2><p>SVM was developed in the 1990s by Vladimir Vapnik and his colleagues. The development of SVM was rooted in statistical learning theory. It introduced the concept of finding the maximum margin hyperplane to separate classes effectively, with extensions to handle non-linear data through kernel functions. SVM gained popularity due to its ability to create powerful classifiers, especially in high-dimensional feature spaces.</p><h3 id="Compare-to-Random-Forest">Compare to Random Forest</h3><p>Random Forest is an ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting, handling both linear and non-linear data well. It’s good for large datasets and provides feature importance but is less interpretable.</p><p>SVM finds the optimal hyperplane to separate classes by maximizing the margin. It works well for smaller, high-dimensional datasets but is computationally expensive for large datasets and harder to interpret.</p><h3 id="Compare-to-Linear-Regression">Compare to Linear Regression</h3><p>The decision function of <strong>Support Vector Machine (SVM)</strong> looks very similar to a <strong>linear function</strong>—and indeed, it shares common elements with <strong>linear regression</strong>. However, the main differences lie in their objectives and the way they handle data:</p><h4 id="Similarities">Similarities</h4><ul><li><strong>Linear Function Form</strong>: Both SVM and Linear Regression use a linear function of the form:<br>$$<br>f(x) = w_1 x_1 + w_2 x_2 + \cdots + w_k x_k + b<br>$$<br>Where $ w_i $ are the weights, $ x_i $ are the features, and $ b $ is the bias term.</li><li><strong>Weight Optimization</strong>: Both models optimize the weights ($ w $) to achieve their goals.</li></ul><h4 id="Key-Differences">Key Differences</h4><ol><li><p><strong>Objective Function</strong>:</p><ul><li><strong>Linear Regression</strong>: The goal is to <strong>minimize the error</strong> (typically the mean squared error) between predicted and actual values. It aims to find the line (or hyperplane) that best fits the data points by minimizing the difference between predictions and true values.</li><li><strong>SVM</strong>: The goal is to <strong>maximize the margin</strong> between different classes. SVM seeks to find a hyperplane that not only separates the classes but does so with the largest possible gap between the nearest points of each class (called <strong>support vectors</strong>). This makes the decision boundary as robust as possible against errors or noise.</li></ul></li><li><p><strong>Loss Function</strong>:</p><ul><li><strong>Linear Regression</strong>: Uses <strong>squared loss</strong> to penalize errors, which means that even small deviations contribute to the overall loss.</li><li><strong>SVM</strong>: Uses a <strong>hinge loss function</strong> for classification, which penalizes misclassifications and ensures a margin of separation. The loss function focuses more on correctly classifying data points with maximum confidence.</li></ul></li><li><p><strong>Problem Type</strong>:</p><ul><li><strong>Linear Regression</strong>: Primarily used for <strong>regression</strong> problems, where the goal is to predict a continuous output.</li><li><strong>SVM</strong>: Primarily used for <strong>classification</strong> (though it can be adapted for regression as <strong>SVR</strong>), where the goal is to classify data points into different categories. In SVM, the function output is interpreted using a sign function, where:<br>$$<br>f(x) = w^T x + b \Rightarrow \text{classify as } \begin{cases}<br>+1, &amp; \text{if } f(x) &gt; 0 \\<br>-1, &amp; \text{if } f(x) &lt; 0<br>\end{cases}<br>$$</li></ul></li><li><p><strong>Margin and Support Vectors</strong>:</p><ul><li><strong>Linear Regression</strong>: There is no concept of a <strong>margin</strong> or <strong>support vectors</strong> in linear regression. It simply finds the line of best fit for all data points.</li><li><strong>SVM</strong>: Introduces the concept of <strong>margin</strong>, which is the distance between the hyperplane and the closest data points from each class. These closest points are called <strong>support vectors</strong>, and they are crucial to defining the decision boundary.</li></ul></li><li><p><strong>Use of Kernels (Non-linearity)</strong>:</p><ul><li><strong>Linear Regression</strong>: Strictly a linear model. To handle non-linearity, you would have to explicitly add polynomial features or transform the features.</li><li><strong>SVM</strong>: Supports <strong>kernel tricks</strong> (such as polynomial or radial basis function kernels) to project data into higher dimensions, allowing it to separate data that isn’t linearly separable in its original space. This feature makes SVM more powerful for complex, non-linear classification problems.</li></ul></li></ol><h4 id="Summary">Summary</h4><ul><li><strong>Linear Regression</strong>: Minimizes prediction error for a best-fit line, used for regression.</li><li><strong>SVM</strong>: Maximizes the margin to find an optimal separating hyperplane, used for classification.</li><li>While both use linear functions, SVM is fundamentally about <strong>classification and margin maximization</strong>, whereas linear regression focuses on <strong>minimizing the difference between predicted and actual continuous values</strong>. SVM also handles non-linearity more effectively through kernels, making it more versatile for complex datasets.</li></ul><h2 id="Overview-of-SVM">Overview of SVM</h2><ul><li>Decision Boundary: $w^T x + b$.</li><li>Classification: $f(x) = sign(w^T x + b)$</li><li>Cost function: Training error cost + $\lambda$ penalty</li></ul><table><thead><tr><th><strong>Number of Features</strong></th><th><strong>Decision Boundary Equation</strong></th><th><strong>Classification Equation</strong></th></tr></thead><tbody><tr><td>1 Feature</td><td>$ w_1 x_1 + b = 0 $</td><td>$ f(x) = \text{sign}(w_1 x_1 + b) $</td></tr><tr><td>2 Features</td><td>$ w_1 x_1 + w_2 x_2 + b = 0 $</td><td>$ f(x) = \text{sign}(w_1 x_1 + w_2 x_2 + b) $</td></tr><tr><td>$ k $ Features</td><td>$ w_1 x_1 + w_2 x_2 + \cdots + w_k x_k + b = 0 $</td><td>$ f(x) = \text{sign}(w_1 x_1 + w_2 x_2 + \cdots + w_k x_k + b) $</td></tr></tbody></table><div class="admonition question"><p class="admonition-title">What does $w^T$ mean</p><p><strong>Explanation</strong>:</p><ul><li>A vector $ w $ is typically represented as a column vector, meaning it has multiple rows and a single column.</li><li>$ w^T $ is the <strong>transpose</strong> of $ w $, which means converting a column vector into a row vector, or vice versa.</li></ul><p><strong>Mathematical Notation</strong>:</p><ul><li>If $ w $ is a column vector with elements:$$w = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}$$</li><li>Then the <strong>transpose</strong> $ w^T $ is w row vector:$$w^T = \begin{bmatrix} w_1 &amp; w_2 &amp; \cdots &amp; w_n \end{bmatrix}$$In <strong>SVM</strong> or <strong>machine learning</strong>, the transpose is often used to indicate a <strong>dot product</strong> operation when combined with another vector or matrix. For example, if you have: $w^T x $, it means you're calculating the <strong>dot product</strong> of vector $ w $ and vector $ x $, which is a scalar value used in calculating distances, projections, or in constructing decision boundaries in algorithms like SVM.</li></ul></div><h3 id="Features">Features</h3><p>$$<br>f(\mathbf{x}) = \text{sign}(\mathbf{w}^T \mathbf{x} + b)<br>$$</p><p>where</p><p>$$<br>\mathbf{x} = \begin{bmatrix} x_0 \ x_1 \end{bmatrix}, \quad \mathbf{w} = \begin{bmatrix} w_0 \ w_1 \end{bmatrix}, \quad \text{and} \quad b \text{ is a scalar.}<br>$$</p><p><strong>Boundary</strong></p><p>The boundary condition is given by:</p><p>$$<br>\begin{bmatrix} w_0 &amp; w_1 \end{bmatrix} \begin{bmatrix} x_0 \ x_1 \end{bmatrix} + b = 0<br>$$</p><p>Solving for $ x_1 $:</p><p>$$<br>w_0 x_0 + w_1 x_1 + b = 0<br>$$</p><p>$$<br>x_1 = -\frac{w_0}{w_1} x_0 - \frac{b}{w_1}<br>$$</p><p><strong>Classification</strong></p><p>The classification function is:</p><p>$$<br>y = \begin{cases}<br>1 &amp; \text{if } x_1 \geq -\frac{w_0}{w_1}x_0 - \frac{b}{w_1} \\<br>-1 &amp; \text{if } x_1 &lt; -\frac{w_0}{w_1}x_0 - \frac{b}{w_1}<br>\end{cases}<br>$$</p><h2 id="Training-Cost">Training Cost</h2><p>The training cost in SVM refers to the computational and resource-related costs involved in training the model, which is an important consideration when choosing an algorithm, especially for larger datasets. SVM’s training cost is influenced by its optimization problem, which involves finding the hyperplane that maximizes the margin while correctly classifying the training data (or with minimal misclassification for soft margins).</p><h3 id="Training-Cost-in-SVM">Training Cost in SVM</h3><ol><li><p><strong>Optimization Complexity</strong>:</p><ul><li>SVM training involves <strong>solving a quadratic optimization problem</strong> to find the best hyperplane.</li><li>This process is complex and takes more computation, especially with <strong>non-linear kernels</strong>.</li></ul></li><li><p><strong>Time Complexity</strong>:</p><ul><li><strong>Linear SVM</strong>: Training time is between $O(n * d)$ and $O(n^2 * d)$, where $ n $ is the number of data points and $ d $ is the number of features.</li><li><strong>Non-linear Kernel SVM</strong>: Training complexity is approximately $O(n^2)$ to $O(n^3)$, making it very expensive for large datasets.</li></ul></li><li><p><strong>Memory Usage</strong>:</p><ul><li>With kernels, SVM stores a <strong>kernel matrix</strong> of size $ n \times n $, which uses a lot of memory if $ n $ is large.</li></ul></li><li><p><strong>Support Vectors</strong>:</p><ul><li>More <strong>support vectors</strong> means more computation during both training and prediction. Complex datasets often need more support vectors.</li></ul></li></ol><h3 id="Why-Care-About-Training-Cost">Why Care About Training Cost?</h3><ul><li><strong>Scalability</strong>: SVM can become impractical for <strong>large datasets</strong> due to the high cost in terms of time and memory.</li><li><strong>Resources</strong>: It requires substantial <strong>CPU and memory</strong>, limiting its use on resource-constrained systems.</li><li><strong>Algorithm Selection</strong>: For small to medium datasets, SVM works well. For large datasets, other methods like <strong>Random Forest</strong> or <strong>SGD</strong> may be better.</li></ul><h3 id="Reducing-Training-Cost">Reducing Training Cost</h3><ol><li><strong>Linear SVM</strong>: Use for linearly separable data—it has lower complexity.</li><li><strong>Approximations</strong>: Use <strong>SGDClassifier</strong> or <strong>kernel approximations</strong> for faster training.</li><li><strong>Data Subset</strong>: Train on a <strong>smaller subset</strong> of data to speed up training.</li></ol><h3 id="Hinge-Loss">Hinge Loss</h3><table><thead><tr><th>Condition</th><th>Cost Function</th><th>Description</th></tr></thead><tbody><tr><td>$y_i \neq \text{sign}(\hat{y}_i)$</td><td>$C(y_ i, \hat{y}_ i) = |y_ i| + 1$</td><td>Large</td></tr><tr><td>$y_i = \text{sign}(\hat{y}_i)$ close</td><td>$C(y_ i, \hat{y}_ i) = |y_ i| + 1$</td><td>Medium</td></tr><tr><td>$y_i = \text{sign}(\hat{y}_i)$ away</td><td>$C(y_ i, \hat{y}_ i) = 0$</td><td>No cost</td></tr><tr><td>General Cost Function</td><td>$C(y_ i, \hat{y}_ i) = \max(0, 1 - y_ i \cdot \hat{y}_ i)$</td><td>-</td></tr></tbody></table><h2 id="Train-a-SVM">Train a SVM</h2><h3 id="Training-Error">Training Error</h3><ul><li>$ \frac{1}{N} \sum_{i=1}^N C(y_i, \hat{y}_ i)$<ul><li>$ = \frac{1}{N} \sum_{i=1}^N \max(0, 1 - y_ i \cdot \hat{y}_ i) $</li><li>$ = \frac{1}{N} \sum_{i=1}^N \max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b)) $</li></ul></li></ul><h3 id="Cost-Function">Cost Function</h3><p>$$ S(\mathbf{w}, b; \lambda) = \frac{1}{N} \sum_{i=1}^N [\max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b))] + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w} $$</p><h3 id="Stochastic-Gradient-Descent">Stochastic Gradient Descent</h3><p>In training a Support Vector Machine (SVM), the primary objective is to minimize the cost function. This cost function often includes terms that measure the classification error and possibly a regularization term. The minimization of the cost function aims to find the best hyperplane that separates the classes while also considering the margin maximization between different classes and controlling model complexity to prevent overfitting.</p><p>$$<br>\mathbf{u} = \begin{bmatrix} \mathbf{w} \ b \end{bmatrix}<br>$$</p><p><strong>Minimize cost function:</strong></p><p>$$<br>g(\mathbf{u}) = \left[ \frac{1}{N} \sum_{i=1}^N g_i(\mathbf{u}) \right] + g_0(\mathbf{u})<br>$$</p><p>where:</p><p>$$<br>g_i(\mathbf{u}) = \max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b))<br>$$</p><p>and:</p><p>$$<br>g_0(\mathbf{u}) = \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}<br>$$</p><p><strong>Iteratively, at step $(n)$:</strong></p><ul><li>Compute descent direction $p^{(n)}$ and step size $\eta$</li><li>Ensure that $g(u^{(n)} + \eta p^{(n)}) \leq g(u^{(n)})$</li><li>Update $u^{(n+1)} = u^{(n)} + \eta p^{(n)}$</li></ul><p><strong>Descent direction:</strong></p><p>$$<br>p^{(n)} = -\nabla g(\mathbf{u}^{(n)})<br>$$</p><p>$$<br>= -\left( \frac{1}{N} \sum_{i=1}^N \nabla g_i(\mathbf{u}) + \nabla g_0(\mathbf{u}) \right)<br>$$</p><p><strong>Estimation through mean of batch:</strong></p><p>$$<br>p^{(n)}_ {N_ b} = -\left( \frac{1}{N_b} \sum_ {j \in \text{batch}} \nabla g_ j(\mathbf{u}) + \nabla g_ 0(\mathbf{u}) \right)<br>$$</p><p><strong>Epoch</strong></p><ul><li>One pass on training set of size $N$</li><li>Each step sees a batch of $N_b$ items</li><li>The dataset is covered in $\frac{N}{N_b}$ steps</li><li>Step size in epoch $e$: $\eta^{(e)} = \frac{m}{e + l}$<ul><li>Constants $m$ and $l$: tune on small subsets</li></ul></li></ul><p><strong>Season</strong></p><ul><li>Constant number of iterations, much smaller than epochs</li><li>Each step sees a batch of $N_b$ items</li><li>Step size in season $s$: $\eta^{(s)} = \frac{m}{s + l}$<ul><li>Constants $m$ and $l$: tune on small subsets</li></ul></li></ul><h3 id="Full-SGD">Full SGD</h3><ul><li><p><strong>Vector u and its gradient:</strong><br>$$ \mathbf{u} = \begin{bmatrix} u_1 \\ \vdots \\ u_d \end{bmatrix}, \quad \nabla g = \begin{bmatrix} \frac{\partial g}{\partial u_1} \\ \vdots \\ \frac{\partial g}{\partial u_d} \end{bmatrix} $$</p></li><li><p><strong>Batches of 1 sample at each training step:</strong><br>$$ N_b = 1 $$</p></li><li><p><strong>Gradient of g(u):</strong><br>$$ \nabla g(\mathbf{u}) = \nabla \left( \max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b)) + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w} \right) $$</p></li><li><p><strong>Update rules for a and b:</strong><br>$$ \begin{bmatrix} \mathbf{w}^{(n+1)} \ b^{(n+1)} \end{bmatrix} = \begin{bmatrix} \mathbf{w}^{(n)} \ b^{(n)} \end{bmatrix} - \eta \begin{bmatrix} \nabla_{\mathbf{w}} \ \nabla_{b} \end{bmatrix} $$</p></li><li><p><strong>Condition for correct classification away from the boundary:</strong><br>$$ y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) \geq 1. \quad \text{Correct, away from boundary} $$<br>$$ \nabla_ {\mathbf{w}} (0 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = \lambda \mathbf{w}, \quad \nabla_ {b}(0 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = 0 $$</p></li><li><p><strong>Condition for classification close to the boundary or incorrect:</strong><br>$$ y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) &lt; 1. \quad \text{Correct, close to boundary, or incorrect} $$<br>$$ \nabla_ {\mathbf{w}} (1 - y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = -y_i \mathbf{x}_ i + \lambda \mathbf{w} $$<br>$$ \nabla_ {b} (1 - y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = -y_i $$</p></li></ul><h3 id="Stops">Stops</h3><p>Stop when</p><ul><li>predefined number of seasons or epochs</li><li>error on held-out data items is smaller than some threshold</li><li>other criteria</li></ul><p><strong>Regularization Constant $ \lambda $</strong></p><ul><li><p>Regularization constant $ \lambda $ in $ g(\mathbf{u}) = \frac{1}{2} \lambda \mathbf{w}^T \mathbf{w} $. Try at different scales (e.g., $ \lambda \in {10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1} $)</p></li><li><p><strong>Procedure for Cross-Validation:</strong></p><ul><li>Split dataset into Test Set and Train Set for cross-validation.</li><li>For each $ \lambda_i $ in set to try, iteratively:<ul><li>Generate a new Fold from Train Set with a Cross-Validation Train Set and Validation Set.</li><li>Using testing $ \lambda_i $, apply Stochastic Gradient Descent (SGD) on Cross-Validation Train Set to find $ \mathbf{w} $ and $ b $.</li><li>Evaluate $ \mathbf{w} $, $ b $, $ \lambda_i $ on Validation Set and record error for current Fold.</li><li>Cross-validation error for chosen $ \lambda_i $ is average error over all the Folds.</li></ul></li><li>Using $ \lambda $ with the lowest cross-validation error, apply SGD on whole training set to get final $ \mathbf{w} $ and $ b $.</li></ul></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Support vector machien is a very commonly used in machine learning</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Random Forest</title>
    <link href="https://karobben.github.io/2024/09/29/AI/randomforest/"/>
    <id>https://karobben.github.io/2024/09/29/AI/randomforest/</id>
    <published>2024-09-30T01:11:25.000Z</published>
    <updated>2024-10-09T22:59:13.565Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Basic-Architectures">Basic Architectures</h2><ol><li><strong>Bootstrap Sampling</strong>: Random subsets of the training data are created with replacement, known as bootstrap samples. Each subset is used to train an individual decision tree.</li><li><strong>Feature Randomness</strong>: For each tree, a random subset of features is considered at each split, reducing correlation among trees and improving generalization.</li><li><strong>Multiple Decision Trees</strong>: Multiple decision trees are grown independently using the bootstrap samples. Each tree makes a prediction for a given input.</li><li><strong>Ensemble Output</strong>: For classification, the output is typically based on majority voting across all trees, while for regression, the final output is an average of all tree predictions.</li></ol><h2 id="Limitations">Limitations</h2><ul><li>Many different trees can lead to similar classifications</li><li>The algorithm to build a decision tree grows each branch just deeply enough to perfectly classify the training examples<ul><li>potential overfit</li></ul></li><li>Randomness in identification of splits: features, thresholds<ul><li>better splits may have not been considered</li></ul></li><li>Addressed through Random Forests</li></ul><h2 id="Basic-of-Random-Forest">Basic of Random Forest</h2><h3 id="Tree-Expanding">Tree Expanding</h3><p>Random forest is build on number of decision trees. For avoiding the overfit, early stop is needed during the tree expanding.</p><ul><li>It stop when<ul><li>depth(branch_node) &gt;= max_depth (manual)</li><li>size(dataset) &lt;= min_leave_size (manual)</li><li>all elements in dataset in same class</li></ul></li></ul><h3 id="Decision-Making">Decision Making</h3><p>The best decision made could be evaluate by the “entropy”.</p><ol><li>2 classes:<br>Here are the formulas from the image converted into Markdown format:</li></ol><ul><li>$ \text{Entropy}(S) = -P(A) \log_2 P(A) - P(B) \log_2 P(B) $<ul><li>$ N = |A| + |B| $</li><li>$ P(A) = \frac{|A|}{N}, \quad P(B) = \frac{|B|}{N} $</li></ul></li></ul><ol start="2"><li>C classes:</li></ol><ul><li>$ \text{Entropy}(S) = - \sum_{i=1}^C P_i \log_2 P_i $<ul><li>Each class $ i $ with probability $ P_i $.</li></ul></li></ul><h3 id="Information-Gain">Information Gain</h3><p><strong>Goal</strong>: The goal is to maximize Information Gain at each split, which corresponds to choosing features that result in subsets with the least entropy, making the data more pure (less mixed) after the split. In Random Forest and decision tree learning, the feature with the highest Information Gain is selected for splitting at each node.</p><ul><li><strong>Definition</strong>: Information Gain measures the reduction in entropy (or uncertainty) after splitting a dataset. It helps to determine the best feature for splitting the data.</li><li><strong>Entropy Before Split</strong>: The initial dataset $ S $ has an entropy $ \text{Entropy}(S) $, which quantifies the impurity or randomness in the dataset.</li><li><strong>Entropy After Split</strong>: When the dataset is split into subsets $ S_l $ and $ S_r $, each subset has its own entropy: $ \text{Entropy}(S_l) $ and $ \text{Entropy}(S_r) $.</li><li><strong>Weighted Entropy</strong>: The weighted average of the entropy of the subsets after the split is given by:<br>$$<br>\text{Entropy}_{\text{after split}} = \frac{|S_l|}{|S|} \text{Entropy}(S_l) + \frac{|S_r|}{|S|} \text{Entropy}(S_r)<br>$$</li><li><strong>Information Gain Calculation</strong>: The Information Gain for the feature $ x^{(i)} $ is calculated as the difference between the entropy before and after the split:<br>$$<br>\text{Information Gain} = \text{Entropy}(S) - \text{Entropy}_{\text{after split}}<br>$$</li></ul><p>The symbols $ |S| $, $ |S_l| $, and $ |S_r| $ represent the <strong>cardinalities</strong> or the <strong>sizes</strong> of the respective sets, meaning they indicate the <strong>number of elements</strong> in each set.</p><h3 id="Missing-Values">Missing Values</h3><ul><li>In splits, if an item misses the feature value that decide where it goes<ul><li>Estimate it based on other examples: mode or mean</li></ul></li><li>Consider only the examples in the corresponding branch</li></ul><h2 id="Decision-Tree-in-Random-Forest">Decision Tree in Random Forest</h2><p>For get the best “Decision” in each branch, iterating through all possible splits at each node can be computationally expensive, especially for large datasets and numerous features. However, decision trees (and Random Forests) use several optimization techniques to find the best split efficiently, while managing computational cost:</p><h3 id="1-Feature-and-Threshold-Selection-Strategy">1. <strong>Feature and Threshold Selection Strategy</strong></h3><ul><li><strong>Greedy Algorithm</strong>: Decision tree algorithms commonly use a greedy approach to split at each node. They do not explore all possible trees but instead make the locally optimal choice (the split that maximizes Information Gain or minimizes entropy) at each step. While this doesn’t guarantee a globally optimal tree, it is computationally efficient.</li><li><strong>Threshold Optimization</strong>: Rather than testing every possible threshold for each feature, the algorithm often considers a subset of thresholds. If the feature is numeric, thresholds are typically evaluated at points between consecutive, sorted feature values.</li></ul><h3 id="2-Random-Forest-Feature-Subsampling">2. <strong>Random Forest Feature Subsampling</strong></h3><ul><li>In <strong>Random Forests</strong>, at each node, only a random subset of features is considered for splitting, rather than evaluating all features. This greatly reduces the number of calculations needed, enhances computational efficiency, and decorrelates the trees in the ensemble (increasing robustness).</li></ul><h3 id="3-Heuristics-to-Reduce-Computation">3. <strong>Heuristics to Reduce Computation</strong></h3><ul><li><strong>Best First Split</strong>: During the process, the split that gives the maximum Information Gain is stored, and the search for a better split continues until the end of the subset of considered thresholds. If no better split is found, the stored one is selected.</li><li><strong>Stopping Conditions</strong>: To further reduce resource usage, decision tree growth is often constrained by stopping criteria such as maximum depth, minimum number of samples per leaf, or if a split provides insufficient improvement.</li></ul><h3 id="4-Approximations-for-Efficiency">4. <strong>Approximations for Efficiency</strong></h3><ul><li><strong>Bin-based Thresholds</strong>: For numerical features, rather than considering every possible value as a split point, values can be grouped into bins. The potential split thresholds are then defined based on these bins.</li><li><strong>Pre-sorting Features</strong>: In some implementations, features are pre-sorted, so determining potential split points for numeric features can be faster.</li></ul><h3 id="5-Iterative-Splitting-and-Best-Split-Finding">5. <strong>Iterative Splitting and Best Split Finding</strong></h3><ul><li>For categorical features, the split can be done in subsets if there are many categories, or by considering binary splits. For numerical features, it evaluates splits between values.</li><li>Yes, it does involve iteration, but the optimizations listed above ensure that this iteration is performed in a manageable and efficient way without explicitly iterating through every possible split for all features.</li></ul><h3 id="Balancing-Efficiency-and-Quality-of-Decision-Trees"><strong>Balancing Efficiency and Quality of Decision Trees</strong></h3><p>The combination of the above techniques allows decision trees to strike a balance between:</p><ol><li><strong>Finding Good Splits</strong>: Even if the splits aren’t absolutely perfect, they are often good enough to form a strong decision tree.</li><li><strong>Limiting Resource Waste</strong>: Efficient search heuristics and optimizations are used to reduce the exhaustive computational cost.</li></ol><h3 id="In-Random-Forest">In Random Forest:</h3><ol><li><strong>Choose $ m = \sqrt{|x|} $ Features at Random</strong><ul><li>Instead of evaluating all features for a potential split, a random subset of features is selected to reduce computation. The number of features selected ($ m $) is proportional to the square root of the total number of features ($ |x| $).</li><li>This is a common technique in Random Forests to decorrelate individual decision trees and make the algorithm <mark>computationally efficient</mark>. It prevents overfitting by introducing randomness and limits the number of features under consideration at each node.</li></ul></li><li><strong>Identify Candidate Splits for the Selected Feature $ x^{(i)} $</strong><ul><li><strong>Feature Sorting</strong>:<ul><li>The feature values ($ x^{(i)} $) can be sorted to determine the best thresholds for splitting the dataset.</li></ul></li><li><strong>Class Boundaries as Thresholds</strong>:<ul><li>The sorted feature values are evaluated to find boundaries between different classes.</li></ul></li><li><strong>Sort Data Items According to Feature Value</strong>:<ul><li>All data points are sorted by their value for the feature $ x^{(i)} $. This allows easy identification of candidate split points.</li></ul></li><li><strong>Adjacent Pairs in Different Classes</strong>:<ul><li>The algorithm looks for adjacent pairs of data points where one belongs to a different class than the other. This suggests a potential decision boundary.</li><li>These pairs, $ (item_0, item_1) $, are identified since they may represent a significant change in class, making them good candidates for splitting.</li></ul></li><li><strong>Threshold Midway Between $ item_0 $ and $ item_1 $</strong>:<ul><li>The threshold for the split is placed midway between these adjacent items from different classes. This ensures that the split captures the difference between the classes as effectively as possible.</li></ul></li></ul></li><li><strong>Randomly Select $ k $ Thresholds</strong><ul><li>To further limit the number of potential splits to evaluate, the algorithm randomly selects $ k $ thresholds from the identified candidate thresholds. This further reduces computational cost while maintaining a good chance of finding an effective split.</li><li>This random sampling balances computational efficiency with the quality of the splits, ensuring that the decision tree doesn’t become too computationally expensive.</li></ul></li><li>Summary<br>The image explains a process that helps reduce the number of potential splits evaluated at each node:<ol><li><strong>Random Subset of Features</strong>: Only a random $ m $ features are considered.</li><li><strong>Identifying Thresholds</strong>: For each selected feature, potential split thresholds are identified by analyzing class boundaries.</li><li><strong>Random Selection of Split Points</strong>: A random subset of the identified thresholds is evaluated.<br>These steps are taken to avoid an exhaustive search, reduce computational resources, and prevent overfitting, particularly in Random Forests where multiple trees are built.</li></ol></li></ol><h3 id="Step-by-Step-Explanation-with-Equations-and-Code">Step-by-Step Explanation with Equations and Code</h3><h4 id="Step-1-Choosing-m-sqrt-x-Features-at-Random">Step 1: Choosing $ m = \sqrt{|x|} $ Features at Random</h4><ul><li>Suppose you have $ |x| $ features in your dataset.</li><li>To decide the split, randomly select $ m $ features to evaluate, where:<br>$$<br>m =  \sqrt{|x|}<br>$$</li></ul><h5 id="Python-Code-Representation">Python Code Representation:</h5><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># data from: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database?resource=download</span><br><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>data = pd.read_csv(<span class="hljs-string">&#x27;/home/yiran/Downloads/diabetes.csv&#x27;</span>)<br>features = d.columns[:-<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># Total number of features</span><br>num_features = <span class="hljs-built_in">len</span>(features)<br><br><span class="hljs-comment"># Choose m features at random</span><br>m = <span class="hljs-built_in">int</span>(np.sqrt(num_features))<br>selected_features = np.random.choice(features, m, replace=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure></div><h4 id="Step-2-Identify-Candidate-Splits-for-a-Feature-x-i">Step 2: Identify Candidate Splits for a Feature $ x^{(i)} $</h4><p>For each feature selected, you need to determine possible thresholds to split the data.</p><h5 id="Steps-in-Code">Steps in Code:</h5><ol><li><p><strong>Sort Feature Values</strong>:</p><ul><li>For the selected feature $ x^{(i)} $, sort the data points by their feature values.</li></ul></li><li><p><strong>Identify Boundaries Between Classes</strong>:</p><ul><li>Find the pairs of data points that belong to different classes.</li><li>Calculate candidate thresholds midway between these pairs.</li></ul></li></ol><h5 id="Equations-for-Finding-Thresholds">Equations for Finding Thresholds:</h5><ul><li><p>Let $ x^{(i)}_j $ represent the feature value for data point $ j $.</p></li><li><p>Sort all values of feature $ x^{(i)} $:<br>$$<br>x^{(i)}_1, x^{(i)}_2, \ldots, x^{(i)}_n \quad \text{where } x^{(i)}_1 &lt; x^{(i)}_2 &lt; \ldots &lt; x^{(i)}_n<br>$$</p></li><li><p>Identify the adjacent pairs that belong to different classes. For a pair of adjacent items $ (x^ {(i)}_ j, x^ {(i)}_ {j+1}) $ from different classes, the candidate threshold $ t_j $ is given by:<br>$$<br>t_ j = \frac{x^ {(i)}_ j + x^ {(i)}_ {j+1}}{2}<br>$$</p></li></ul><h5 id="Python-Code-Representation-v2">Python Code Representation:</h5><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Assume we have a data structure `data` which contains features and labels</span><br><span class="hljs-comment"># We are focusing on the selected feature xi</span><br><br>data = TB.T.to_dict()<br>data = [data[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> data]<br><br><span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> selected_features:<br>    <span class="hljs-comment"># Sort data based on the selected feature&#x27;s value</span><br>    sorted_data = <span class="hljs-built_in">sorted</span>(data, key=<span class="hljs-keyword">lambda</span> d: d[feature])<br>    <span class="hljs-comment"># Find candidate thresholds</span><br>    candidate_thresholds = []<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sorted_data) - <span class="hljs-number">1</span>):<br>        <span class="hljs-comment"># If the class label changes between adjacent items</span><br>        <span class="hljs-keyword">if</span> sorted_data[j][<span class="hljs-string">&#x27;Outcome&#x27;</span>] != sorted_data[j + <span class="hljs-number">1</span>][<span class="hljs-string">&#x27;Outcome&#x27;</span>]:<br>            <span class="hljs-comment"># Find the midpoint between two adjacent feature values</span><br>            threshold = (sorted_data[j][feature] + sorted_data[j + <span class="hljs-number">1</span>][feature]) / <span class="hljs-number">2</span><br>            candidate_thresholds.append(threshold)<br></code></pre></td></tr></table></figure></div><div class="admonition note"><p class="admonition-title">note</p><p>In the code <code>sorted_data = sorted(data, key=lambda d: d[feature])</code></p><pre>feature_value_pairs = [(d, d[feature]) for d in data]# Step 2: Sort based on the feature valuefeature_value_pairs_sorted = sorted(feature_value_pairs, key=lambda pair: pair[1])# Step 3: Extract the sorted data pointssorted_data = [pair[0] for pair in feature_value_pairs_sorted]</pre></div><h4 id="Step-3-Randomly-Select-k-Thresholds">Step 3: Randomly Select $ k $ Thresholds</h4><ul><li>Randomly pick $ k $ thresholds from the candidate thresholds identified in the previous step to reduce computation.</li></ul><h5 id="Equation">Equation:</h5><ul><li>Suppose $ T = { t_1, t_2, \ldots, t_p } $ is the set of candidate thresholds.</li><li>Select $ k $ thresholds randomly from $ T $:<br>$$<br>T_{\text{selected}} = { t_{i_1}, t_{i_2}, \ldots, t_{i_k} }, \quad \text{where } i_j \in {1, \ldots, p}<br>$$</li></ul><h5 id="Python-Code-Representation-v3">Python Code Representation:</h5><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><br>some_predefined_k = <span class="hljs-number">15</span><br><span class="hljs-comment"># Number of thresholds to randomly select</span><br>k = <span class="hljs-built_in">min</span>(<span class="hljs-built_in">len</span>(candidate_thresholds), some_predefined_k)<br><br><span class="hljs-comment"># Randomly select k thresholds from candidate_thresholds</span><br>selected_thresholds = np.random.choice(candidate_thresholds, k, replace=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure></div><h4 id="Step-4-Compute-Information-Gain-for-Each-Split">Step 4: Compute Information Gain for Each Split</h4><ul><li>Iterate through the selected thresholds to compute the Information Gain and select the best one.</li></ul><h5 id="Equation-for-Information-Gain">Equation for Information Gain:</h5><ul><li><p>For a given threshold $ t $, split the data into two subsets:<br>$$<br>S_l = { x \in S : x^{(i)} \le t }, \quad S_r = { x \in S : x^{(i)} &gt; t }<br>$$</p></li><li><p>Compute the weighted entropy after the split:<br>$$<br>\text{Entropy}_{\text{after}} = \frac{|S_l|}{|S|}\text{Entropy}(S_l) + \frac{|S_r|}{|S|}\text{Entropy}(S_r)<br>$$</p></li><li><p>Compute the Information Gain:<br>$$<br>\text{Information Gain} = \text{Entropy}(S) - \text{Entropy}_{\text{after}}<br>$$</p></li></ul><h5 id="Python-Code-Representation-v4">Python Code Representation:</h5><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">entropy</span>(<span class="hljs-params">data</span>):</span><br>    <span class="hljs-comment"># Assume `data` has a function to calculate entropy</span><br>    labels = [d[<span class="hljs-string">&#x27;Outcome&#x27;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> data]<br>    <span class="hljs-comment"># Count the occurrences of each label</span><br>    label_counts = Counter(labels)<br>    total_count = <span class="hljs-built_in">len</span>(data)<br>    <span class="hljs-comment"># Calculate the entropy</span><br>    ent = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> label_counts.values():<br>        <span class="hljs-comment"># Calculate the probability of each label</span><br>        p = count / total_count<br>        <span class="hljs-comment"># Add to entropy, using the formula -p * log2(p)</span><br>        <span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0</span>:<br>            ent -= p * np.log2(p)<br>    <span class="hljs-keyword">return</span> ent<br><br>best_gain = -np.inf<br>best_threshold = <span class="hljs-literal">None</span><br><br><span class="hljs-keyword">for</span> threshold <span class="hljs-keyword">in</span> selected_thresholds:<br>    <span class="hljs-comment"># Split the data into left and right based on the threshold</span><br>    left_split = [d <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> sorted_data <span class="hljs-keyword">if</span> d[feature] &lt;= threshold]<br>    right_split = [d <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> sorted_data <span class="hljs-keyword">if</span> d[feature] &gt; threshold]<br>    <br>    <span class="hljs-comment"># Calculate the weighted entropy of the two subsets</span><br>    p_left = <span class="hljs-built_in">len</span>(left_split) / <span class="hljs-built_in">len</span>(sorted_data)<br>    p_right = <span class="hljs-built_in">len</span>(right_split) / <span class="hljs-built_in">len</span>(sorted_data)<br>    <br>    entropy_after = p_left * entropy(left_split) + p_right * entropy(right_split)<br>    gain = entropy(sorted_data) - entropy_after<br>    <br>    <span class="hljs-comment"># Update the best gain and threshold</span><br>    <span class="hljs-keyword">if</span> gain &gt; best_gain:<br>        best_gain = gain<br>        best_threshold = threshold<br></code></pre></td></tr></table></figure></div><h4 id="Summary">Summary</h4><ol><li><strong>Select Features Randomly</strong>: $ m = \sqrt{|x|} $ features are selected randomly to evaluate.</li><li><strong>Determine Candidate Thresholds</strong>: For each feature, the data is sorted, and class boundaries are used to identify potential split points.</li><li><strong>Random Threshold Selection</strong>: From the candidate thresholds, a random subset is chosen to reduce computational cost.</li><li><strong>Calculate Information Gain</strong>: Evaluate the Information Gain for each threshold to find the best split.</li></ol><p>These steps ensure that the decision tree algorithm efficiently finds good splits without exhaustively considering all possible splits. The randomness helps reduce computational costs and enhances the model’s robustness, especially in Random Forests.</p><h2 id="Another-Example">Another Example</h2><ol><li><a href="https://github.com/HaoranTang/Applied-Machine-Learning/blob/main/ClassifyingImages.ipynb">Applied-Machine-Learning/ClassifyingImages.ipynb</a></li><li>Quick ChatGPT example:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Step 1: Import Libraries</span><br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><br><span class="hljs-comment"># Step 2: Load and Prepare Data</span><br><span class="hljs-comment"># We&#x27;ll use the Iris dataset as an example</span><br>iris = load_iris()<br>X, y = iris.data, iris.target<br><br><span class="hljs-comment"># Split the data into training and test sets (80% train, 20% test)</span><br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># Step 3: Train the Random Forest Classifier</span><br>clf = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, random_state=<span class="hljs-number">42</span>)  <span class="hljs-comment"># 100 trees in the forest</span><br>clf.fit(X_train, y_train)<br><br><span class="hljs-comment"># Step 4: Make Predictions and Evaluate</span><br>y_pred = clf.predict(X_test)<br><br><span class="hljs-comment"># Calculate the accuracy</span><br>accuracy = accuracy_score(y_test, y_pred)<br>print(<span class="hljs-string">f&#x27;Accuracy: <span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.2</span>f&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure></div><hr><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Random Forest</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>DLVO theory: Atom Interaction</title>
    <link href="https://karobben.github.io/2024/08/16/LearnNotes/DLVOtheory/"/>
    <id>https://karobben.github.io/2024/08/16/LearnNotes/DLVOtheory/</id>
    <published>2024-08-16T21:27:54.000Z</published>
    <updated>2024-08-17T04:33:44.970Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DLVO-Theory">DLVO Theory</h2><table><thead><tr><th style="text-align:center"><img src="https://ars.els-cdn.com/content/image/3-s2.0-B0080431526016223-gr3.gif" alt="DLVO theory"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.sciencedirect.com/science/article/abs/pii/B0080431526016223">© J.H. Adair; 2001</a></td></tr></tbody></table><p>DLVO theory is named after Derjaguin, Landau, Verwey, and Overbeek, who developed it in the 1940s. It describes the forces between charged surfaces interacting through a liquid medium. The theory combines two main types of forces:</p><ol><li><strong>Van der Waals forces:</strong> These are attractive forces that arise from induced electrical interactions between molecules or atoms.</li><li><strong>Electrostatic double-layer forces:</strong> These are repulsive forces that occur due to the overlap of electrical double layers surrounding charged particles.</li></ol><p>The balance between these forces determines whether particles will aggregate (if the attractive forces dominate) or remain stable in suspension (if the repulsive forces dominate). This theory is widely used in colloid chemistry, environmental science, and materials science to understand and predict the stability of colloidal dispersions.</p><p>The DLVO theory describes the interaction energy $ U_{total} $ between two colloidal particles as the sum of the Van der Waals attraction $ U_{VdW} $ and the electrostatic repulsion $ U_{elec} $. The general form of the DLVO potential is given by:</p><p>$$ U_{total}(h) = U_{VdW}(h) + U_{elec}(h) $$</p><p>where $ h $ is the distance between the surfaces of the particles.</p><h3 id="Van-der-Waals-Attraction-U-VdW">Van der Waals Attraction ($ U_{VdW} $)</h3><p>The Van der Waals attraction energy between two spherical particles of radius $ R $ at a separation distance $ h $ is given by:</p><p>$$ U_{VdW}(h) = - \frac{A}{6} \left( \frac{2R^2}{h(2R + h)} + \frac{2R^2}{(2R + h)^2} + \ln \left( \frac{h}{2R + h} \right) \right) $$</p><ul><li>$ A $: Hamaker constant (typically around $ 10^{-20} $ J for biological systems)</li><li>$ R $: Radius of the amino acid (approx. $ 0.5 $ nm or $ 0.5 \times 10^{-9} $ m)</li><li>$ h $: Separation distance between the particles</li></ul><div class="admonition note"><p class="admonition-title">What if the atoms are different?</p><p>$R^2= R_1 * R_2$$2R = R_1 + R_2$</p></div><h3 id="Electrostatic-Repulsion-U-elec">Electrostatic Repulsion ($ U_{elec} $)</h3><p>The electrostatic repulsion energy between two spherical particles with surface potential $ \psi_0 $ and radius $ R $ in a medium with Debye length $ \kappa^{-1} $ (which is related to the ionic strength of the medium) is given by:</p><p>$$ U_{elec}(h) = 2 \pi \epsilon R \psi_0^2 \ln \left( 1 + \exp(-\kappa h) \right) $$</p><ul><li>$ \epsilon $: Permittivity of the medium (water, typically $ 80 \times 8.854 \times 10^{-12} $ F/m)</li><li>$ \psi_0 $: Surface potential (approx. $ 25 $ mV or $ 25 \times 10^{-3} $ V)</li><li>$ \kappa $: Inverse Debye length (for a Debye length of $ 1 $ nm, $ \kappa \approx 10^9 $ m$^{-1}$)</li><li>$ h $: Separation distance between the particles</li></ul><h3 id="Total-Interaction-Energy">Total Interaction Energy</h3><p>Combining these two expressions, the total interaction energy is:</p><p>$$ U_{total}(h) = - \frac{A}{6} \left( \frac{2R^2}{h(2R + h)} + \frac{2R^2}{(2R + h)^2} + \ln \left( \frac{h}{2R + h} \right) \right) + 2 \pi \epsilon R \psi_0^2 \ln \left( 1 + \exp(-\kappa h) \right) $$</p><p>This equation allows us to predict whether the colloidal particles will repel each other and remain stable in suspension or attract each other and aggregate, depending on the balance of the attractive and repulsive forces.</p><h3 id="Separation-Distance-h">Separation Distance (h)</h3><ul><li>If the radii $ R_1 $ and $ R_2 $ of two spherical particles are known, and the center-to-center distance between them is $ D $, the separation distance $ h $ is calculated as:<br>$$<br>h = D - (R_1 + R_2)<br>$$</li><li>For identical particles with the same radius $ R $, it simplifies to:<br>$$<br>h = D - 2R<br>$$</li></ul><p>In the code snippet provided, the parameters can be categorized into <strong>constant parameters</strong> (those that remain the same across different residues) and <strong>variable parameters</strong> (those that may change depending on the specific residues or the system under consideration).</p><h2 id="Parameters">Parameters</h2><h3 id="Constant-Parameters">Constant Parameters:</h3><ol><li><p><strong>$ A $</strong> (Hamaker constant):</p><ul><li><strong>Value:</strong> $ 1 \times 10^{-20} $ J</li><li><strong>Description:</strong> This is a material-specific constant that depends on the nature of the interacting particles and the medium. For biological molecules in water, it’s often taken as a constant.</li></ul></li><li><p><strong>$ \epsilon $</strong> (Permittivity of the medium):</p><ul><li><strong>Value:</strong> $ 80 \times 8.854 \times 10^{-12} $ F/m (Permittivity of water)</li><li><strong>Description:</strong> The permittivity of the medium (usually water in biological contexts) is a constant based on the dielectric properties of the solvent.</li></ul></li><li><p><strong>$ \kappa $</strong> (Inverse Debye length):</p><ul><li><strong>Value:</strong> $ 1 \times 10^9 $ m$^{-1}$</li><li><strong>Description:</strong> The inverse Debye length is related to the ionic strength of the medium and is often considered constant under specific conditions, such as physiological ionic strength.</li></ul></li></ol><h3 id="Variable-Parameters">Variable Parameters:</h3><ol><li><p><strong>$ R $</strong> (Radius of the amino acid):</p><ul><li><strong>Value:</strong> $ 0.5 \times 10^{-9} $ m (0.5 nm)</li><li><strong>Description:</strong> The radius could vary slightly between different amino acids, especially when considering side chains. The value used here is an approximation and might need adjustment for specific residues.</li></ul></li><li><p><strong>$ \psi_0 $</strong> (Surface potential):</p><ul><li><strong>Value:</strong> $ 25 \times 10^{-3} $ V (25 mV)</li><li><strong>Description:</strong> The surface potential can vary depending on the charge state of the amino acid side chains. For example, charged residues like lysine or aspartic acid will have different surface potentials compared to neutral residues like alanine.</li></ul></li><li><p><strong>$ h $</strong> (Separation distance):</p><ul><li><strong>Value:</strong> Range from $ 0.1 \times 10^{-9} $ m to $ 10 \times 10^{-9} $ m</li><li><strong>Description:</strong> The separation distance between two residues or atoms is the primary variable in these calculations, often determined by the 3D structure of the protein or molecular complex being studied.</li></ul></li></ol><h2 id="Who-to-Know-the-R-and-h">Who to Know the R and h?</h2><p>To calculate the radius of amino acids such as valine (V) and phenylalanine (F), you’re generally referring to an approximation of the <strong>van der Waals (VDW) radius</strong> or the <strong>effective radius</strong> of the entire amino acid side chain. This radius can be used in models like DLVO theory to represent the size of the interacting particle.</p><h3 id="Methods-to-Determine-the-Radius">Methods to Determine the Radius:</h3><ol><li><p><strong>Van der Waals Radius of Atoms:</strong></p><ul><li>The van der Waals radius is an inherent property of atoms and can be summed up to approximate the radius of a molecule or side chain. For example, the VDW radius for carbon is about 1.7 Å, and for hydrogen, it’s about 1.2 Å.</li></ul></li><li><p><strong>Effective Radius from Crystal Structures:</strong></p><ul><li>If you have a crystal structure or molecular model, you can measure the effective radius of the side chain by considering the spatial extent of the side chain atoms. This is often done using software tools that can calculate the solvent-accessible surface area (SASA) or by directly measuring distances in a molecular viewer.</li></ul></li><li><p><strong>Using Approximate Values from Literature:</strong></p><ul><li>For many applications, approximate radii for amino acids are available in the literature based on their typical side chain sizes.</li></ul></li></ol><h3 id="Approximate-Radii-for-Valine-V-and-Phenylalanine-F">Approximate Radii for Valine (V) and Phenylalanine (F):</h3><ul><li><p><strong>Valine (V):</strong></p><ul><li>Valine has a branched, non-polar side chain. Its effective radius is often approximated as <strong>~3.0 Å (0.3 nm)</strong>.</li></ul></li><li><p><strong>Phenylalanine (F):</strong></p><ul><li>Phenylalanine has a larger, aromatic side chain. Its effective radius is typically around <strong>~3.5-4.0 Å (0.35-0.4 nm)</strong>.</li></ul></li></ul><p>These values are not exact but are generally used in theoretical calculations.</p><h3 id="Calculation-Example">Calculation Example:</h3><p>If you need to calculate the interaction between valine (V) and phenylalanine (F), you could use these approximate radii:</p><ul><li><strong>Valine (V):</strong> $ R_V \approx 0.3 $ nm</li><li><strong>Phenylalanine (F):</strong> $ R_F \approx 0.35 $ nm</li></ul><p>The separation distance $ h $ would then be calculated based on the center-to-center distance $ D $ between the residues:</p><p>$$<br>h = D - (R_V + R_F)<br>$$</p><p>If $ D $ is known (from a crystal structure or a model), this formula gives the separation distance $ h $ between the residues’ surfaces.</p><h3 id="Tools-for-More-Accurate-Measurements">Tools for More Accurate Measurements:</h3><ul><li><strong>Molecular Visualization Software (e.g., PyMOL, Chimera):</strong> You can load a protein structure and measure the distance between specific atoms or calculate the van der Waals surface.</li><li><strong>Computational Tools:</strong> Software packages like CHARMM, AMBER, or GROMACS can provide detailed calculations based on molecular dynamics or energy minimization, giving more precise values for radii in specific contexts.</li></ul><h2 id="Python-Script">Python Script</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">van_der_waals</span>(<span class="hljs-params">h, A, R</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Calculate Van der Waals attraction energy.&quot;&quot;&quot;</span><br>    term1 = (<span class="hljs-number">2</span> * R**<span class="hljs-number">2</span>) / (h * (<span class="hljs-number">2</span> * R + h))<br>    term2 = (<span class="hljs-number">2</span> * R**<span class="hljs-number">2</span>) / (<span class="hljs-number">2</span> * R + h)**<span class="hljs-number">2</span><br>    term3 = np.log(h / (<span class="hljs-number">2</span> * R + h))<br>    <span class="hljs-keyword">return</span> - (A / <span class="hljs-number">6</span>) * (term1 + term2 + term3)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">electrostatic_repulsion</span>(<span class="hljs-params">h, epsilon, R, psi_0, kappa</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Calculate electrostatic repulsion energy.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * np.pi * epsilon * R * psi_0**<span class="hljs-number">2</span> * np.log(<span class="hljs-number">1</span> + np.exp(-kappa * h))<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dlvo_total</span>(<span class="hljs-params">h, A, R, epsilon, psi_0, kappa</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Calculate total DLVO interaction energy.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> van_der_waals(h, A, R) + electrostatic_repulsion(h, epsilon, R, psi_0, kappa)<br><br><span class="hljs-comment"># Parameters</span><br>A = <span class="hljs-number">1e-20</span>  <span class="hljs-comment"># Hamaker constant in J</span><br>R = <span class="hljs-number">1e-7</span>   <span class="hljs-comment"># Radius of particles in m</span><br>epsilon = <span class="hljs-number">80</span> * <span class="hljs-number">8.854e-12</span>  <span class="hljs-comment"># Permittivity of water in F/m</span><br>psi_0 = <span class="hljs-number">25e-3</span>  <span class="hljs-comment"># Surface potential in V</span><br>kappa = <span class="hljs-number">1e8</span>    <span class="hljs-comment"># Inverse Debye length in 1/m</span><br>h = np.linspace(<span class="hljs-number">5e-10</span>, <span class="hljs-number">1e-7</span>, <span class="hljs-number">400</span>)  <span class="hljs-comment"># Separation distance in m</span><br><br><span class="hljs-comment"># Calculate DLVO potential</span><br>U_vdw = van_der_waals(h, A, R)<br>U_elec = electrostatic_repulsion(h, epsilon, R, psi_0, kappa)<br>U_total = dlvo_total(h, A, R, epsilon, psi_0, kappa)<br><br><span class="hljs-comment"># Plot results</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.plot(h * <span class="hljs-number">1e9</span>, U_vdw, label=<span class="hljs-string">&#x27;Van der Waals Attraction&#x27;</span>, linestyle=<span class="hljs-string">&#x27;-.&#x27;</span>)<br>plt.plot(h * <span class="hljs-number">1e9</span>, U_elec, label=<span class="hljs-string">&#x27;Electrostatic Repulsion&#x27;</span>, linestyle=<span class="hljs-string">&#x27;-.&#x27;</span>)<br>plt.plot(h * <span class="hljs-number">1e9</span>, U_total, label=<span class="hljs-string">&#x27;Total DLVO Potential&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Separation Distance (nm)&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Interaction Energy (J)&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;DLVO Theory Interaction Energy&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/GMxKF8T.png" alt=""></p><h2 id="Simplified-System">Simplified System</h2><h3 id="Empirical-Formula-in-the-Context-of-Your-Problem"><strong>Empirical Formula in the Context of Your Problem</strong></h3><p>A general empirical formula for the interaction energy $ \Delta G_{i,j} $ between two residues $ H_i $ and $ A_j $ might look like this:</p><p>$$<br>\Delta G_{i,j} = V_{\text{LJ}}(r_{ij}) + V_{\text{Coulomb}}(r_{ij})<br>$$</p><p>Where:</p><ul><li>$ r_{ij} $ is the distance between residue $ H_i $ and residue $ A_j $.</li><li>$ V_{\text{LJ}}(r_{ij}) $ represents the van der Waals interaction.</li><li>$ V_{\text{Coulomb}}(r_{ij}) $ represents the electrostatic interaction.</li></ul><h3 id="Lennard-Jones-Potential-van-der-Waals-Interactions"><strong>Lennard-Jones Potential (van der Waals Interactions)</strong></h3><p>The Lennard-Jones potential is a commonly used empirical formula to describe the van der Waals forces between two non-bonded atoms or molecules. It has the form:</p><p>$$<br>V_{\text{LJ}}( r ) = 4\epsilon \left[ \left(\frac{\sigma}{r}\right)^{12} - \left(\frac{\sigma}{r}\right)^{6} \right]<br>$$</p><ul><li>$ V_{\text{LJ}}( r ) $ is the potential energy as a function of distance $ r $ between two particles.</li><li>$ \epsilon $ is the depth of the potential well, representing the strength of the interaction.</li><li>$ \sigma $ is the distance at which the potential energy is zero (often related to the size of the atoms/molecules).</li><li>$ r $ is the distance between the two particles.</li></ul><p>The term $ \left(\frac{\sigma}{r}\right)^{12} $ represents the repulsive interaction at short distances (due to Pauli exclusion principle), and the term $ \left(\frac{\sigma}{r}\right)^{6} $ represents the attractive van der Waals forces at longer distances.</p><h3 id="Coulomb’s-Law-Electrostatic-Interactions"><strong>Coulomb’s Law (Electrostatic Interactions)</strong></h3><p>Coulomb’s law describes the electrostatic interaction between two charged particles:</p><p>$$<br>V_{\text{Coulomb}}( r ) = \frac{k_e \cdot q_1 \cdot q_2}{r}<br>$$</p><ul><li>$ V_{\text{Coulomb}}( r ) $ is the potential energy between two charges.</li><li>$ k_e $ is Coulomb’s constant ($ 8.9875 \times 10^9 , \text{N} \cdot \text{m}<sup>2/\text{C}</sup>2 $ in vacuum).</li><li>$ q_1 $ and $ q_2 $ are the charges of the two interacting particles.</li><li>$ r $ is the distance between the two charges.</li></ul><h3 id="Python-Script-v2">Python Script</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> prody <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Load the protein structure</span><br>structure = parsePDB(<span class="hljs-string">&#x27;your_structure.pdb&#x27;</span>)<br><br><span class="hljs-comment"># Select the side chains of the residues of interest</span><br>residue1_sidechain = structure.select(<span class="hljs-string">&#x27;resid 10 and sidechain&#x27;</span>)<br>residue2_sidechain = structure.select(<span class="hljs-string">&#x27;resid 20 and sidechain&#x27;</span>)<br><br><span class="hljs-comment"># Function to calculate interaction energy considering only side chains</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_interaction_energy</span>(<span class="hljs-params">residue1, residue2, cutoff=<span class="hljs-number">5.0</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Calculate the van der Waals and electrostatic interaction energy </span><br><span class="hljs-string">    between two residues&#x27; side chains using a simple empirical formula.</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    :param residue1: ProDy atom group for the first residue side chain</span><br><span class="hljs-string">    :param residue2: ProDy atom group for the second residue side chain</span><br><span class="hljs-string">    :param cutoff: Distance cutoff for interaction (in Å)</span><br><span class="hljs-string">    :return: Tuple of (vdW energy, electrostatic energy)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># van der Waals parameters (simplified example)</span><br>    epsilon = <span class="hljs-number">0.1</span>  <span class="hljs-comment"># Depth of the potential well (kcal/mol)</span><br>    sigma = <span class="hljs-number">3.5</span>  <span class="hljs-comment"># Distance at which the potential is zero (Å)</span><br>    <br>    <span class="hljs-comment"># Coulomb constant (for electrostatic energy calculation)</span><br>    k_e = <span class="hljs-number">8.9875517873681764e9</span>  <span class="hljs-comment"># N m² C⁻² (can be adjusted for unit compatibility)</span><br>    <br>    <span class="hljs-comment"># Simplified charges for electrostatic calculation</span><br>    charge1 = np.<span class="hljs-built_in">sum</span>([atom.getCharge() <span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> residue1])<br>    charge2 = np.<span class="hljs-built_in">sum</span>([atom.getCharge() <span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> residue2])<br>    <br>    vdW_energy = <span class="hljs-number">0.0</span><br>    electrostatic_energy = <span class="hljs-number">0.0</span><br><br>    <span class="hljs-comment"># Calculate pairwise interactions considering only side chains</span><br>    <span class="hljs-keyword">for</span> atom1 <span class="hljs-keyword">in</span> residue1:<br>        <span class="hljs-keyword">for</span> atom2 <span class="hljs-keyword">in</span> residue2:<br>            distance = np.linalg.norm(atom1.getCoords() - atom2.getCoords())<br>            <span class="hljs-keyword">if</span> distance &lt; cutoff:<br>                <span class="hljs-comment"># van der Waals energy (Lennard-Jones potential)</span><br>                vdW_energy += <span class="hljs-number">4</span> * epsilon * ((sigma / distance)**<span class="hljs-number">12</span> - (sigma / distance)**<span class="hljs-number">6</span>)<br>                <span class="hljs-comment"># Electrostatic energy (Coulomb&#x27;s law)</span><br>                electrostatic_energy += k_e * (charge1 * charge2) / distance<br>    <span class="hljs-keyword">return</span> vdW_energy, electrostatic_energy<br><br><span class="hljs-comment"># Calculate interaction energy</span><br>vdW_energy, electrostatic_energy = calculate_interaction_energy(residue1_sidechain, residue2_sidechain)<br>print(<span class="hljs-string">f&quot;van der Waals Energy: <span class="hljs-subst">&#123;vdW_energy:<span class="hljs-number">.2</span>f&#125;</span> kcal/mol&quot;</span>)<br>print(<span class="hljs-string">f&quot;Electrostatic Energy: <span class="hljs-subst">&#123;electrostatic_energy:<span class="hljs-number">.2</span>f&#125;</span> kcal/mol&quot;</span>)<br></code></pre></td></tr></table></figure></div><details><summary>Codes for the plot</summary><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">h = np.linspace(<span class="hljs-number">3.3</span>, <span class="hljs-number">10</span>, <span class="hljs-number">400</span>)  <span class="hljs-comment"># Separation distance in m</span><br><span class="hljs-comment"># Plot results</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.plot(h, [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> h], linestyle = <span class="hljs-string">&#x27;-.&#x27;</span>)<br>plt.plot(h, [calculate_interaction_energy(i)  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> h], color = <span class="hljs-string">&#x27;salmon&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Separation Distance (Åm)&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Interaction Energy&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Lennard-Jones Potential&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div></details><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/wmcPi2K.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">In this plot, it shows the change of the Lennard-Jones Potential with the change of the distance when the $\sigma = 3.5$</td></tr></tbody></table><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">DLVO theory describes the forces between charged surfaces interacting through a liquid medium. The theory combines two main types of forces</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="other" scheme="https://karobben.github.io/categories/Notes/other/"/>
    
    
    <category term="Physics" scheme="https://karobben.github.io/tags/Physics/"/>
    
  </entry>
  
  <entry>
    <title>Kernel Density Estimation (KDE)</title>
    <link href="https://karobben.github.io/2024/08/16/AI/KernelDensityEstimation/"/>
    <id>https://karobben.github.io/2024/08/16/AI/KernelDensityEstimation/</id>
    <published>2024-08-16T20:41:30.000Z</published>
    <updated>2024-10-09T23:01:25.526Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kernel-Density-Estimation-KDE">Kernel Density Estimation (KDE)</h2><p><strong>Kernel Density Estimation (KDE)</strong> is a non-parametric method to estimate the probability density function (PDF) of a random variable based on a finite set of data points. Unlike parametric methods, which assume that the underlying data follows a specific distribution (like normal, exponential, etc.), KDE makes no such assumptions and can model more complex data distributions.</p><h3 id="How-KDE-Works">How KDE Works:</h3><ol><li><p><strong>Kernel Function</strong>: The kernel function is a smooth, continuous, symmetric function that is centered on each data point. The most commonly used kernel is the Gaussian (normal) kernel, but other kernels like Epanechnikov, triangular, and uniform can also be used.</p></li><li><p><strong>Bandwidth (Smoothing Parameter)</strong>: The bandwidth is a crucial parameter that controls the smoothness of the KDE. It determines the width of the kernel functions. A smaller bandwidth leads to a more sensitive, less smooth estimate, while a larger bandwidth produces a smoother, less sensitive estimate.</p></li><li><p><strong>Summation of Kernels</strong>: KDE constructs the overall density estimate by summing the contributions of each kernel function across all data points. Each data point contributes a small “bump” to the estimate, and the sum of these bumps forms the estimated density function.</p></li></ol><h3 id="KDE-Formula">KDE Formula:</h3><p>Given a set of $ n $ data points $ x_1, x_2, \ldots, x_n $, the KDE at a point $ x $ is calculated as:</p><p>$$<br>\hat{f}(x) = \frac{1}{n \cdot h} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)<br>$$</p><p>Where:</p><ul><li>$ \hat{f}(x) $ is the estimated density at point $ x $.</li><li>$ n $ is the number of data points.</li><li>$ h $ is the bandwidth.</li><li>$ K $ is the kernel function.</li><li>$ x_i $ are the observed data points.</li></ul><h3 id="Example-of-KDE">Example of KDE:</h3><p>Imagine you have a dataset of people’s heights. Rather than assuming the heights follow a specific distribution (like normal), KDE allows you to estimate the distribution directly from the data, which may reveal subtle features like bimodal distributions (e.g., a mix of two distinct groups).</p><h3 id="Advantages-of-KDE">Advantages of KDE:</h3><ul><li><strong>Flexible</strong>: KDE doesn’t assume any specific form of the distribution, making it suitable for complex and unknown distributions.</li><li><strong>Smooth Estimation</strong>: It provides a smooth estimate of the density function, which can be more informative than histograms.</li></ul><h3 id="Disadvantages-of-KDE">Disadvantages of KDE:</h3><ul><li><strong>Choice of Bandwidth</strong>: The performance of KDE heavily depends on the choice of bandwidth. Too small a bandwidth can lead to overfitting, while too large a bandwidth can oversmooth important features.</li><li><strong>Computationally Intensive</strong>: KDE can be computationally intensive, especially for large datasets and high-dimensional data.</li></ul><h3 id="Applications-of-KDE">Applications of KDE:</h3><ul><li><strong>Data Visualization</strong>: KDE is often used to visualize the distribution of data, particularly in one-dimensional and two-dimensional cases.</li><li><strong>Anomaly Detection</strong>: KDE can be used to detect outliers by identifying areas of low probability density.</li><li><strong>Density-Based Clustering</strong>: In clustering methods like DBSCAN, KDE can help define regions of high density.</li></ul><h2 id="How-Do-It-in-Python">How Do It in Python</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KernelDensity<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br><span class="hljs-comment"># Step 2: Prepare Your Data</span><br><span class="hljs-comment"># Example list of values</span><br>data_list = [<span class="hljs-number">1.2</span>, <span class="hljs-number">2.3</span>, <span class="hljs-number">2.4</span>, <span class="hljs-number">2.5</span>, <span class="hljs-number">3.1</span>, <span class="hljs-number">3.6</span>, <span class="hljs-number">3.8</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">4.2</span>, <span class="hljs-number">5.0</span>] * <span class="hljs-number">30</span><br><span class="hljs-comment"># Convert the list to a NumPy array and reshape it for the model</span><br>data = np.array(data_list).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-comment"># Step 3: Fit the Kernel Density Estimation Model</span><br><span class="hljs-comment"># Fit the KDE model</span><br>kde = KernelDensity(kernel=<span class="hljs-string">&#x27;gaussian&#x27;</span>, bandwidth=<span class="hljs-number">0.2</span>).fit(data)<br><span class="hljs-comment"># Step 4: (Optional) Plot the Estimated Density</span><br><span class="hljs-comment"># Define a range of values</span><br>x_range = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">6</span>, <span class="hljs-number">1000</span>).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-comment"># Estimate density for the entire range</span><br>log_density = kde.score_samples(x_range)<br>density = np.exp(log_density)<br><span class="hljs-comment"># Plot the density</span><br>sns.kdeplot(data_list, bw_adjust=<span class="hljs-number">0.5</span>)<br>plt.plot(x_range, density)<br>plt.title(<span class="hljs-string">&quot;Kernel Density Estimation&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Value&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Density&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/Tr6nMEI.png" alt="KDE plot"></th></tr></thead><tbody><tr><td style="text-align:center"></td></tr></tbody></table><h2 id="Save-and-Load-the-Model">Save and Load the Model</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pickle<br><br><span class="hljs-comment"># Fit the KDE model (assuming you have already done this)</span><br><span class="hljs-comment"># kde = KernelDensity(kernel=&#x27;gaussian&#x27;, bandwidth=0.2).fit(data)</span><br><br><span class="hljs-comment"># Save the model to a file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;kde_model.pkl&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>    pickle.dump(kde, file)<br><br><span class="hljs-comment"># Load the model from the file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;kde_model.pkl&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>    loaded_kde = pickle.load(file)<br><br><span class="hljs-comment"># Value for which you want to estimate the density</span><br>value = <span class="hljs-number">3.5</span><br><br><span class="hljs-comment"># Estimate the density using the loaded model</span><br>log_density = loaded_kde.score_samples([[value]])<br>density = np.exp(log_density)<br>print(<span class="hljs-string">f&quot;Density of the value <span class="hljs-subst">&#123;value&#125;</span> using loaded model: <span class="hljs-subst">&#123;density[<span class="hljs-number">0</span>]:<span class="hljs-number">.6</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Kernel Density Estimation (KDE)</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="Regression" scheme="https://karobben.github.io/categories/Machine-Learning/Regression/"/>
    
    
    <category term="Regression" scheme="https://karobben.github.io/tags/Regression/"/>
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Taylor Series and Its Applications in Machine Learning</title>
    <link href="https://karobben.github.io/2024/08/09/AI/taylorseries/"/>
    <id>https://karobben.github.io/2024/08/09/AI/taylorseries/</id>
    <published>2024-08-09T22:40:44.000Z</published>
    <updated>2024-10-09T23:03:07.711Z</updated>
    
    <content type="html"><![CDATA[<p>The Taylor Series is a fundamental mathematical tool that finds applications across various domains, including machine learning. In this post, we’ll explore what the Taylor Series is, how it is used in machine learning, and the significant impact it can have on <strong>optimizing</strong> machine learning models. Here are some good videos to explain the basic of the Taylor Series: <a href="https://www.youtube.com/watch?v=3d6DsjIBzJ4">Taylor series | Chapter 11, Essence of calculus</a>, <a href="https://www.youtube.com/watch?v=LkLVMJQAj6A">Visualization of the Taylor Series</a>, <a href="https://www.youtube.com/watch?v=EYjBnnUJTP8">3 Applications of Taylor Series: Integrals, Limits, &amp; Series</a>, and <a href="https://www.youtube.com/watch?v=eX1hvWxmJVE">Dear Calculus 2 Students, This is why you’re learning Taylor Series</a></p><h2 id="What-is-the-Taylor-Series"><strong>What is the Taylor Series?</strong></h2><p>The Taylor Series is a mathematical concept that allows us to <strong>approximate complex functions</strong> using an infinite sum of terms, calculated from the derivatives of the function at a specific point. It essentially breaks down a function into a <strong>polynomial</strong> that closely approximates the function near a given point.</p><p>The general formula for the Taylor Series of a function $ f(x) $ around a point $ a $ is:</p><p>$$<br>f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!} (x - a)^n<br>$$</p><p>Where $ f^{(n)}(a) $ represents the $ n $-th derivative of $ f(x) $ at point $ a $, and $ n! $ is the factorial of $ n $.</p><p>This approximation is particularly useful when dealing with functions that are difficult to compute directly, as it allows us to work with simpler polynomial expressions instead.</p><h2 id="Taylor-Series-for-the-Cosine-Function"><strong>Taylor Series for the Cosine Function</strong></h2><p><img src="https://imgur.com/20cgEEk.png" alt="Taylor Series for the Cosine Function"></p><p>The cosine function, $ \cos(x) $, is a smooth and periodic function (<mark>line in black</mark>) that oscillates between -1 and 1. While it can be computed directly using trigonometric tables or built-in functions in programming languages, these computations can be resource-intensive, especially for small embedded systems or in scenarios requiring real-time processing.</p><p>The Taylor Series provides a way to approximate $ \cos(x) $ using a polynomial expansion around a specific point, typically $ x = 0 $ (the Maclaurin series, a special case of the Taylor Series). The Taylor Series for $ \cos(x) $ is given by:</p><p>$$<br>\cos(x) = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \dots<br>$$</p><p>This series can be written as:</p><p>$$<br>\cos(x) = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!} x^{2n}<br>$$</p><p>Where:</p><ul><li>$ (-1)^n $ alternates the sign of each term.</li><li>$ (2n)! $ is the factorial of $ 2n $, ensuring that the series converges.</li><li>$ x^{2n} $ means that only even powers of $ x $ are used, reflecting the symmetry of the cosine function.</li></ul><h3 id="How-the-Approximation-Works"><strong>How the Approximation Works</strong></h3><p>The beauty of the Taylor Series lies in its ability to approximate $ \cos(x) $ with just a few terms, depending on the desired accuracy.</p><ul><li><p><strong>First Few Terms</strong>: If you only take the first two terms (up to $ x^2 $), the approximation is:<br>$$<br>\cos(x) \approx 1 - \frac{x^2}{2}<br>$$<br>This provides a reasonable approximation for $ \cos(x) $ when $ x $ is close to 0, capturing the initial downward curve of the cosine function.</p></li><li><p><strong>Adding More Terms</strong>: As you include more terms (e.g., $ x^4 $, $ x^6 $), the approximation becomes increasingly accurate, even for values of $ x $ further from 0. Each additional term refines the curve, making the polynomial more closely match the actual cosine function.</p></li></ul><h3 id="Practical-Use-and-Computational-Benefits"><strong>Practical Use and Computational Benefits</strong></h3><p>In practical applications, such as in computer graphics, signal processing, or physics simulations, using the Taylor Series to approximate $ \cos(x) $ can significantly reduce computational cost. Instead of performing the full trigonometric calculation, which might involve iterative or complex operations, a system can compute a few polynomial terms, which are far less demanding.</p><ul><li><p><strong>Example</strong>: In embedded systems where processing power is limited, calculating $ \cos(x) $ using the Taylor Series with just a few terms can save time and energy, which is crucial in battery-powered devices.</p></li><li><p><strong>Trade-off</strong>: There is always a trade-off between the number of terms used and the accuracy of the approximation. For most practical purposes, using 4 to 6 terms provides a good balance between accuracy and computational efficiency.</p></li></ul><h3 id="Beyond-Cosine-General-Use-in-Trigonometric-Functions"><strong>Beyond Cosine: General Use in Trigonometric Functions</strong></h3><p>The approach used to approximate $ \cos(x) $ can also be applied to other trigonometric functions like $ \sin(x) $ and $ \tan(x) $. Each of these functions has its own Taylor Series expansion, enabling similar approximations and computational savings.</p><h2 id="Applications-of-Taylor-Series-in-Machine-Learning"><strong>Applications of Taylor Series in Machine Learning</strong></h2><p>While the Taylor Series is a powerful mathematical tool on its own, its applications in machine learning are particularly noteworthy, especially in the context of optimization algorithms and model behavior analysis.</p><h3 id="1-Gradient-Descent-and-Optimization"><strong>1. Gradient Descent and Optimization</strong></h3><p>In machine learning, gradient descent is a widely used optimization technique that minimizes a loss function by iteratively adjusting model parameters. The Taylor Series plays a crucial role in understanding and improving this process.</p><ul><li><p><strong>Basic Gradient Descent</strong>:</p><ul><li>Gradient descent uses the first-order Taylor approximation of the loss function to update parameters. However, the basic gradient descent approach can be slow and sensitive to the choice of the learning rate, often requiring careful tuning to avoid issues like overshooting or slow convergence.</li></ul></li><li><p><strong>Newton’s Method Using Taylor Series</strong>:</p><ul><li>By incorporating the second-order Taylor expansion of the loss function, Newton’s method leverages the Hessian matrix (a matrix of second derivatives) to make more informed updates. This results in faster and more stable convergence, especially near the optimum, although it comes at the cost of increased computational complexity.</li></ul></li></ul><p><strong>Before vs. After Applying the Taylor Series</strong>:</p><ul><li><strong>Before</strong>: Gradient descent can be slow and sensitive, requiring many iterations to reach a solution.</li><li><strong>After</strong>: Newton’s method, using the Taylor Series, accelerates convergence and provides more stability, particularly in challenging optimization landscapes.</li></ul><h3 id="2-Understanding-Model-Behavior"><strong>2. Understanding Model Behavior</strong></h3><p>The Taylor Series also helps in linearizing non-linear models, which is essential for understanding how small changes in input features affect the model’s output.</p><ul><li><strong>Linearization of Non-Linear Models</strong>: By approximating non-linear functions (like activation functions in neural networks) with a Taylor Series, we can analyze the local behavior of these functions. This is particularly useful for sensitivity analysis, where understanding the impact of small input perturbations is crucial for model robustness.</li></ul><h3 id="3-Regularization-and-Generalization"><strong>3. Regularization and Generalization</strong></h3><p>Regularization techniques, which are used to prevent overfitting in machine learning models, can also be viewed through the lens of the Taylor Series. By penalizing higher-order terms in the Taylor expansion, regularization methods like L2 regularization (Ridge) help in controlling model complexity and improving generalization.</p><h2 id="Real-World-Example-Logistic-Regression-and-Taylor-Series"><strong>Real-World Example: Logistic Regression and Taylor Series</strong></h2><p>To illustrate the practical application of the Taylor Series in machine learning, consider a logistic regression model used to classify emails as spam or not. The model uses a sigmoid function to predict probabilities, and the goal is to minimize the binary cross-entropy loss function.</p><ul><li><p><strong>Without Taylor Series</strong>: Using basic gradient descent, the model may take many iterations to converge, with convergence being highly dependent on the chosen learning rate.</p></li><li><p><strong>With Taylor Series (Newton’s Method)</strong>: By applying the Taylor Series, specifically the second-order approximation, the model can achieve faster and more stable convergence, even if each iteration is more computationally intensive.</p></li></ul><p>In this case, applying the Taylor Series through Newton’s method can drastically reduce the number of iterations required to reach an optimal solution, highlighting the power of this mathematical tool in machine learning optimization.</p><h2 id="Conclusion"><strong>Conclusion</strong></h2><p>The Taylor Series is more than just a mathematical concept; it’s a powerful tool that underpins several key techniques in machine learning. From optimizing models with gradient descent to understanding the behavior of complex functions, the Taylor Series enables us to make more accurate and efficient decisions in model training and evaluation. Whether you’re dealing with logistic regression or deep learning, understanding and applying the Taylor Series can significantly enhance your machine learning practice.</p><p>By incorporating second-order information through the Taylor Series, you can achieve faster convergence, better stability, and a deeper understanding of your models, ultimately leading to more robust and effective machine learning solutions.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">The Taylor Series is a mathematical tool that approximates complex functions with polynomials, playing a crucial role in machine learning optimization. It enhances gradient descent by incorporating second-order information, leading to faster and more stable convergence. Additionally, it aids in linearizing non-linear models and informs regularization techniques. This post explores the significance of the Taylor Series in improving model training efficiency and understanding model behavior. $$\cos(x) = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!} x^{2n}$$</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="Math" scheme="https://karobben.github.io/categories/Machine-Learning/Math/"/>
    
    
    <category term="Math" scheme="https://karobben.github.io/tags/Math/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>FoldX</title>
    <link href="https://karobben.github.io/2024/07/06/Bioinfor/foldx/"/>
    <id>https://karobben.github.io/2024/07/06/Bioinfor/foldx/</id>
    <published>2024-07-06T22:11:38.000Z</published>
    <updated>2024-07-23T02:49:21.088Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Run-the-FoldX">Run the FoldX</h2><p>In this example, I am using the <strong>7ekb</strong> as example</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># download the pdb</span><br>wget https://files.rcsb.org/view/7ekb.pdb<br><span class="hljs-comment"># Repair the PDB. After repaired it, you&#x27;ll get the 7ekb_Repair.pdb for the next step</span><br>FoldX --<span class="hljs-built_in">command</span>=RepairPDB --pdb=7ekb.pdb<br><span class="hljs-comment"># calculate the free energy of the PDB  </span><br>FoldX --<span class="hljs-built_in">command</span>=Stability  --pdb=7ekb_Repair.pdb<br></code></pre></td></tr></table></figure></div><h2 id="RepairPDB">RepairPDB</h2><h3 id="Why-Repair-PDB">Why Repair PDB?</h3><p>According to ChatGPT4o, <code>RepairPDB</code> command in FoldX is a crucial step to ensure the quality and integrity of your PDB file before performing stability calculations or other analyses. Also, you could found more information from <a href="https://foldxsuite.crg.eu/command/RepairPDB">document</a></p><ol><li><p><strong>Fix Structural Issues</strong>:</p><ul><li><strong>Correcting Errors</strong>: PDB files obtained from experiments like X-ray crystallography or cryo-EM often have missing atoms, residues, or other structural issues that can affect downstream analyses. <code>RepairPDB</code> fixes these issues to ensure a complete and accurate structure.</li><li><strong>Adding Missing Atoms</strong>: The command can add missing atoms, such as hydrogen atoms, which are essential for energy calculations.</li></ul></li><li><p><strong>Standardizing the Structure</strong>:</p><ul><li><strong>Normalization</strong>: <code>RepairPDB</code> standardizes the structure to ensure that all residues and atoms are in the correct format and positions. This includes correcting bond lengths and angles to standard values.</li><li><strong>Removing Non-standard Residues</strong>: It can remove or correct non-standard residues and ligands that might interfere with calculations.</li></ul></li><li><p><strong>Improving Energy Calculations</strong>:</p><ul><li><strong>Optimizing Geometry</strong>: The command optimizes the geometry of the protein, ensuring that the atomic positions are energetically favorable. This leads to more accurate stability and free energy calculations.</li><li><strong>Minimizing Steric Clashes</strong>: It identifies and resolves steric clashes (where atoms are too close to each other), which can distort energy calculations.</li></ul></li><li><p><strong>Ensuring Compatibility</strong>:</p><ul><li><strong>Consistency</strong>: Running <code>RepairPDB</code> ensures that your PDB file is compatible with FoldX’s algorithms, reducing the risk of errors during subsequent steps.</li></ul></li></ol><div class="admonition question"><p class="admonition-title">How does the Output looks like?</p></div><pre>Residue LYSH222 has high Energy, we mutate it to itselfRepair Residue ID= LYSH222BackHbond       =               -317.22SideHbond       =               -137.87Energy_VdW      =               -476.42Electro         =               -15.23Energy_SolvP    =               628.80Energy_SolvH    =               -624.90Energy_vdwclash =               15.60energy_torsion  =               9.33backbone_vdwclash=              143.43Entropy_sidec   =               245.04Entropy_mainc   =               632.72water bonds     =               0.00helix dipole    =               -0.35loop_entropy    =               0.00cis_bond        =               4.50disulfide       =               -13.95kn electrostatic=               -0.25partial covalent interactions = 0.00Energy_Ionisation =             1.07Entropy Complex =               0.00-----------------------------------------------------------Total          =   -49.10</pre><p>It took me <mark>2m 48s</mark>. It only work in single thread and cannot move on multiple threads. I guess because it works by following the order of the AA and the <code>Total</code> is depending on the previous values. So, it can’t work on multiple threads.</p><p>Here is the result of before and after repairing. The RMS=0.01 which means it almost the same. But the slightly different are mainly focus on the loos area. In the picture present below, the left panel with green structure is the raw pdb file from PDB database. The light blue structure on the right is the corrected by FoldX. Red structure is antigen. As I marked on the left panel, 2 beta-sheets and 1 alpha helix are deleted and become random loop. Those area from the antibody are very closing to the antigen. So, technically, random loop would make more sense to me.</p><p><img src="https://imgur.com/i8aPrTy.png" alt=""></p><h2 id="Stability-Calculations">Stability Calculations</h2><p>After repaired the PDB file, you can get the result immediately.</p><pre>   ********************************************   ***                                      ***   ***             FoldX 5.1 (c)            ***   ***                                      ***   ***     code by the FoldX Consortium     ***   ***                                      ***   ***     Jesper Borg, Frederic Rousseau   ***   ***    Joost Schymkowitz, Luis Serrano   ***   ***    Peter Vanhee, Erik Verschueren    ***   ***     Lies Baeten, Javier Delgado      ***   ***       and Francois Stricher          ***   *** and any other of the 9! permutations ***   ***   based on an original concept by    ***   ***   Raphael Guerois and Luis Serrano   ***   ********************************************Stability >>>1 models read: 7ekb_Repair.pdbBackHbond       =               -332.04SideHbond       =               -163.29Energy_VdW      =               -481.14Electro         =               -17.42Energy_SolvP    =               626.91Energy_SolvH    =               -633.28Energy_vdwclash =               13.20energy_torsion  =               9.65backbone_vdwclash=              144.57Entropy_sidec   =               259.27Entropy_mainc   =               634.24water bonds     =               0.00helix dipole    =               -0.40loop_entropy    =               0.00cis_bond        =               4.50disulfide       =               -13.95kn electrostatic=               -0.41partial covalent interactions = 0.00Energy_Ionisation =             1.14Entropy Complex =               0.00-----------------------------------------------------------Total          =   -93.01FINISHING STABILITY ANALYSIS OPTIONYour file run OKEnd time of FoldX: Sat Jul  6 17:23:18 2024Total time spend: 0.85 seconds.</pre><h2 id="Mutation-Energy-Change-Calculation">Mutation Energy Change Calculation</h2><p>With FoldX, you can predicted the mutations effects when you have the wild type structure. The command <code>BuildModel</code> could generate the new pdb structure with the ‘mutate_file’ you write. Here is an example of <code>mutate_file</code>:</p><pre>AA4P,FD4P;AA4F,QD4F;</pre><p>In this example, it would generate 2 new structures. For the first one, in chain A, the mutation is A4P, in chain D, the mutation is F4P. The “;” means the first mutate process is done. It would read the second line to create the another mutation file. You don’t need to calculate the mutation energy difference between before and after again. Because all they are saved in the file (*.fxout) as <code>tsv</code> format.</p><p>For running it, you just need to run like:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">FoldX --<span class="hljs-built_in">command</span>=BuildModel --pdb=protein_Repair.pdb --mutant-file=individual_list.txt<br></code></pre></td></tr></table></figure></div><p>An interesting thing is for the mutate-file, it has to start with “individual_list” or ‘mutate_file’ or you’ll get error. After the job is down, you’ll got 4 outputs: “Average_<em>.fxout&quot;, &quot;Dif_</em>.fxout”, “PdbList_<em>.fxout&quot;, and &quot;Raw_</em>.fxout”. Here is some details about those outputs:</p><ol><li><p><strong>Average_*.fxout</strong>:</p><ul><li>This file contains the average energy values from multiple runs of the BuildModel command. FoldX often performs multiple simulations to generate an average value to improve reliability and account for variability in the calculations.</li><li>It includes averaged energy terms like van der Waals interactions, electrostatics, solvation, and total energy for the mutated model.</li></ul></li><li><p><strong>Dif_*.fxout</strong>:</p><ul><li>This file contains the differences in energy values between the wild-type and mutated proteins.</li><li>It shows the ΔΔG (difference in free energy change) due to the introduced mutation(s), which helps in understanding the stability change caused by the mutation.</li></ul></li><li><p><strong>PdbList_*.fxout</strong>:</p><ul><li>This file lists the PDB files generated during the mutation process.</li><li>It includes the names of the mutated PDB files that FoldX generated, which you can further analyze or visualize using molecular visualization tools.</li></ul></li><li><p><strong>Raw_*.fxout</strong>:</p><ul><li>This file contains the raw energy values for each individual run of the BuildModel command.</li><li>It provides detailed energy components for each simulation, such as van der Waals interactions, electrostatics, solvation, and other energy terms for the mutated model.</li></ul></li></ol><div class="admonition note"><p class="admonition-title">Notice</p><ol><li>For the <code>mutate_file</code>, you can't add any extra expressions like space in it.</li><li>According to the <a href="https://pdb101.rcsb.org/learn/guide-to-understanding-pdb-data/protein-primary-sequences">PDB document</a>, the SEQRES and ATOM records may include only a portion of the molecule. So if your sequence was extracted from the PDB file, the numbering of it may incorrect. For achieve the correct position, you may like to extract extract the sequence from cif format.</li></ol></div><h3 id="BuildModel-in-Action">BuildModel in Action</h3><p>Here, the test data is from Qi Wen Teo<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> CR9114 (<strong>4FQI</strong>) as example. I randomly choose 4FQI as the standard. In the paper, they mutated the resi through 93 to 102 (kabat numbering) which is 97 to 110. So, we could do it with a <code>mutate_file</code>. For FoldX, it only recognize the digital numbering. But in antibody (show below) sometimes was numbered by kabat numbering or something similar methods. So it may contain numbering like 100A, 100B, etc. They can’t recognized by FoldX and we need to renumbering them. Pymol is very complicated in this kind of task. But Biopython could handle it very well. You could using the script from <a href="https://github.com/Karobben/Bio_tools">Karobben/Bio_tools</a> with code: <code>python PDBreNumbering.py -i 4FQI_Repair.pdb -o renumbered.pdb</code></p><pre>ATOM   4942  CD1 TYR H 100     -17.095  54.149 -23.690  1.00 17.83      C    C  ATOM   4943  CD2 TYR H 100     -15.503  54.016 -21.927  1.00 17.51      C    C  ATOM   4944  CE1 TYR H 100     -16.055  54.246 -24.606  1.00 18.76      C    C  ATOM   4945  CE2 TYR H 100     -14.431  54.115 -22.848  1.00 20.56      C    C  ATOM   4946  CZ  TYR H 100     -14.740  54.248 -24.173  1.00 21.30      C    C  ATOM   4947  OH  TYR H 100     -13.735  54.378 -25.146  1.00 22.06      C    O  ATOM   4948  N   TYR H 100A    -19.396  56.114 -18.971  1.00 19.55      C    N  ATOM   4949  CA  TYR H 100A    -20.277  56.072 -17.797  1.00 21.40      C    C  ATOM   4950  C   TYR H 100A    -21.609  56.741 -18.067  1.00 25.46      C    C  ATOM   4951  O   TYR H 100A    -22.655  56.288 -17.527  1.00 25.98      C    O  ATOM   4952  CB  TYR H 100A    -19.611  56.821 -16.587  1.00 18.28      C    C  ATOM   4953  CG  TYR H 100A    -18.192  56.412 -16.276  1.00 19.12      C    C  ATOM   4954  CD1 TYR H 100A    -17.753  55.092 -16.396  1.00 21.46      C    C  </pre><p>Script to create the <code>mutate_file</code>. In this script, the target region is from number 97-110 and the sequence is “ARHGNYYYYSGMDV”.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">WT = <span class="hljs-built_in">list</span>(<span class="hljs-string">&quot;ARHGNYYYYSGMDV&quot;</span>)<br>All20 = <span class="hljs-built_in">list</span>(<span class="hljs-string">&quot;ARNDCEQGHILKMFPSTWYV&quot;</span>)<br>Num = <span class="hljs-number">96</span><br>sublist = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> WT:<br>    Num += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> ii <span class="hljs-keyword">in</span> All20:<br>        <span class="hljs-keyword">if</span> i != ii:<br>             sublist += [<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;i&#125;</span>H<span class="hljs-subst">&#123;Num&#125;</span><span class="hljs-subst">&#123;ii&#125;</span>;&quot;</span>]<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;individual_list.txt&quot;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> F:<br>    F.write(<span class="hljs-string">&quot;\n&quot;</span>.join(sublist))<br></code></pre></td></tr></table></figure></div><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># repair the PDB</span><br>FoldX --<span class="hljs-built_in">command</span>=RepairPDB --pdb=4FQI.pdb<br><span class="hljs-comment"># renumbering the resi for FoldX</span><br>python PDBreNumbering.py -i 4FQI_Repair.pdb -o renumbered.pdb<br><span class="hljs-comment"># calculate the results </span><br>FoldX --<span class="hljs-built_in">command</span>=BuildModel --pdb=renumbered.pdb --mutant-file=individual_list.txt<br></code></pre></td></tr></table></figure></div><p>After that, the result is saved in the file <code>Raw_renumbered.fxout</code>. The table was started at line 9. We could use R to sorting and compare the experiment result. For the experiment result, you can download from <a href="https://github.com/nicwulab/CR9114_LC_CDRH3_screen/blob/main/result/CDRH3_KD_table_summary.csv">nicwulab/CR9114_LC_CDRH3_screen</a></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(ggplot2)<br>library(reshape2)<br>library(stringr)<br><br>TB &lt;- read.csv(<span class="hljs-string">&#x27;Raw_renumbered.fxout&#x27;</span>, skip = <span class="hljs-number">8</span>, sep = <span class="hljs-string">&#x27;\t&#x27;</span>)<br>TB$Type &lt;- <span class="hljs-string">&quot;Mute&quot;</span><br>TB$Type[grep(<span class="hljs-string">&quot;WT_&quot;</span>, TB$Pdb)] &lt;- <span class="hljs-string">&quot;WT&quot;</span><br>TB &lt;- TB[<span class="hljs-built_in">c</span>(<span class="hljs-string">&#x27;Pdb&#x27;</span>, <span class="hljs-string">&#x27;total.energy&#x27;</span>, <span class="hljs-string">&#x27;Type&#x27;</span>)]<br>TB$Pdb &lt;- str_remove(TB$Pdb, <span class="hljs-string">&quot;WT_&quot;</span>)<br>TBM &lt;- reshape(TB, idvar = <span class="hljs-string">&#x27;Pdb&#x27;</span>, timevar = <span class="hljs-string">&#x27;Type&#x27;</span>, direction = <span class="hljs-string">&#x27;wide&#x27;</span>)<br>colnames(TBM) &lt;- str_remove(colnames(TBM), <span class="hljs-string">&#x27;total.energy.&#x27;</span>)<br><br>Anno &lt;- read.csv(<span class="hljs-string">&#x27;individual_list.txt&#x27;</span>, header = <span class="hljs-built_in">F</span>)<br>TBM$Anno &lt;- str_remove(Anno$V1, <span class="hljs-string">&quot;;&quot;</span>)<br>TBM$Diff = TBM$Mute - TBM$WT<br><br>library(scales)<br>library(readr)<br>library(tidyr)<br>library(dplyr)<br>library(gridExtra)<br><br>aa_level &lt;- rev(<span class="hljs-built_in">c</span>(<span class="hljs-string">&#x27;E&#x27;</span>,<span class="hljs-string">&#x27;D&#x27;</span>,<span class="hljs-string">&#x27;R&#x27;</span>,<span class="hljs-string">&#x27;K&#x27;</span>,<span class="hljs-string">&#x27;H&#x27;</span>,<span class="hljs-string">&#x27;Q&#x27;</span>,<span class="hljs-string">&#x27;N&#x27;</span>,<span class="hljs-string">&#x27;S&#x27;</span>,<span class="hljs-string">&#x27;T&#x27;</span>,<span class="hljs-string">&#x27;P&#x27;</span>,<span class="hljs-string">&#x27;G&#x27;</span>,<span class="hljs-string">&#x27;C&#x27;</span>,<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-string">&#x27;V&#x27;</span>,<span class="hljs-string">&#x27;I&#x27;</span>,<span class="hljs-string">&#x27;L&#x27;</span>,<span class="hljs-string">&#x27;M&#x27;</span>,<span class="hljs-string">&#x27;F&#x27;</span>,<span class="hljs-string">&#x27;Y&#x27;</span>,<span class="hljs-string">&#x27;W&#x27;</span>,<span class="hljs-string">&#x27;_&#x27;</span>))<br><br>df &lt;- read_csv(<span class="hljs-string">&#x27;CDRH3_KD_table_summary.csv&#x27;</span>) %&gt;%<br>  filter(grepl(<span class="hljs-string">&#x27;CR9114&#x27;</span>,ID)) %&gt;%<br>  mutate(log10_Kd=log10(Kd)) %&gt;%<br>  filter((log10_Kd &lt; -<span class="hljs-number">8</span> &amp; p.value &lt; <span class="hljs-number">0.2</span>) | (log10_Kd &gt;= -<span class="hljs-number">8</span>)) %&gt;%<br>  mutate(Mutation=gsub(<span class="hljs-string">&#x27;CR9114_&#x27;</span>,<span class="hljs-string">&quot;&quot;</span>,ID)) %&gt;%<br>  filter(Mutation != <span class="hljs-string">&#x27;WT&#x27;</span>) %&gt;%<br>  mutate(resi=str_sub(Mutation,<span class="hljs-number">1</span>,-<span class="hljs-number">2</span>)) %&gt;%<br>  mutate(aa=str_sub(Mutation,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>)) %&gt;%<br>  filter(aa %in% aa_level) %&gt;%<br>  mutate(aa=factor(aa,levels=aa_level)) %&gt;%<br>  complete(resi, aa) %&gt;%<br>  mutate(Pos=str_sub(resi,<span class="hljs-number">2</span>,-<span class="hljs-number">1</span>)) %&gt;%<br>  mutate(Pos=<span class="hljs-built_in">as.numeric</span>(<span class="hljs-built_in">as.character</span>(Pos))) %&gt;%<br>  arrange(Pos) %&gt;%<br>  mutate(resi=factor(resi,levels=unique(resi))) %&gt;%<br>  mutate(log10_Kd=case_when(str_sub(resi,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)==aa ~ log10(<span class="hljs-number">5.19e-10</span>), <span class="hljs-literal">TRUE</span> ~ log10_Kd)) %&gt;%<br>  mutate(Mutation=paste(resi,aa,sep=<span class="hljs-string">&#x27;&#x27;</span>)) %&gt;%<br>  select(Mutation, resi, Pos, aa, log10_Kd)<br><br>df$Pos = df$Pos + <span class="hljs-number">96</span><br>df$Anno &lt;- paste(gsub(<span class="hljs-string">&quot;[0-9]&quot;</span>, <span class="hljs-string">&quot;&quot;</span>, df$resi), df$Pos, df$aa, sep = <span class="hljs-string">&#x27;&#x27;</span>)<br><br>remove_second_letter &lt;- <span class="hljs-keyword">function</span>(x) &#123;<br>  paste0(substr(x, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), substr(x, <span class="hljs-number">3</span>, nchar(x)))<br>&#125;<br><br>TBM$Anno &lt;- sapply( TBM$Anno, remove_second_letter)<br>TBM$log10_K &lt;- df$log10_Kd[match(TBM$Anno, df$Anno)]<br>TBMF &lt;- TBM[!<span class="hljs-built_in">is.na</span>(TBM$log10_K),]<br><br>ggplot(TBMF, aes(Diff, log10_K )) + geom_point() + <br>  geom_smooth(method = <span class="hljs-string">&#x27;lm&#x27;</span>) + <br>  theme_bw()<br><br>ggplot(TBMF[TBMF$Diff &lt;= <span class="hljs-number">2</span>,], aes(Diff, log10_K )) + geom_point() + <br>  geom_smooth(method = <span class="hljs-string">&#x27;lm&#x27;</span>) +<br>  geom_vline( xintercept = <span class="hljs-number">0</span>, linetype = <span class="hljs-number">4</span>) + <br>  geom_hline( yintercept = -<span class="hljs-number">9.28</span>, linetype = <span class="hljs-number">4</span>) + <br>  theme_bw()<br></code></pre></td></tr></table></figure></div><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/zstiOTG.png" alt=""></td><td style="text-align:center"><img src="https://imgur.com/Ceu7diL.png" alt=""></td></tr></tbody></table><p>According to this plot, the correlation between experiments and the prediction is terrible. I think the main reason is because all those positions are located on CDRH3 region which not only they are random loop, but also the key site to determine the binding affinity of the antibody. So, the prediction result would be extrimly hard. But I think the result is not totally useless. At least when the Δ G of the complex predicted became more stable ($\Delta_ {mutate} - \Delta_ {wt} &lt; 0$), most of experiments results are very closing to the wild type.</p><table><thead><tr><th>Mute</th><th>WT</th><th>Anno</th><th>Diff</th><th>log10_K</th></tr></thead><tbody><tr><td>-111.068</td><td>-110.525</td><td>D109E</td><td>-0.543</td><td>-9.444906</td></tr><tr><td>-110.702</td><td>-110.512</td><td>D109Q</td><td>-0.190</td><td>-9.422508</td></tr><tr><td>-110.532</td><td>-110.525</td><td>D109C</td><td>-0.007</td><td>-9.343902</td></tr><tr><td>-111.447</td><td>-110.503</td><td>D109M</td><td>-0.944</td><td>-9.296702</td></tr><tr><td>-112.243</td><td>-110.512</td><td>G100M</td><td>-1.731</td><td>-9.222573</td></tr><tr><td>-111.185</td><td>-110.503</td><td>D109T</td><td>-0.682</td><td>-9.180450</td></tr><tr><td>-110.902</td><td>-110.857</td><td>S106M</td><td>-0.045</td><td>-9.170053</td></tr><tr><td>-111.330</td><td>-110.525</td><td>D109R</td><td>-0.805</td><td>-9.156767</td></tr><tr><td>-112.243</td><td>-110.504</td><td>D109Y</td><td>-1.739</td><td>-9.136677</td></tr><tr><td>-111.697</td><td>-110.893</td><td>G100I</td><td>0.804</td><td>-9.114074</td></tr><tr><td>-112.226</td><td>-110.512</td><td>G100K</td><td>-1.714</td><td>-9.100179</td></tr><tr><td>-111.195</td><td>-110.525</td><td>G100C</td><td>-0.670</td><td>-9.075721</td></tr><tr><td>-113.282</td><td>-110.512</td><td>G100R</td><td>-2.770</td><td>-9.057495</td></tr><tr><td>-111.521</td><td>-111.362</td><td>Y104F</td><td>-0.159</td><td>-9.040850</td></tr><tr><td>-110.971</td><td>-110.611</td><td>G107F</td><td>-0.360</td><td>-9.038579</td></tr><tr><td>-111.158</td><td>-110.503</td><td>D109F</td><td>-0.655</td><td>-9.028260</td></tr><tr><td>-110.824</td><td>-110.503</td><td>D109L</td><td>-0.321</td><td>-9.026410</td></tr><tr><td>-112.106</td><td>-110.820</td><td>G100N</td><td>-1.286</td><td>-8.995671</td></tr><tr><td>-111.164</td><td>-110.470</td><td>V110L</td><td>-0.694</td><td>-8.978111</td></tr><tr><td>-111.718</td><td>-110.818</td><td>G100H</td><td>-0.900</td><td>-8.869666</td></tr><tr><td>-111.044</td><td>-110.873</td><td>S106N</td><td>-0.171</td><td>-8.545155</td></tr><tr><td>-110.944</td><td>-110.837</td><td>N101H</td><td>-0.107</td><td>-8.423659</td></tr></tbody></table><h3 id="How-it-work">How it work?</h3><p>Here is the corrected version of your text:</p><p>According to the documentation: This is the workhorse of the FoldX mutation engine. This command ensures that whenever you are mutating a protein, you always move the same neighbors in the WT and in the mutant, producing for each mutant PDB a corresponding PDB for its WT. Each mutation will move different neighbors, and therefore you need different WT references.</p><p>From the experience above, I think it works under the assumption that the structure of the protein won’t change due to the point mutation. As shown below, even though the amino acid changed from <strong>Y</strong> to <strong>R</strong>, the position remains unchanged, and the RMSD is 0. So, I think it would be more reliable when this amino acid is in the alpha helix or beta sheet. When a point mutation happens in these regions, the rough structure remains relatively the same. However, when the mutation occurs in the loop region, the result would be less reliable. According to Yuan M, et al.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>, when the mutation only happens in the V gene of the antibody, the correct ratio was about 70%. But according to the test above, the result is inconsistent and lacks convinciveness.</p><p><img src="https://imgur.com/JpXnrlY.png" alt=""></p><h2 id="Interface-Analysis">Interface Analysis</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># after repair the PDB</span><br>FoldX --<span class="hljs-built_in">command</span>=AnalyseComplex --pdb=4FQI_Repair.pdb --analyseComplexChains=A,B,H,L<br><span class="hljs-comment"># output: Indiv_energies_4FQI_Repair_AC.fxout  Interaction_4FQI_Repair_AC.fxout  Interface_Residues_4FQI_Repair_AC.fxout  Summary_4FQI_Repair_AC.fxout</span><br></code></pre></td></tr></table></figure></div><ul><li><strong>Indiv_energies_4FQI_Repair_AC</strong>:<br>This file contains individual energy contributions of residues to the overall stability of the protein complex. To be notice, the total energy of the complex is not equals to the sum of individual total energy.</li><li><strong>Interaction_4FQI_Repair_AC</strong>:<br>It records detailed interaction between each chain pairs. I think <strong>Interaction Energy</strong> is one of most important results from it.</li><li><strong>Summary_4FQI_Repair_AC.fxout</strong>:<br>It contains only a few important columns from the <em>Interaction_4FQI_Repair_AC.fxout</em>.</li><li><strong>Interface_Residues_4FQI_Repair_AC.fxout</strong>:<br>It records all residues in the interface in a list.</li></ul><div class="admonition note"><p class="admonition-title">Notice</p><p>Before running <code>AnalyseComplex</code>, you should renumber the residues as well. Residue numbers like 100A and 100B in an antibody can both be shown as 100, leading to multiple entries like YH100 in the list. After renumbering the whole PDB with the script above, the results would become &quot;YH103, YH104, YH105&quot;.Additionally, the <code>Interface_residues</code> results may contain some unusual entries such as <strong>oA11</strong>. Technically, this means that in chain A, position 11, there is a non-typical residue, such as a Zn ion.</p></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p><a href="https://www.sciencedirect.com/science/article/pii/S2211124723014225">Teo Q W, Wang Y, Lv H, et al. Stringent and complex sequence constraints of an IGHV1-69 broadly neutralizing antibody to influenza HA stem[J]. Cell reports, 2023, 42(11).</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Yuan M, Feng Z, Lv H, et al. Widespread impact of immunoglobulin V-gene allelic polymorphisms on antibody reactivity[J]. Cell Reports, 2023, 42(10). <a href="#fnref2" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">FoldX</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/"/>
    
    
    <category term="Protein Structure" scheme="https://karobben.github.io/tags/Protein-Structure/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
  </entry>
  
  <entry>
    <title>Nanopre and PacBio based Genome Assembly</title>
    <link href="https://karobben.github.io/2024/06/30/Bioinfor/LongReads/"/>
    <id>https://karobben.github.io/2024/06/30/Bioinfor/LongReads/</id>
    <published>2024-06-30T20:50:52.000Z</published>
    <updated>2024-07-05T16:33:10.980Z</updated>
    
    <content type="html"><![CDATA[<p>Related Papers:</p><ul><li>Rayamajhi N, Cheng C H C, Catchen J M. Evaluating Illumina-, Nanopore-, and PacBio-based genome assembly strategies with the bald notothen, Trematomus borchgrevinki[J]. G3, 2022, 12(11): jkac192. <a href="https://academic.oup.com/g3journal/article/12/11/jkac192/6651842">Paper</a></li><li>van Rengs W M J, Schmidt M H W, Effgen S, et al. A chromosome scale tomato genome built from complementary PacBio and Nanopore sequences alone reveals extensive linkage drag during breeding[J]. The Plant Journal, 2022, 110(2): 572-588. <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/tpj.15690">Paper</a></li><li>Murigneux V, Rai S K, Furtado A, et al. Comparison of long-read methods for sequencing and assembly of a plant genome[J]. GigaScience, 2020, 9(12): giaa146. <a href="https://academic.oup.com/gigascience/article/9/12/giaa146/6042729">Paper</a></li></ul><h2 id="Evaluating-Illumina-Nanopore-and-PacBio-based-genome-assembly-strategies-with-the-bald-notothen-Trematomus-borchgrevinki">Evaluating Illumina-, Nanopore-, and PacBio-based genome assembly strategies with the bald notothen, Trematomus borchgrevinki</h2><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left">In this paper, they did long reads assembly and Long-reads, short-reads hybrid assembly comparing. The experiment organism is “Trematomus borchgrevinki” (<strong>fish</strong>), a cold specialized Antarctic notothenioid fish with an estimated genome size of 1.28 Gb</td><td style="text-align:center"><img src="https://www.researchgate.net/profile/Gregory-Sloop/publication/364608485/figure/fig6/AS:11431281091299426@1666380222030/Trematomus-bernacchii-Courtesy-of-Zureks-and-Wikimedia-Commons.png" alt=""><br><a href="https://www.researchgate.net/publication/364608485_The_Cardiovascular_System_of_Antarctic_Icefish_Appears_to_Have_Been_Designed_to_Utilize_Hemoglobinless_Blood?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Gregory Sloop</a></td></tr></tbody></table><p>Sequencing Size:</p><ul><li>Nanopore: 24.29 Gb</li><li>PacBio: 118.42 Gb</li></ul><p><strong>Hybrid</strong> assemblies can generate <mark>higher contiguity</mark> they tend to suffer from lower quality. <strong>long-read-only assemblies</strong> can be optimized for <mark>contiguity</mark> by subsampling length-restricted raw reads. Long-read contig assembly is the current <strong>best choice</strong> and that assemblies from phase I and phase II were of lower quality.</p><p>Strategies:</p><ul><li>Long-reads and short-reads hybrid: quickmerge<ol><li>Long-reads assembly independently: <code>Canu</code> and <code>WTDBG2</code> assembly, assessed with <code>QUAST</code></li><li>2 rounds of polishing with <code>Pilon</code>. (First round: SNPs adn indels, Second round: local reassembly)</li><li>Gap filling with <code>PBJELLY</code></li></ol></li><li>Long-reads only was assembly by variaties of tools. The yacrd (Marijon et al. 2020) it the tool to identify potential <strong>chimeric reads</strong><ol><li><code>WTDBG2</code> was used to do the assembly</li></ol></li></ul><p>For long-reads, comparing to short-reads assembled genome, it has high continuity but also more number of duplicated BUSCO genes. Chimeric reads are exist. In this paper, they also applied the subsampling to deleted chimeric reads. By cooperate with the limiting reads lengths, the PacBio reads assembly results could be improved. The number of contigs dropped from 10,848 to 4,409 with only 70 Gb of data (generated by sampling minimum and maximum read lengths of 10 and 40 kb)</p><p>In this paper, the data shows that the assembly results from ONT reads are not as good as those from PacBio reads. However, because they used very different methods for pre-processing reads and assembly, the results are somewhat incomparable. Therefore, we can only conclude that the PacBio pipeline is more advanced.</p><h2 id="Comparison-of-long-read-methods-for-sequencing-and-assembly-of-a-plant-genome">Comparison of long-read methods for sequencing and assembly of a plant genome</h2><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left">This paper targets <em><strong>Macadamia jansenii</strong></em>, a type of tree. The PacBio data surprised others because it has higher coverage and longer reads than the typical ONT data. Therefore, <strong>cross-comparison is meaningless</strong>. However, they assembled the genome using <strong>multiple tools</strong>, so the <strong>internal data-type comparison</strong> is still valuable.</td><td style="text-align:center"><img src="https://www.researchgate.net/profile/Ian-Cock/publication/264458556/figure/fig1/AS:392523655729155@1470596340564/Macadamia-integriflora-leaves-and-flowers-photographed-accessed-from-Wikipedia-Commons.png" alt="Macadamia integriflora"><a href="https://www.researchgate.net/publication/264458556_Evaluation_of_the_potential_of_Macadamia_integriflora_extracts_as_antibacterial_food_agents?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Ian Edwin Cock</a></td></tr></tbody></table><table><thead><tr><th>Category</th><th>ONT</th><th>PacBio</th><th>BGI</th></tr></thead><tbody><tr><td><strong>Mean Reads Length</strong></td><td>7,962</td><td>20,575</td><td>2 × 100</td></tr><tr><td><strong>Assembly</strong></td><td>Redbean v2.5, Flye v2.5, and Canu</td><td>Redbean v2.5, Flye v2.5, and Canu</td><td>SuperPlus v1.0, Supernova v2.1.1, TGS-GapCloser</td></tr><tr><td><strong>Data Size</strong></td><td>24.9 Gb</td><td>65.2 Gb</td><td>74.5 Gb</td></tr><tr><td><strong>Largest Contigs</strong></td><td>9,683,794</td><td>23,824,472</td><td>517,998</td></tr><tr><td><strong>Scaffold</strong></td><td>5,332</td><td>5,446</td><td>5,065</td></tr><tr><td><strong>Scaffold N50</strong></td><td>3.52</td><td>3.50</td><td>3.54</td></tr><tr><td><strong>Contigs</strong></td><td>6,022</td><td>5,717</td><td>19,954</td></tr><tr><td><strong>Contigs N50(M)</strong></td><td>1.04</td><td>1.60</td><td>0.036</td></tr><tr><td><strong>BUSCO</strong></td><td>1,963 (92.5)</td><td>1,983 (93.5)</td><td>1,873 (88.3)</td></tr></tbody></table><p>Hybrid assembly:</p><ol><li>MaSuRCA v3.3.3: Illumina + ONT/PacBio</li><li>Flye v2.5 to perform the final assembly</li></ol><p>Diploid de novo genome assembly: PacBio reads was performed with FALCON v1.3.0</p><p>Assembly Evaluation: QUAST v5.0.2;  publicly available reference genome of M. integrifolia v2 (Genbank accession: GCA_900631585.1); subjected to BUSCO v3.0.2 with the eudicotyledons_odb10 database (2,121 genes).</p><p>In this result, the PacBio sequences dominate everything. This is because ultra-long ONT reads were not used here. Consequently, not only the length of the reads but also the accuracy and coverage of the ONT reads are lower than those of the PacBio. This comparison is extremely uneven. By <strong>comparing different long-reads assembly tools</strong> (Redbean, Flye, Falcon, Canu, Raven), the <mark>Rave</mark> is the best for both PacBio and ONT data. An interesting thing is, according to the paper, Rave supports the GPU-accelerate. But in this research, they only given 12 threads for Rave though, technically, we could give more than 1,000 of threads if we have a professional GPU.</p><h2 id="A-chromosome-scale-tomato-genome-built-from-complementary-PacBio-and-Nanopore-sequences-alone-reveals-extensive-linkage-drag-during-breeding">A chromosome scale tomato genome built from complementary PacBio and Nanopore sequences alone reveals extensive linkage drag during breeding</h2><pre class="mermaid">graph LR;　　Nanopore-->|NECAT|Assembly1;　　PacBio-->|Hifiasm|Assembly2;    Assembly1-->|quickmerge| Sinlge;    Assembly2-->|quickmerge| Sinlge</pre><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left">In this research, they target into the cultivated <strong>tomato</strong> (Solanum lycopersicum). They applied PacBio HiFi and ONT Nanopore sequencing to develop <strong>independent</strong>. After then, they <strong>merged the HiFi and ONT assemblies</strong> to generate a long-read-only assembly where all 12 chromosomes were represented as 12 contiguous sequences (N50 = 68.5 Mbp).</td><td style="text-align:center"><img src="https://www.researchgate.net/profile/Gbenga-Orunmolase/publication/371350058/figure/fig1/AS:11431281165942236@1686127227760/Tomato-Solanum-lycopersicum.jpg" alt=""><br><a href="https://www.researchgate.net/publication/371350058_MICROORGANISMS_ASSOCIATED_WITH_SOFT_ROT_OF_TOMATOES?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Gbenga Emmanuel Orunmolase</a></td></tr></tbody></table><table><thead><tr><th></th><th>ONT</th><th>PacBio HiFi</th></tr></thead><tbody><tr><td><strong>Data Size</strong></td><td>100~ Gb (x2)</td><td>20~ Gb (x2)</td></tr><tr><td><strong>Assembled Contigs</strong></td><td>100~300</td><td>700~2000</td></tr><tr><td><strong>Computational Costs</strong></td><td>3 days with 256 threads (NECAT)</td><td>2 hours with 128 threads (Hifiasm)</td></tr></tbody></table><p>Although the <strong>ONT has fewer contigs</strong>, it has a lower BUSCO (complete) percentage due to uncorrectable base errors. For the <mark>saturating test</mark>, they found that for PacBio HiFi reads, <strong>20 Gb</strong> would be enough to finish a good assembly with <strong>Hifiasm</strong>. For longer ONT reads, <strong>50 Gb</strong> could do a similar job with <strong>NECAT</strong>. After that, they conducted the <mark>merge test</mark> because they found partial complementarity of the assemblies as the breakpoints were different. After merging the two results, they obtained 12 super contigs, which correspond to the 12 chromosomes. Along with these 12 super contigs, they also obtained 54 contigs that could not be assembled into the 12 chromosomes; these could be chloroplast, mitochondrial, rDNA, and satellite repeat-derived sequences.</p><h3 id="MbTMV-assembly-pipeline-Merge-ONT-and-PacBio-results">MbTMV assembly pipeline (Merge ONT and PacBio results)</h3><ol><li>Assembly result polishing</li><li>nucmer (part of mummer v.4.0.0rc1) with the -l parameter to prevent invalid contig links</li><li>quickmerge<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> was used to merge 2 assemblies with the parameter -c 7.0.</li></ol><p>A very interesting thing is they use a customized script to convert the Salsa2 output to Hi-C file and plot the contact plot with jucibox</p><hr><p>A recent comparison pointed out that PacBio HiFi reads tend to lead to better assembly of the barley (Hordeum vulgare) genome than ONT (Mascher et al., 2021)</p><style>pre {//  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Chakraborty M, Baldwin-Brown J G, Long A D, et al. Contiguous and accurate de novo assembly of metazoan genomes with modest long read coverage[J]. Nucleic acids research, 2016, 44(19): e147-e147. <a href="https://github.com/mahulchak/quickmerge">GitHub</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">Nanopre and PacBio based Genome Assembly</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/WGS/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="Genome" scheme="https://karobben.github.io/tags/Genome/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/tags/WGS/"/>
    
  </entry>
  
</feed>
