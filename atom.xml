<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Karobben</title>
  
  <subtitle>Engjoy~</subtitle>
  <link href="https://karobben.github.io/atom.xml" rel="self"/>
  
  <link href="https://karobben.github.io/"/>
  <updated>2025-04-10T04:51:41.888Z</updated>
  <id>https://karobben.github.io/</id>
  
  <author>
    <name>Karobben</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>AlphaFold</title>
    <link href="https://karobben.github.io/2025/04/05/AI/alphafold/"/>
    <id>https://karobben.github.io/2025/04/05/AI/alphafold/</id>
    <published>2025-04-05T23:33:11.000Z</published>
    <updated>2025-04-10T04:51:41.888Z</updated>
    
    <content type="html"><![CDATA[<h2 id="AlphaFold2">AlphaFold2</h2><p>Main source from:</p><ul><li>YouTube:<ul><li><a href="https://www.youtube.com/watch?v=jTO6odQNp90">Kendrew Lecture 2021 pt2 - John Jumper</a>: first author of AlphaFold2</li><li><a href="https://www.youtube.com/watch?v=qjFgthkKxcA">Review and discussion of AlphaFold3; 2024</a>: Sergey Ovchinnikov, MIT</li><li><a href="https://www.youtube.com/watch?v=yJKfn6rvHmg">MRC Laboratory of Molecular Biology; 2024</a></li><li><a href="https://www.youtube.com/watch?v=7q8Uw3rmXyE">What Is AlphaFold? | NEJM</a></li><li><a href="https://www.youtube.com/watch?v=3gSy_yN9YBo">how AlphaFold <em>actually</em> works</a></li></ul></li><li>Papers:<ul><li>Jumper J, Evans R, Pritzel A, et al. Highly accurate protein structure prediction with AlphaFold[J]. nature, 2021, 596(7873): 583-589.</li><li>Abramson J, Adler J, Dunger J, et al. Accurate structure prediction of biomolecular interactions with AlphaFold 3[J]. Nature, 2024, 630(8016): 493-500.</li></ul></li></ul><p><img src="https://imgur.com/a0ULWSB.png" alt=""></p><p>Main architecture of the AlphaFold2:</p><ul><li>Input<ul><li><strong>MSA</strong>: Multiple Sequence Alignment, to align the sequences to gain the evolutionary information. The idea is: When residues pair-mutated, they are highly likely to be contact with each other. So, the model could learn the contact map from the MSA.</li><li><strong>Pairing</strong>: It might the <strong>most important</strong> part for the model. Captures pairwise relationships between residues.<ul><li>At the beginning, there are almost no information.</li><li><strong>triangle inequality</strong>: ij+kj&gt;=ik (i, j, k are the residues)</li></ul></li><li><strong>Structure Database</strong>: The model will use the structure database to get the information of the protein structure.</li><li><mark>MSA results</mark> would be used to generate the <strong>MSA representation</strong>. <mark>Pairing</mark> and <mark>Structure Database</mark> would be used to generate the <strong>Pair representation</strong>.</li></ul></li><li>Evoformer<ul><li>The Evoformer is a transformer model which is used to learn the MSA representation and Pair representation. The Evoformer will use the MSA representation and Pair representation to refine the <strong>MSA representation</strong> and <strong>Pair representation</strong></li></ul></li><li>Structure<ul><li>The structure module would use the <strong>representations</strong> to do rotation and translation to generate the final structure.</li></ul></li><li>Recycle:<ul><li>The final structure would be collected and used to refine the <strong>refined representations</strong> in the <strong>Evoformer</strong> module and <strong>Structure</strong> module multiple times.</li></ul></li></ul><h2 id="Multiple-Sequence-Alignment-MSA">Multiple Sequence Alignment (MSA)</h2><p>Multiple sequences are critical for the prediction of protein structures. The more sequences you have, the better the prediction. Even taking away the sequences in PDB databse, AF2 cans till give a very good prediction. “The sequence is very very clear for the structure” (John)</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:left">MSA is important</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/yaOCXym.png" alt=""></td><td style="text-align:left">How does the Multiple Sequence Alignment (MSA) help: the sequences alignment could help to encode the residues contact map. When a residues mutated, the contact residues are likely to be mutated, too. This kind of features could be captured by models.</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/agW7Dyw.png" alt=""></td><td style="text-align:left">Without the MSA, the predicted result is terrible.</td></tr></tbody></table><table><thead><tr><th style="text-align:center">MAS</th><th style="text-align:center">pLDDT</th><th style="text-align:center">PAE</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/537sn8D.png" alt=""></td><td style="text-align:center"><img src="https://imgur.com/HzSz3t5.png" alt=""></td><td style="text-align:center"><img src="https://imgur.com/BtkO7wh.png" alt=""></td></tr></tbody></table><p>As you can see from this graphic, the depth of the MSA results are difference. The depth would effect the final quality of the model. And the depth of the MSA is also highly positive associated with the pLDDT score. The more sequences you have, the better the prediction. Except the depth, the pLDDT score is also highly related to the second structures. Random loop usually get a low pLDDT.</p><p><strong>PAE</strong>’s concept is pretty similar to pLDDT. pLDDT is predicts the single residue confidence, while PAE is predicts the pairwise residue confidence. The higher the PAE score, the more confident the model is about the distance between two residues.</p><h2 id="Results">Results</h2><ul><li><strong>pLDDT</strong>: a per-atom confidence estimate on a 0-100 scale where a higher value indicates higher confidence.</li><li><strong>PAE</strong> (predicted aligned error): estimate of the error in the relative position and orientation between two tokens in the predicted structure.</li><li><strong>pTM</strong> and <strong>ipTM</strong> scores: the <mark>predicted template modeling</mark> (pTM) score and the interface predicted template modeling (ipTM) score are both derived from a measure called the template modeling ™ score. This measures the accuracy of the <s>entire structure</s></li></ul><p>In alphafold2, you’ll get 5 results for each seed because there are 5 different models which trained slightly different. The model would rank the results by pLDDT, pTM, and, ipTM.</p><p>You may tell the false positive based on the PEA score</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/xXFSAfk.png" alt="False Negative Prediction"></td><td style="text-align:left"><strong>False Negative</strong>: The results from the top is predicted incorrectly. The PAE score between chain with different color is very high. The results from the bottom is predicted correctly. The PAE score between chain with different color is very low which means the result my reliable.</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/QeZG4is.png" alt="False Positive"></td><td style="text-align:left"><strong>False Positive</strong>: Though the PAE are very good, but they are not supposed to be interact to each other. It could be caused by some protein from other families may interacted in this way. So, the model learned this possibilities.</td></tr></tbody></table><table><thead><tr><th style="text-align:center"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41586-021-03819-2/MediaObjects/41586_2021_3819_Fig4_HTML.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">Ablation results on two target sets: the CASP14 set of domains (n = 87 protein domains) and the PDB test set of chains with template coverage of ≤30% at 30% identity (n = 2,261 protein chains). Domains are scored with GDT and chains are scored with lDDT-Cα. The ablations are reported as a difference compared with the average of the three baseline seeds. Means (points) and 95% bootstrap percentile intervals (error bars) are computed using bootstrap estimates of 10,000 samples. b, Domain GDT trajectory over 4 recycling iterations and 48 Evoformer blocks on CASP14 targets LmrP (T1024) and Orf8 (T1064) where D1 and D2 refer to the individual domains as defined by the CASP assessment. Both T1024 domains obtain the correct structure early in the network, whereas the structure of T1064 changes multiple times and requires nearly the full depth of the network to reach the final structure. Note, 48 Evoformer blocks comprise one recycling iteration.</td></tr></tbody></table><h2 id="Limitations">Limitations</h2><p>As you can expect, MSA brings huge convenience to the model. But it also brings the major limitation: it not sensitive to the mutations. In some cases, a point mutation would make the inner structure or interaction between proteins fail. But the MSA would not be able to capture this kind of information and database lacking such structures. So, the model would have high false positive rate. This features makes antibody-antigen interaction prediction very hard. (It was said that RoseTTAFold can do much better on handling mutations.)</p><p>On the other hand, as long as AF relies on the MSA, it is not good at fast mutated proteins interactions. For example, the virus protein and antibodies. Virus mutation every years and antibodies mutated every few days<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>. MSA would more reliable on the co-evolution over thousands or millions of years. But for the fast mutated proteins, the MSA would not be able to capture the co-evolution information. So, the model would not be able to learn the structure from scratch.</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/lLYMCB4.png" alt=""></td><td style="text-align:left">It is also not hard to imagine that the model would not be able to predict the structure of the protein which has no homologous sequences. The model is not able to learn the structure from scratch.</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/BCTQZeZ.png" alt=""></td><td style="text-align:left">It seems like AF2 can get a much better DockQ score then the other models. But the this kind of compairation is very tricky</td></tr></tbody></table><p>John Jumper claimed that the AF2-multimer “performs poorly on antibody interactions”, could be caused by “still miss quite a few interactions”</p><h2 id="AlphaFold3">AlphaFold3</h2><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/fQM7ut8.png" alt=""></th></tr></thead><tbody></tbody></table><p>In AF3, John Jumper became the first corresponding author with Demis. The first author is Josh Abramson.</p><p>The main difference between AF2 and AF3 is that AF3 moved the structure model out of the cycling and only put the Pairformer in the cycling. The structure model is only used once. And it is replaced by the Diffusion model. Meanwhile, the contribution of MSA was changed. In Af3, there is an independent MSA module and only 4 blocks. If Af2, MSA would used for 48 blocks. This change makes the training much faster. For 5000 tokens, AF2 would takes about 2 to 3 days. But now, it only takes about 20 minutes.</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/2TEVTBI.png" alt="AF3 vs Af2"></td><td style="text-align:left">AF3 made some improvements on monomers and prtotein-protein interaction prediction caompare to AF2. It seems like the AF3 made huge improvement on the protein-antibody prediction. But according to Sergey, this comparision is very tricky. They didn’t take the results by single try. They actually test different seeds independently to get over 1000 results. And then, they took the best result. So, the results are not very reliable. This hind may tell use that diffusion model may not solve the sampling problem well</td></tr></tbody></table><ul><li>reproducibility: For AF model, if you change the random seeds, the result would be totally different. So, when we try to use it to predict the protein-antibody, we would got all different results and it is hard to tell which one is the true positive.</li></ul><div class="admonition note"><p class="admonition-title">Why Diffusion model doesn't work well in protein-antibody prediction?</p><p>Personally, I think the main limitation still bring from the MSA. The homology modeling method would be ok for protein-protein prediction because this type of interaction could be described and protein family-family interaction. But for antibodies, the interaction is very specific and restricted to the unique loop region which mostly not exist in the any of database. Meanwhile, the contribution of the residues are extremely unbalanced. Some residues credit as motif plays the most important role in the recognition. Rest of residues are maybe play the role in supporting the structure, complementary to the surface to present the motif in a correct position, etc. So, antibodies are more sensitive to the surface structure. But the diffusion model would fail in the initial interface prediction. And during the de-noise process, this error would be amplified. Meanwhile, the model doesn't know if the antibody can dock to the target or not. During the diffusion steps, it only tries is best to dock on it during the de-noise steps and make false positive result.</p></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p><a href="https://www.youtube.com/watch?v=oSTHQQYoGQs">Jeffrey Gray: Artificial Intelligence Tools for Antibody Engineering and Protein Docking; 2024; YouTube</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">AlphaFold2</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="LM" scheme="https://karobben.github.io/categories/Machine-Learning/LM/"/>
    
    <category term="Protein" scheme="https://karobben.github.io/categories/Machine-Learning/LM/Protein/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="AI" scheme="https://karobben.github.io/tags/AI/"/>
    
    <category term="3D" scheme="https://karobben.github.io/tags/3D/"/>
    
    <category term="Protein Structure" scheme="https://karobben.github.io/tags/Protein-Structure/"/>
    
  </entry>
  
  <entry>
    <title>Hemagglutinin, the Influenza Virus Protein</title>
    <link href="https://karobben.github.io/2025/03/13/LearnNotes/hemagglutinin/"/>
    <id>https://karobben.github.io/2025/03/13/LearnNotes/hemagglutinin/</id>
    <published>2025-03-13T17:45:19.000Z</published>
    <updated>2025-03-13T21:16:52.223Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://cdn.rcsb.org/pdb101/motm/302/H5_1JSN_1RVT.png" alt=""><br><a href="https://pdb101.rcsb.org/motm/302">© PDB</a></td><td style="text-align:left">Influenza is able to enter and infect cells through the action of hemagglutinin, which recognizes and attaches to specific molecules on the cell surface. Most hemagglutinins have been found to target sialic acids, a family of nine-carbon sugar molecules that are commonly found at the tips of glycans, or sugar chains, that are attached to proteins and lipids on the cell surface. Sialic acid can be linked to glycans in different ways.</td></tr></tbody></table><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://www.pnas.org/cms/10.1073/pnas.1810927115/asset/7e80d138-dd37-4523-b210-b015a43262b9/assets/graphic/pnas.1810927115fig04.jpeg" alt=""><br><a href="https://www.pnas.org/doi/10.1073/pnas.1810927115">© Donald J. Benton, 2018</a></td><td style="text-align:left">(A) Cryotomogram section showing cross-section of a liposome with examples of HAs tilted with respect to the lipid bilayer (white boxes). (Scale bar: 20 nm.)<br>(B) Gallery of subtomograms of tilted HA in liposomes. Images in the second row are identical to those above but indicate HAs (blue) and liposome bilayer (red lines). (Scale bar: 10 nm.)</td></tr></tbody></table><h2 id="Conformation-Change-of-Hemagglutinin">Conformation Change of Hemagglutinin</h2><table><thead><tr><th style="text-align:center"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41586-020-2333-6/MediaObjects/41586_2020_2333_Fig1_HTML.png?as=webp" alt=""></th><th style="text-align:left"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41586-020-2333-6/MediaObjects/41586_2020_2333_Fig3_HTML.png?as=webp" alt=""></th></tr></thead><tbody></tbody></table><p><a href="https://www.nature.com/articles/s41586-020-2333-6?fromPaywallRec=false">© Donald J. Benton, 2020</a></p><p>At low pH in endosomes, about pH 5.5, depending on the virus strain, fusion of virus and endosomal membranes is activated in a process that involves extensive changes in HA conformation<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p><h2 id="Anchor-of-the-HA-Protein">Anchor of the HA Protein</h2><p>Donald J. Benton, et al., observe the structure of the region that anchors HA in the virus membrane as <strong>a bundle of three α-helices</strong> that are joined to the ectodomain by flexible linkages. And the <strong>fab</strong>, HA−FISW84, binds to near there (<a href="https://www.rcsb.org/structure/6HJQ">6HJQ</a>) my affects the <strong>flexibility</strong> of the linker and prevents the conformational changes that are necessary for membrane <strong>fusion</strong><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>.</p><table><thead><tr><th style="text-align:center"><img src="https://www.pnas.org/cms/10.1073/pnas.1810927115/asset/c53d13e7-4b4c-4de9-8bb5-57449231b859/assets/graphic/pnas.1810927115fig02.jpeg" alt=""></th><th style="text-align:left">The structure of the membrane-associated region. Detailed views of (A) tilted and (B) straight micelles, as shown in Fig. 1 B and C respectively.</th></tr></thead><tbody></tbody></table><blockquote><p><a href="https://www.pnas.org/doi/10.1073/pnas.1810927115">© Donald J. Benton, 2018</a>: The flexible linker region (purple) runs between Gly-175, at the C terminus of the 160 helix, and Gly-182 and extends to the N termini of the α-helices of a trimeric α-helical bundle, residues 186 to 203 (cyan). (C) The amino acid sequence of the transmembrane domain. The sequence shown begins at the 160 helix to the C terminus of HA2. Color-coded block diagrams indicate the positions of these structural elements in the sequence.</p></blockquote><h2 id="Fusion-Model">Fusion Model</h2><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://journals.asm.org/cms/10.1128/jvi.00240-16/asset/65d32174-509e-4279-8b22-72a0325989fa/assets/graphic/zjv9991818150002.jpeg" alt=""></td><td style="text-align:left">A 3.2-nm-thick computational slice through a reconstructed cryo-electron tomogram shows several examples of HA bridging (blue arrowheads) between virus particles and 80% DOPC:20% Chol liposomes after 30 s of incubation at pH 5.5<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></td></tr></tbody></table><table><thead><tr><th style="text-align:center"><img src="https://journals.asm.org/cms/10.1128/jvi.00240-16/asset/e96f8f67-a919-4f4b-9c14-047f5ccec992/assets/graphic/zjv9991818150003.jpeg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://journals.asm.org/cms/10.1128/jvi.00240-16/asset/2cbd36cf-fc05-48a9-a525-a9e2ff260d61/assets/graphic/zjv9991818150010.jpeg" alt=""></td></tr><tr><td style="text-align:center"><a href="https://journals.asm.org/doi/10.1128/jvi.00240-16">© Lee lab</a></td></tr></tbody></table><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>PA Bullough, FM Hughson, JJ Skehel, DC Wiley, Structure of influenza haemagglutinin at the pH of membrane fusion. Nature 371, 37–43 (1994). <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p><a href="https://www.pnas.org/doi/10.1073/pnas.1810927115">Benton D J, Nans A, Calder L J, et al. Influenza hemagglutinin membrane anchor[J]. Proceedings of the National Academy of Sciences, 2018, 115(40): 10112-10117.</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p><a href="https://journals.asm.org/doi/10.1128/jvi.00240-16">Gui L, Ebner J L, Mileant A, et al. Visualization and sequencing of membrane remodeling leading to influenza virus fusion[J]. Journal of virology, 2016, 90(15): 6948-6962.</a> <a href="#fnref3" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">Hemagglutinin is a protein found on the surface of the influenza virus.</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Notes/Biology/"/>
    
    <category term="Immunity" scheme="https://karobben.github.io/categories/Notes/Biology/Immunity/"/>
    
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Antibody" scheme="https://karobben.github.io/tags/Antibody/"/>
    
    <category term="Immunology" scheme="https://karobben.github.io/tags/Immunology/"/>
    
    <category term="Influenza" scheme="https://karobben.github.io/tags/Influenza/"/>
    
  </entry>
  
  <entry>
    <title>Protein Loop Refinement</title>
    <link href="https://karobben.github.io/2025/03/03/Bioinfor/protein-loop/"/>
    <id>https://karobben.github.io/2025/03/03/Bioinfor/protein-loop/</id>
    <published>2025-03-03T15:19:36.000Z</published>
    <updated>2025-03-04T02:26:40.556Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DaReUS-Loop-2018-Web-Server-Only">DaReUS-Loop 2018 (Web Server Only)</h2><table><thead><tr><th style="text-align:center"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-018-32079-w/MediaObjects/41598_2018_32079_Fig1_HTML.png" alt=""></th></tr></thead><tbody></tbody></table><p>DaReUS-Loop (Data-based approach using Remote or Unrelated Structures for Loop modeling) (<a href="https://mobyle.rpbs.univ-paris-diderot.fr/cgi-bin/portal.py#forms::DaReUS-Loop">Web Server</a>) is a data-based approach that identifies loop candidates mining the complete set of experimental structures available in the Protein Data Bank. Candidate filtering relies on local conformation profile-profile comparison, together with physico-chemical scoring<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>. This work is the extension of using Binet-Cauchy kernel to mine large collections of structures efficiently<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>.</p><p>The database they used: CASP11, CASP12 and HOMSTRAD.</p><p>In this paper, They classified protein loop-modeling tools are generally into three main categories:</p><ol><li><strong>Knowledge-based (Template-based) Methods</strong>:<ul><li>These approaches utilize structural repositories to extract observed loop conformations that match specific sequences and geometric constraints. By mining databases of known protein structures, they identify loop candidates that fit the target region, offering computational efficiency. However, their effectiveness is limited by the availability of suitable loop conformations in existing structural data.</li><li>Exp: SuperLooper2<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> mines the Loop In Protein (LIP) database;<ul><li>MoMA-LoopSampler<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>: a knowledge-based method that uses a database of loop fragments to model missing segments in proteins. It has <a href="https://moma.laas.fr/results/">web server</a> but queue is too long to wait.</li></ul></li></ul></li></ol><ol start="2"><li><strong>Ab Initio (De Novo) Methods</strong>:<ul><li>These techniques involve sampling a wide range of possible loop conformations <strong>without relying on existing structural templates</strong>. They <strong>dependent on energy optimization techniques and are consequently highly time consuming</strong>. They often employ <strong>exhaustive searches</strong> of the loop’s torsional angles to explore conformational space. While capable of modeling loops without suitable templates, ab initio methods are computationally intensive and typically more successful with <strong>shorter loops</strong> due to the vast number of possible conformations.</li><li>Exp:Rosetta Next-Generation KIC (NGK)<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>, GalaxyLoop-PS2<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup></li></ul></li></ol><ol start="3"><li><strong>Hybrid Methods</strong>:<ul><li>Combining elements of both knowledge-based and ab initio approaches, hybrid methods use small structural fragments from databases within an ab initio sampling framework. This integration aims to balance computational efficiency with modeling accuracy, leveraging known structural motifs to guide the exploration of conformational space.</li><li>Exp: Sphinx<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup></li></ul></li></ol><h3 id="How-to-Use">How to Use</h3><p>In the <a href="https://mobyle.rpbs.univ-paris-diderot.fr/cgi-bin/portal.py#forms::DaReUS-Loop">Web Server</a>, you can upload your protein structure file and the sequences.<br>For the sequences, you need to <mark>mask your loop with gap (‘-’)</mark>.</p><p>In my case, I masked a 14 amino acid loop in the sequence and this is the log:</p><pre>[00:00:00] Please bookmark this page to have access to your results![00:00:00] 1/14: Verifying input files[00:00:00] Warning! Only the first chain from the input PDB is considered![00:00:11] 2/14: BCLoopSearch[00:01:42] 3/14: clustering[00:01:59] 4/14: Conformational profiles[00:04:16] Found 353 hits for Loop1_AMTMVVASFFQYYA[00:04:16] 5/14: Measuring Jensen Shannon distances10%..20%..30%..40%..50%..60%..70%..80%..90%..100%[00:08:06] 6/14: selecting top candidates per loop[00:08:21] 7/14: preparing top candidates for minimization[00:08:43] 8/14: positioning linker side chains[00:09:36] 9/14: minimization[00:11:01] 10/14: Scoring the candidates[00:11:01] ... 3 jobs already in the queue, please wait ...[00:23:16] 11/14: Measuring energy values (KORP potential)[00:23:45] 12/14: Preparing the final energy report[00:23:55] 13/14: Generating combinatorial models[00:24:04] 14/14: detecting possible clashes between candidates[00:24:34] ... 3 jobs already in the queue, please wait ...[00:26:46] ... 2 jobs already in the queue, please wait ...[00:27:52] Finished</pre><p><img src="https://imgur.com/vzrRJXM.png" alt=""></p><p>In the end, you can download the top  10 models.<br>I compared it with the original structure and the loop is refined and find <strong>none of them are making sense</strong>. Another biggest problem is that they <strong>only</strong> took the chain with masked loops and <strong>disgard the rest of other chains</strong>. In this case, lots of results would have a serious <strong>atom crush</strong> problem. So, it takes about half an hour to get the result and the result is not good even for a very small protein.</p><p>By checking the performance from the paper, it also doesn’t show a dramatic improvements.</p><p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-018-32079-w/MediaObjects/41598_2018_32079_Fig5_HTML.png" alt=""></p><h2 id="Foldseek-2024">Foldseek 2024</h2><p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41587-023-01773-0/MediaObjects/41587_2023_1773_Fig1_HTML.png" alt=""></p><p>FOldSeek<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> also has a <a href="https://search.foldseek.com">web server</a>. It also deposited there codes on Github at <a href="https://github.com/steineggerlab/foldseek">steineggerlab/foldseek</a>. After that, the also development foldseek-multimer<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> for protein complex alignment.</p><details><summary>Abstract</summary>As structure prediction methods are generating millions of publicly available protein structures, searching these databases is becoming a bottleneck. Foldseek aligns the structure of a query protein against a database by describing tertiary amino acid interactions within proteins as sequences over a structural alphabet. Foldseek decreases computation times by four to five orders of magnitude with 86%, 88% and 133% of the sensitivities of Dali, TM-align and CE, respectively.</details><p>It is a fast loop searching tool which is very powerful if you want to use it to search the loop in a large database. It is a very good tool if you want to development a new knowledge-based loop refinement tool. <mark>You can’t use it to refine the loop directly.</mark></p><h2 id="KarmaLoop-2024">KarmaLoop 2024</h2><table><thead><tr><th style="text-align:center"><img src="https://spj.science.org/cms/10.34133/research.0408/asset/ccac7451-395e-4551-b33e-ed4cf0bd212d/assets/graphic/research.0408.fig.001.jpg" alt=""></th></tr></thead><tbody></tbody></table><p>KarmaLoop<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup> is an <a href="https://github.com/karma211225/KarmaLoop">open source</a> deep learning-based framework designed for rapid and precise full-atom protein loop modeling. It addresses the challenges of predicting loop conformations, which are critical for accurate protein structure determination.</p><p><strong>Key Features of KarmaLoop</strong>:</p><ul><li><p><strong>Full-Atom Modeling</strong>: Unlike many methods that focus primarily on backbone atoms, KarmaLoop predicts the positions of both backbone and side-chain heavy atoms, providing comprehensive loop conformations.</p></li><li><p><strong>Deep Learning Architecture</strong>: The framework employs advanced neural network architectures, including Graph Transformers and Geometric Vector Perceptrons, to effectively capture intricate structural patterns within proteins.</p></li><li><p><strong>High Accuracy and Efficiency</strong>: KarmaLoop has demonstrated superior performance in both accuracy and computational speed compared to traditional and other deep learning-based methods. For instance, it achieved average root-mean-square deviations (RMSDs) of 1.77 Å and 1.95 Å on the CASP13+14 and CASP15 benchmark datasets, respectively, with a significant speed advantage over other methods. citeturn0search0</p></li><li><p><strong>Versatility</strong>: The tool has shown effectiveness in modeling general protein loops as well as specific regions like antibody complementarity-determining region (CDR) H3 loops, which are known for their structural complexity.</p></li></ul><h3 id="How-to-Use-v2">How to Use</h3><p>It is very easy to installing.<br>Before you refine you loop, you need to truncate you structure with the script they provide and convert it into graphic. Then, you can use their pre-trained model to refine your structure.</p><p><mark>The problem</mark> is the script to make the graphic has different features on the model the give you. So, you can use their model, at all. It seems they updated the codes for model alone, but forget to update the codes for graphic making. Someone asked this question on the github but didn’t get any response.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style><h2 id="PLM-GAN">PLM-GAN</h2><p>PLM-GAN looks like a good model. But they deposited their trained model on github as a large file and we can’t download it any more. So, we can’t test the performace of it.</p><p>It applies Generative Adversarial Networks (GANs) to model loop structures, leveraging the generator-discriminator framework to produce realistic loop conformations. It utilizes the pix2pix GAN model to generate and inpaint protein distance matrices, facilitating the folding of protein structures</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Karami Y, Guyon F, De Vries S, et al. DaReUS-Loop: accurate loop modeling using fragments from remote or unrelated proteins[J]. Scientific reports, 2018, 8(1): 13673. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Guyon F, Tufféry P. Fast protein fragment similarity scoring using a binet–cauchy kernel[J]. Bioinformatics, 2014, 30(6): 784-791. <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Ismer J, Rose A S, Tiemann J K S, et al. SL2: an interactive webtool for modeling of missing segments in proteins[J]. Nucleic acids research, 2016, 44(W1): W390-W394. <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>MoMA-LoopSampler: a web server to exhaustively sample protein loop conformations <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>Stein A, Kortemme T. Improvements to robotics-inspired conformational sampling in rosetta[J]. PloS one, 2013, 8(5): e63090. <a href="#fnref5" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p>Park H, Lee G R, Heo L, et al. Protein loop modeling using a new hybrid energy function and its application to modeling in inaccurate structural environments[J]. PloS one, 2014, 9(11): e113811. <a href="#fnref6" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p>Marks C, Nowak J, Klostermann S, et al. Sphinx: merging knowledge-based and ab initio approaches to improve protein loop prediction[J]. Bioinformatics, 2017, 33(9): 1346-1353. <a href="#fnref7" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p>Van Kempen M, Kim S S, Tumescheit C, et al. Fast and accurate protein structure search with Foldseek[J]. Nature biotechnology, 2024, 42(2): 243-246. <a href="#fnref8" class="footnote-backref">↩︎</a></p></li><li id="fn9" class="footnote-item"><p>Kim W, Mirdita M, Levy Karin E, et al. Rapid and sensitive protein complex alignment with foldseek-multimer[J]. Nature Methods, 2025: 1-4. <a href="#fnref9" class="footnote-backref">↩︎</a></p></li><li id="fn10" class="footnote-item"><p>Wang T, Zhang X, Zhang O, et al. Highly accurate and efficient deep learning paradigm for full-atom protein loop modeling with KarmaLoop[J]. Research, 2024, 7: 0408. <a href="#fnref10" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">Protein Loop refinement, Paper Read</summary>
    
    
    
    <category term="Paper" scheme="https://karobben.github.io/categories/Paper/"/>
    
    
    <category term="AI" scheme="https://karobben.github.io/tags/AI/"/>
    
    <category term="Protein" scheme="https://karobben.github.io/tags/Protein/"/>
    
    <category term="Structure" scheme="https://karobben.github.io/tags/Structure/"/>
    
  </entry>
  
  <entry>
    <title>AlexNet</title>
    <link href="https://karobben.github.io/2025/02/06/AI/alexnet/"/>
    <id>https://karobben.github.io/2025/02/06/AI/alexnet/</id>
    <published>2025-02-06T21:39:05.000Z</published>
    <updated>2025-02-06T23:18:49.849Z</updated>
    
    <content type="html"><![CDATA[<h2 id="AlexNet">AlexNet</h2><p>AlexNet is a convolutional neural network that won the ImageNet Large Scale Visual Recognition Challenge in 2012. It was designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. The network has eight layers, five of which are convolutional layers and three are fully connected layers. It uses ReLU activation functions, dropout for regularization, and data augmentation techniques to improve performance. AlexNet significantly advanced the field of deep learning and computer vision.</p><p>In the structure illustration, it was usually split into 2 parts. It is because backing that time, GPU has limited memory. So, they split each layer into 2 parts, and each part is trained on a different GPU.</p><p><img src="https://upload.wikimedia.org/wikipedia/commons/c/cc/Comparison_image_neural_networks.svg" alt="AlexNet, wikipedia"></p><p>As you can see from this illustration, the input imge is 227x227x3, which is then passed through five convolutional layers, followed by three fully connected layers. The output is a 1000-dimensional vector representing the probabilities of the image belonging to each of the 1000 classes in the ImageNet dataset.</p><p>Let’s break down the architecture of AlexNet:</p><ol><li><strong>Input Layer</strong>: The input layer takes an image of size 227x227x3 (height, width, channels or RGB values).</li><li><strong>Convolutional Layer 1</strong>: The first convolutional layer applies 96 filters of size 11x11 with a stride of 4, resulting in an output of size 55x55x96. This layer uses ReLU activation and is followed by a max pooling layer with a 3x3 window and a stride of 2.<ul><li>What does ReLU do? It introduces non-linearity into the model by replacing all negative pixel values in the feature map with zero. By doing this, the model can learm more complex patterns in the data. So, it basically makes the negative values as a closed signal.</li></ul></li><li><strong>Convolutional Layer 2</strong>: The second convolutional layer applies 256 filters of size 5x5 with a stride of 1, resulting in an output of size 27x27x256. This layer also uses ReLU activation and is followed by a max pooling layer with a 3x3 window and a stride of<br>…</li><li>Finally, we got a 5x5x256 feature map. This is then flattened into a 1D vector of size 6400. It then passes through three fully connected layers with 4096, 4096, and 1000 neurons, respectively. The final layer uses a softmax activation function to output the probabilities of the image belonging to each of the 1000 classes in the ImageNet dataset.</li></ol><p>So, as you can see, the original image contains 227x227x3 = 154,587. In daily life, the picture we using are 1920*1080, which could have 6,220,800 input. With this convolutional techniques, we can reduce the size of the image while still keeping the important information.</p><div class="admonition note"><p class="admonition-title">**Terminology** Explained</p><ul><li><strong>filters</strong>: The filters are also known as kernels, and they are used to detect features in the input image. In this case, the first convolutional layer uses 96 filters of size 11x11.</li><li><strong>stride</strong>: The stride is the number of pixels by which the filter is moved across the image. You can also think of it as the step size. In this case, the stride is 4, which means that the filter is moved 4 pixels at a time. <strong>55</strong> = (227 - 11) / 4 + 1</li><li><strong>5.6 times of decreasing</strong>:  Now, instsad of 227x227x3, we have 55x55x96. You can image that the image is now much smaller, but it still contains a lot of information. Though, the &quot;pixels&quot; are less than the original image, each information from the single pixel is now represented by 96 values rather than 3 values. By multiplying the number of pixels with the number of filters, we can see that the amount of information has increased significantly. (227x227x3 = 154,587, 55x55x9 = 27225)</li><li><strong>ReLU activation</strong>: The ReLU activation function is applied to introduce non-linearity into the model. It replaces all negative pixel values in the feature map with zero.</li><li><strong>Max Pooling</strong>: The max pooling layer reduces the spatial dimensions of the feature map by taking the <strong>maximum value in each 3x3 window</strong> with a stride of 2. This helps to reduce the computational complexity and makes the model more robust to variations in the input image. The difference between the convolutional layer and the max pooling layer is that the convolutional layer applies a filter to the input image, while the max pooling layer reduces the spatial dimensions of the feature map by taking the maximum value in each window.</li></ul></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">AlexNet is a convolutional neural network that won the ImageNet Large Scale Visual Recognition Challenge in 2012.</summary>
    
    
    
    <category term="AI" scheme="https://karobben.github.io/categories/AI/"/>
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/AI/Machine-Learning/"/>
    
    
    <category term="neuralnetworks" scheme="https://karobben.github.io/tags/neuralnetworks/"/>
    
    <category term="deeplearning" scheme="https://karobben.github.io/tags/deeplearning/"/>
    
    <category term="computer-vision" scheme="https://karobben.github.io/tags/computer-vision/"/>
    
  </entry>
  
  <entry>
    <title>esm, Evolutionary Scale Modeling</title>
    <link href="https://karobben.github.io/2025/01/22/Bioinfor/esm/"/>
    <id>https://karobben.github.io/2025/01/22/Bioinfor/esm/</id>
    <published>2025-01-22T16:06:18.000Z</published>
    <updated>2025-01-22T16:21:58.294Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Basic-Use">Basic Use</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> esm<br><br><span class="hljs-comment"># Load ESM-2 model</span><br>model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()<br>batch_converter = alphabet.get_batch_converter()<br>model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># disables dropout for deterministic results</span><br><br><span class="hljs-comment"># Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)</span><br>data = [<br>    (<span class="hljs-string">&quot;protein1&quot;</span>, <span class="hljs-string">&quot;MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG&quot;</span>),<br>    (<span class="hljs-string">&quot;protein2&quot;</span>, <span class="hljs-string">&quot;KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE&quot;</span>),<br>    (<span class="hljs-string">&quot;protein2 with mask&quot;</span>,<span class="hljs-string">&quot;KALTARQQEVFDLIRD&lt;mask&gt;ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE&quot;</span>),<br>    (<span class="hljs-string">&quot;protein3&quot;</span>,  <span class="hljs-string">&quot;K A &lt;mask&gt; I S Q&quot;</span>),<br>]<br>batch_labels, batch_strs, batch_tokens = batch_converter(data)<br>batch_lens = (batch_tokens != alphabet.padding_idx).<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># Extract per-residue representations (on CPU)</span><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    results = model(batch_tokens, repr_layers=[<span class="hljs-number">33</span>], return_contacts=<span class="hljs-literal">True</span>)<br>token_representations = results[<span class="hljs-string">&quot;representations&quot;</span>][<span class="hljs-number">33</span>]<br><br><span class="hljs-comment"># Generate per-sequence representations via averaging</span><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> token 0 is always a beginning-of-sequence token, so the first residue is token 1.</span><br>sequence_representations = []<br><span class="hljs-keyword">for</span> i, tokens_len <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(batch_lens):<br>    sequence_representations.append(token_representations[i, <span class="hljs-number">1</span> : tokens_len - <span class="hljs-number">1</span>].mean(<span class="hljs-number">0</span>))<br><br><span class="hljs-comment"># Look at the unsupervised self-attention map contact predictions</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">for</span> (_, seq), tokens_len, attention_contacts <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(data, batch_lens, results[<span class="hljs-string">&quot;contacts&quot;</span>]):<br>    plt.matshow(attention_contacts[: tokens_len, : tokens_len])<br>    plt.title(seq)<br>    plt.show()<br></code></pre></td></tr></table></figure></div><h2 id="Get-the-Embedding-for-Each-Residues">Get the Embedding for Each Residues</h2><p><strong>ESM</strong> (like many transformer-based models) uses “special tokens” plus padding so that all sequences in a batch have the same length. Specifically:</p><ol><li><p><strong>Start and End Tokens</strong>: For any single sequence of length ( n ), the ESM model prepends a start token and appends an end token. That gives you ( n + 2 ) positions for a single sequence.</p></li><li><p><strong>Batch Processing Requires Padding</strong>: When you process multiple sequences in a single batch, they all get padded (on the right) to match the length of the <em>longest</em> sequence in the batch. So if the longest sequence has ( n ) residues, <em>all</em> sequences become length ( n + 2 ) (including the special tokens), and shorter sequences get padding tokens to fill in the gap.</p></li></ol><p>Hence, whether a sequence originally has ( k ) residues or ( m ) residues, in a batch whose <em>longest</em> sequence is ( n ) residues, everyone ends up with a vector length of ( n + 2 ). This ensures the entire input tensor in the batch has a uniform shape.</p><p>Here is an example of extract the embedding by following codes above:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">Seq_Embeding = &#123;i[<span class="hljs-number">0</span>]:token_representations[<span class="hljs-number">0</span>][:<span class="hljs-built_in">len</span>(i[<span class="hljs-number">1</span>])+<span class="hljs-number">2</span>] <span class="hljs-keyword">for</span> i,ii <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(data,token_representations) &#125;<br><span class="hljs-comment"># also, this is for remove the start and end</span><br>Seq_Embeding = &#123;i[<span class="hljs-number">0</span>]:token_representations[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>:<span class="hljs-built_in">len</span>(i[<span class="hljs-number">1</span>])+<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> i,ii <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(data,token_representations) &#125;<br></code></pre></td></tr></table></figure></div><p>The embedding results form batch and single chain are general the same but slightly different. If you embedding them one by one and calculate the difference, you’ll find there are slight different. According to the ChatGPT, it could be caused by:</p><ol><li><p><strong>Position Embeddings</strong></p><ul><li>ESM (like most Transformer models) uses positional embeddings. If the model sees a “longer” padded batch, the position indices for each token can differ from the single-sequence scenario, so the sequence’s tokens may be mapped to slightly different (learned) position embeddings.</li></ul></li><li><p><strong>Attention Masking and Context</strong></p><ul><li>In a batched setting, the model creates a larger attention mask (covering all tokens up to the longest sequence in the batch). Although it’s not supposed to mix information across sequences, the internal computations (e.g., how attention is batched or chunked) can differ from the single-sequence forward pass, leading to small numeric discrepancies.</li></ul></li><li><p><strong>Dropout or Other Stochastic Layers</strong></p><ul><li>If your model isn’t in <code>eval()</code> mode (or if dropout is enabled for any reason), you’ll get random differences each pass. Always ensure <code>model.eval()</code> and (ideally) a fixed random seed for more reproducible outputs.</li></ul></li><li><p><strong>Floating-Point Rounding</strong></p><ul><li>GPU parallelization can cause minor floating-point differences, especially between batched and single-inference calls. These are typically very small numerical deviations.</li></ul></li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">esm, Evolutionary Scale Modeling</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Protein Structure" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Protein-Structure/"/>
    
    
    <category term="AI" scheme="https://karobben.github.io/tags/AI/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Biochmistry" scheme="https://karobben.github.io/tags/Biochmistry/"/>
    
  </entry>
  
  <entry>
    <title>PCA</title>
    <link href="https://karobben.github.io/2025/01/06/AI/ai-pca/"/>
    <id>https://karobben.github.io/2025/01/06/AI/ai-pca/</id>
    <published>2025-01-07T05:00:57.000Z</published>
    <updated>2025-01-08T18:06:58.301Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Example Dataset</strong><br>Suppose we have the following dataset with 3 data points and 2 features ($x_1$, $x_2$):</p><p>$$<br>X =<br>\begin{bmatrix}<br>2.5 &amp; 2.4 \\<br>0.5 &amp; 0.7 \\<br>2.2 &amp; 2.9<br>\end{bmatrix}<br>$$</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">X = np.array([[<span class="hljs-number">2.5</span>, <span class="hljs-number">2.4</span>], [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.7</span>], [<span class="hljs-number">2.2</span>, <span class="hljs-number">2.9</span>]])<br></code></pre></td></tr></table></figure></div><h2 id="Step-1-Center-the-Data">Step 1: Center the Data</h2><p>First, subtract the mean of each feature from the dataset to center it:</p><ol><li><p>Compute the means:</p><ul><li>Mean of $x_1$: $\text{mean}(x_1) = \frac{2.5 + 0.5 + 2.2}{3} = 1.73$</li><li>Mean of $x_2$: $\text{mean}(x_2) = \frac{2.4 + 0.7 + 2.9}{3} = 2.0$</li></ul></li><li><p>Subtract the means:<br>$$<br>X_{ \text{centered} } =<br>\begin{bmatrix}<br>2.5 - 1.73 &amp; 2.4 - 2.0 \\<br>0.5 - 1.73 &amp; 0.7 - 2.0 \\<br>2.2 - 1.73 &amp; 2.9 - 2.0<br>\end{bmatrix} =<br>\begin{bmatrix}<br>0.77 &amp; 0.4 \\<br>-1.23 &amp; -1.3 \\<br>0.47 &amp; 0.9<br>\end{bmatrix}<br>$$</p></li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">X_centered = X - X.mean(axis = <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure></div><h3 id="Step-2-Compute-the-Covariance-Matrix">Step 2: Compute the Covariance Matrix</h3><p>The covariance matrix shows how the features are related. Calculate it as:<br>$$<br>\text{Cov}(X) = \frac{1}{n-1} X_{\text{centered}}^\top X_{\text{centered}}<br>$$</p><ol><li><p>Compute $X_{\text{centered}}^\top X_{\text{centered}}$:<br>$$<br>X_{\text{centered}}^\top X_{\text{centered}} =<br>\begin{bmatrix}<br>0.77 &amp; -1.23 &amp; 0.47 \\<br>0.4 &amp; -1.3 &amp; 0.9<br>\end{bmatrix}<br>\begin{bmatrix}<br>0.77 &amp; 0.4 \\<br>-1.23 &amp; -1.3 \\<br>0.47 &amp; 0.9<br>\end{bmatrix} =<br>\begin{bmatrix}<br>2.32 &amp; 2.33 \\<br>2.33 &amp; 2.66<br>\end{bmatrix}<br>$$</p></li><li><p>Divide by $n-1 = 2$ (since $n=3$):<br>$$<br>\text{Cov}(X) =<br>\begin{bmatrix}<br>1.163 &amp; 1.165 \\<br>1.165 &amp; 1.33<br>\end{bmatrix}<br>$$</p></li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># based on the equation</span><br>n = <span class="hljs-number">3</span><br>CovX = X_centered.T@X_centered /(n-<span class="hljs-number">1</span>)<br><span class="hljs-comment"># using function from numpy</span><br>np.cov(X.T)<br></code></pre></td></tr></table></figure></div><pre>array([[1.16333333, 1.165     ],       [1.165     , 1.33      ]])</pre><h3 id="Step-3-Eigenvalues-and-Eigenvectors">Step 3: Eigenvalues and Eigenvectors</h3><p>Find the eigenvalues ($\lambda$) and eigenvectors ($u$) of the covariance matrix.</p><ol><li>Solve $\text{det}(\text{Cov}(X) - \lambda I) = 0$ for $\lambda$:<br>$$<br>\text{det}<br>\begin{bmatrix}<br>1.163 - \lambda &amp; 1.165 \\<br>1.165 &amp; 1.33 - \lambda<br>\end{bmatrix}<br>= 0<br>$$<br>This results in eigenvalues:<br>$$<br>\lambda_1 = 2.41, \quad \lambda_2 = 0.08<br>$$</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Lambda</span>(<span class="hljs-params">a,b,c</span>):</span><br>    lambda1 = (-b+np.sqrt(b**<span class="hljs-number">2</span> - <span class="hljs-number">4</span>*a*c))/<span class="hljs-number">2</span>/a<br>    lambda2 = (-b-np.sqrt(b**<span class="hljs-number">2</span> - <span class="hljs-number">4</span>*a*c))/<span class="hljs-number">2</span>/a<br>    <span class="hljs-keyword">return</span> lambda1, lambda2<br><br>a = <span class="hljs-number">1</span><br>b = -(CovX[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>] + CovX[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]) <br>c = CovX[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]*CovX[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>] - CovX[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]*CovX[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br><br>lm = Lambda(a,b,c)<br></code></pre></td></tr></table></figure></div><pre>(2.414643312171381, 0.07869002116195278)</pre><ol start="2"><li>Find eigenvectors ($u$):<br>Solve $(\text{Cov}(X) - \lambda I)u = 0$ for each $\lambda$. The eigenvectors are:<br>$$<br>u_1 = \begin{bmatrix} 8.48  \\ -9.11 \end{bmatrix}, \quad<br>$$</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">tmp1 = CovX - lm[<span class="hljs-number">0</span>]* np.eye(CovX.shape[<span class="hljs-number">0</span>])<br>xx = tmp.<span class="hljs-built_in">sum</span>()<br>x2 = xx[<span class="hljs-number">0</span>]/xx[<span class="hljs-number">1</span>]<br>Length = np.sqrt(xx[<span class="hljs-number">0</span>]**<span class="hljs-number">2</span>+xx[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span>)<br>x1 = <span class="hljs-number">1</span>/Length<br>x2 /=Length * -<span class="hljs-number">1</span><br><br>u = np.array([x1, x2])<br>print(u)<br></code></pre></td></tr></table></figure></div><pre>array([ 8.47987336, 9.10811172])</pre><ol start="3"><li>Variance of this component</li></ol><p>$$<br>Variance = \frac{max(\lambda _1, \lambda _2)}{\lambda _1+\lambda _2}<br>$$</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">max</span>(lm)/<span class="hljs-built_in">sum</span>(lm)<br></code></pre></td></tr></table></figure></div><pre>0.9684</pre><h3 id="Step-4-Project-Data-onto-Principal-Components">Step 4: Project Data onto Principal Components</h3><p>Use the top eigenvector ($u_1$) to project the data into 1D (reduce dimensionality):</p><p>$$<br>X_{\text{projected}} = X_{\text{centered}} \cdot u_1<br>$$</p><ol><li>Compute the projection:<br>$$<br>X_{\text{projected}} =<br>\begin{bmatrix}<br>0.77 &amp; 0.4 \\<br>-1.23 &amp; -1.3 \\<br>0.47 &amp; 0.9<br>\end{bmatrix}<br>\begin{bmatrix}<br>8.48 \\<br>-9.11<br>\end{bmatrix} =<br>\begin{bmatrix}<br>10.14 \\<br>-22.30 \\<br>12.15<br>\end{bmatrix}<br>$$</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">X_centered*u<br></code></pre></td></tr></table></figure></div><pre>array([ 10.14448093, -22.29905572,  12.15457479])</pre><h3 id="Final-Results">Final Results:</h3><ol><li><p><strong>Principal Components</strong>:</p><ul><li>The first principal component explains most of the variance ($97%$).</li></ul></li><li><p><strong>Transformed Data</strong>:</p><ul><li>The dataset in 1D space:<br>$$<br>X_{\text{projected}} = \begin{bmatrix}    10.14 \\ -22.30 \\ 12.15 \end{bmatrix}<br>$$</li></ul></li></ol><hr><h2 id="How-to-calculate-Eigenvalues">How to calculate Eigenvalues</h2><h3 id="Step-3-1-Find-Eigenvalues">Step 3.1: Find Eigenvalues</h3><p>Eigenvalues are the roots of the <strong>characteristic equation</strong>:<br>$$<br>\text{det}(\text{Cov}(X) - \lambda I) = 0<br>$$</p><p>$$<br>\text{Cov}(X) =<br>\begin{bmatrix}<br>2.01   &amp; 1.91 \\<br>1.91 &amp; 2.03<br>\end{bmatrix}<br>$$</p><ol><li><p>Subtract $\lambda I$ from $\text{Cov}(X)$:<br>$$<br>\text{Cov}(X) - \lambda I =<br>\begin{bmatrix}<br>2.01 - \lambda &amp; 1.91 \\<br>1.91 &amp; 2.03 - \lambda<br>\end{bmatrix}<br>$$</p></li><li><p>Compute the determinant of this matrix:<br>$$<br>\text{det}(\text{Cov}(X) - \lambda I) = (2.01 - \lambda)(2.03 - \lambda) - (1.91)^2<br>$$</p></li><li><p>Expand the determinant:<br>$$<br>\text{det}(\text{Cov}(X) - \lambda I) = (2.01)(2.03) - (2.01)\lambda - (2.03)\lambda + \lambda^2 - 1.91^2<br>$$<br>$$<br>= \lambda^2 - (2.01 + 2.03)\lambda + (2.01 \cdot 2.03 - 1.91^2)<br>$$</p></li><li><p>Simplify:<br>$$<br>\lambda^2 - 4.04\lambda + (4.0803 - 3.6481) = 0<br>$$<br>$$<br>\lambda^2 - 4.04\lambda + 0.4322 = 0<br>$$</p></li><li><p>Solve this quadratic equation using the quadratic formula:<br>$$<br>\lambda = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}<br>$$<br>Here:</p></li></ol><ul><li>$a = 1$, $b = -4.04$, $c = 0.4322$</li></ul><p>$$<br>\lambda = \frac{-(-4.04) \pm \sqrt{(-4.04)^2 - 4(1)(0.4322)}}{2(1)}<br>$$<br>$$<br>\lambda = \frac{4.04 \pm \sqrt{16.3216 - 1.7288}}{2}<br>$$<br>$$<br>\lambda = \frac{4.04 \pm \sqrt{14.5928}}{2}<br>$$<br>$$<br>\lambda = \frac{4.04 \pm 3.82}{2}<br>$$</p><ol start="6"><li>Compute the two eigenvalues:<br>$$<br>\lambda_1 = \frac{4.04 + 3.82}{2} = 3.96, \quad \lambda_2 = \frac{4.04 - 3.82}{2} = 0.08<br>$$</li></ol><h3 id="Step-3-2-Find-Eigenvectors">Step 3.2: Find Eigenvectors</h3><p>For each eigenvalue $\lambda$, solve $(\text{Cov}(X) - \lambda I)u = 0$.</p><h4 id="For-lambda-1-3-96">For $\lambda_1 = 3.96$:</h4><ol><li><p>Substitute $\lambda_1$ into $\text{Cov}(X) - \lambda I$:<br>$$<br>\text{Cov}(X) - 3.96 I =<br>\begin{bmatrix}<br>2.01 - 3.96 &amp; 1.91 \\<br>1.91 &amp; 2.03 - 3.96<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>-1.95 &amp; 1.91 \\<br>1.91 &amp; -1.93<br>\end{bmatrix}<br>$$</p></li><li><p>Solve the equation:<br>$$<br>\begin{bmatrix}<br>-1.95 &amp; 1.91 \\<br>1.91 &amp; -1.93<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_1 \\<br>x_2<br>\end{bmatrix}<br>= 0<br>$$</p></li></ol><p>This expands to two equations:<br>$$<br>-1.95x_1 + 1.91x_2 = 0<br>$$<br>$$<br>1.91x_1 - 1.93x_2 = 0<br>$$</p><ol start="3"><li><p>Simplify:<br>$$<br>x_2 = \frac{1.95}{1.91}x_1 \quad \text{(from the first equation)}<br>$$</p></li><li><p>Normalize the vector (scale so that the length is 1):<br>$$<br>u_1 = \begin{bmatrix}<br>0.71 \\<br>0.71<br>\end{bmatrix}<br>$$</p></li></ol><h4 id="For-lambda-2-0-08">For $\lambda_2 = 0.08$:</h4><ol><li><p>Substitute $\lambda_2$ into $\text{Cov}(X) - \lambda I$:<br>$$<br>\text{Cov}(X) - 0.08 I =<br>\begin{bmatrix}<br>2.01 - 0.08 &amp; 1.91 \\<br>1.91 &amp; 2.03 - 0.08<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>1.93 &amp; 1.91 \\<br>1.91 &amp; 1.95<br>\end{bmatrix}<br>$$</p></li><li><p>Solve the equation:<br>$$<br>1.93x_1 + 1.91x_2 = 0<br>$$<br>$$<br>1.91x_1 + 1.95x_2 = 0<br>$$</p></li><li><p>Simplify:<br>$$<br>x_2 = -\frac{1.93}{1.91}x_1<br>$$</p></li><li><p>Normalize the vector:<br>$$<br>u_2 = \begin{bmatrix}<br>-0.71 \\<br>0.71<br>\end{bmatrix}<br>$$</p></li></ol><h3 id="Final-Result">Final Result:</h3><ul><li><strong>Eigenvalues</strong>:<br>$$<br>\lambda_1 = 3.96, \quad \lambda_2 = 0.08<br>$$</li><li><strong>Eigenvectors</strong>:<br>$$<br>u_1 = \begin{bmatrix} 0.71 \\ 0.71 \end{bmatrix}, \quad<br>u_2 = \begin{bmatrix} -0.71 \\ 0.71 \end{bmatrix}<br>$$</li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">PCA</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>High Dimension Data</title>
    <link href="https://karobben.github.io/2025/01/01/AI/highdimension/"/>
    <id>https://karobben.github.io/2025/01/01/AI/highdimension/</id>
    <published>2025-01-01T21:23:33.000Z</published>
    <updated>2025-01-07T05:00:08.685Z</updated>
    
    <content type="html"><![CDATA[<h2 id="High-Dimensional-Data">High Dimensional Data</h2><p>$$<br>\begin{bmatrix}<br>\mathbf{x}_ 1 \\<br>\mathbf{x}_ 2 \\<br>\vdots \\<br>\mathbf{x}_ N<br>\end{bmatrix} =<br>\begin{bmatrix}<br>x_1^{(1)} &amp; x_1^{(2)} &amp; \cdots &amp; x_1^{(d)} \\<br>x_2^{(1)} &amp; x_2^{(2)} &amp; \cdots &amp; x_2^{(d)} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>x_N^{(1)} &amp; x_N^{(2)} &amp; \cdots &amp; x_N^{(d)}<br>\end{bmatrix}<br>$$</p><h3 id="Mean-and-Covariance-of-High-Dimensional-Data">Mean and Covariance of High-Dimensional Data</h3><p>When working with high-dimensional data, it is important to understand the <strong>mean</strong> and <strong>covariance matrix</strong>, which are essential statistical measures that summarize the data’s location and spread.</p><hr><h3 id="1-Mean-Vector">1. <strong>Mean Vector</strong></h3><p>For a dataset with $ n $ samples and $ d $ features (dimensions):</p><ul><li>Let $ \mathbf{X} \in \mathbb{R}^{n \times d} $ be the dataset, where each row $ \mathbf{x}_i \in \mathbb{R}^d $ represents a data point and each column corresponds to a feature.</li></ul><h4 id="Definition">Definition:</h4><p>The mean vector $ \mathbf{\mu} \in \mathbb{R}^d $ is defined as:<br>$$<br>\mathbf{\mu} = \frac{1}{n} \sum_{i=1}^n \mathbf{x}_i<br>$$</p><h4 id="Element-Wise">Element-Wise:</h4><p>The $ j $-th element of the mean vector is:<br>$$<br>\mu_j = \frac{1}{n} \sum_{i=1}^n x_{ij}, \quad j = 1, 2, \dots, d<br>$$<br>Where $ x_{ij} $ is the $ j $-th feature of the $ i $-th sample.</p><h3 id="2-Covariance-Matrix">2. <strong>Covariance Matrix</strong></h3><p>The covariance matrix $ \mathbf{\Sigma} \in \mathbb{R}^{d \times d} $ captures the pairwise relationships between features.</p><h4 id="Definition-v2">Definition:</h4><p>$$<br>\mathbf{\Sigma} = \frac{1}{n-1} \sum_{i=1}^n (\mathbf{x}_i - \mathbf{\mu})(\mathbf{x}_i - \mathbf{\mu})^T<br>$$</p><h4 id="Element-Wise-v2">Element-Wise:</h4><p>The $ (j, k) $-th entry of the covariance matrix is:<br>$$<br>\Sigma_{jk} = \frac{1}{n-1} \sum_{i=1}^n (x_{ij} - \mu_j)(x_{ik} - \mu_k)<br>$$<br>Where:</p><ul><li>$ \Sigma_{jk} $: Covariance between feature $ j $ and feature $ k $.</li><li>$ x_{ij} $: Value of the $ j $-th feature for the $ i $-th sample.</li></ul><h4 id="Properties">Properties:</h4><ul><li>$ \Sigma_{jj} $: Variance of feature $ j $.</li><li>$ \Sigma_{jk} $: Correlation between features $ j $ and $ k $ if scaled by their standard deviations.</li><li>The matrix $ \mathbf{\Sigma} $ is symmetric: $ \Sigma_{jk} = \Sigma_{kj} $.</li></ul><h3 id="3-Matrix-Representation">3. <strong>Matrix Representation</strong></h3><p>Using matrix notation, the mean vector $ \mathbf{\mu} $ and covariance matrix $ \mathbf{\Sigma} $ can be computed efficiently:</p><h4 id="Mean-Vector">Mean Vector:</h4><p>$$<br>\mathbf{\mu} = \frac{1}{n} \mathbf{X}^T \mathbf{1}<br>$$<br>Where:</p><ul><li>$ \mathbf{X}^T $: Transpose of the data matrix.</li><li>$ \mathbf{1} $: A column vector of ones with size $ n $.</li></ul><h4 id="Covariance-Matrix">Covariance Matrix:</h4><p>$$<br>\mathbf{\Sigma} = \frac{1}{n-1} (\mathbf{X} - \mathbf{1} \mathbf{\mu}^T )^T (\mathbf{X} - \mathbf{1} \mathbf{\mu}^T)<br>$$</p><h3 id="Summary-of-Notation">Summary of Notation</h3><table><thead><tr><th><strong>Symbol</strong></th><th><strong>Meaning</strong></th></tr></thead><tbody><tr><td>$ \mathbf{\mu} $</td><td>Mean vector of the dataset ($ d $-dimensional).</td></tr><tr><td>$ \mathbf{\Sigma} $</td><td>Covariance matrix ($ d \times d $).</td></tr><tr><td>$ \mu_j $</td><td>Mean of the $ j $-th feature.</td></tr><tr><td>$ \Sigma_{jk} $</td><td>Covariance between feature $ j $ and $ k $.</td></tr></tbody></table><h3 id="Example-of-the-Covariance">Example of the Covariance</h3><p>The <strong>Iris dataset</strong> has been successfully loaded. Here’s a brief look at the dataset:</p><table><thead><tr><th><strong>Sepal Length (cm)</strong></th><th><strong>Sepal Width (cm)</strong></th><th><strong>Petal Length (cm)</strong></th><th><strong>Petal Width (cm)</strong></th></tr></thead><tbody><tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td></tr><tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td></tr><tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td></tr><tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td></tr><tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td></tr></tbody></table><p>Let’s compute the <strong>mean</strong> and <strong>covariance matrix</strong> for this dataset and visualize their insights.</p><h3 id="Results-from-the-Iris-Dataset">Results from the Iris Dataset:</h3><h4 id="Mean-Vector-v2"><strong>Mean Vector</strong>:</h4><p>The mean of each feature (measured in centimeters) is:</p><ul><li><strong>Sepal Length</strong>: $ 5.843 $</li><li><strong>Sepal Width</strong>: $ 3.057 $</li><li><strong>Petal Length</strong>: $ 3.758 $</li><li><strong>Petal Width</strong>: $ 1.199 $</li></ul><h4 id="Covariance-Matrix-v2"><strong>Covariance Matrix</strong>:</h4><p>The covariance matrix is:<br>$$<br>\mathbf{\Sigma} =<br>\begin{bmatrix}<br>0.6857 &amp; -0.0424 &amp; 1.2743 &amp; 0.5163 \\<br>-0.0424 &amp; 0.1900 &amp; -0.3297 &amp; -0.1216 \\<br>1.2743 &amp; -0.3297 &amp; 3.1163 &amp; 1.2956 \\<br>0.5163 &amp; -0.1216 &amp; 1.2956 &amp; 0.5810<br>\end{bmatrix}<br>$$</p><h4 id="Interpretation">Interpretation:</h4><ol><li><p><strong>Diagonal Entries</strong>:</p><ul><li>These are the variances of the features:<ul><li>Variance of <strong>Sepal Length</strong>: $ 0.6857 $</li><li>Variance of <strong>Sepal Width</strong>: $ 0.1900 $</li><li>Variance of <strong>Petal Length</strong>: $ 3.1163 $</li><li>Variance of <strong>Petal Width</strong>: $ 0.5810 $</li></ul></li></ul></li><li><p><strong>Off-Diagonal Entries</strong>:</p><ul><li>These represent covariances between pairs of features:<ul><li><strong>Positive covariance</strong> (e.g., $ 1.2743 $ between Sepal Length and Petal Length) suggests a positive relationship.</li><li><strong>Negative covariance</strong> (e.g., $ -0.3297 $ between Sepal Width and Petal Length) suggests a negative relationship.</li></ul></li></ul></li></ol><p>The <strong>Covariance Matrix with Species Encoding</strong> is as follows:</p><table><thead><tr><th>Feature</th><th>Sepal Length (cm)</th><th>Sepal Width (cm)</th><th>Petal Length (cm)</th><th>Petal Width (cm)</th><th>Species Encoded</th></tr></thead><tbody><tr><td><strong>Sepal Length (cm)</strong></td><td>0.6857</td><td>-0.0424</td><td>1.2743</td><td>0.5163</td><td>0.5309</td></tr><tr><td><strong>Sepal Width (cm)</strong></td><td>-0.0424</td><td>0.1900</td><td>-0.3297</td><td>-0.1216</td><td>-0.1523</td></tr><tr><td><strong>Petal Length (cm)</strong></td><td>1.2743</td><td>-0.3297</td><td>3.1163</td><td>1.2956</td><td>1.3725</td></tr><tr><td><strong>Petal Width (cm)</strong></td><td>0.5163</td><td>-0.1216</td><td>1.2956</td><td>0.5810</td><td>0.5973</td></tr><tr><td><strong>Species Encoded</strong></td><td>0.5309</td><td>-0.1523</td><td>1.3725</td><td>0.5973</td><td>0.6711</td></tr></tbody></table><h3 id="Key-Observations">Key Observations:</h3><ol><li><p><strong>Species Encoded Relationships</strong>:</p><ul><li>Positive covariance with features like petal length ($1.3725$) and petal width ($0.5973$).</li><li>Indicates that these features strongly vary with the species.</li></ul></li><li><p><strong>Feature Variability</strong>:</p><ul><li>Variance (diagonal values) is high for petal length ($3.1163$), meaning it varies most across the dataset.</li><li>Sepal width ($0.1900$) has the least variance.</li></ul></li></ol><p><img src="https://imgur.com/gTRpILR.png" alt=""></p><div id="chart_bar" style="width: 100%; height: 400px;"></div><div class="admonition note"><p class="admonition-title">What can we get from this results?</p><p>As you can see, <strong>Petal Length</strong> has the largest variance (var = 3.1163). Meanwhile, it also has the highest covariance with species (1.37). This indicates that much of its variance is explained by the species, making Petal Length a potentially good feature for species classification.</p></div><h2 id="Transformations">Transformations</h2><p>High-dimensional data transformation refers to the process of modifying or converting data that exists in a high-dimensional space (i.e., data with a large number of features or variables) into a more manageable or meaningful representation. This transformation can involve reducing dimensions, re-organizing data, or mapping it to a different space while preserving important information or relationships.</p><ol><li><p><strong>Source Dataset (${x}$)</strong> and <strong>Target Dataset (${m}$)</strong>:</p><ul><li>The target dataset ${m_i}$ is generated by applying a rotation and translation to the source dataset:<br>$$<br>m_i = A x_i + b<br>$$</li><li>Here, $A$ is the rotation matrix, and $b$ is the translation vector.</li></ul></li><li><p><strong>Mean Transformation</strong>:</p><ul><li>The mean of the transformed dataset (${m}$) can be expressed as:<br>$$<br>\text{mean}({m}) = A \cdot \text{mean}({x}) + b<br>$$</li></ul></li><li><p><strong>Covariance Transformation</strong>:</p><ul><li>The covariance matrix of the transformed dataset (${m}$) is derived as:<br>$$<br>\text{Covmat}({m}) = A \cdot \text{Covmat}({x}) \cdot A^\top<br>$$</li><li>This shows how the covariance matrix of the source dataset transforms under a linear transformation.</li><li>The covariance matrix of ${x}$ is defined as:<br>$$<br>\text{Covmat}({x}) = \frac{1}{N} \sum_i (x_i - \text{mean}({x}))(x_i - \text{mean}({x}))^\top<br>$$</li></ul></li></ol><h2 id="Eigenvector-and-Eigenvalue">Eigenvector and Eigenvalue</h2><p>Imagine you’re analyzing data (like in machine learning or physics). Eigenvalues and eigenvectors can:</p><ul><li>Find patterns: In large data (like PCA), eigenvectors show the “directions” of most variation, and eigenvalues tell how important each direction is.</li><li>Simplify problems: Diagonalization makes hard matrix computations easier.</li></ul><h3 id="1-Eigenvector-u-and-Eigenvalue-lambda"><strong>1. Eigenvector ($u$) and Eigenvalue ($\lambda$)</strong></h3><ul><li>An <strong>eigenvector</strong> $u$ of a matrix $S$ is a vector that does not change direction when $S$ is applied to it. Instead, it is scaled by a factor $\lambda$, the <strong>eigenvalue</strong>:<br>$$<br>S u = \lambda u<br>$$<ul><li>$S$: A square matrix.</li><li>$u$: An eigenvector (non-zero vector).</li><li>$\lambda$: The corresponding eigenvalue.</li></ul></li></ul><h3 id="2-Symmetric-Matrices-S"><strong>2. Symmetric Matrices ($S$)</strong></h3><ul><li>If $S$ is symmetric ($S = S^\top$), it has special properties:<ul><li>All eigenvalues are <strong>real</strong>.</li><li>Eigenvectors corresponding to distinct eigenvalues are <strong>orthogonal</strong>:<br>$$<br>u_i \perp u_j \quad \text{if} \quad i \neq j<br>$$</li><li>Eigenvectors can also be <strong>normalized</strong> to form an orthonormal set ($|u| = 1$).</li></ul></li></ul><h3 id="3-Orthonormal-Matrix-U"><strong>3. Orthonormal Matrix ($U$)</strong></h3><ul><li>By stacking all the eigenvectors of $S$ as columns into a matrix $U$:<br>$$<br>U = [u_1, u_2, \dots, u_d]<br>$$<ul><li>$U$ is an <strong>orthonormal matrix</strong>, meaning:<br>$$<br>U^\top U = I \quad \text{(identity matrix)}<br>$$</li></ul></li></ul><h3 id="4-Eigenvalues-as-a-Diagonal-Matrix-Lambda"><strong>4. Eigenvalues as a Diagonal Matrix ($\Lambda$)</strong></h3><ul><li>Arrange eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_d$ into a diagonal matrix:<br>$$<br>\Lambda =<br>\begin{bmatrix}<br>\lambda_1 &amp; 0 &amp; \dots &amp; 0 \\<br>0 &amp; \lambda_2 &amp; \dots &amp; 0 \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>0 &amp; 0 &amp; \dots &amp; \lambda_d<br>\end{bmatrix}<br>$$</li></ul><h3 id="5-Diagonalization"><strong>5. Diagonalization</strong></h3><ul><li>If $S$ is symmetric, it can be <strong>diagonalized</strong> using its eigenvectors and eigenvalues:<br>$$<br>S = U \Lambda U^\top<br>$$<ul><li>$U$: Matrix of eigenvectors.</li><li>$\Lambda$: Diagonal matrix of eigenvalues.</li></ul></li></ul><h3 id="6-Key-Properties-of-Diagonalization"><strong>6. Key Properties of Diagonalization</strong></h3><ul><li>Simplifies computations, e.g., powers of $S$:<br>$$<br>S^k = U \Lambda^k U^\top<br>$$<ul><li>$\Lambda^k$ is simply the diagonal matrix with each eigenvalue raised to the power $k$.</li></ul></li><li>Used in many fields such as:<ul><li>Principal Component Analysis (PCA).</li><li>Solving differential equations.</li><li>Modal analysis in engineering.</li></ul></li></ul><h2 id="Principal-Component-Analysis-PCA">Principal Component Analysis (PCA)</h2><p>Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of data while retaining as much variance as possible. It identifies new axes, called principal components, which are uncorrelated and align with the directions of maximum variance. PCA transforms the data to these new axes, ranks the components by their variance (eigenvalues), and allows dimensionality reduction by selecting the top components.</p><h3 id="PCA-in-3-Steps-More-Accurate-Breakdown">PCA in 3 Steps (More Accurate Breakdown):</h3><ol><li><p><strong>Transformation (Centering the Data)</strong>:</p><ul><li>Before applying PCA, you need to <strong>center</strong> the data by subtracting the mean of each feature. This step ensures that the principal components (axes of maximum variance) pass through the origin.</li><li>Mathematically:<br>$$<br>x_{\text{centered}} = x - \text{mean}(x)<br>$$</li></ul></li><li><p><strong>Rotation (Find Eigenvalues and Eigenvectors)</strong>:</p><ul><li>The goal of PCA is to find the directions (principal components) where the data has the most variance.</li><li>This involves computing the <strong>eigenvectors</strong> and <strong>eigenvalues</strong> of the covariance matrix:<br>$$<br>S = \frac{1}{n} X^\top X<br>$$<ul><li>The eigenvectors represent the new axes (principal components).</li><li>The eigenvalues indicate how much variance is captured by each axis.</li></ul></li><li><strong>Rotation</strong> refers to aligning the data along the directions of these principal components.</li></ul></li><li><p><strong>Dimensional Reduction (Keep Principal Components)</strong>:</p><ul><li>After identifying the principal components, you can choose the top $k$ components with the highest eigenvalues (the directions of the most variance) and ignore the rest.</li><li>This step reduces the dimensionality while retaining as much information as possible.</li></ul></li></ol><h3 id="1-Original-Dataset">1. <strong>Original Dataset</strong>:</h3><ul><li>The dataset ${x}$:<ul><li>It has $d$ features (dimensions).</li><li>Each data point is a vector in a $d$-dimensional space.</li></ul></li></ul><h3 id="2-Step-1-Covariance-Matrix">2. <strong>Step 1: Covariance Matrix</strong>:</h3><ul><li>The covariance matrix captures how features are correlated. It is computed as:<br>$$<br>\text{Covmat}({x}) = \frac{1}{N} \sum_{i=1}^N \left( x_i - \text{mean}({x}) \right) \left( x_i - \text{mean}({x}) \right)^\top<br>$$</li><li>PCA works by <strong>diagonalizing</strong> this covariance matrix.</li></ul><h3 id="3-Step-2-Eigen-Decomposition">3. <strong>Step 2: Eigen Decomposition</strong>:</h3><ul><li>Decompose the covariance matrix into <strong>eigenvalues ($\lambda$)</strong> and <strong>eigenvectors ($u$)</strong>:<br>$$<br>U^\top \text{Covmat}({x}) U = \Lambda<br>$$<ul><li>$U$: Matrix of eigenvectors (principal components).</li><li>$\Lambda$: Diagonal matrix of eigenvalues (variance explained by each principal component).</li></ul></li></ul><h3 id="4-Step-3-Choose-s-Principal-Components">4. <strong>Step 3: Choose $s$ Principal Components</strong>:</h3><ul><li>Eigenvalues represent the <strong>variance</strong> explained by each principal component. They are sorted in descending order.</li><li>To reduce dimensions:<ul><li>Choose the top $s$ eigenvalues that explain the most variance.</li><li>Often, the ratio is calculated:<br>$$<br>\frac{\sum_{j=s+1}^d \lambda_j}{\sum_{j=1}^d \lambda_j}<br>$$<ul><li>This ratio helps decide $s$ by ensuring the <strong>remaining variance (error)</strong> is small.</li></ul></li><li>Plotting $\lambda_i$ vs. $i$ (as shown in the slide) can help visualize where most variance is captured (the “elbow” point).</li></ul></li></ul><h3 id="5-Step-4-Project-Data-to-Lower-Dimensions">5. <strong>Step 4: Project Data to Lower Dimensions</strong>:</h3><ul><li>Once you have selected $s$ principal components, project the original data onto this lower-dimensional space:<br>$$<br>\hat{x}_ i = \sum_ {j=1}^s \left[ u_ j^\top (x_ i - \text{mean}({x})) \right] u_j + \text{mean}({x})<br>$$<ul><li>Here:<ul><li>$u_j$: The eigenvectors corresponding to the top $s$ eigenvalues.</li><li>$\hat{x}_i$: The low-dimensional representation of $x_i$.</li></ul></li></ul></li></ul><h3 id="6-Visualization-from-the-Slide">6. <strong>Visualization from the Slide</strong>:</h3><ul><li>The graph shows the eigenvalues ($\lambda_i$) vs. their indices ($i$):<ul><li>The blue curve represents the sorted eigenvalues.</li><li>The orange circle highlights the “elbow” point, which suggests the optimal number of principal components to retain.</li></ul></li></ul><h3 id="Summary-of-PCA-Calculation">Summary of PCA Calculation:</h3><ol><li>Center the data (subtract the mean).</li><li>Compute the covariance matrix.</li><li>Find eigenvalues and eigenvectors of the covariance matrix.</li><li>Choose the top $s$ eigenvalues to decide the number of principal components.</li><li>Project the data onto the top $s$ eigenvectors to get a reduced representation.</li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style><script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script><script type="text/javascript">  google.charts.load('current', { packages: ['corechart'] });  google.charts.setOnLoadCallback(drawChart);  function drawChart() {    // Data for the bar chart with error bars    var data = google.visualization.arrayToDataTable([      ['Feature', 'Setosa Mean', { role: 'interval' }, { role: 'interval' },                 'Versicolor Mean', { role: 'interval' }, { role: 'interval' },                 'Virginica Mean', { role: 'interval' }, { role: 'interval' }],      ['Sepal Length',       5.006, 4.8, 5.2,  // Setosa       5.936, 5.7, 6.2,  // Versicolor       6.588, 6.4, 6.8], // Virginica      ['Sepal Width',       3.428, 3.2, 3.6,  // Setosa       2.770, 2.6, 2.9,  // Versicolor       2.974, 2.8, 3.1], // Virginica      ['Petal Length',       1.462, 1.3, 1.6,  // Setosa       4.260, 4.0, 4.5,  // Versicolor       5.552, 5.3, 5.8], // Virginica      ['Petal Width',       0.246, 0.2, 0.3,  // Setosa       1.326, 1.2, 1.5,  // Versicolor       2.026, 1.9, 2.2]  // Virginica    ]);    // Chart options    var options = {      title: 'Mean Values with Error Bars by Species',      hAxis: { title: 'Feature' },      vAxis: { title: 'Mean Value', minValue: 0 },      legend: { position: 'top' },      bar: { groupWidth: '75%' },    };    // Draw the chart    var chart = new google.visualization.ColumnChart(document.getElementById('chart_bar'));    chart.draw(data, options);  }</script>]]></content>
    
    
    <summary type="html">High Dimension Data</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>AI: Logistic Regression</title>
    <link href="https://karobben.github.io/2024/12/30/AI/ai-logistic-reg/"/>
    <id>https://karobben.github.io/2024/12/30/AI/ai-logistic-reg/</id>
    <published>2024-12-31T02:03:29.000Z</published>
    <updated>2024-12-31T04:01:44.345Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Logistic-Regression">Logistic Regression</h2><p>Logistic regression is a <strong>supervised machine learning algorithm</strong> used for <strong>binary classification</strong> tasks. Unlike linear regression, which predicts continuous values, logistic regression predicts the <strong>probability</strong> that a given input belongs to a certain class.</p><h3 id="Key-Concepts-in-Logistic-Regression">Key Concepts in Logistic Regression</h3><ol><li><p><strong>Logistic Function (Sigmoid Function)</strong>:</p><ul><li>Logistic regression uses the <strong>sigmoid function</strong> to map predicted values to probabilities:<br>$$<br>\sigma(z) = \frac{1}{1 + e^{-z}}<br>$$<ul><li>$ z = X \beta $: Linear combination of features.</li><li>The output of $ \sigma(z) $ is always between 0 and 1, representing the probability.</li></ul></li></ul></li><li><p><strong>Logit Link Function</strong>:</p><ul><li>The logit function is the natural logarithm of the odds (log-odds) of the binary outcome:<br>$$<br>g(\theta) = \log\left(\frac{P(y=1|X)}{P(y=0|X)}\right)<br>$$</li><li>It transforms probabilities into log-odds:<br>$$<br>g(\theta) = X^T\beta<br>$$</li></ul></li><li><p><strong>Inverse Link Function</strong>:</p><ul><li>To map the log-odds ($ X^T\beta $) back to probabilities, we use the <strong>inverse of the logit function</strong>:<br>$$<br>P(y=1|X, \beta) = \frac{e<sup>{X</sup>T\beta}}{1 + e<sup>{X</sup>T\beta}}<br>$$<ul><li>This is the <strong>sigmoid function</strong>, which outputs probabilities between 0 and 1.</li></ul></li></ul></li><li><p><strong>Decision Boundary</strong>:</p><ul><li>For binary classification:<ul><li>If $ \sigma(z) \geq 0.5 $, classify the input as Class 1.</li><li>If $ \sigma(z) &lt; 0.5 $, classify the input as Class 0.</li></ul></li></ul></li><li><p><strong>Log-Likelihood</strong>:</p><ul><li>Logistic regression optimizes the <strong>log-likelihood</strong> instead of minimizing residuals (like in linear regression):<br>$$<br>\ell(\beta) = \sum_{i=1}^n \left[ y_i \ln(\hat{y}_i) + (1 - y_i) \ln(1 - \hat{y}_i) \right]<br>$$<br>Where:<ul><li>$ \hat{y}_i = \sigma(z_i) $: Predicted probability.</li><li>$ y_i $: Actual class (0 or 1).</li></ul></li></ul></li><li><p><strong>Negative Log-Likelihood</strong>:</p><ul><li>The optimization process in machine learning (and statistics) often involves <strong>minimizing</strong> a cost function. Since the log-likelihood is a measure of fit (higher is better), we take its <strong>negative</strong> to convert the maximization problem into a <strong>minimization problem</strong>:</li><li>$$ -\ln L(\beta) = -\sum_{i=1}^n \left[ y_i X_i^T \beta - \ln(1 + e^ {X_i^ T \beta}) \right] $$</li></ul></li><li><p><strong>Optimization</strong>:</p><ul><li>The goal is to find the coefficients $ \beta $ that maximize the log-likelihood using algorithms like <strong>Gradient Descent</strong> or <strong>Newton’s Method</strong>.</li></ul></li></ol><div class="admonition note"><p class="admonition-title">What is a Link Function?</p><p>A <strong>link function</strong> connects the <strong>linear predictor</strong> ($ X\beta $) to the <strong>mean of the response variable</strong> in a generalized linear model (GLM). It provides a transformation that ensures the predicted values from the model stay within the valid range for the response variable.</p></div><div class="admonition note"><p class="admonition-title">Why negative Log-likelihood function?</p><ul><li>The negative log-likelihood is used to simplify optimization by turning a maximization problem into a minimization one.</li><li>The formula on the slide and in my explanation are equivalent, just written in slightly different forms.</li></ul></div><h3 id="Applications-of-Logistic-Regression">Applications of Logistic Regression</h3><ul><li>Binary classification problems such as:<ul><li>Email spam detection (Spam/Not Spam).</li><li>Disease diagnosis (Positive/Negative).</li><li>Customer churn prediction (Churn/No Churn).</li></ul></li></ul><h3 id="Practical-Example-Binary-Classification-with-Logistic-Regression">Practical Example: Binary Classification with Logistic Regression</h3><p>Below is a Python example using scikit-learn:</p><h4 id="Problem-Predict-whether-a-person-has-heart-disease-based-on-two-features">Problem: Predict whether a person has heart disease based on two features.</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_classification<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score, confusion_matrix, classification_report<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># Generate synthetic binary classification dataset</span><br>X, y = make_classification(n_samples=<span class="hljs-number">200</span>, n_features=<span class="hljs-number">2</span>, n_informative=<span class="hljs-number">2</span>, n_redundant=<span class="hljs-number">0</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># Split the dataset into training and testing sets</span><br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># Fit logistic regression model</span><br>model = LogisticRegression()<br>model.fit(X_train, y_train)<br><br><span class="hljs-comment"># Make predictions</span><br>y_pred = model.predict(X_test)<br><br><span class="hljs-comment"># Evaluate the model</span><br>accuracy = accuracy_score(y_test, y_pred)<br>print(<span class="hljs-string">&quot;Accuracy:&quot;</span>, accuracy)<br>print(<span class="hljs-string">&quot;Confusion Matrix:\n&quot;</span>, confusion_matrix(y_test, y_pred))<br>print(<span class="hljs-string">&quot;Classification Report:\n&quot;</span>, classification_report(y_test, y_pred))<br><br><span class="hljs-comment"># Plot decision boundary</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>x_min, x_max = X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br>y_min, y_max = X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br>xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="hljs-number">0.1</span>),<br>                     np.arange(y_min, y_max, <span class="hljs-number">0.1</span>))<br>Z = model.predict(np.c_[xx.ravel(), yy.ravel()])<br>Z = Z.reshape(xx.shape)<br><br>plt.contourf(xx, yy, Z, alpha=<span class="hljs-number">0.8</span>)<br>plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y, edgecolor=<span class="hljs-string">&#x27;k&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br>plt.title(<span class="hljs-string">&quot;Logistic Regression Decision Boundary&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Feature 1&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Feature 2&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/DvGwCmw.png" alt=""></p><h2 id="Logistic-Regression-for-Multiclass-Classification">Logistic Regression for Multiclass Classification</h2><p>Logistic regression can be extended to handle <strong>multiclass classification problems</strong> where the target variable has more than two classes. The two common approaches are <strong>One-vs-Rest (OvR)</strong> and <strong>Softmax (Multinomial)</strong> logistic regression.</p><h3 id="One-vs-Rest-OvR">One-vs-Rest (OvR)</h3><h4 id="Overview">Overview:</h4><ul><li>In OvR, a separate binary classifier is trained for each class.</li><li>For a class $ k $, the classifier treats:<ul><li>$ y = k $ as <strong>positive (1)</strong>.</li><li>$ y \neq k $ as <strong>negative (0)</strong>.</li></ul></li><li>Each classifier predicts the probability of the input belonging to its class.</li></ul><h4 id="Prediction">Prediction:</h4><ul><li>For a new data point, the class with the <strong>highest probability</strong> is chosen:<br>$$<br>\hat{y} = \arg\max_{k} P(y = k | x)<br>$$</li></ul><h3 id="Softmax-Multinomial-Logistic-Regression">Softmax (Multinomial) Logistic Regression</h3><p>Softmax logistic regression generalizes binary logistic regression to multiple classes. Instead of fitting separate binary classifiers, it predicts the probability for all classes simultaneously using the <strong>softmax function</strong>.</p><h4 id="Softmax-Function">Softmax Function:</h4><p>$$<br>P(y = k | x) = \frac{e^{X \beta_k}}{\sum_{j=1}^K e^{X \beta_j}}<br>$$<br>Where:</p><ul><li>$ K $: Total number of classes.</li><li>$ \beta_k $: Coefficients for class $ k $.</li><li>$ P(y = k | x) $: Probability of class $ k $ given the input $ x $.</li></ul><h4 id="Prediction-v2">Prediction:</h4><ul><li>For a new data point, the class with the highest softmax probability is chosen:<br>$$<br>\hat{y} = \arg\max_{k} P(y = k | x)<br>$$</li></ul><h3 id="Summary-of-Methods">Summary of Methods:</h3><table><thead><tr><th><strong>Method</strong></th><th><strong>When to Use</strong></th><th><strong>Advantages</strong></th><th><strong>Disadvantages</strong></th></tr></thead><tbody><tr><td><strong>One-vs-Rest</strong></td><td>Small datasets with a limited number of classes.</td><td>Easy to implement, interpretable.</td><td>Can struggle with overlapping classes.</td></tr><tr><td><strong>Softmax</strong></td><td>When normalized probabilities across classes are needed.</td><td>Probabilities are calibrated.</td><td>Computationally expensive.</td></tr></tbody></table><p><strong>Softmax approach</strong> (also called multinomial logistic regression)</p><ol><li><p><strong>C-Class Classification</strong>:</p><ul><li>The goal is to classify the target variable $ y $ into one of $ C $ classes:<br>$$<br>y \in {0, 1, \dots, C-1}<br>$$</li></ul></li><li><p><strong>Discrete Probability Distribution</strong>:</p><ul><li>The probabilities $ \theta_0, \theta_1, \dots, \theta_{C-1} $ represent the likelihood of a data point belonging to each class.</li><li>These probabilities satisfy:<br>$$<br>\theta_i \in [0, 1] \quad \text{and} \quad \sum_{i=0}^{C-1} \theta_i = 1<br>$$</li></ul></li><li><p><strong>Link Function</strong>:</p><ul><li>The relationship between the linear model ($ X\beta $) and the class probabilities is established using the <strong>Softmax function</strong>:<br>$$<br>g(\theta) = \log \left( \frac{\theta_i}{1 - \sum_{u=0}^{C-1} \theta_u} \right) = X^T \beta<br>$$</li></ul></li><li><p><strong>Class Probabilities</strong>:</p><ul><li>For each class $ i $, the probability is computed as:<br>$$<br>P(y = i | X, \beta) = \frac{e^ {X^ T \beta_i}}{1 + \sum_{j=0}^ {C-1} e^ {X^ T \beta_j}}<br>$$</li><li>For the last class $ C-1 $, the probability is:<br>$$<br>P(y = C-1 | X, \beta) = \frac{1}{1 + \sum_{i=0}^{C-2} e^ {X^T \beta_i}}<br>$$</li></ul></li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">AI: Logistic Regression</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Linear Model Optimization</title>
    <link href="https://karobben.github.io/2024/12/30/AI/linearoptimal/"/>
    <id>https://karobben.github.io/2024/12/30/AI/linearoptimal/</id>
    <published>2024-12-30T20:59:30.000Z</published>
    <updated>2024-12-31T03:33:25.115Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Measure-Information">Measure Information</h2><p>Both Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are measures used to evaluate the quality of statistical models, particularly in the context of selecting the best model size or complexity.</p><ol><li><p><strong>Why Use AIC and BIC?</strong></p><ul><li>When building statistical or machine learning models, we often face the challenge of balancing <strong>model fit</strong> (how well the model explains the data) with <strong>model simplicity</strong> (avoiding overfitting).</li><li>AIC and BIC are metrics that help in selecting the best model by incorporating penalties for the number of parameters used in the model.</li></ul></li><li><p><strong>Akaike Information Criterion (AIC):</strong></p><ul><li>AIC estimates the relative quality of a model for a given dataset.</li><li>Formula:<br>$$<br>\text{AIC} = 2k - 2 \ln(L)<br>$$<ul><li>$ k $: Number of parameters in the model.</li><li>$ L $: Likelihood of the model (how well it fits the data).</li></ul></li><li><strong>Objective</strong>: Choose the model with the <strong>lowest AIC value</strong>, which balances fit and complexity.</li></ul></li><li><p><strong>Bayesian Information Criterion (BIC):</strong></p><ul><li>Similar to AIC, BIC adds a stronger penalty for model complexity to account for overfitting.</li><li>Formula:<br>$$<br>\text{BIC} = k \ln(n) - 2 \ln(L)<br>$$<ul><li>$ n $: Number of observations in the dataset.</li><li>$ k $: Number of parameters in the model.</li></ul></li><li><strong>Objective</strong>: Choose the model with the <strong>lowest BIC value</strong> for a balance between fit and simplicity, especially when sample size $ n $ is large.</li></ul></li><li><p><strong>Key Difference Between AIC and BIC:</strong></p><ul><li>AIC focuses on model quality and is less strict about model size.</li><li>BIC penalizes complexity more heavily, making it more conservative in selecting simpler models.</li></ul></li></ol><p><strong>Applications</strong>:</p><ul><li>Model selection in regression, time-series analysis, and machine learning.</li><li>Comparing models with different numbers of features or parameters.</li><li>Evaluating trade-offs between underfitting and overfitting.</li></ul><p>Would you like a detailed example or visual demonstration of how AIC and BIC are used?</p><table><thead><tr><th><strong>Criterion</strong></th><th><strong>Formula</strong></th><th><strong>Focus</strong></th><th><strong>Penalty for Complexity</strong></th><th><strong>Use Case</strong></th><th><strong>Objective</strong></th></tr></thead><tbody><tr><td><strong>Akaike Information Criterion (AIC)</strong></td><td>$ 2k - 2\ln(L) $</td><td>Model fit vs. simplicity</td><td>Proportional to $ k $</td><td>Choosing models that balance goodness-of-fit and simplicity</td><td>Minimize AIC</td></tr><tr><td><strong>Bayesian Information Criterion (BIC)</strong></td><td>$ k\ln(n) - 2\ln(L) $</td><td>Model fit vs. parsimony</td><td>Stronger penalty with $ \ln(n) $</td><td>Suitable for large datasets and emphasizing simpler models</td><td>Minimize BIC</td></tr><tr><td><strong>Penalty Strength</strong></td><td>Moderate</td><td>High</td><td><strong>Depends on Sample Size ($ n $)</strong></td><td>Larger datasets lead to stricter penalties in BIC</td><td></td></tr><tr><td><strong>Common Application</strong></td><td>Time-series, regression, machine learning</td><td>Model selection across varying complexity</td><td>Multi-model comparison</td><td>Best when balancing underfitting and overfitting</td><td></td></tr></tbody></table><ol><li><p><strong>AIC</strong>:</p><ul><li>Prefers models with a better balance between complexity and fit.</li><li>Less conservative than BIC, suitable for small datasets or exploratory analysis.</li></ul></li><li><p><strong>BIC</strong>:</p><ul><li>Stronger emphasis on simplicity.</li><li>More appropriate for larger datasets or when avoiding overfitting is crucial.</li></ul></li><li><p><strong>Choosing Between AIC and BIC</strong>:</p><ul><li>Use <strong>AIC</strong> if you prioritize model quality over strict simplicity.</li><li>Use <strong>BIC</strong> if simplicity and generalization are more important.</li></ul></li></ol><h3 id="Likelihood">Likelihood</h3><p>When calculating AIC or BIC, the likelihood refers to <strong>how well the model trained on the training data</strong> explains the same training data. The likelihood is not calculated on the test data, as AIC and BIC are measures of model quality on the training dataset itself.</p><h3 id="Likelihood-in-AIC-BIC-Context">Likelihood in AIC/BIC Context:</h3><ol><li><strong>Training Data</strong>:<ul><li>We use the model parameters (e.g., coefficients in regression) estimated from the training data to calculate the likelihood of the training data.</li></ul></li><li><strong>Likelihood Calculation</strong>:<ul><li>For a model trained on the training data, the likelihood is the probability (or density) of the observed training data under the model:<br>$$<br>L(\theta | \text{Training Data}) = \prod_{i=1}^n f(y_i | \theta)<br>$$<br>Where:<ul><li>$ y_i $: Observed target value.</li><li>$ \theta $: Model parameters estimated during training.</li><li>$ f(y_i | \theta) $: Probability density of $ y_i $ under the model.</li></ul></li></ul></li><li><strong>Log-Likelihood for AIC/BIC</strong>:<ul><li>Instead of working with $ L $, we calculate the <strong>log-likelihood</strong> to simplify computations:<br>$$<br>\ln L(\theta | \text{Training Data}) = \sum_{i=1}^n \ln f(y_i | \theta)<br>$$</li></ul></li></ol><h3 id="Steps-to-Calculate-Likelihood-for-AIC-BIC">Steps to Calculate Likelihood for AIC/BIC:</h3><ol><li>Train the Model:<ul><li>Use the training data to estimate the model parameters ($ \theta $).</li></ul></li><li>Calculate Predictions ($ \hat{y}_i $):<ul><li>Predict the mean or central tendency of the model for each training data point.</li></ul></li><li>Calculate Residuals and Likelihood:<ul><li>Assume a distribution for the residuals (e.g., normal distribution).</li><li>For a normal distribution:<br>$$<br>f(y_i | \hat{y}_i, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \hat{y}_i)^ 2}{2\sigma^ 2}\right)<br>$$</li><li>The log-likelihood becomes:<br>$$<br>\ln L = \sum_{i=1}^n \left[ -\frac{1}{2} \ln(2\pi\sigma^2) - \frac{(y_i - \hat{y}_i)^ 2}{2\sigma^ 2} \right]<br>$$</li></ul></li></ol><p>$\sigma$ represents the <strong>standard deviation of the residuals</strong></p><h3 id="Example-Using-Training-Data-to-Calculate-Likelihood">Example: Using Training Data to Calculate Likelihood</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Simulated training data</span><br>X_train = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>y_train = np.array([<span class="hljs-number">1.2</span>, <span class="hljs-number">2.3</span>, <span class="hljs-number">2.8</span>, <span class="hljs-number">4.1</span>, <span class="hljs-number">5.3</span>])<br><br><span class="hljs-comment"># Train a linear regression model</span><br>model = LinearRegression()<br>model.fit(X_train, y_train)<br>y_pred_train = model.predict(X_train)<br><br><span class="hljs-comment"># Calculate residuals and variance</span><br>residuals = y_train - y_pred_train<br>sigma_squared = np.var(residuals, ddof=<span class="hljs-number">1</span>)  <span class="hljs-comment"># Variance of residuals</span><br><br><span class="hljs-comment"># Calculate log-likelihood</span><br>n = <span class="hljs-built_in">len</span>(y_train)<br>log_likelihood = -<span class="hljs-number">0.5</span> * n * np.log(<span class="hljs-number">2</span> * np.pi * sigma_squared) - np.<span class="hljs-built_in">sum</span>((residuals**<span class="hljs-number">2</span>) / (<span class="hljs-number">2</span> * sigma_squared))<br><br><span class="hljs-comment"># AIC and BIC</span><br>k = <span class="hljs-number">2</span>  <span class="hljs-comment"># Number of parameters (intercept + slope)</span><br>aic = <span class="hljs-number">2</span> * k - <span class="hljs-number">2</span> * log_likelihood<br>bic = k * np.log(n) - <span class="hljs-number">2</span> * log_likelihood<br><br>&#123;<span class="hljs-string">&quot;Log-Likelihood&quot;</span>: log_likelihood, <span class="hljs-string">&quot;AIC&quot;</span>: aic, <span class="hljs-string">&quot;BIC&quot;</span>: bic&#125;<br></code></pre></td></tr></table></figure></div><h3 id="Example">Example</h3><p>Source: <a href="https://www.youtube.com/watch?v=HOqHI53x9Go">Model selection with AIC and AICc</a><br><img src="https://imgur.com/K4j8QGS.png" alt=""></p><h2 id="Forward-stagewise-regression-and-Backward-stagewise-regression">Forward stagewise regression and Backward stagewise regression</h2><p><strong>Backward stagewise regression</strong> and <strong>Forward stagewise regression</strong> are methods for variable selection and model fitting, primarily used in regression contexts. They are stepwise procedures for adding or removing predictors in a systematic way to improve model performance or interpretability.</p><h3 id="Backward-Stagewise-Regression"><strong>Backward Stagewise Regression</strong></h3><h4 id="Overview">Overview:</h4><ul><li>Starts with a <strong>full model</strong> (all predictors included).</li><li>Gradually <strong>removes predictors</strong> one by one, based on a criterion (e.g., p-value, AIC, or adjusted $ R^2 $).</li><li>The goal is to find a smaller, simpler model without significantly compromising the fit.</li></ul><h4 id="Procedure">Procedure:</h4><ol><li>Begin with a model containing all predictors.</li><li>Evaluate the significance of each predictor (e.g., using p-values).</li><li>Remove the <strong>least significant predictor</strong> (highest p-value) that exceeds a predefined</li></ol><p>threshold (e.g., $p &gt; 0.05$).</p><ol start="4"><li>Refit the model and repeat the process until all remaining predictors are statistically significant or meet the stopping criteria.</li></ol><h4 id="Advantages">Advantages:</h4><ul><li>Simple and interpretable.</li><li>Useful for removing irrelevant predictors in high-dimensional datasets.</li></ul><h4 id="Disadvantages">Disadvantages:</h4><ul><li>Can miss optimal combinations of predictors.</li><li>Sensitive to multicollinearity among predictors.</li></ul><h3 id="2-Forward-Stagewise-Regression">2. <strong>Forward Stagewise Regression</strong></h3><h4 id="Overview-v2">Overview:</h4><ul><li>Starts with an <strong>empty model</strong> (no predictors included).</li><li>Gradually <strong>adds predictors</strong> one at a time, based on a criterion (e.g., reducing residual sum of squares or improving AIC/BIC).</li><li>The goal is to build a model step-by-step, adding only significant predictors.</li></ul><h4 id="Procedure-v2">Procedure:</h4><ol><li>Begin with an empty model.</li><li>Evaluate all predictors not yet in the model, adding the one that most improves the model fit (e.g., the one with the smallest p-value or largest improvement in $ R^2 $).</li><li>Refit the model and repeat the process until no additional predictors meet the inclusion criteria.</li></ol><h4 id="Advantages-v2">Advantages:</h4><ul><li>Can handle datasets with a large number of predictors.</li><li>Less likely to overfit compared to starting with a full model.</li></ul><h4 id="Disadvantages-v2">Disadvantages:</h4><ul><li>Ignores potential joint effects of predictors (e.g., interactions).</li><li>May miss the best subset of predictors.</li></ul><h3 id="Key-Differences-Between-Backward-and-Forward-Stagewise-Regression">Key Differences Between Backward and Forward Stagewise Regression</h3><table><thead><tr><th>Feature</th><th>Backward Stagewise</th><th>Forward Stagewise</th></tr></thead><tbody><tr><td><strong>Starting Point</strong></td><td>Full model (all predictors).</td><td>Empty model (no predictors).</td></tr><tr><td><strong>Procedure</strong></td><td>Removes predictors iteratively.</td><td>Adds predictors iteratively.</td></tr><tr><td><strong>Use Case</strong></td><td>Small datasets with fewer predictors.</td><td>Large datasets with many predictors.</td></tr><tr><td><strong>Limitations</strong></td><td>May retain redundant predictors.</td><td>May miss joint effects of predictors.</td></tr></tbody></table><h3 id="When-to-Use-Each-Method">When to Use Each Method?</h3><ul><li><p><strong>Backward Stagewise</strong>:</p><ul><li>When you suspect many predictors are irrelevant.</li><li>When computational resources are not a concern (since fitting starts with a large model).</li></ul></li><li><p><strong>Forward Stagewise</strong>:</p><ul><li>When you have a large number of predictors and computational efficiency is critical.</li><li>When you want a simpler starting point and add complexity gradually.</li></ul></li></ul><h3 id="A-Quick-Example">A Quick Example</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Re-import necessary libraries after environment reset</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_regression<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br><br><span class="hljs-comment"># Generate a dataset with 100 samples and 10 features</span><br>np.random.seed(<span class="hljs-number">42</span>)<br>X, y = make_regression(n_samples=<span class="hljs-number">100</span>, n_features=<span class="hljs-number">10</span>, noise=<span class="hljs-number">10</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># Initialize model and variables for Forward Stagewise Regression</span><br>selected_features = []<br>remaining_features = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]))<br>forward_scores = []<br><br><span class="hljs-comment"># Forward Stagewise Regression</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(remaining_features)):<br>    scores = []<br>    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> remaining_features:<br>        <span class="hljs-comment"># Fit a model with the current feature added</span><br>        features_to_test = selected_features + [feature]<br>        model = LinearRegression().fit(X[:, features_to_test], y)<br>        score = model.score(X[:, features_to_test], y)  <span class="hljs-comment"># R^2 score</span><br>        scores.append((score, feature))<br>    <br>    <span class="hljs-comment"># Select the feature with the highest R^2 score</span><br>    scores.sort(reverse=<span class="hljs-literal">True</span>)<br>    best_score, best_feature = scores[<span class="hljs-number">0</span>]<br>    forward_scores.append(best_score)<br>    selected_features.append(best_feature)<br>    remaining_features.remove(best_feature)<br><br><span class="hljs-comment"># Results of Forward Stagewise Regression</span><br>selected_features_forward = selected_features  <span class="hljs-comment"># Save selected features for clarity</span><br><br><span class="hljs-comment"># Backward Stagewise Regression</span><br>selected_features_backward = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]))<br>backward_scores = []<br><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(selected_features_backward) - <span class="hljs-number">1</span>):<br>    scores = []<br>    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> selected_features_backward:<br>        <span class="hljs-comment"># Fit a model with the current feature removed</span><br>        features_to_test = [f <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> selected_features_backward <span class="hljs-keyword">if</span> f != feature]<br>        model = LinearRegression().fit(X[:, features_to_test], y)<br>        score = model.score(X[:, features_to_test], y)  <span class="hljs-comment"># R^2 score</span><br>        scores.append((score, feature))<br>    <br>    <span class="hljs-comment"># Remove the feature with the smallest impact on R^2 score</span><br>    scores.sort(reverse=<span class="hljs-literal">True</span>)<br>    best_score, worst_feature = scores[-<span class="hljs-number">1</span>]<br>    backward_scores.append(best_score)<br>    selected_features_backward.remove(worst_feature)<br><br><span class="hljs-comment"># Plot R^2 scores for Forward and Backward Stagewise Regression</span><br>plt.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))<br><br><span class="hljs-comment"># Forward Stagewise Regression</span><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(forward_scores) + <span class="hljs-number">1</span>), forward_scores, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">&#x27;Forward Stagewise&#x27;</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>)<br><br><span class="hljs-comment"># Backward Stagewise Regression</span><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(backward_scores), <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>), backward_scores, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">&#x27;Backward Stagewise&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br><br><span class="hljs-comment"># Formatting the plot</span><br>plt.title(<span class="hljs-string">&quot;R² Scores During Forward and Backward Stagewise Regression&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Number of Features&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;R² Score&quot;</span>)<br>plt.xticks(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(forward_scores) + <span class="hljs-number">1</span>))<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.tight_layout()<br>plt.show()<br></code></pre></td></tr></table></figure></div><script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script><script type="text/javascript">  google.charts.load('current', { packages: ['corechart'] });  google.charts.setOnLoadCallback(drawChart);  function drawChart() {    var data = google.visualization.arrayToDataTable([          ['Number of Features', 'Forward Stagewise R²', 'Backward Stagewise R²'],          [1, 0.287, null],          [2, 0.511, null],          [3, 0.698, null],          [4, 0.801, null],          [5, 0.918, null],          [6, 0.988, null],          [7, 0.995, null],          [8, 0.997, null],          [9, 0.997, null],          [9, 0.997, 0.787],          [8, null, 0.643],          [7, null, 0.441],          [6, null, 0.285],          [5, null, 0.176],          [4, null, 0.076],          [3, null, 0.045],          [2, null, 0.011],          [1, null, 0.002],        ]);    var options = {      title: 'R² Scores During Forward and Backward Stagewise Regression',      hAxis: { title: 'Number of Features' },      vAxis: { title: 'R² Score', minValue: 0, maxValue: 1 },      legend: { position: 'top' },      colors: ['blue', 'red'],    };    var chart = new google.visualization.LineChart(document.getElementById('chart_div'));    chart.draw(data, options);  }</script><div id="chart_div" style="width: 100%  ; height: 300px"></div><h3 id="Limitations">Limitations</h3><p><strong>Forward and Backward Stagewise Regression</strong> can become computationally expensive and impractical when dealing with a <strong>large number of features (e.g., 1000+ features)</strong> because:</p><ol><li><strong>High Computational Cost</strong>:<ul><li>Both methods involve iteratively adding or removing features, which requires fitting a model at each step. For large datasets, this becomes infeasible.</li></ul></li><li><strong>Potential Overfitting</strong>:<ul><li>With a large number of features, stepwise methods might select features that fit noise in the data rather than actual patterns.</li></ul></li><li><strong>Ignoring Interactions</strong>:<ul><li>These methods do not account for interactions between features, which can lead to suboptimal feature selection.</li></ul></li></ol><p><strong>Alternative Methods for Large Feature Spaces</strong></p><table><thead><tr><th><strong>Method</strong></th><th><strong>Description</strong></th><th><strong>Advantages</strong></th><th><strong>Disadvantages</strong></th><th><strong>Best Use Case</strong></th></tr></thead><tbody><tr><td><strong>Lasso Regression (L1)</strong></td><td>Shrinks coefficients and sets some to exactly zero for feature selection.</td><td>- Efficient for high-dimensional data.<br>- Automatically selects features.<br>- Prevents overfitting.</td><td>- May ignore correlated features.<br>- Requires hyperparameter tuning ($ \lambda $).</td><td>When many features are irrelevant, and sparse solutions are desired.</td></tr><tr><td><strong>Elastic Net</strong></td><td>Combines L1 (Lasso) and L2 (Ridge) regularization.</td><td>- Balances feature selection and handling multicollinearity.<br>- Suitable for correlated features.</td><td>- More complex than Lasso.<br>- Requires tuning of both $ \lambda $ and $ \alpha $.</td><td>When predictors are highly correlated, and feature selection is needed.</td></tr><tr><td><strong>Recursive Feature Elimination (RFE)</strong></td><td>Iteratively removes the least important features based on a chosen model.</td><td>- Works with any estimator (e.g., linear, tree-based).<br>- Provides a rank of feature importance.</td><td>- Computationally expensive.<br>- Sensitive to model choice and training data.</td><td>When model-specific feature ranking is required.</td></tr><tr><td><strong>Principal Component Analysis (PCA)</strong></td><td>Reduces dimensionality by transforming features into uncorrelated components that capture most variance.</td><td>- Handles high-dimensional data well.<br>- Removes multicollinearity.<br>- No need for target variable.</td><td>- Components are linear combinations of features, losing interpretability.<br>- Not ideal for feature selection.</td><td>When reducing dimensionality is more important than interpretability.</td></tr><tr><td><strong>Tree-Based Feature Importance</strong></td><td>Uses models like Random Forest or Gradient Boosting to rank feature importance.</td><td>- Naturally handles non-linearity.<br>- Accounts for feature interactions.<br>- Fast for large datasets.</td><td>- Can be biased toward high-cardinality features.<br>- Does not directly reduce feature count.</td><td>When using tree-based models or ranking feature importance is a priority.</td></tr><tr><td><strong>Mutual Information</strong></td><td>Measures the statistical dependency between features and the target variable.</td><td>- Non-parametric.<br>- Detects non-linear relationships.</td><td>- Computationally expensive for many features.<br>- Does not handle feature interactions.</td><td>When quantifying feature relevance to the target variable without assumptions is needed.</td></tr><tr><td><strong>Feature Clustering</strong></td><td>Groups similar features into clusters and uses cluster representatives for modeling.</td><td>- Reduces redundancy in correlated features.<br>- Scales well with high-dimensional data.</td><td>- May lose specific feature contributions.<br>- Requires a meaningful distance metric.</td><td>When dealing with highly correlated features or datasets with groups of similar features.</td></tr><tr><td><strong>Embedding-Based Methods</strong></td><td>Uses deep learning or models like word2vec to transform features into a lower-dimensional space.</td><td>- Captures complex relationships between features.<br>- Flexible for large feature spaces.</td><td>- Requires advanced techniques and computational resources.<br>- May lose interpretability.</td><td>When handling very high-dimensional data (e.g., text, genomic data) with complex dependencies.</td></tr></tbody></table><h4 id="Recommendations">Recommendations:</h4><ul><li><strong>Lasso Regression</strong>: If feature selection is the goal and the data has many irrelevant features.</li><li><strong>Elastic Net</strong>: If features are highly correlated and Lasso alone may struggle.</li><li><strong>PCA</strong>: When interpretability is less important, and you want to reduce dimensionality.</li><li><strong>Tree-Based Importance</strong>: For datasets where feature importance ranking is needed, especially with tree-based models.</li><li><strong>Feature Clustering</strong>: For correlated features where redundancy needs to be reduced.</li></ul><h2 id="M-Estimators">M-Estimators</h2><p><strong>M-Estimators</strong> (Maximum Likelihood-type Estimators) are a general class of estimators in statistics used for robust parameter estimation. They extend the principle of Maximum Likelihood Estimation (MLE) to allow for more flexibility and robustness, especially in the presence of outliers or non-normal errors.</p><h3 id="What-Are-M-Estimators">What Are M-Estimators?</h3><ol><li><p><strong>Definition</strong>:</p><ul><li>M-Estimators generalize Maximum Likelihood Estimators by minimizing a <strong>loss function</strong> (also called the objective function) over the parameters of interest.</li></ul></li><li><p><strong>Loss Function</strong>:</p><ul><li>The core idea is to minimize a function of residuals:<br>$$<br>\hat{\theta} = \arg\min_{\theta} \sum_{i=1}^n \rho\left(\frac{r_i}{\sigma}\right)<br>$$<br>Where:<ul><li>$ r_i = y_i - f(x_i, \theta) $: Residual (difference between observed and predicted values).</li><li>$ \rho(\cdot) $: A loss function that determines the contribution of residuals.</li><li>$ \sigma $: Scale parameter (controls the spread).</li></ul></li></ul></li><li><p><strong>Goal</strong>:</p><ul><li>Instead of focusing purely on minimizing squared residuals (like in Ordinary Least Squares), M-Estimators allow for more flexible functions to make the estimator <strong>less sensitive to outliers</strong>.</li></ul></li></ol><h3 id="Examples-of-M-Estimators">Examples of M-Estimators</h3><table><thead><tr><th><strong>Type</strong></th><th><strong>Loss Function ($ \rho $)</strong></th><th><strong>Characteristics</strong></th></tr></thead><tbody><tr><td><strong>Ordinary Least Squares (OLS)</strong></td><td>$ \rho( r ) = r^2 $</td><td>Highly sensitive to outliers. Minimizes sum of squared errors.</td></tr><tr><td><strong>Huber Loss</strong></td><td>$\rho( r) = \begin{cases} r^2 &amp; \text{if } |r| \leq c \\ 2c|r| - c^2 &amp; \text{if } |r| &gt; c\end{cases}$</td><td>Combines squared loss (for small residuals) and absolute loss (for large residuals).</td></tr><tr><td><strong>Tukey’s Biweight</strong></td><td>$\rho( r ) = \begin{cases}  c^2\left(1 - \left[1 - \left(\frac{r}{c}\right)^ 2\right]^ 3\right) &amp; \text{if } |r| \leq c \\ c^2 &amp; \text{if } |r| &gt; c \end{cases}$</td><td>Completely ignores residuals larger than a threshold $ c $.</td></tr><tr><td><strong>Huberized Absolute Loss</strong></td><td>$\rho( r) = |r|$</td><td>Linear penalty, robust but less efficient.</td></tr></tbody></table><h3 id="Advantages-of-M-Estimators">Advantages of M-Estimators</h3><ol><li><strong>Robustness to Outliers</strong></li><li><strong>Flexibility</strong></li><li><strong>Generalization of MLE</strong>:<ul><li>MLE is a special case of M-Estimators, making them widely applicable in parametric settings.</li></ul></li></ol><h3 id="When-to-Use-M-Estimators">When to Use M-Estimators?</h3><ol><li><strong>Presence of Outliers</strong>:</li><li><strong>Non-Normal Errors</strong>:</li><li><strong>Heavy-Tailed Distributions</strong>:</li></ol><h3 id="Practical-Example-Using-Huber-Loss">Practical Example: Using Huber Loss</h3><p>Below is an example of applying <strong>Huber Loss</strong> to regression in Python:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> HuberRegressor<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># Simulate data with outliers</span><br>np.random.seed(<span class="hljs-number">42</span>)<br>X = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>y = <span class="hljs-number">3</span> * X.flatten() + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size=X.shape[<span class="hljs-number">0</span>])<br>y[::<span class="hljs-number">10</span>] += <span class="hljs-number">20</span>  <span class="hljs-comment"># Add outliers every 10th point</span><br><br><span class="hljs-comment"># Fit Ordinary Least Squares (OLS) Regression</span><br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br>ols = LinearRegression().fit(X, y)<br><br><span class="hljs-comment"># Fit Huber Regression</span><br>huber = HuberRegressor(epsilon=<span class="hljs-number">1.35</span>).fit(X, y)<br><br><span class="hljs-comment"># Plot the results</span><br>plt.scatter(X, y, color=<span class="hljs-string">&quot;blue&quot;</span>, label=<span class="hljs-string">&quot;Data with Outliers&quot;</span>)<br>plt.plot(X, ols.predict(X), color=<span class="hljs-string">&quot;red&quot;</span>, label=<span class="hljs-string">&quot;OLS Regression Line&quot;</span>)<br>plt.plot(X, huber.predict(X), color=<span class="hljs-string">&quot;green&quot;</span>, label=<span class="hljs-string">&quot;Huber Regression Line&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;Comparison of OLS and Huber Regression&quot;</span>)<br>plt.legend()<br>plt.xlabel(<span class="hljs-string">&quot;X&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;y&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div><script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script><script type="text/javascript">  google.charts.load('current', { packages: ['corechart'] });  google.charts.setOnLoadCallback(drawChart);  function drawChart() {    // Data for OLS and Huber Regression    var data = google.visualization.arrayToDataTable([['X', 'Observed Data', 'OLS Regression', 'Huber Regression'], [0.0, 20.496714153011233, 2.3618406112691552, -0.05895399071098967], [0.10101010101010101, 0.16476600185911838, 2.6554630935183825, 0.24599617373579663], [0.20202020202020202, 1.2537491441612985, 2.9490855757676098, 0.550946338182583], [0.30303030303030304, 2.4321207654989347, 3.242708058016837, 0.8558965026293694], [0.40404040404040403, 0.9779678373978762, 3.5363305402660643, 1.1608466670761555], [0.5050505050505051, 1.2810145582023347, 3.8299530225152916, 1.465796831522942], [0.6060606060606061, 3.39739463368921, 4.123575504764519, 1.7707469959697284], [0.7070707070707071, 2.88864685036503, 4.417197987013745, 2.0756971604165146], [0.8080808080808081, 1.9547680383074721, 4.710820469262973, 2.380647324863301], [0.9090909090909091, 3.2698327708586916, 5.0044429515122, 2.6855974893100876], [1.0101010101010102, 22.566885337490568, 5.298065433761428, 2.990547653756874], [1.1111111111111112, 2.8676035797630766, 5.591687916010654, 3.29549781820366], [1.2121212121212122, 3.878325907929671, 5.8853103982598824, 3.6004479826504467], [1.3131313131313131, 2.0261136947361416, 6.178932880509109, 3.905398147097233], [1.4141414141414141, 2.5175064099112094, 6.472555362758336, 4.210348311544019], [1.5151515151515151, 3.983167016213572, 6.766177845007563, 4.5152984759908055], [1.6161616161616161, 3.835653728150425, 7.059800327256791, 4.820248640437591], [1.7171717171717171, 5.465762484110425, 7.353422809506018, 5.125198804884378], [1.8181818181818181, 4.546521379024243, 7.647045291755245, 5.430148969331165], [1.9191919191919191, 4.345272056240466, 7.940667774004472, 5.73509913377795], [2.0202020202020203, 27.526254829527616, 8.2342902562537, 6.040049298224737], [2.121212121212121, 6.1378600631498275, 8.527912738502927, 6.344999462671523], [2.2222222222222223, 6.734194871354591, 8.821535220752153, 6.64994962711831], [2.323232323232323, 5.5449487834835125, 9.11515770300138, 6.954899791565095], [2.4242424242424243, 6.728344548202091, 9.40878018525061, 7.259849956011883], [2.525252525252525, 7.6866801654674415, 9.702402667499836, 7.564800120458668], [2.6262626262626263, 6.727794301365576, 9.996025149749062, 7.8697502849054555], [2.727272727272727, 8.557516200163853, 10.289647631998289, 8.17470044935224], [2.8282828282828283, 7.88420979492968, 10.583270114247517, 8.479650613799027], [2.929292929292929, 8.49618503808551, 10.876892596496743, 8.784600778245814], [3.0303030303030303, 28.489202478679694, 11.170515078745971, 9.0895509426926], [3.131313131313131, 11.24621757844833, 11.464137560995198, 9.394501107139385], [3.2323232323232323, 9.683472472231763, 11.757760043244426, 9.699451271586172], [3.3333333333333335, 8.9422890710441, 12.051382525493654, 10.00440143603296], [3.4343434343434343, 11.125575215133491, 12.34500500774288, 10.309351600479745], [3.5353535353535355, 9.385216956089582, 12.638627489992109, 10.614301764926532], [3.6363636363636362, 11.117954504095664, 12.932249972241335, 10.919251929373319], [3.7373737373737375, 9.252451088241438, 13.225872454490563, 11.224202093820105], [3.8383838383838382, 10.186965466253085, 13.51949493673979, 11.52915225826689], [3.9393939393939394, 12.015043054050942, 13.813117418989018, 11.834102422713677], [4.040404040404041, 32.85967870120753, 14.106739901238244, 12.139052587160464], [4.141414141414141, 12.595610705432392, 14.40036238348747, 12.444002751607249], [4.242424242424242, 12.611624444884486, 14.693984865736699, 12.748952916054035], [4.343434343434343, 12.729199334713742, 14.987607347985925, 13.053903080500822], [4.444444444444445, 11.854811342965906, 15.281229830235153, 13.358853244947609], [4.545454545454545, 12.916519427968927, 15.57485231248438, 13.663803409394394], [4.646464646464646, 13.47875516843415, 15.868474794733606, 13.96875357384118], [4.747474747474747, 15.299546468643157, 16.162097276982834, 14.273703738287967], [4.848484848484849, 14.889072835023008, 16.45571975923206, 14.578653902734755], [4.94949494949495, 13.085444693122115, 16.74934224148129, 14.883604067181542], [5.05050505050505, 35.47559912090995, 17.042964723730513, 15.188554231628325], [5.151515151515151, 15.069463174129137, 17.336587205979743, 15.493504396075112], [5.252525252525253, 15.080653757269799, 17.630209688228973, 15.7984545605219], [5.353535353535354, 16.67228234944693, 17.9238321704782, 16.103404724968687], [5.454545454545454, 17.394635886132313, 18.217454652727426, 16.40835488941547], [5.555555555555555, 17.597946785782863, 18.511077134976652, 16.713305053862257], [5.656565656565657, 16.13047944647433, 18.80469961722588, 17.018255218309044], [5.757575757575758, 16.96351489687606, 19.09832209947511, 17.32320538275583], [5.858585858585858, 17.907021007161138, 19.39194458172433, 17.628155547202617], [5.959595959595959, 18.854333005910238, 19.68556706397356, 17.933105711649404], [6.0606060606060606, 37.70264394397289, 19.979189546222788, 18.23805587609619], [6.161616161616162, 18.299189508184668, 20.272812028472018, 18.543006040542977], [6.262626262626262, 17.681543813872757, 20.56643451072124, 18.84795620498976], [6.363636363636363, 17.89470246682842, 20.86005699297047, 19.152906369436547], [6.4646464646464645, 20.20646521633359, 21.153679475219697, 19.457856533883334], [6.565656565656566, 21.05320972554052, 21.447301957468927, 19.762806698330124], [6.666666666666667, 19.927989878419666, 21.740924439718153, 20.06775686277691], [6.767676767676767, 21.306563200922326, 22.03454692196738, 20.372707027223694], [6.8686868686868685, 20.96769663110824, 22.328169404216606, 20.67765719167048], [6.96969696969697, 20.263971154485787, 22.621791886465832, 20.982607356117267], [7.070707070707071, 41.573516817629624, 22.915414368715062, 21.287557520564054], [7.171717171717171, 23.053188081617485, 23.209036850964285, 21.592507685010837], [7.2727272727272725, 21.782355779071864, 23.502659333213515, 21.897457849457627], [7.373737373737374, 23.685855777026127, 23.79628181546274, 22.202408013904414], [7.474747474747475, 19.80449732015268, 24.08990429771197, 22.5073581783512], [7.575757575757575, 23.54917523164795, 24.383526779961194, 22.812308342797984], [7.6767676767676765, 23.117350098541202, 24.677149262210424, 23.11725850724477], [7.777777777777778, 23.034325982867465, 24.97077174445965, 23.422208671691557], [7.878787878787879, 23.728124412899138, 25.26439422670888, 23.727158836138344], [7.979797979797979, 21.951825024793045, 25.558016708958103, 24.03210900058513], [8.080808080808081, 44.02275235458673, 25.851639191207333, 24.337059165031917], [8.181818181818182, 24.902567116966292, 26.14526167345656, 24.642009329478704], [8.282828282828282, 26.32637889322636, 26.438884155705786, 24.946959493925487], [8.383838383838384, 24.633244933241507, 26.732506637955016, 25.251909658372277], [8.484848484848484, 24.646051851652267, 27.026129120204242, 25.55685982281906], [8.585858585858587, 25.25581871399122, 27.319751602453472, 25.86180998726585], [8.686868686868687, 26.976008178308135, 27.613374084702695, 26.166760151712634], [8.787878787878787, 26.692387473296044, 27.90699656695192, 26.47171031615942], [8.88888888888889, 26.136906462899628, 28.20061904920115, 26.776660480606207], [8.98989898989899, 27.482964402810325, 28.494241531450378, 27.081610645052994], [9.09090909090909, 47.36980482207531, 28.787864013699604, 27.386560809499777], [9.191919191919192, 28.54440256629047, 29.081486495948834, 27.691510973946567], [9.292929292929292, 27.176734784910522, 29.375108978198057, 27.99646113839335], [9.393939393939394, 27.854156035220416, 29.66873146044729, 28.30141130284014], [9.494949494949495, 28.09274033171633, 29.962353942696513, 28.606361467286924], [9.595959595959595, 27.324363839746667, 30.25597642494574, 28.91131163173371], [9.696969696969697, 29.38702936797367, 30.54959890719497, 29.2162617961805], [9.797979797979798, 29.65499466611928, 30.843221389444196, 29.521211960627284], [9.8989898989899, 29.70208315361216, 31.136843871693426, 29.826162125074074], [10.0, 29.765412866624853, 31.430466353942652, 30.131112289520857]]);    // Chart options    var options = {      title: 'OLS vs Huber Regression',      hAxis: { title: 'X', minValue: 0 },      vAxis: { title: 'y', minValue: 0 },      pointSize: 2,      legend: { position: 'right' },      series: {        0: { color: 'black', pointShape: 'circle' }, // Observed data points        1: { color: 'red', lineWidth: 2, pointSize: 0 },          // OLS Regression line        2: { color: 'green', lineWidth: 2, pointSize: 0 },        // Huber Regression line      },    };    // Render the chart    var chart = new google.visualization.ScatterChart(document.getElementById('chart_div2'));    chart.draw(data, options);  }</script><div id="chart_div2" style="width: 100%; height: 400px;"></div><hr><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Linear Model Optimization</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Regularization</title>
    <link href="https://karobben.github.io/2024/12/30/AI/regularization/"/>
    <id>https://karobben.github.io/2024/12/30/AI/regularization/</id>
    <published>2024-12-30T16:50:04.000Z</published>
    <updated>2024-12-30T18:18:52.803Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Quick-View">Quick View</h2><p><strong>Video Tutorial</strong>:</p><ul><li><a href="https://www.youtube.com/watch?v=Q81RR3yKn30">StatQuest with Josh Starmer: Regularization Part 1: Ridge (L2) Regression</a></li><li><a href="https://www.youtube.com/watch?v=NGf0voTMlcs">StatQuest with Josh Starmer: Regularization Part 2: Lasso (L1) Regression</a></li><li><a href="https://www.youtube.com/watch?v=1dKRdX9bfIo">StatQuest with Josh Starmer: Regularization Part 3: Elastic Net Regression</a></li></ul><h3 id="What-is-Regularization">What is Regularization?</h3><p><strong>Regularization</strong> is a technique used in machine learning and regression to prevent <strong>overfitting</strong> by adding a penalty to the loss function. The penalty discourages overly complex models and large coefficients, helping the model generalize better to unseen data.</p><h3 id="Why-Do-We-Need-Regularization">Why Do We Need Regularization?</h3><ol><li><p><strong>Overfitting</strong>:</p><ul><li>When a model becomes too complex, it memorizes the training data, leading to poor performance on test data.</li><li>Example: In polynomial regression, high-degree polynomials might perfectly fit the training data but fail to generalize.</li></ul></li><li><p><strong>Ill-Conditioned Data</strong>:</p><ul><li>When predictors are highly correlated or there are many predictors relative to observations, the regression model can become unstable.</li></ul></li><li><p><strong>Bias-Variance Tradeoff</strong>:</p><ul><li>Regularization introduces some bias but reduces variance, improving the model’s robustness.</li></ul></li></ol><h3 id="Types-of-Regularization-Why-Ridge-Lasso-and-Elastic-Net">Types of Regularization: Why Ridge, Lasso, and Elastic Net?</h3><p>These are three popular regularization methods used for linear regression:</p><h4 id="1-Ridge-Regression-L2-Regularization">1. <strong>Ridge Regression (L2 Regularization)</strong>:</h4><ul><li><p><strong>Penalty</strong>: Adds the squared magnitude of coefficients to the loss function.<br>$$<br>\text{Loss Function: } \sum_{i=1}^n (y_i - \hat{y}_ i)^2 + \lambda \sum_ {j=1}^p \beta_ j^2<br>$$</p><ul><li>$ \lambda $: Regularization parameter (controls penalty strength).</li><li>$ \beta_j $: Coefficients of predictors.</li></ul></li><li><p><strong>Effect</strong>:</p><ul><li>Shrinks coefficients towards zero, but never makes them exactly zero.</li><li>Reduces the impact of less important predictors without removing them entirely.</li></ul></li><li><p><strong>Use Case</strong>:</p><ul><li>Works well when many predictors are correlated.</li></ul></li></ul><h4 id="2-Lasso-Regression-L1-Regularization">2. <strong>Lasso Regression (L1 Regularization)</strong>:</h4><ul><li><p><strong>Penalty</strong>: Adds the absolute value of coefficients to the loss function.<br>$$<br>\text{Loss Function: } \sum_ {i=1}^n (y_ i - \hat{y}_ i)^2 + \lambda \sum_ {j=1}^p |\beta_ j|<br>$$</p></li><li><p><strong>Effect</strong>:</p><ul><li>Can shrink some coefficients to exactly zero, effectively performing <strong>feature selection</strong>.</li><li>Helps in creating sparse models by keeping only the most relevant predictors.</li></ul></li><li><p><strong>Use Case</strong>:</p><ul><li>Useful when you expect only a subset of predictors to be important.</li></ul></li></ul><h4 id="3-Elastic-Net-Regression">3. <strong>Elastic Net Regression</strong>:</h4><ul><li><p><strong>Penalty</strong>: Combines both L1 (lasso) and L2 (ridge) penalties.<br>$$<br>\text{Loss Function: } \sum_{i=1}^n (y_ i - \hat{y}_ i)^2 + \lambda_ 1 \sum_{j=1}^p |\beta_ j| + \lambda_ 2 \sum_{j=1}^p \beta_ j^2<br>$$</p></li><li><p><strong>Effect</strong>:</p><ul><li>Balances the strengths of Ridge and Lasso regression.</li><li>Retains the ability to perform feature selection (like Lasso) while handling multicollinearity (like Ridge).</li></ul></li><li><p><strong>Use Case</strong>:</p><ul><li>Best when there are many predictors and some are correlated, but feature selection is also desired.</li></ul></li></ul><h3 id="Comparison-of-Regularization-Methods">Comparison of Regularization Methods:</h3><table><thead><tr><th><strong>Method</strong></th><th><strong>Penalty</strong></th><th><strong>Effect on Coefficients</strong></th><th><strong>Use Case</strong></th></tr></thead><tbody><tr><td><strong>Ridge</strong></td><td>$ \beta_j^2 $</td><td>Shrinks coefficients, no zeros.</td><td>Multicollinearity or many predictors.</td></tr><tr><td><strong>Lasso</strong></td><td>$|\beta_j|$</td><td>Shrinks coefficients to zero.</td><td>Feature selection with fewer predictors.</td></tr><tr><td><strong>Elastic Net</strong></td><td>$|\beta_j| + \beta_j^2 $</td><td>Combination of Ridge and Lasso.</td><td>Multicollinearity with feature selection.</td></tr></tbody></table><h3 id="Why-Are-They-Discussed-Together">Why Are They Discussed Together?</h3><ul><li>All three are <strong>extensions of linear regression</strong>.</li><li>They <strong>regularize the model</strong> to prevent overfitting, but they differ in the type of penalty they impose on the coefficients.</li></ul><h2 id="Ridge-Regression">Ridge Regression</h2><h3 id="Ridge-Regression-Loss-Function">Ridge Regression Loss Function</h3><p>The Ridge regression modifies the Ordinary Least Squares (OLS) cost function by adding a penalty (regularization term) to the sum of squared coefficients:</p><p>$$<br>\text{Loss} = \sum_ {i=1}^n \left( y_ i - \hat{y}_ i \right)^2 + \lambda \sum_{j=1}^p \beta_ j^2<br>$$</p><p>Where:</p><ul><li>$ y_i $: Observed target value.</li><li>$ \hat{y}_i $: Predicted value ($ \hat{y}_i = X_i \cdot \beta $).</li><li>$ \beta_j $: Coefficients of the regression model.</li><li>$ \lambda $: Regularization parameter (also called penalty parameter).</li></ul><h3 id="Ridge-Coefficient-Solution">Ridge Coefficient Solution</h3><p>The Ridge regression coefficients are obtained by solving the following optimization problem:</p><p>$$<br>\min_{\beta} \{ \|y - X\beta\|^2 + \lambda \|\beta\|^2  \}<br>$$</p><ol><li><p><strong>Matrix Form</strong>:</p><ul><li>Rewrite the problem in matrix notation:<br>$$<br>\min_{\beta} \{ (y - X\beta)^T (y - X\beta) + \lambda \beta^T \beta \}<br>$$</li></ul></li><li><p><strong>Solution for $ \beta $</strong>:</p><ul><li>Differentiating the loss function with respect to $ \beta $, we get:<br>$$<br>\beta = \left( X^T X + \lambda I \right)^{-1} X^T y<br>$$</li><li>Here:<ul><li>$ X^T X $: Correlation matrix of predictors.</li><li>$ \lambda I $: Regularization term, where $ I $ is the identity matrix.</li><li>$ \lambda $: Controls the trade-off between minimizing the squared error and penalizing large coefficients.</li></ul></li></ul></li></ol><h3 id="Why-Add-lambda-I">Why Add $ \lambda I $?</h3><ul><li>Inverse of $ X^T X $ might not exist if the predictors are highly correlated or there are fewer observations than predictors (multicollinearity).</li><li>Adding $ \lambda I $ ensures that $ X^T X + \lambda I $ is always invertible.</li></ul><h3 id="Finding-the-Optimal-lambda">Finding the Optimal $ \lambda $</h3><ol><li><p><strong>Grid Search with Cross-Validation</strong>:</p><ul><li>Evaluate the model’s performance (e.g., Mean Squared Error) for different values of $ \lambda $.</li><li>Use k-fold cross-validation to select the $ \lambda $ that minimizes validation error.</li></ul></li><li><p><strong>Mathematical Insight</strong>:</p><ul><li>When $ \lambda = 0 $: Ridge reduces to Ordinary Least Squares (OLS).</li><li>As $ \lambda \to \infty $: Coefficients $ \beta \to 0 $ (model becomes very simple).</li></ul></li><li><p><strong>Validation-Based Optimization</strong>:</p><ul><li>Define a range of $ \lambda $ values (e.g., $ \lambda = [0.001, 0.01, 0.1, 1, 10, 100] $).</li><li>For each $ \lambda $, perform cross-validation and select the value with the lowest error.</li></ul></li></ol><h3 id="Example-Finding-lambda-with-Cross-Validation">Example: Finding $ \lambda $ with Cross-Validation</h3><p>Here’s Python code to find the optimal $ \lambda $ using grid search:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score<br><br><span class="hljs-comment"># Define a range of lambda (alpha) values</span><br>alphas = np.logspace(-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">50</span>)  <span class="hljs-comment"># Lambda values from 0.001 to 1000</span><br><span class="hljs-comment"># Compute cross-validated MSE and standard deviation for each alpha</span><br>mse_values = []<br>std_errors = []<br><br><span class="hljs-keyword">for</span> alpha <span class="hljs-keyword">in</span> alphas:<br>    ridge = Ridge(alpha=alpha)<br>    scores = -cross_val_score(ridge, X_train, y_train, scoring=<span class="hljs-string">&quot;neg_mean_squared_error&quot;</span>, cv=<span class="hljs-number">5</span>)<br>    mse_values.append(scores.mean())<br>    std_errors.append(scores.std())<br><br><span class="hljs-comment"># Plot the results with error bars</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.errorbar(alphas, mse_values, yerr=std_errors, fmt=<span class="hljs-string">&#x27;o&#x27;</span>, linestyle=<span class="hljs-string">&#x27;-&#x27;</span>, label=<span class="hljs-string">&#x27;Cross-validated MSE&#x27;</span>, capsize=<span class="hljs-number">3</span>)<br>plt.xscale(<span class="hljs-string">&#x27;log&#x27;</span>)  <span class="hljs-comment"># Log scale for alpha</span><br>plt.xlabel(<span class="hljs-string">&quot;Lambda (α)&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Mean Squared Error (MSE)&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;Finding Optimal Lambda (α) for Ridge Regression with Error Bars&quot;</span>)<br>plt.axvline(alphas[np.argmin(mse_values)], color=<span class="hljs-string">&#x27;red&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">f&quot;Optimal λ = <span class="hljs-subst">&#123;alphas[np.argmin(mse_values)]:<span class="hljs-number">.3</span>f&#125;</span>&quot;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br><br><span class="hljs-comment"># Output optimal lambda</span><br>optimal_lambda_error_bar = alphas[np.argmin(mse_values)]<br>optimal_lambda_error_bar<br></code></pre></td></tr></table></figure></div><pre>0.21209508879201902</pre><p><img src="https://imgur.com/ir98xgb.png" alt=""></p><h3 id="Key-Insights">Key Insights</h3><ol><li><p><strong>Ridge Regression Purpose</strong>:</p><ul><li>Penalizes large coefficients to reduce model complexity and improve generalization.</li></ul></li><li><p><strong>Finding $ \lambda $</strong>:</p><ul><li>Perform grid search with cross-validation to select $ \lambda $ that minimizes validation error.</li></ul></li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Simulate noisier data</span><br>np.random.seed(<span class="hljs-number">42</span>)<br>n_samples = <span class="hljs-number">100</span><br>X = np.random.rand(n_samples, <span class="hljs-number">2</span>)  <span class="hljs-comment"># Two predictors</span><br>X[:, <span class="hljs-number">1</span>] = X[:, <span class="hljs-number">0</span>] + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, size=n_samples)  <span class="hljs-comment"># Add stronger multicollinearity</span><br>y = <span class="hljs-number">4</span> * X[:, <span class="hljs-number">0</span>] + <span class="hljs-number">3</span> * X[:, <span class="hljs-number">1</span>] + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, size=n_samples)  <span class="hljs-comment"># More noise in the data</span><br><br><span class="hljs-comment"># Randomly sample 20% for training</span><br>train_indices = np.random.choice(<span class="hljs-built_in">range</span>(n_samples), size=<span class="hljs-built_in">int</span>(<span class="hljs-number">0.2</span> * n_samples), replace=<span class="hljs-literal">False</span>)<br>test_indices = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples) <span class="hljs-keyword">if</span> i <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> train_indices]<br><br>X_train, y_train = X[train_indices], y[train_indices]<br>X_test, y_test = X[test_indices], y[test_indices]<br><br><span class="hljs-comment"># Ordinary Least Squares Regression</span><br>ols_model = LinearRegression()<br>ols_model.fit(X_train, y_train)<br>y_pred_ols = ols_model.predict(X_test)<br><br><span class="hljs-comment"># Ridge Regression</span><br>ridge_model = Ridge(alpha=<span class="hljs-number">1.0</span>)  <span class="hljs-comment"># Alpha is equivalent to λ</span><br>ridge_model.fit(X_train, y_train)<br>y_pred_ridge = ridge_model.predict(X_test)<br><br><span class="hljs-comment"># Evaluate models</span><br>mse_ols = mean_squared_error(y_test, y_pred_ols)<br>mse_ridge = mean_squared_error(y_test, y_pred_ridge)<br><br><span class="hljs-comment"># Return updated MSE and coefficients</span><br>mse_results_updated = &#123;<br>    <span class="hljs-string">&quot;Mean Squared Error (OLS)&quot;</span>: mse_ols,<br>    <span class="hljs-string">&quot;Mean Squared Error (Ridge)&quot;</span>: mse_ridge,<br>    <span class="hljs-string">&quot;OLS Coefficients&quot;</span>: ols_model.coef_,<br>    <span class="hljs-string">&quot;Ridge Coefficients&quot;</span>: ridge_model.coef_,<br>&#125;<br><br>mse_results_updated<br><br><br><span class="hljs-comment"># Correct the regression line plotting using predicted results</span><br><br><span class="hljs-comment"># Generate predictions for the entire feature range for consistent straight lines</span><br>X_range = np.linspace(X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>(), X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>(), <span class="hljs-number">100</span>).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>X_range_full = np.hstack([X_range, X_range + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, size=X_range.shape)])<br><br><span class="hljs-comment"># Predict the regression lines for OLS and Ridge models</span><br>y_ols_line = ols_model.predict(X_range_full)<br>y_ridge_line = ridge_model.predict(X_range_full)<br><br><span class="hljs-comment"># Plot regression results</span><br>plt.figure(figsize=(<span class="hljs-number">14</span>, <span class="hljs-number">6</span>))<br><br><span class="hljs-comment"># OLS Regression</span><br>plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>plt.scatter(X_test[:, <span class="hljs-number">0</span>], y_test, color=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&quot;Test Set&quot;</span>, alpha=<span class="hljs-number">0.7</span>)<br>plt.scatter(X_train[:, <span class="hljs-number">0</span>], y_train, color=<span class="hljs-string">&#x27;orange&#x27;</span>, label=<span class="hljs-string">&quot;Training Set&quot;</span>, alpha=<span class="hljs-number">0.7</span>)<br>plt.plot(X_range[:, <span class="hljs-number">0</span>], y_ols_line, color=<span class="hljs-string">&#x27;red&#x27;</span>, label=<span class="hljs-string">&quot;OLS Regression Line&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;OLS Regression&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Feature 1&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Target (y)&quot;</span>)<br>plt.legend()<br><br><span class="hljs-comment"># Ridge Regression</span><br>plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>plt.scatter(X_test[:, <span class="hljs-number">0</span>], y_test, color=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&quot;Test Set&quot;</span>, alpha=<span class="hljs-number">0.7</span>)<br>plt.scatter(X_train[:, <span class="hljs-number">0</span>], y_train, color=<span class="hljs-string">&#x27;orange&#x27;</span>, label=<span class="hljs-string">&quot;Training Set&quot;</span>, alpha=<span class="hljs-number">0.7</span>)<br>plt.plot(X_range[:, <span class="hljs-number">0</span>], y_ridge_line, color=<span class="hljs-string">&#x27;purple&#x27;</span>, label=<span class="hljs-string">&quot;Ridge Regression Line&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;Ridge Regression&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Feature 1&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Target (y)&quot;</span>)<br>plt.legend()<br><br>plt.tight_layout()<br>plt.show()<br><br><span class="hljs-comment"># Plot feature contributions (coefficients) as bar plots</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>x_labels = [<span class="hljs-string">&#x27;Feature 1&#x27;</span>, <span class="hljs-string">&#x27;Feature 2&#x27;</span>]<br><br><span class="hljs-comment"># OLS Coefficients</span><br>plt.bar(x_labels, ols_model.coef_, label=<span class="hljs-string">&#x27;OLS Coefficients&#x27;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br><br><span class="hljs-comment"># Ridge Coefficients</span><br>plt.bar(x_labels, ridge_model.coef_, label=<span class="hljs-string">&#x27;Ridge Coefficients&#x27;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>)<br><br><span class="hljs-comment"># Add title and labels</span><br>plt.title(<span class="hljs-string">&quot;Comparison of Feature Contributions (OLS vs Ridge)&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Coefficient Value&quot;</span>)<br>plt.legend()<br>plt.show()<br></code></pre></td></tr></table></figure></div><pre>{'Mean Squared Error (OLS)': 3.747535481239866, 'Mean Squared Error (Ridge)': 3.7344119726941427, 'OLS Coefficients': array([4.0641917 , 3.31246222]), 'Ridge Coefficients': array([2.93270253, 2.91932805])}</pre><p><img src="https://imgur.com/tUDhjah.png" alt=""><br><img src="https://imgur.com/dJzQZyh.png" alt=""></p><p>In this specific example, ridge regression slight reduced the mean squared error by reducing the contribution of <strong>feature 1</strong>. Contribution of the <strong>feature 1</strong> and <strong>feature 2</strong> are almost the same (Blue color in barplot). The linear regression plot was updated by removing the effects of <strong>feature 2</strong>.</p><h2 id="Compare-3-Methods">Compare 3 Methods</h2><p>Code continue from above</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> ElasticNet<br><br><span class="hljs-comment"># Perform Elastic Net Regression</span><br>elastic_net_model = ElasticNet(alpha=<span class="hljs-number">0.1</span>, l1_ratio=<span class="hljs-number">0.5</span>)  <span class="hljs-comment"># Alpha controls regularization strength, l1_ratio balances Lasso and Ridge</span><br>elastic_net_model.fit(X_train, y_train)<br>y_pred_elastic_net = elastic_net_model.predict(X_test)<br><br><span class="hljs-comment"># Evaluate Elastic Net model</span><br>mse_elastic_net = mean_squared_error(y_test, y_pred_elastic_net)<br><br><span class="hljs-comment"># Plot feature contributions (coefficients) for Elastic Net</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.bar(x_labels, ols_model.coef_, label=<span class="hljs-string">&quot;OLS Coefficients&quot;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&quot;red&quot;</span>)<br>plt.bar(x_labels, ridge_model.coef_, label=<span class="hljs-string">&quot;Ridge Coefficients&quot;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&quot;blue&quot;</span>)<br>plt.bar(x_labels, lasso_model.coef_, label=<span class="hljs-string">&quot;Lasso Coefficients&quot;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&quot;green&quot;</span>)<br>plt.bar(x_labels, elastic_net_model.coef_, label=<span class="hljs-string">&quot;Elastic Net Coefficients&quot;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&quot;purple&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;Comparison of Feature Contributions (OLS, Ridge, Lasso, Elastic Net)&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Coefficient Value&quot;</span>)<br>plt.legend()<br>plt.show()<br><br><span class="hljs-comment"># Return MSE and coefficients for Elastic Net</span><br>mse_results_elastic_net = &#123;<br>    <span class="hljs-string">&quot;Mean Squared Error (OLS)&quot;</span>: mse_ols,<br>    <span class="hljs-string">&quot;Mean Squared Error (Ridge)&quot;</span>: mse_ridge,<br>    <span class="hljs-string">&quot;Mean Squared Error (Lasso)&quot;</span>: mse_lasso,<br>    <span class="hljs-string">&quot;Mean Squared Error (Elastic Net)&quot;</span>: mse_elastic_net,<br>    <span class="hljs-string">&quot;OLS Coefficients&quot;</span>: ols_model.coef_,<br>    <span class="hljs-string">&quot;Ridge Coefficients&quot;</span>: ridge_model.coef_,<br>    <span class="hljs-string">&quot;Lasso Coefficients&quot;</span>: lasso_model.coef_,<br>    <span class="hljs-string">&quot;Elastic Net Coefficients&quot;</span>: elastic_net_model.coef_,<br>&#125;<br><br>mse_results_elastic_net<br></code></pre></td></tr></table></figure></div><pre>{'Mean Squared Error (OLS)': 3.747535481239866, 'Mean Squared Error (Ridge)': 3.7344119726941427, 'Mean Squared Error (Lasso)': 3.681549054209485, 'Mean Squared Error (Elastic Net)': 3.810867636824328, 'OLS Coefficients': array([4.0641917 , 3.31246222]), 'Ridge Coefficients': array([2.93270253, 2.91932805]), 'Lasso Coefficients': array([3.3579283 , 2.97769008]), 'Elastic Net Coefficients': array([2.72149455, 2.71825096])}</pre><p><img src="https://imgur.com/ExhrW4P.png" alt=""></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Regularization is a way to make sure our model doesn&#39;t become too complicated. It ensures the model doesn’t overfit the training data while still making good predictions on new data. Think of it as adding a &#39;&lt;b&gt;rule&lt;/b&gt;&#39; or &#39;&lt;b&gt;constraint&lt;/b&gt;&#39; that prevents the model from relying too much on any specific feature or predictor.</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>pyrosetta</title>
    <link href="https://karobben.github.io/2024/12/20/Bioinfor/pyrosetta/"/>
    <id>https://karobben.github.io/2024/12/20/Bioinfor/pyrosetta/</id>
    <published>2024-12-21T00:06:54.000Z</published>
    <updated>2024-12-23T21:35:44.191Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Loop-Regenerate-Codes-From-ChatGPT">Loop Regenerate (Codes From ChatGPT)</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> init, Pose, get_fa_scorefxn<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.loops <span class="hljs-keyword">import</span> Loops, Loop<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.loops.loop_mover.perturb <span class="hljs-keyword">import</span> LoopMover_Perturb_KIC<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.loops.loop_mover.refine <span class="hljs-keyword">import</span> LoopMover_Refine_KIC, LoopMover_Refine_CCD<br><br><span class="hljs-comment">#from pyrosetta.rosetta.core.import_pose import pose_from_pdbstring as pose_from_pdb</span><br><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span>  pose_from_pdb<br><br><span class="hljs-comment"># 1. Initialize PyRosetta</span><br>init()<br><br><span class="hljs-comment"># 2. Load Your Protein Pose</span><br>pose = pose_from_pdb( <span class="hljs-string">&quot;test.pdb&quot;</span>)<br><br>n_chains = pose.num_chains()<br><span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>    start_res = pose.chain_begin(chain_index)<br>    end_res = pose.chain_end(chain_index)<br>    print(<span class="hljs-string">f&quot;Chain <span class="hljs-subst">&#123;chain_index&#125;</span>: residues <span class="hljs-subst">&#123;start_res&#125;</span> to <span class="hljs-subst">&#123;end_res&#125;</span>&quot;</span>)<br><br>loops_by_chain = &#123;&#125;<br><br><span class="hljs-comment"># Iterate over chains</span><br>n_chains = pose.num_chains()<br><span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>    start_res = pose.chain_begin(chain_index)<br>    end_res = pose.chain_end(chain_index)<br><br>    <span class="hljs-comment"># Extract secondary structure substring for this chain</span><br>    chain_secstruct = secstruct[start_res-<span class="hljs-number">1</span>:end_res]<br><br>    loop_regions = []<br>    current_loop_start = <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># Identify loop regions as stretches of &#x27;L&#x27;</span><br>    <span class="hljs-keyword">for</span> i, s <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(chain_secstruct, start=start_res):<br>        <span class="hljs-keyword">if</span> s == <span class="hljs-string">&#x27;L&#x27;</span>:<br>            <span class="hljs-keyword">if</span> current_loop_start <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                current_loop_start = i<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> current_loop_start <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                loop_regions.append((current_loop_start, i-<span class="hljs-number">1</span>))<br>                current_loop_start = <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># Check if a loop extends to the end of the chain</span><br>    <span class="hljs-keyword">if</span> current_loop_start <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        loop_regions.append((current_loop_start, end_res))<br><br>    <span class="hljs-comment"># Extract sequences for each loop region</span><br>    <span class="hljs-comment"># Store them in a dictionary keyed by chain index</span><br>    chain_loops = []<br>    <span class="hljs-keyword">for</span> (loop_start, loop_end) <span class="hljs-keyword">in</span> loop_regions:<br>        <span class="hljs-comment"># Extract the sequence of the loop</span><br>        loop_seq = <span class="hljs-string">&quot;&quot;</span>.join([pose.residue(r).name1() <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(loop_start, loop_end+<span class="hljs-number">1</span>)])<br>        chain_loops.append(&#123;<br>            <span class="hljs-string">&quot;start&quot;</span>: loop_start,<br>            <span class="hljs-string">&quot;end&quot;</span>: loop_end,<br>            <span class="hljs-string">&quot;sequence&quot;</span>: loop_seq<br>        &#125;)<br><br>    loops_by_chain[chain_index] = chain_loops<br><br>n_chains = pose.num_chains()<br><span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>    start_res = pose.chain_begin(chain_index)<br>    end_res = pose.chain_end(chain_index)<br>    print(<span class="hljs-string">f&quot;Chain <span class="hljs-subst">&#123;chain_index&#125;</span>: residues <span class="hljs-subst">&#123;start_res&#125;</span> to <span class="hljs-subst">&#123;end_res&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 3. Define the Loop(s) You Want to Remodel</span><br><span class="hljs-comment"># Suppose you want to remodel the loop from residues 45 to 55.</span><br><span class="hljs-comment"># Choose a cut point (ideally inside the loop), typically near the middle.</span><br>loop_start = <span class="hljs-number">593</span><br>loop_end = <span class="hljs-number">608</span><br>cutpoint = <span class="hljs-number">601</span><br><br>loops = Loops()<br>loops.add_loop( Loop(loop_start, loop_end, cutpoint) )<br><br><span class="hljs-comment"># 4. Set Up a Scorefunction</span><br>scorefxn = get_fa_scorefxn()<br><br><span class="hljs-comment"># 5. Set Up the Loop Remodeling Protocol</span><br><span class="hljs-comment"># You have multiple options: </span><br><span class="hljs-comment"># Example: Use KIC Perturb and then Refine</span><br>perturb_mover = LoopMover_Perturb_KIC(loops)<br>perturb_mover.set_scorefxn(scorefxn)<br><br>refine_mover = LoopMover_Refine_KIC(loops)<br>refine_mover.set_scorefxn(scorefxn)<br><br><span class="hljs-comment"># Alternatively, you might use CCD refinement:</span><br><span class="hljs-comment"># refine_mover = LoopMover_Refine_CCD(loops)</span><br><span class="hljs-comment"># refine_mover.set_scorefxn(scorefxn)</span><br><br><span class="hljs-comment"># 6. Optionally: Set Up Monte Carlo or Repeats</span><br><span class="hljs-comment"># Often you do multiple trials and pick the best model.</span><br><br><span class="hljs-comment"># 7. Apply the Movers</span><br><span class="hljs-comment"># First do perturbation</span><br>perturb_mover.apply(pose)<br><br><span class="hljs-comment"># Then refine</span><br>refine_mover.apply(pose)<br><br><span class="hljs-comment"># After this, you should have a remodeled loop region.</span><br><span class="hljs-comment"># You can save the resulting structure to a PDB file:</span><br>pose.dump_pdb(<span class="hljs-string">&quot;remodeled_loop.pdb&quot;</span>)<br></code></pre></td></tr></table></figure></div><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/eJG1S0x.png" alt="Raw loop"></td><td style="text-align:left">Raw loop</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/Lu1BdXo.png" alt="Predicted loop"></td><td style="text-align:left">Predicted loop by ussing ImmuneBuilder. The Predicted results has some trouble in the CDRH3 region. And if we place it in the corrected position and it has crush.</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/l41Fw8X.png" alt="Reconstructed loop"></td><td style="text-align:left">Rosetta reconstructed loop by using the code above. Rosetta takes lots of time to reconstruct the loop and the result is terrible. The loop inseted into a very wired and unlikly position</td></tr></tbody></table><h2 id="Loop-Regenerate-Codes-From-Tutorial">Loop Regenerate (Codes From Tutorial)</h2><p><img src="https://imgur.com/1Op8NKe.png" alt=""></p><p>In the Tutorial 9.01, it use 2 structure: 1) the complete structure and 2) the structure has gap. The missing parts is range from 29~31. It not only deleted 5 residues, but also split it into 2 chains.</p><p>Because it was in a separate chain, the index 28 and 29 is the C terminal and N terminal in the chain with gap. The selected residues is 28 and 32 when they are in the original structure</p><h3 id="How-it-works-in-antibody-CDRH3">How it works in antibody CDRH3</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Notebook setup</span><br><span class="hljs-keyword">import</span> pyrosettacolabsetup; pyrosettacolabsetup.install_pyrosetta()<br><span class="hljs-keyword">import</span> pyrosetta; pyrosetta.init()<br><br><span class="hljs-comment"># py3Dmol setup (if there&#x27;s an error, make sure you have &#x27;py3Dmol&#x27; and &#x27;ipywidgets&#x27; pip installed)</span><br><span class="hljs-keyword">import</span> glob<br><span class="hljs-keyword">import</span> logging<br>logging.basicConfig(level=logging.INFO)<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pyrosetta.distributed<br><span class="hljs-keyword">import</span> pyrosetta.distributed.io <span class="hljs-keyword">as</span> io<br><span class="hljs-keyword">import</span> pyrosetta.distributed.viewer <span class="hljs-keyword">as</span> viewer<br><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> pose_from_pdb<br><br>input_pose = pose_from_pdb(<span class="hljs-string">&#x27;/mnt/Data/PopOS/Data_Ana/Wu/PigAntiBodies/AB_regine/data/14-1_ImmuneCorrect_partial.pdb&#x27;</span>)<br>input_pose_no_loop = pose_from_pdb(<span class="hljs-string">&#x27;/mnt/Data/PopOS/Data_Ana/Wu/PigAntiBodies/AB_regine/data/14-1_ImmuneCorrect_partial_Noloop.pdb&#x27;</span>)<br><br><br>helix_selector = pyrosetta.rosetta.core.select.residue_selector.SecondaryStructureSelector(<span class="hljs-string">&quot;H&quot;</span>)<br>loop_selector = pyrosetta.rosetta.core.select.residue_selector.SecondaryStructureSelector(<span class="hljs-string">&quot;L&quot;</span>)<br><br>modules = [<br>    viewer.setBackgroundColor(color=<span class="hljs-string">&quot;black&quot;</span>),<br>    viewer.setStyle(residue_selector=helix_selector, cartoon_color=<span class="hljs-string">&quot;blue&quot;</span>, label=<span class="hljs-literal">False</span>, radius=<span class="hljs-number">0</span>),<br>    viewer.setStyle(residue_selector=loop_selector, cartoon_color=<span class="hljs-string">&quot;yellow&quot;</span>, label=<span class="hljs-literal">False</span>, radius=<span class="hljs-number">0</span>),<br>    viewer.setZoomTo(residue_selector=loop_selector)<br>]<br><br><span class="hljs-comment">#view = viewer.init(input_pose, window_size=(800, 600), modules=modules).show()</span><br><span class="hljs-comment">#view = viewer.init(input_pose_no_loop, window_size=(800, 600), modules=modules).show()</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Chain_num</span>(<span class="hljs-params">pose</span>):</span><br>    n_chains = pose.num_chains()<br>    <span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>        start_res = pose.chain_begin(chain_index)<br>        end_res = pose.chain_end(chain_index)<br>        print(<span class="hljs-string">f&quot;Chain <span class="hljs-subst">&#123;chain_index&#125;</span>: residues <span class="hljs-subst">&#123;start_res&#125;</span> to <span class="hljs-subst">&#123;end_res&#125;</span>&quot;</span>)<br><br>Chain_num(input_pose)<br>Chain_num(input_pose_no_loop)<br><br>Start = input_pose_no_loop.chain_end(<span class="hljs-number">1</span>)<br>Len = input_pose.chain_end(<span class="hljs-number">1</span>) - input_pose_no_loop.chain_end(<span class="hljs-number">2</span>)<br>End = Start + Len<br>Miss_Seq = <span class="hljs-string">&quot;&quot;</span>.join([input_pose.residue(i).name1() <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Start, End)])<br><br><span class="hljs-comment">##The c terminus of one helix</span><br>print(input_pose_no_loop.residue(Start).name())<br><span class="hljs-comment">#The N terminus of the other helix</span><br>print(input_pose_no_loop.residue(Start+<span class="hljs-number">1</span>).name())<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mutate_position</span>(<span class="hljs-params">pose,position,mutate</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;A simple function to mutate an amino acid given a pose number&#x27;&#x27;&#x27;</span><br>    mr = pyrosetta.rosetta.protocols.simple_moves.MutateResidue()<br>    mr.set_target(position)<br>    mr.set_res_name(mutate)<br>    mr.apply(pose)<br><br><br><span class="hljs-comment">##Mutate both 28 and 29 to ALA</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">In tutorial, they mutated the residues into ALA. It says is make a pose that can be applied by GenKic. I am not sure it is because of the GenKic algorithem or GenKic function. I possible could be that GenKic function doesn&#x27;t accept the resideus from either side do the termianl. So, we need to remove them and add them back. Because my goal is get a better conformation of the loop, I just relpace it with itself to do the lateral test.</span><br><span class="hljs-string">Also, I tried to use ALA at the begining, too. The folding resutls are not promissing.</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>Resi1 = input_pose_no_loop.residue(Start).name3()<br>Resi2 = input_pose_no_loop.residue(Start+<span class="hljs-number">1</span>).name3()<br><br>mutate_position(input_pose_no_loop,Start,Resi1)<br>mutate_position(input_pose_no_loop,Start+<span class="hljs-number">1</span>,Resi2)<br><span class="hljs-keyword">assert</span>(input_pose_no_loop.residue(Start).name() == Resi1)<br><span class="hljs-keyword">assert</span>(input_pose_no_loop.residue(Start+<span class="hljs-number">1</span>).name() == Resi2)<br><br><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> Pose<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">slice_pose</span>(<span class="hljs-params">p,start,end</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Take a pose object and return from start, end</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    sliced = Pose()<br>    <span class="hljs-keyword">if</span> end &gt; p.size() <span class="hljs-keyword">or</span> start &gt; p.size():<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;end/start slice is longer than total lenght of pose &#123;&#125; &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(start,end)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start,end+<span class="hljs-number">1</span>):<br>        sliced.append_residue_by_bond(p.residue(i))<br>    <span class="hljs-keyword">return</span> sliced<br><br><span class="hljs-comment">##Pose object 1 - helix_AB all the way up to residue 28</span><br>helix_ab_pose = slice_pose(input_pose_no_loop,<span class="hljs-number">1</span>,Start)<br><span class="hljs-comment">##Pose object 2 - helix C and the reaminder of the pose</span><br><span class="hljs-comment">#helix_c_pose = slice_pose(input_pose_no_loop,Start+1,input_pose_no_loop.size())</span><br>helix_c_pose = slice_pose(input_pose_no_loop,Start+<span class="hljs-number">1</span>,input_pose_no_loop.chain_end(<span class="hljs-number">2</span>))<br><br><span class="hljs-comment"># We&#x27;re just going to quicky add in pdb info so that our viewing commands work</span><br>add_pdb_info_mover = pyrosetta.rosetta.protocols.simple_moves.AddPDBInfoMover()<br>add_pdb_info_mover.apply(helix_ab_pose)<br>add_pdb_info_mover.apply(helix_c_pose)<br><span class="hljs-comment"># Here&#x27;s the second part</span><br><span class="hljs-comment">#view = viewer.init(helix_c_pose, window_size=(800, 600), modules=modules).show()</span><br><span class="hljs-comment"># Here&#x27;s the first object</span><br><span class="hljs-comment">#view = viewer.init(helix_ab_pose, window_size=(800, 600), modules=modules).show()</span><br><br><span class="hljs-comment"># Here&#x27;s the second object</span><br><span class="hljs-comment">#view = viewer.init(helix_c_pose, window_size=(800, 600), modules=modules).show()</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">crudely_connect_w_loop</span>(<span class="hljs-params">n_term_pose,c_term_pose,connect_with</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    The function will take two poses and join them with a loop</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Keep in mind this is just joined as far as the pose is concerned. The bond angles and lenghts will be sub-optimal</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    one_to_three = &#123;<br>    <span class="hljs-string">&#x27;A&#x27;</span>: <span class="hljs-string">&#x27;ALA&#x27;</span>,<br>    <span class="hljs-string">&#x27;C&#x27;</span>: <span class="hljs-string">&#x27;CYS&#x27;</span>,<br>    <span class="hljs-string">&#x27;D&#x27;</span>: <span class="hljs-string">&#x27;ASP&#x27;</span>,<br>    <span class="hljs-string">&#x27;E&#x27;</span>: <span class="hljs-string">&#x27;GLU&#x27;</span>,<br>    <span class="hljs-string">&#x27;F&#x27;</span>: <span class="hljs-string">&#x27;PHE&#x27;</span>,<br>    <span class="hljs-string">&#x27;G&#x27;</span>: <span class="hljs-string">&#x27;GLY&#x27;</span>,<br>    <span class="hljs-string">&#x27;H&#x27;</span>: <span class="hljs-string">&#x27;HIS&#x27;</span>,<br>    <span class="hljs-string">&#x27;I&#x27;</span>: <span class="hljs-string">&#x27;ILE&#x27;</span>,<br>    <span class="hljs-string">&#x27;K&#x27;</span>: <span class="hljs-string">&#x27;LYS&#x27;</span>,<br>    <span class="hljs-string">&#x27;L&#x27;</span>: <span class="hljs-string">&#x27;LEU&#x27;</span>,<br>    <span class="hljs-string">&#x27;M&#x27;</span>: <span class="hljs-string">&#x27;MET&#x27;</span>,<br>    <span class="hljs-string">&#x27;N&#x27;</span>: <span class="hljs-string">&#x27;ASN&#x27;</span>,<br>    <span class="hljs-string">&#x27;P&#x27;</span>: <span class="hljs-string">&#x27;PRO&#x27;</span>,<br>    <span class="hljs-string">&#x27;Q&#x27;</span>: <span class="hljs-string">&#x27;GLN&#x27;</span>,<br>    <span class="hljs-string">&#x27;R&#x27;</span>: <span class="hljs-string">&#x27;ARG&#x27;</span>,<br>    <span class="hljs-string">&#x27;S&#x27;</span>: <span class="hljs-string">&#x27;SER&#x27;</span>,<br>    <span class="hljs-string">&#x27;T&#x27;</span>: <span class="hljs-string">&#x27;THR&#x27;</span>,<br>    <span class="hljs-string">&#x27;V&#x27;</span>: <span class="hljs-string">&#x27;VAL&#x27;</span>,<br>    <span class="hljs-string">&#x27;Y&#x27;</span>: <span class="hljs-string">&#x27;TYR&#x27;</span>,<br>    <span class="hljs-string">&#x27;W&#x27;</span>: <span class="hljs-string">&#x27;TRP&#x27;</span>&#125;<br><br>    pose_a = Pose()<br>    pose_a.assign(n_term_pose)<br><br>    pose_b = Pose()<br>    pose_b.assign(c_term_pose)<br><br>    <span class="hljs-comment"># Setup CHEMICAL MANAGER TO MAKE NEW RESIDUES</span><br>    chm = pyrosetta.rosetta.core.chemical.ChemicalManager.get_instance()<br>    rts = chm.residue_type_set(<span class="hljs-string">&#x27;fa_standard&#x27;</span>)<br>    get_residue_object = <span class="hljs-keyword">lambda</span> x: pyrosetta.rosetta.core.conformation.ResidueFactory.create_residue(<br>        rts.name_map(x))<br><br>    <span class="hljs-comment"># Will keep track of indexing of rebuilt loop</span><br>    rebuilt_loop = []<br><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Iterate through string turning each letter into a residue object and then</span><br><span class="hljs-string">    appending it to the N term pose&#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">for</span> one_letter <span class="hljs-keyword">in</span> connect_with:<br>        resi = get_residue_object(one_to_three[one_letter])<br>        pose_a.append_residue_by_bond(resi, <span class="hljs-literal">True</span>)<br>        pose_a.set_omega(pose_a.total_residue(), <span class="hljs-number">180.</span>)<br>        rebuilt_loop.append(pose_a.total_residue())<br><br>    <span class="hljs-comment">##ADD the C term pose to the end of the loop we just appended</span><br>    <span class="hljs-keyword">for</span> residue_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, pose_b.total_residue()+<span class="hljs-number">1</span>):<br>        pose_a.append_residue_by_bond(<br>            pose_b.residue(residue_index))<br><br>    print(<span class="hljs-string">&quot;Joined NTerm and CTerm pose with loop &#123;&#125; at residues &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(connect_with,rebuilt_loop))<br>    <span class="hljs-keyword">return</span> pose_a<br><br><span class="hljs-comment">#Returns a pose that is connected, but sub-optimal geometry</span><br>gk_input_pose = crudely_connect_w_loop(helix_ab_pose,helix_c_pose,Miss_Seq)<br><span class="hljs-comment">#gk_input_pose = crudely_connect_w_loop(helix_ab_pose,helix_c_pose,Miss_Seq)</span><br><br>print(Miss_Seq)<br><span class="hljs-keyword">for</span> chain <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>,input_pose_no_loop.num_chains()+<span class="hljs-number">1</span>):<br>    print(chain)<br>    gk_input_pose.append_pose_by_jump(input_pose_no_loop.split_by_chain(chain), gk_input_pose.total_residue())<br><br>Chain_num(helix_ab_pose)<br>Chain_num(gk_input_pose)<br>Chain_num(input_pose)<br><br><br><br><span class="hljs-keyword">from</span> additional_scripts.GenKic <span class="hljs-keyword">import</span> GenKic<br><br><span class="hljs-comment">##All that GenKic needs is the loop residue list</span><br>loop_residues = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Start,End+<span class="hljs-number">2</span>)]<br>gk_object = GenKic(loop_residues)<br><br><span class="hljs-comment">##Let&#x27;s set the closure attempt to 500000</span><br>gk_object.set_closure_attempts(<span class="hljs-number">500000</span>)<br>gk_object.set_min_solutions(<span class="hljs-number">10</span>)<br><br><br><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> ScoreFunction<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_bb_only_sfxn</span>():</span><br>    scorefxn = ScoreFunction()<br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.fa_atr, <span class="hljs-number">1</span>)    <span class="hljs-comment"># full-atom attractive score</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.fa_rep, <span class="hljs-number">0.55</span>)    <span class="hljs-comment"># full-atom repulsive score</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.hbond_sr_bb, <span class="hljs-number">1</span>)    <span class="hljs-comment"># short-range hbonding</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.hbond_lr_bb, <span class="hljs-number">1</span>)    <span class="hljs-comment"># long-range hbonding</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.rama_prepro, <span class="hljs-number">0.45</span>)    <span class="hljs-comment"># ramachandran score</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.omega, <span class="hljs-number">0.4</span>)    <span class="hljs-comment"># omega torsion score</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.p_aa_pp, <span class="hljs-number">0.625</span>)<br>    <span class="hljs-keyword">return</span> scorefxn<br><br><span class="hljs-comment">##Grab BB Only SFXN</span><br>bb_only_sfxn = get_bb_only_sfxn()<br><br><span class="hljs-comment">##Pass it to GK</span><br>gk_object.set_scorefxn(bb_only_sfxn)<br><br>gk_object.set_selector_type(<span class="hljs-string">&#x27;lowest_energy_selector&#x27;</span>)<br><span class="hljs-comment">#First lets set alll mainchain omega values to 180 degrees in our loop. We don&#x27;t want to include residue after the last anchor residue as that could potentially not exist.</span><br><span class="hljs-keyword">for</span> res_num <span class="hljs-keyword">in</span> loop_residues[:-<span class="hljs-number">1</span>]:<br>    gk_object.set_dihedral(res_num, res_num + <span class="hljs-number">1</span>, <span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;N&quot;</span>, <span class="hljs-number">180.1</span>)<br><br><span class="hljs-comment">###Or there is a convienience function within the class that does the same thing</span><br>gk_object.set_omega_angles()<br><br><span class="hljs-keyword">for</span> res_num <span class="hljs-keyword">in</span> loop_residues:<br>    gk_object.randomize_backbone_by_rama_prepro(res_num)<br><span class="hljs-comment">##This will grab the GK instance and apply everything we have set to our pose</span><br>gk_object.get_instance().apply(gk_input_pose)<br><br><span class="hljs-comment">##You can see, we perturbed the loop, but we did not tell GK to close the bond</span><br><span class="hljs-comment">#view = viewer.init(gk_input_pose, window_size=(800, 600), modules=modules).show()</span><br><br>gk_object.close_normal_bond(End,End+<span class="hljs-number">1</span>) <span class="hljs-comment">#or gk_object.close_normal_bond(loop_residues[-2],loop_residues[-1])</span><br>gk_object.get_instance().apply(gk_input_pose)<br><br><span class="hljs-comment">#view = viewer.init(gk_input_pose, window_size=(800, 600), modules=modules).show()</span><br><br><span class="hljs-comment">##The first residue in our loop definition will be confiend to alpha-helical rama space</span><br>gk_object.set_filter_backbone_bin(loop_residues[<span class="hljs-number">0</span>],<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-built_in">bin</span>=<span class="hljs-string">&#x27;ABBA&#x27;</span>)<br><span class="hljs-comment">##The last residue in our loop definition will be confiend to alpha-helical rama space</span><br>gk_object.set_filter_backbone_bin(loop_residues[-<span class="hljs-number">1</span>],<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-built_in">bin</span>=<span class="hljs-string">&#x27;ABBA&#x27;</span>)<br><br>gk_object.set_filter_loop_bump_check()<br><br><br><span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> gk_object.pivot_residues:<br>    gk_object.set_filter_rama_prepro(r,cutoff=<span class="hljs-number">0.5</span>)<br><br><span class="hljs-comment">##Grab GK instance</span><br>gk_instance = gk_object.get_instance()<br><span class="hljs-comment">##apply it to the pose</span><br>gk_instance.apply(gk_input_pose)<br><span class="hljs-comment">##The Input pose with no loop, the reference pose we are trying to recreate and the GK pose</span><br>poses = [input_pose_no_loop, input_pose, gk_input_pose]<br><span class="hljs-comment"># view = viewer.init(poses) + viewer.setStyle()</span><br><span class="hljs-comment"># view()</span><br><br><span class="hljs-comment">#gk_input_pose.dump_pdb(&quot;final_pose.pdb&quot;)</span><br><br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.relax <span class="hljs-keyword">import</span> FastRelax<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.core.kinematics <span class="hljs-keyword">import</span> MoveMap<br><br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.relax <span class="hljs-keyword">import</span> FastRelax<br><br><br><br>gk_input_pose.dump_pdb(<span class="hljs-string">&quot;loop.pdb&quot;</span>)<br><br><span class="hljs-comment"># loop alone</span><br>relax = FastRelax()<br>relax.set_scorefxn(bb_only_sfxn)<br>movemap = MoveMap()<br><span class="hljs-comment">#for res in loop_residues:</span><br><span class="hljs-comment">#  movemap.set_bb(res, True)</span><br>relax.set_movemap(movemap)<br>relax.apply(gk_input_pose)<br><br><br>gk_input_pose.dump_pdb(<span class="hljs-string">&quot;loop_relaxe.pdb&quot;</span>)<br><br><span class="hljs-comment"># relax: Full</span><br>relax = FastRelax()<br>relax.set_scorefxn(bb_only_sfxn)<br>relax.apply(gk_input_pose)<br><br><br>gk_input_pose.dump_pdb(<span class="hljs-string">&quot;loop_full_relaxe.pdb&quot;</span>)<br></code></pre></td></tr></table></figure></div><p>In this script, I am not only test the loop reconstruction, but also add relaxation steps. Here is the results from different methods. It seems like no matter how you try, it is hard to reconstruct this loop in Rosetta.</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/iyjWPH6.png" alt="Reconstructed Loop"></td><td style="text-align:center"><img src="https://imgur.com/ZX450qV.png" alt="Loop-reconstruction and Relaxation"></td></tr><tr><td style="text-align:center"><img src="https://imgur.com/wTwR6P5.png" alt="Relaxation only"></td><td style="text-align:center"><img src="https://imgur.com/BRtUTD1.png" alt="Realxation and then loop rexonstruction"></td></tr></tbody></table><h2 id="How-to-check-the-Chain-and-the-number-of-residues">How to check the Chain and the number of residues</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> init, pose_from_pdb<br><br><span class="hljs-comment"># 1. Initialize PyRosetta</span><br>init()<br><span class="hljs-comment"># 2. Load Your Protein Pose</span><br>pose = pose_from_pdb( <span class="hljs-string">&quot;data/14-1_ImmuneCorrect.pdb&quot;</span>)<br><br><span class="hljs-comment"># 3. Count and print the result</span><br><br>n_chains = pose.num_chains()<br><span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>    start_res = pose.chain_begin(chain_index)<br>    end_res = pose.chain_end(chain_index)<br>    print(<span class="hljs-string">f&quot;Chain <span class="hljs-subst">&#123;chain_index&#125;</span>: residues <span class="hljs-subst">&#123;start_res&#125;</span> to <span class="hljs-subst">&#123;end_res&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure></div><pre>Chain 1: residues 1 to 322Chain 2: residues 323 to 493Chain 3: residues 494 to 620Chain 4: residues 621 to 729</pre><h2 id="Get-the-Second-Structure">Get the Second Structure</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> init, pose_from_pdb<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.core.scoring.dssp <span class="hljs-keyword">import</span> Dssp<br><br><span class="hljs-comment"># 1. Initialize PyRosetta</span><br>init()<br><span class="hljs-comment"># 2. Load Your Protein Pose</span><br>pose = pose_from_pdb( <span class="hljs-string">&quot;data/14-1_ImmuneCorrect.pdb&quot;</span>)<br><br><span class="hljs-comment"># Run DSSP to get secondary structure</span><br>dssp = Dssp(pose)<br>secstruct = dssp.get_dssp_reduced_IG_as_L_secstruct()<br></code></pre></td></tr></table></figure></div><pre>LLEEEEELELLLLLLEEEELLEEEEEELLEEELEELLLLLLEEEELLELLEELLLELHHHHHHLLLLLLLLLLLLLLLLEEELLLLLELLLLLLLELLHHHHHHHLLLELLLEEEELLLLLLLLLLEELLLLEHLHLLLLLLELLLLEEEEEELLLLLLLEEEEEELLLLLLEEEEEEEEELLLHHHHHHHHLLLLLLEEEEELLLEEEELLLLLLLLLLLLLLLEEEEEEEEELLLLEEEEEELLLEEEELEEEELLELLLLLEEELLLLEEEEEELEELLLLLEL...</pre><div class="admonition note"><p class="admonition-title">What does it mean?</p><ul><li>H: Alpha-Helix</li><li>E: Beta-Strand</li><li>L: Loop or Irregular Region</li></ul></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">pyrosetta</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Heatmap with GGplot</title>
    <link href="https://karobben.github.io/2024/12/15/R/ggplot-heatmap/"/>
    <id>https://karobben.github.io/2024/12/15/R/ggplot-heatmap/</id>
    <published>2024-12-15T06:23:10.000Z</published>
    <updated>2024-12-15T07:20:26.023Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Data-Ready">Data Ready</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r"><span class="hljs-comment"># Library</span><br>library(ggplot2)<br><br><span class="hljs-comment"># Dummy data</span><br>x &lt;- <span class="hljs-built_in">LETTERS</span>[<span class="hljs-number">1</span>:<span class="hljs-number">20</span>]<br>y &lt;- paste0(<span class="hljs-string">&quot;var&quot;</span>, seq(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>))<br>data &lt;- expand.grid(X=x, Y=y)<br>data$Z &lt;- runif(<span class="hljs-number">400</span>, <span class="hljs-number">0</span>, <span class="hljs-number">5</span>)<br> <br><span class="hljs-comment"># Heatmap </span><br>p &lt;- ggplot(data, aes(X, Y, fill= Z)) + <br>  geom_tile()<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/lxVWCkQ.png" alt="heatmap in ggplot"></p><pre>  X    Y         Z1 A var1 2.56291662 B var1 4.91312173 C var1 0.12522194 D var1 2.66059005 E var1 1.23435786 F var1 4.7347760</pre><h2 id="Cluster-column-and-rows">Cluster column and rows</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(stringr)<br><br>data2 &lt;- reshape(data, idvar=<span class="hljs-string">&#x27;X&#x27;</span>, timevar= <span class="hljs-string">&#x27;Y&#x27;</span>, direction= <span class="hljs-string">&#x27;wide&#x27;</span>)<br>row.names(data2) &lt;- data2$X<br>colnames(data2) &lt;- str_remove(colnames(data2), <span class="hljs-string">&#x27;Z.&#x27;</span>)<br>data2 &lt;- data2[-<span class="hljs-number">1</span>]<br><br>ClusLevel &lt;- <span class="hljs-keyword">function</span>(data2)&#123;<br>    tmp &lt;- hclust(dist(data2))<br>    <span class="hljs-built_in">return</span>(tmp$labels[tmp$order])<br>&#125;<br><br>data$X &lt;- factor(data$X, level = ClusLevel(data2))<br>data$Y &lt;- factor(data$Y, level = ClusLevel(t(data2)))<br><br>p &lt;- ggplot(data, aes(X, Y, fill= Z)) + <br>  geom_tile()<br><br><br></code></pre></td></tr></table></figure></div><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/4Lq2Nhf.png" alt="Heatmap"></th></tr></thead><tbody><tr><td style="text-align:center">The cluster resutls are not very clear. It is expectable because the generated dataset doesn’t has any kind of relationship at all</td></tr></tbody></table><h2 id="Classic-RdYlBu-Palette">Classic RdYlBu Palette</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r"><span class="hljs-comment">## Best color for heatmap</span><br>library(RColorBrewer)<br>colorRampPalette(rev(brewer.pal(n = 7,name = &quot;RdYlBu&quot;))) -&gt; cc<br><br>p + scale_fill_gradientn(colors=cc(<span class="hljs-number">100</span>))<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/PYhCSUc.png" alt=""></p><h2 id="Set-Limits-and-Change-Color">Set Limits and Change Color</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">Heatmape &lt;- <span class="hljs-keyword">function</span>(TB, x, y, fill, minpoint = <span class="hljs-literal">FALSE</span>, midpoint = <span class="hljs-literal">FALSE</span>, maxpoint = <span class="hljs-literal">FALSE</span>,<br>                    legend.name = <span class="hljs-built_in">expression</span>(paste(<span class="hljs-string">&quot;Δ&quot;</span>, <span class="hljs-built_in">log</span>[<span class="hljs-number">10</span>], <span class="hljs-string">&quot;(&quot;</span>, K[D], <span class="hljs-string">&quot;)&quot;</span>, sep = <span class="hljs-string">&#x27;&#x27;</span>)),<br>                    axis.title.x = <span class="hljs-string">&#x27;X&#x27;</span>,<br>                    axis.title.y = <span class="hljs-string">&#x27;Y&#x27;</span>,<br>                    colors = <span class="hljs-built_in">c</span>(<span class="hljs-string">&quot;Firebrick4&quot;</span>, <span class="hljs-string">&quot;white&quot;</span>, <span class="hljs-string">&quot;royalblue4&quot;</span>) <br>                    )&#123;<br>    <span class="hljs-keyword">if</span>(minpoint == <span class="hljs-literal">FALSE</span>)&#123;<br>        minpoint = <span class="hljs-built_in">min</span>(TB[[fill]], na.rm = <span class="hljs-literal">TRUE</span>)<br>    &#125;<br>    <span class="hljs-keyword">if</span>(midpoint == <span class="hljs-literal">FALSE</span>)&#123;<br>        midpoint = mean(<span class="hljs-built_in">c</span>(<span class="hljs-built_in">min</span>(TB[[fill]], na.rm = <span class="hljs-literal">TRUE</span>), <span class="hljs-built_in">min</span>(TB[[fill]], na.rm = <span class="hljs-literal">TRUE</span>)))<br>    &#125;<br>    <span class="hljs-keyword">if</span>(maxpoint == <span class="hljs-literal">FALSE</span>)&#123;<br>        maxpoint = <span class="hljs-built_in">max</span>(TB[[fill]], na.rm = <span class="hljs-literal">TRUE</span>)<br>    &#125;<br>    P &lt;- ggplot(TB, aes(TB[[x]], TB[[y]], fill = TB[[fill]])) + geom_tile() +<br>      scale_fill_gradientn(<br>          colors = colors, <br>          values = scales::rescale(<span class="hljs-built_in">c</span>(minpoint, midpoint, maxpoint)),  <span class="hljs-comment"># Set the key values</span><br>          oob = scales::squish,<br>          na.value = <span class="hljs-string">&quot;gray&quot;</span>,<br>          limit = <span class="hljs-built_in">c</span>(minpoint, maxpoint)) +<br>      labs(x = axis.title.x, y = axis.title.y, fill = legend.name) +<br>      theme_linedraw() + <br>      coord_trans(expand = <span class="hljs-number">0</span>) + <br>      theme(panel.background= element_rect (<span class="hljs-string">&#x27;gray&#x27;</span>,), panel.grid = element_blank())<br>    <span class="hljs-built_in">return</span>(P)<br>&#125;<br><br>Heatmape(data, <span class="hljs-string">&quot;X&quot;</span>, <span class="hljs-string">&quot;Y&quot;</span>, <span class="hljs-string">&quot;Z&quot;</span>) + ggtitle(<span class="hljs-string">&quot;With default scale&quot;</span>)<br>Heatmape(data, <span class="hljs-string">&quot;X&quot;</span>, <span class="hljs-string">&quot;Y&quot;</span>, <span class="hljs-string">&quot;Z&quot;</span>, minpoint=<span class="hljs-number">1.5</span>, midpoint =<span class="hljs-number">2</span>, maxpoint=<span class="hljs-number">2.5</span>) + ggtitle(<span class="hljs-string">&quot;given value range&quot;</span>)<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/kgMkccx.png" alt=""></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Heatmap with GGplot</summary>
    
    
    
    <category term="R" scheme="https://karobben.github.io/categories/R/"/>
    
    <category term="Plot" scheme="https://karobben.github.io/categories/R/Plot/"/>
    
    <category term="GGPLOT" scheme="https://karobben.github.io/categories/R/Plot/GGPLOT/"/>
    
    
    <category term="R" scheme="https://karobben.github.io/tags/R/"/>
    
    <category term="ggplot" scheme="https://karobben.github.io/tags/ggplot/"/>
    
  </entry>
  
  <entry>
    <title>GGplot: Prism style</title>
    <link href="https://karobben.github.io/2024/12/15/R/ggplot-prismStyle/"/>
    <id>https://karobben.github.io/2024/12/15/R/ggplot-prismStyle/</id>
    <published>2024-12-15T06:07:28.000Z</published>
    <updated>2024-12-19T00:47:27.075Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Barplot">Barplot</h2><h3 id="Data-Prepare">Data Prepare</h3><p>This code would use the inner data set <code>chickwts</code> as example to calculate the mean value and sd value to use as bar height and error bar</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(dplyr)<br><br>result &lt;- chickwts %&gt;%<br>  group_by(feed) %&gt;%<br>  summarise(mean = mean(weight), sd = sd(weight))<br></code></pre></td></tr></table></figure></div><p>Data <code>chickwts</code>:</p><pre>  weight      feed1    179 horsebean2    160 horsebean3    136 horsebean4    227 horsebean</pre><p>Data frame after converted:</p><pre># A tibble: 6 × 3  feed       mean    sd  <fct>     <dbl> <dbl>1 casein     324.  64.42 horsebean  160.  38.63 linseed    219.  52.24 meatmeal   277.  64.95 soybean    246.  54.16 sunflower  329.  48.8</pre><h3 id="Plot-the-Plot-and-Define-the-Theme">Plot the Plot and Define the Theme</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(ggplot2)<br><br>Prim_bar &lt;- <span class="hljs-keyword">function</span>(p)&#123;<br>  P &lt;- p + theme(panel.background = element_blank(),<br>          axis.line = element_line(size = <span class="hljs-number">1</span>),<br>          axis.ticks = element_line(colour = <span class="hljs-string">&quot;black&quot;</span>, size = <span class="hljs-number">1</span>),<br>          axis.ticks.length = unit(<span class="hljs-number">.25</span>, <span class="hljs-string">&#x27;cm&#x27;</span>),<br>          axis.text = element_text(size = <span class="hljs-number">15</span>),<br>          axis.text.x = element_text(angle = <span class="hljs-number">45</span>, vjust = <span class="hljs-number">1</span>, hjust = <span class="hljs-number">1</span>),<br>          axis.title = element_text(size = <span class="hljs-number">20</span>),<br>          plot.title = element_text(hjust = <span class="hljs-number">.5</span>, size = <span class="hljs-number">25</span>)) +<br>  scale_y_continuous(expand = <span class="hljs-built_in">c</span>(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0</span>)) +<br>  ggtitle(<span class="hljs-string">&#x27;plot&#x27;</span>)<br>  <span class="hljs-built_in">return</span>(P)<br>&#125;<br><br>p &lt;- ggplot(result, aes(feed, mean)) +<br>    geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = <span class="hljs-number">.3</span>, size = <span class="hljs-number">1</span>) +<br>geom_bar(stat = <span class="hljs-string">&#x27;identity&#x27;</span>, color = <span class="hljs-string">&#x27;black&#x27;</span>, size = <span class="hljs-number">1</span>, width = <span class="hljs-number">.6</span>, fill = <span class="hljs-string">&#x27;Gainsboro&#x27;</span>)<br><br>Prim_bar(p)<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/omoKjMU.png" alt="Apply the Prism Themes to ggplot"></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">GGplot: Prism style</summary>
    
    
    
    <category term="R" scheme="https://karobben.github.io/categories/R/"/>
    
    <category term="Plot" scheme="https://karobben.github.io/categories/R/Plot/"/>
    
    <category term="GGPLOT" scheme="https://karobben.github.io/categories/R/Plot/GGPLOT/"/>
    
    
    <category term="R" scheme="https://karobben.github.io/tags/R/"/>
    
    <category term="Plot" scheme="https://karobben.github.io/tags/Plot/"/>
    
    <category term="ggplot" scheme="https://karobben.github.io/tags/ggplot/"/>
    
  </entry>
  
  <entry>
    <title>OpenMM, Molecular Dynamic Simulation</title>
    <link href="https://karobben.github.io/2024/12/08/Bioinfor/openMM/"/>
    <id>https://karobben.github.io/2024/12/08/Bioinfor/openMM/</id>
    <published>2024-12-08T17:48:03.000Z</published>
    <updated>2024-12-19T02:29:45.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Install">Install</h2><p>More detailed installations: <a href="http://docs.openmm.org/latest/userguide/application/01_getting_started.html">OpenMM User Guide</a></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">conda create -n openmm python=3.9 -y<br>conda activate openmm<br>conda install -c conda-forge openmm<br></code></pre></td></tr></table></figure></div><p><strong>Test the installation</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">python -m openmm.testInstallation<br></code></pre></td></tr></table></figure></div><pre>OpenMM Version: 8.2Git Revision: 53770948682c40bd460b39830d4e0f0fd3a4b868There are 4 Platforms available:1 Reference - Successfully computed forces2 CPU - Successfully computed forces3 CUDA - Successfully computed forces1 warning generated.1 warning generated.4 OpenCL - Successfully computed forcesMedian difference in forces between platforms:Reference vs. CPU: 6.29538e-06Reference vs. CUDA: 6.75176e-06CPU vs. CUDA: 7.49106e-07Reference vs. OpenCL: 6.75018e-06CPU vs. OpenCL: 7.64529e-07CUDA vs. OpenCL: 1.757e-07All differences are within tolerance.</pre><h2 id="Use-the-GPU-in-Simulation">Use the GPU in Simulation</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> openmm <span class="hljs-keyword">import</span> Platform<br><br><span class="hljs-comment"># define the platform</span><br>platform = Platform.getPlatformByName(<span class="hljs-string">&#x27;CUDA&#x27;</span>)<br>properties = &#123;<span class="hljs-string">&#x27;DeviceIndex&#x27;</span>: <span class="hljs-string">&#x27;1&#x27;</span>, <span class="hljs-string">&#x27;Precision&#x27;</span>: <span class="hljs-string">&#x27;mixed&#x27;</span>&#125; <br><br><span class="hljs-comment"># add the parameter in simulation</span><br>simulation = app.Simulation(pdb.topology, system, integrator, platform, properties)<br></code></pre></td></tr></table></figure></div><p>To be notice: Evene though, you implied that the GPU parameter in the code, it still heavily relies on the CPU.</p><h2 id="PDB-fix">PDB fix</h2><p>Before you run the simulation, you may need to fix the PDB first.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> openmm <span class="hljs-keyword">import</span> app<br><span class="hljs-keyword">from</span> openmm.unit <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> pdbfixer <span class="hljs-keyword">import</span> PDBFixer<br><span class="hljs-keyword">from</span> openmm.app <span class="hljs-keyword">import</span> PDBFile<br><br><br><span class="hljs-comment"># repair the PDB</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">PDB_fix</span>(<span class="hljs-params">INPUT, OUPUT</span>):</span><br>    fixer = PDBFixer(filename=INPUT)<br>    fixer.findMissingResidues()<br>    fixer.findNonstandardResidues()<br>    fixer.replaceNonstandardResidues()<br>    fixer.findMissingAtoms()<br>    fixer.addMissingAtoms()<br>    fixer.addMissingHydrogens(<span class="hljs-number">7.0</span>)<br>    <span class="hljs-comment">#fixer.addSolvent(fixer.topology.getUnitCellDimensions())</span><br>    <span class="hljs-comment"># Remove problematic water molecules and add correct TIP3P water</span><br>    fixer.removeHeterogens(keepWater=<span class="hljs-literal">True</span>)<br>    PDBFile.writeFile(fixer.topology, fixer.positions, <span class="hljs-built_in">open</span>(OUPUT, <span class="hljs-string">&#x27;w&#x27;</span>))<br><br>PDB_fix(<span class="hljs-string">&#x27;best.pdb&#x27;</span>, <span class="hljs-string">&#x27;best_fix.pdb&#x27;</span>)<br></code></pre></td></tr></table></figure></div><h2 id="Anchor-a-protein">Anchor a protein</h2><p><mark>Code was not tested because my protein has hydrophobic surface and it would crush in the water environment</mark></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Anchor protein 1 by restraining its atoms</span><br>anchor_force = mm.CustomExternalForce(<span class="hljs-string">&#x27;0.5 * k * (x^2 + y^2 + z^2)&#x27;</span>)<br>anchor_force.addPerParticleParameter(<span class="hljs-string">&#x27;k&#x27;</span>)<br><br><span class="hljs-comment"># Add position restraints to protein 1</span><br><span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> pdb.topology.atoms():<br>    <span class="hljs-keyword">if</span> atom.residue.chain.<span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>]:  <span class="hljs-comment"># Assume protein 1 is in chain A</span><br>        anchor_force.addParticle(atom.index, [<span class="hljs-number">1000</span>])  <span class="hljs-comment"># High force constant</span><br><br>system.addForce(anchor_force)<br></code></pre></td></tr></table></figure></div><h2 id="Pull-Protein-Force-Apply">Pull Protein Force Apply</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Apply a pulling force to protein 2</span><br>pulling_force = mm.CustomExternalForce(<span class="hljs-string">&#x27;k_pull * (x - x0)^2&#x27;</span>)<br>pulling_force.addPerParticleParameter(<span class="hljs-string">&#x27;k_pull&#x27;</span>)<br>pulling_force.addPerParticleParameter(<span class="hljs-string">&#x27;x0&#x27;</span>)<br><br><span class="hljs-comment"># Add pulling force to atoms of protein 2 (e.g., chain B)</span><br><span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> pdb.topology.atoms():<br>    <span class="hljs-keyword">if</span> atom.residue.chain.<span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>]:  <span class="hljs-comment"># Assume protein 2 is in chain B</span><br>        pulling_force.addParticle(atom.index, [-<span class="hljs-number">1</span>, <span class="hljs-number">1.0</span>])  <span class="hljs-comment"># Adjust constants</span><br><br>system.addForce(pulling_force)<br></code></pre></td></tr></table></figure></div><h2 id="Change-Record">Change Record</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">simulation.reporters.append(app.StateDataReporter(<br>    <span class="hljs-string">&#x27;best_fix_sld_tr.csv&#x27;</span>, <span class="hljs-number">10</span>, step=<span class="hljs-literal">True</span>, potentialEnergy=<span class="hljs-literal">True</span>, temperature=<span class="hljs-literal">True</span>))<br>simulation.reporters.append(app.PDBReporter(<span class="hljs-string">&#x27;best_fix_sld_tr.pdb&#x27;</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure></div><h2 id="Save-the-structure-as-cif">Save the structure as cif</h2><p>During the simulation, you may wants to add lots of wather molecular. When the number of molecular over than 100,000, pdb format can’t handle it anymore. So you want to save it as <code>cif</code> format.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;best_fix_sld.cif&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    app.PDBxFile.writeFile(modeller.topology, modeller.positions, f)<br></code></pre></td></tr></table></figure></div><h2 id="Save-the-structure-from-the-simulation">Save the structure from the simulation</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># After finishing your MD steps:</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;final_structure.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br></code></pre></td></tr></table></figure></div><h2 id="In-Action-Protein-in-Water">In Action: Protein in Water</h2><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/JnRSG6l.gif" alt=""></td><td style="text-align:left">With the help of openMM toolkit, you can simulate the protein in water easily. Here is the code from the <a href="https://openmm.github.io/openmm-cookbook/latest/notebooks/tutorials/protein_in_water.html">document</a>.</td></tr></tbody></table><p>In the simulation, the main codes is explained:</p><ol><li>read the pdb file with <code>PDBFile</code></li><li>Specify the force filed.</li><li>Clean water and add water as a period box. In this step, you can add the water in the force filed based on the size of the filed. Our you can test small filed. The filed is a period box (replicated infinitely in all directions) which means when the object moves to the end of one side, it will not run out of the box, but coming back from <strong>against face</strong>.</li><li>Setup the integrator:<ul><li><code>forcefield.createSystem</code>:<ul><li><code>modeller.topology</code>: you’ll add you molecular (protein)</li><li><code>nonbondedMethod=PME</code>: <font title='ChatGPT o1' color=gray>specifies how long-range electrostatic interactions. Simply cutting them off at a certain radius can introduce errors. <strong>PME</strong> (Particle Mesh Ewald) uses a combination of direct space calculations (for short distances) and reciprocal space calculations (using fast Fourier transforms) to accurately handle these interactions.</font></li><li><code>nonbondedCutoff=1.0*nanometer</code>: <font title='ChatGPT o1' color=gray>When using a cutoff-based approach (like nonbondedCutoff=1.0*nanometer), the simulation engine directly calculates the vdW (Lennard-Jones) interactions only between pairs of atoms that are within that 1 nm cutoff distance. If two atoms are farther apart than 1 nm, their vdW interactions are not explicitly computed.</font></li></ul></li><li><code>integrator</code>: This is the place to given the value of the Tm and time scale.</li></ul></li><li><code>simulation.minimizeEnergy()</code>: Start to minimize local Energy.</li><li>Setup report: set up the report to record the energy change and save the trajectory.</li><li>Simulate in the <mark>NVT equillibration</mark> and <mark>NPT production MD</mark> condition.<ul><li><code>1*bar</code>: bar is a standard measure of pressure, and 1 bar is approximately equal to atmospheric pressure at sea level.</li></ul></li></ol><div class="admonition note"><p class="admonition-title">What is NVT and NPT?</p><p><font title='ChatGPT o1' color=gray>In the NVT (constant Number of particles, Volume, and Temperature) ensemble, the system is thermally equilibrated at a fixed volume to achieve a stable temperature distribution. This step ensures that any initial structural distortions and non-equilibrium distributions of kinetic energy dissipate, providing a well-relaxed starting point. Following NVT equilibration, the system is often subjected to an NPT (constant Number of particles, Pressure, and Temperature) ensemble, where both temperature and pressure are maintained constant. This allows the simulation box volume to fluctuate to the pressure target, enabling the system’s density and structure to equilibrate under more experimentally relevant conditions. The transition from NVT to NPT thus facilitates a smooth pathway from initial equilibration to realistic production conditions, offering a balanced and physically representative environment for subsequent analyses of structural, thermodynamic, and dynamic properties.</font></p></div><table><thead><tr><th>Feature</th><th>NVT Equilibration</th><th>NPT Production MD</th></tr></thead><tbody><tr><td>Ensemble</td><td>Canonical (NVT)</td><td>Isothermal–Isobaric (NPT)</td></tr><tr><td>Variables Held Fixed</td><td>Number of particles (N), Volume (V), Temperature (T)</td><td>Number of particles (N), Pressure §, Temperature (T)</td></tr><tr><td>Volume Adjustment</td><td>Fixed volume</td><td>Volume fluctuates to maintain target pressure</td></tr><tr><td>Pressure Control</td><td>Not controlled, can fluctuate</td><td>Actively controlled via a barostat</td></tr><tr><td>Typical Use</td><td>Initial temperature equilibration after energy minimization</td><td>Production runs to simulate conditions resembling experimental environments</td></tr><tr><td>Realism</td><td>Less physically representative of ambient conditions (volume fixed)</td><td>More realistic: system adapts to pressure, resulting in stable density</td></tr><tr><td>Common Duration</td><td>Shorter (tens to hundreds of picoseconds)</td><td>Longer (nanoseconds to microseconds) for data collection</td></tr><tr><td>Outcome</td><td>Thermally equilibrated structure at given T</td><td>Equilibrium structure and dynamics at given P and T, suitable for analysis</td></tr></tbody></table><h2 id="Protein-Relaxation-Test">Protein Relaxation Test</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sys <span class="hljs-keyword">import</span> stdout<br><span class="hljs-keyword">from</span> openmm.app <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> openmm <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> openmm.unit <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># Input files</span><br>pdb_filename = <span class="hljs-string">&#x27;best_fix.pdb&#x27;</span>  <span class="hljs-comment"># Your starting protein structure (from cryo-EM)</span><br>forcefield_files = [<span class="hljs-string">&#x27;amber14-all.xml&#x27;</span>, <span class="hljs-string">&#x27;amber14/tip3pfb.xml&#x27;</span>]  <span class="hljs-comment"># Force fields</span><br>ionic_strength = <span class="hljs-number">0.15</span>*molar<br><br><span class="hljs-comment"># Load the PDB</span><br>pdb = PDBFile(pdb_filename)<br><br><span class="hljs-comment"># Create a forcefield object</span><br>forcefield = ForceField(*forcefield_files)<br><br><span class="hljs-comment"># Create a model of the system with solvent</span><br><span class="hljs-comment"># Add a water box around the protein (10 Å padding)</span><br>modeller = Modeller(pdb.topology, pdb.positions)<br><span class="hljs-comment">#modeller.addSolvent(forcefield, model=&#x27;tip3p&#x27;, boxSize=Vec3(10,10,20)*nanometer, ionicStrength=ionic_strength)</span><br>modeller.addSolvent(forcefield, padding=<span class="hljs-number">1.0</span>*nanometer, ionicStrength=ionic_strength)<br><br><span class="hljs-comment"># Create the system</span><br>system = forcefield.createSystem(<br>    modeller.topology,<br>    nonbondedMethod=PME,<br>    nonbondedCutoff=<span class="hljs-number">1.0</span>*nanometer,<br>    constraints=HBonds,<br>    hydrogenMass=<span class="hljs-number">4</span>*amu<br>)<br><br><span class="hljs-comment"># Add a thermostat and barostat for later (NPT)</span><br>temperature = <span class="hljs-number">300</span>*kelvin<br>pressure = <span class="hljs-number">1</span>*bar<br>friction = <span class="hljs-number">1</span>/picosecond<br>timestep = <span class="hljs-number">0.002</span>*picoseconds<br><br>system.addForce(MonteCarloBarostat(pressure, temperature))<br><br><span class="hljs-comment"># Create integrator (for equilibration and production)</span><br>integrator = LangevinIntegrator(temperature, friction, timestep)<br><br><span class="hljs-comment"># Create simulation object</span><br>platform = Platform.getPlatformByName(<span class="hljs-string">&#x27;CUDA&#x27;</span>)  <span class="hljs-comment"># or &#x27;CUDA&#x27;/&#x27;OpenCL&#x27; if available</span><br>simulation = Simulation(modeller.topology, system, integrator, platform)<br>simulation.context.setPositions(modeller.positions)<br><br><span class="hljs-comment"># Minimization</span><br>print(<span class="hljs-string">&quot;Minimizing...&quot;</span>)<br>simulation.minimizeEnergy(maxIterations=<span class="hljs-number">1000</span>)<br><br><span class="hljs-comment"># save the last structure from simulation</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_mini.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br>    <br><span class="hljs-comment"># NVT Equilibration: Remove barostat and fix volume for initial temp equilibration</span><br><span class="hljs-comment"># (Optional step: you can also start directly with NPT if you prefer)</span><br>forces = &#123; force.__class__.__name__: force <span class="hljs-keyword">for</span> force <span class="hljs-keyword">in</span> system.getForces() &#125;<br>system.removeForce(<span class="hljs-built_in">list</span>(forces.keys()).index(<span class="hljs-string">&#x27;MonteCarloBarostat&#x27;</span>))<br>simulation.context.reinitialize(preserveState=<span class="hljs-literal">True</span>)<br><br>simulation.context.setVelocitiesToTemperature(temperature)<br>print(<span class="hljs-string">&quot;Equilibrating under NVT conditions...&quot;</span>)<br>simulation.reporters.append(StateDataReporter(stdout, <span class="hljs-number">1000</span>, step=<span class="hljs-literal">True</span>, potentialEnergy=<span class="hljs-literal">True</span>, temperature=<span class="hljs-literal">True</span>))<br>simulation.reporters.append(DCDReporter(<span class="hljs-string">&#x27;relax.dcd&#x27;</span>, <span class="hljs-number">1000</span>))  <span class="hljs-comment"># Save a frame every 1000 steps</span><br><br>print(<span class="hljs-string">&#x27;start simulation&#x27;</span>)<br>simulation.step(<span class="hljs-number">50000</span>)  <span class="hljs-comment"># ~100 ps of NVT equilibration (adjust as needed)</span><br><br><span class="hljs-comment"># save the last structure from simulation</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_NVT.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br><br><span class="hljs-comment"># Re-introduce NPT conditions (barostat)</span><br>system.addForce(MonteCarloBarostat(pressure, temperature))<br>simulation.context.reinitialize(preserveState=<span class="hljs-literal">True</span>)<br><br>print(<span class="hljs-string">&quot;Equilibrating under NPT conditions...&quot;</span>)<br><span class="hljs-comment"># Remove old reporters and add a new one</span><br>simulation.step(<span class="hljs-number">50000</span>)  <span class="hljs-comment"># Another ~100 ps (adjust as needed)</span><br><br><span class="hljs-comment"># save the last structure from simulation</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_NPT.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br><br><span class="hljs-comment"># Now we have an equilibrated system at NPT.</span><br><span class="hljs-comment"># This is where you might start your production run.</span><br><br>production_steps = <span class="hljs-number">250000</span>  <span class="hljs-comment"># ~500 ps of production (adjust as needed)</span><br><span class="hljs-comment">#simulation.reporters.append(PDBReporter(&#x27;output_production.pdb&#x27;, 5000)) # Save frames every 10 ps</span><br>simulation.reporters.append(StateDataReporter(<span class="hljs-string">&#x27;production_log.csv&#x27;</span>, <span class="hljs-number">1000</span>, step=<span class="hljs-literal">True</span>, time=<span class="hljs-literal">True</span>, potentialEnergy=<span class="hljs-literal">True</span>,<br>                                               kineticEnergy=<span class="hljs-literal">True</span>, totalEnergy=<span class="hljs-literal">True</span>, temperature=<span class="hljs-literal">True</span>,<br>                                               volume=<span class="hljs-literal">True</span>, density=<span class="hljs-literal">True</span>))<br><br>print(<span class="hljs-string">&quot;Running Production MD...&quot;</span>)<br>simulation.step(production_steps)<br>print(<span class="hljs-string">&quot;Done!&quot;</span>)<br><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_final.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br></code></pre></td></tr></table></figure></div><p>In the final simulation, your protein may on the “edge” of the box. So, we need to adjust the relative position of the protein</p><table><thead><tr><th style="text-align:center">Before Recenter</th><th style="text-align:center">After Recenter</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/1obBq2B.png" alt=""></td><td style="text-align:center"><img src="https://imgur.com/kyKbB8I.png" alt=""></td></tr></tbody></table><h2 id="Recenter-the-Protein">Recenter the Protein</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mdtraj <span class="hljs-keyword">as</span> md<br><br><span class="hljs-comment"># Load the trajectory and topology</span><br>traj = md.load(<span class="hljs-string">&#x27;relax.dcd&#x27;</span>, top=<span class="hljs-string">&#x27;relax_test_mini.cif&#x27;</span>)<br>traj = traj.image_molecules()<br><br><span class="hljs-comment"># Re-center coordinates so the protein is centered in the box</span><br>centered_traj = traj.center_coordinates()<br><span class="hljs-comment"># Save the re-centered trajectory as a multi-model PDB (one MODEL per frame)</span><br><span class="hljs-comment">#centered_traj.save_pdb(&#x27;centered_system.pdb&#x27;)</span><br>centered_traj.save_dcd(<span class="hljs-string">&#x27;centered_system.dcd&#x27;</span>)<br></code></pre></td></tr></table></figure></div><h2 id="Trouble-Shot">Trouble Shot</h2><h3 id="No-template-found-for-residue-30730-HOH">No template found for residue 30730 (HOH)</h3><p>Citation: <a href=""></a><br>Error Code:</p><pre>ValueError: No template found for residue 30730 (HOH).  The set of atoms matches HOH, but the bonds are different.</pre><p>Bug reason: <a href="https://github.com/openmm/openmm/issues/3393">The PDB format doesn’t support models with more than 100,000 atoms.</a></p><p>How to solve: save the output as <code>cif</code> format by using <code>PDBxFile</code></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="DIFF"><figure class="iseeu highlight /diff"><table><tr><td class="code"><pre><code class="hljs diff"><span class="hljs-deletion">- with open(&#x27;best_fix_sld.pdb&#x27;, &#x27;w&#x27;) as f:</span><br><span class="hljs-addition">+ with open(&#x27;best_fix_sld.cif&#x27;, &#x27;w&#x27;) as f:</span><br><span class="hljs-deletion">-    app.PDBFile.writeFile(modeller.topology, modeller.positions, f)</span><br><span class="hljs-addition">+    app.PDBxFile.writeFile(modeller.topology, modeller.positions, f)</span><br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">OpenMM, Molecular Dynamic Simulation</summary>
    
    
    
    <category term="Python" scheme="https://karobben.github.io/categories/Python/"/>
    
    <category term="Bio" scheme="https://karobben.github.io/categories/Python/Bio/"/>
    
    
    <category term="openMM" scheme="https://karobben.github.io/tags/openMM/"/>
    
    <category term="Molecular Dynamic Simulation" scheme="https://karobben.github.io/tags/Molecular-Dynamic-Simulation/"/>
    
  </entry>
  
  <entry>
    <title>HDF5 Data Format Introduction</title>
    <link href="https://karobben.github.io/2024/10/23/AI/hdf5/"/>
    <id>https://karobben.github.io/2024/10/23/AI/hdf5/</id>
    <published>2024-10-24T02:35:09.000Z</published>
    <updated>2024-10-30T17:49:23.621Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Structure-of-hdf5">Structure of hdf5</h2><p><mark>Key Features of HDF5</mark>:</p><ol><li>Hierarchical Structure: HDF5 files are organized like a file system, with “groups” that act like directories and “datasets” that act like files. This allows for complex, hierarchical data storage.</li><li>Efficient Storage: HDF5 is optimized for storing and retrieving large datasets. It uses compression techniques (like GZIP or SZIP) to reduce file size without losing data.</li><li>Cross-platform Compatibility: The format is portable across different platforms and operating systems, meaning that HDF5 files can be used on Windows, macOS, Linux, etc.</li><li>Self-describing Format: HDF5 files include metadata that describe the contents of the file. This makes it easy to understand the data structure without additional documentation.</li><li>Multidimensional Data: HDF5 supports storing complex, multidimensional data (such as arrays, tables, images, etc.).</li><li>Supports Many Data Types: It can store data in various types, such as integers, floats, strings, and more.</li></ol><pre>/root                (Group)    /experiment1     (Group)        /data        (Dataset)        /info        (Dataset)    /experiment2     (Group)        /data        (Dataset)        /info        (Dataset)</pre><p>Use Cases:</p><ul><li><strong>Scientific Data</strong>: For example, storing results from simulations, satellite data, or genome sequences.</li><li><strong>Machine Learning</strong>: Large training datasets can be stored in HDF5 format for efficient access during training.</li><li><strong>Image Storage</strong>: Storing large collections of images or medical imaging data (e.g., MRI scans).</li></ul><h2 id="Show-all-Names-of-Groups-and-Data">Show all Names of Groups and Data</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> h5py<br><br><span class="hljs-comment"># Open the file in read mode</span><br><span class="hljs-keyword">with</span> h5py.File(<span class="hljs-string">&#x27;file1.h5&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">print_hdf5_structure</span>(<span class="hljs-params">group, indent=<span class="hljs-number">0</span></span>):</span><br>        <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> group.keys():<br>            item = group[key]<br>            print(<span class="hljs-string">&quot;  &quot;</span> * indent + <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;key&#125;</span>: <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(item)&#125;</span>&quot;</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(item, h5py.Group):<br>                print_hdf5_structure(item, indent + <span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># Print structure from the root</span><br>    print_hdf5_structure(f)<br></code></pre></td></tr></table></figure></div><h2 id="How-to-Merge-Multiple-hdf5-Files">How to Merge Multiple hdf5 Files</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> h5py<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Function to recursively copy/merge the structure and data from source_group to target_group</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">copy_and_merge</span>(<span class="hljs-params">source_group, target_group</span>):</span><br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> source_group.keys():<br>        item = source_group[key]<br>        <span class="hljs-comment"># If the item is a group, we create the same group in the target and copy its contents</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(item, h5py.Group):<br>            <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> target_group:<br>                target_group.create_group(key)<br>            copy_and_merge(item, target_group[key])  <span class="hljs-comment"># Recursive call to merge the group&#x27;s contents</span><br>        <span class="hljs-comment"># If the item is a dataset, we merge it</span><br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(item, h5py.Dataset):<br>            <span class="hljs-comment"># If the dataset doesn&#x27;t exist in the target file, copy it</span><br>            <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> target_group:<br>                target_group.create_dataset(key, data=item[:])<br>            <span class="hljs-comment"># If the dataset exists, concatenate the data along the first axis</span><br>            <span class="hljs-keyword">else</span>:<br>                existing_data = target_group[key][:]<br>                new_data = item[:]<br>                <span class="hljs-comment"># Concatenate datasets along the first axis</span><br>                merged_data = np.concatenate((existing_data, new_data), axis=<span class="hljs-number">0</span>)<br>                <span class="hljs-comment"># Delete the old dataset and replace it with the merged one</span><br>                <span class="hljs-keyword">del</span> target_group[key]<br>                target_group.create_dataset(key, data=merged_data)<br><br><span class="hljs-comment"># Function to merge multiple HDF5 files and save the result to a new file</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge_multiple_hdf5</span>(<span class="hljs-params">files, output_file</span>):</span><br>    <span class="hljs-comment"># Create a new HDF5 file to store the merged result</span><br>    <span class="hljs-keyword">with</span> h5py.File(output_file, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> target_file:<br>        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:<br>            <span class="hljs-keyword">with</span> h5py.File(file, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> source_file:<br>                <span class="hljs-comment"># Merge the contents of each source file into the target file</span><br>                copy_and_merge(source_file, target_file)<br>        print(<span class="hljs-string">f&quot;All files have been merged into <span class="hljs-subst">&#123;output_file&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># List of HDF5 files to be merged</span><br>files_to_merge = [<span class="hljs-string">&#x27;file1.h5&#x27;</span>, <span class="hljs-string">&#x27;file2.h5&#x27;</span>, <span class="hljs-string">&#x27;file3.h5&#x27;</span>]  <span class="hljs-comment"># Add as many files as needed</span><br><span class="hljs-comment"># Specify the output file where the merged data will be saved</span><br>output_file = <span class="hljs-string">&#x27;merged_output.h5&#x27;</span><br><span class="hljs-comment"># Merge the files and save the result</span><br>merge_multiple_hdf5(files_to_merge, output_file)<br></code></pre></td></tr></table></figure></div><p>Explanation of the Code:</p><ol><li><strong><code>copy_and_merge</code> function</strong> remains the same, recursively merging groups and datasets from the source to the target.</li><li><strong><code>merge_multiple_hdf5</code> function</strong>:<ul><li>Accepts a list of HDF5 files (<code>files</code>) and an <code>output_file</code> name.</li><li>It creates a new HDF5 file (<code>output_file</code>) in <strong>write mode</strong> (<code>'w'</code>).</li><li>It loops through each file in the list, opens it in <strong>read mode</strong> (<code>'r'</code>), and calls the <code>copy_and_merge</code> function to copy the contents into the newly created file.</li><li>After all files are merged, it saves the result as <code>output_file</code>.</li></ul></li></ol><p>!!! note Key Points:<br>- Each dataset is merged by <strong>concatenating along the first axis</strong>. If you need to merge along a different axis or have more complex merging rules, we can adjust the code.<br>- Make sure the datasets you’re merging are compatible (same dimensionality along non-concatenated axes).</p><h2 id="Change-the-Group-Names">Change the Group Names</h2><p>To rename a group in an HDF5 file using <code>h5py</code>, you can’t directly change the group’s name. Instead, you can <strong>copy the group to a new group with the desired name</strong>, and then <strong>delete the original group</strong>.</p><p>Here’s how you can rename the group “4skj” to “4skj_10086”:</p><h2 id="Step-by-Step-Code">Step-by-Step Code:</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> h5py<br><span class="hljs-keyword">import</span> shutil<br><br><span class="hljs-comment"># Function to rename a group in an HDF5 file</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rename_group</span>(<span class="hljs-params">hdf5_file, old_group_name, new_group_name</span>):</span><br>    <span class="hljs-comment"># Open the file in read/write mode</span><br>    <span class="hljs-keyword">with</span> h5py.File(hdf5_file, <span class="hljs-string">&#x27;r+&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-comment"># Check if the group exists</span><br>        <span class="hljs-keyword">if</span> old_group_name <span class="hljs-keyword">in</span> f:<br>            <span class="hljs-comment"># Copy the old group to the new group</span><br>            f.copy(old_group_name, new_group_name)<br>            <span class="hljs-comment"># Delete the old group</span><br>            <span class="hljs-keyword">del</span> f[old_group_name]<br>            print(<span class="hljs-string">f&quot;Group &#x27;<span class="hljs-subst">&#123;old_group_name&#125;</span>&#x27; has been renamed to &#x27;<span class="hljs-subst">&#123;new_group_name&#125;</span>&#x27;&quot;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            print(<span class="hljs-string">f&quot;Group &#x27;<span class="hljs-subst">&#123;old_group_name&#125;</span>&#x27; does not exist in the file.&quot;</span>)<br><br><span class="hljs-comment"># Rename the group in the HDF5 file</span><br>hdf5_file = <span class="hljs-string">&#x27;file1.h5&#x27;</span>  <span class="hljs-comment"># Replace with your actual file path</span><br>old_group_name = <span class="hljs-string">&#x27;4skj&#x27;</span>  <span class="hljs-comment"># Original group name</span><br>new_group_name = <span class="hljs-string">&#x27;4skj_10086&#x27;</span>  <span class="hljs-comment"># New group name</span><br><br>rename_group(hdf5_file, old_group_name, new_group_name)<br></code></pre></td></tr></table></figure></div><ol><li><strong>Check if the group exists</strong>: The script checks if the group <code>&quot;4skj&quot;</code> exists in the HDF5 file.</li><li><strong>Copy the group</strong>: It uses the <code>f.copy()</code> function to copy the group and its contents to a new group with the desired name (<code>&quot;4skj_10086&quot;</code>).</li><li><strong>Delete the old group</strong>: After copying, the original group is deleted with <code>del f[old_group_name]</code>.</li><li><strong>Save changes</strong>: Since the file is opened in <code>'r+'</code> mode (read/write), all changes are saved automatically.</li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">HDF5 (Hierarchical Data Format version 5) is a file format designed for efficiently storing and organizing large, complex datasets. It uses a hierarchical structure of **groups** (like directories) and **datasets** (like files) to store data, supporting multidimensional arrays, metadata, and a wide variety of data types. Key advantages include **compression**, **cross-platform compatibility**, and the ability to handle large datasets that don’t fit in memory. It’s widely used in fields like scientific computing, machine learning, and bioinformatics due to its efficiency and flexibility.</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="Data Format" scheme="https://karobben.github.io/categories/Machine-Learning/Data-Format/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data" scheme="https://karobben.github.io/tags/Data/"/>
    
    <category term="AI" scheme="https://karobben.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Render Your Protein in Blender with Molecular Nodes</title>
    <link href="https://karobben.github.io/2024/10/19/Bioinfor/blender-molecular-nodes/"/>
    <id>https://karobben.github.io/2024/10/19/Bioinfor/blender-molecular-nodes/</id>
    <published>2024-10-19T14:53:38.000Z</published>
    <updated>2024-11-02T22:56:43.703Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Who-to-Install-Molecular-Nodes-for-Blender">Who to Install Molecular Nodes for Blender</h2><p>First, you should download Blender yourself. Instead of the latest version, opt for a stable version because the newest release may have bugs or be incompatible with Molecular Nodes. I tried version 4.40, but when I changed the style of the molecule to Ribbon or another style, Blender crashed and closed itself. Then, I switched to version 4.2.2, and it worked fine.</p><p>As following the figure “Install the Extension”, you can find this plugin and install it. Once you down installation, you can find there is some thing new pops up like its show in figure “Update in Scene”. In this new module, you could download the pdb online or when you have pdb in the “Cache Downloads” directory, you could also load it with “Molecular Nodes”. When you load the molecular, it looks terrible. You need to follow the figure “Render By Cycles” and “Start Render” to get a normal view of molecular(“Atoms View”).</p><p><img src="https://imgur.com/uCbxiP9.png" alt="Install the Extension"></p><p><img src="https://imgur.com/9fEK7wf.png" alt="Update in Scene"></p><p><img src="https://imgur.com/ehHyKKP.png" alt="Render By Cycles"><br><img src="https://imgur.com/1DMegwb.png" alt="Start Render"><br><img src="https://imgur.com/LoFewhU.png" alt="Atoms View"></p><h2 id="Add-a-Pure-Perfect-Background">Add a Pure Perfect Background</h2><p>Source: <a href="https://www.youtube.com/watch?v=aegiN7XeLow">YouTube: EMPossible</a></p><p><img src="https://imgur.com/Wl0ea71.png" alt="Pure White Background"></p><p>How to set:</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/UndFMUw.png" alt="Set pure background"></td><td style="text-align:left">select <li>“Render” → “Film” → “Transparent”<li>“Render” → “Color Management” → “View Transform” → “Starndard”</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/jxy0bGJ.png" alt="Set Compositing"></td><td style="text-align:left">Set the Compositing. And that’s it. Go to rendering and it woud add an Perfectwhite at the background</td></tr></tbody></table><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># codes for set a pure white background</span><br><span class="hljs-comment"># Those codes only working on the first few steps.</span><br><span class="hljs-comment"># I didn&#x27;t figure how to use script to make the Geometry Nodes </span><br><span class="hljs-comment"># So, After you run those 3 commands, you still need to started from step 3 in the second pictures to manually finish the Geometry Nodes setting.</span><br>bpy.context.scene.render.engine = <span class="hljs-string">&#x27;CYCLES&#x27;</span><br>bpy.context.scene.cycles.device = <span class="hljs-string">&#x27;GPU&#x27;</span><br>bpy.context.space_data.shading.<span class="hljs-built_in">type</span> = <span class="hljs-string">&#x27;RENDERED&#x27;</span><br><br>bpy.context.<span class="hljs-built_in">object</span>.scale[<span class="hljs-number">0</span>] = <span class="hljs-number">3.5</span><br>bpy.context.<span class="hljs-built_in">object</span>.scale[<span class="hljs-number">1</span>] = <span class="hljs-number">3.5</span><br>bpy.context.<span class="hljs-built_in">object</span>.scale[<span class="hljs-number">2</span>] = <span class="hljs-number">3.5</span><br><br><br>bpy.context.scene.render.film_transparent = <span class="hljs-literal">True</span><br>bpy.context.scene.view_settings.view_transform = <span class="hljs-string">&#x27;Standard&#x27;</span><br>bpy.context.scene.use_nodes = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure></div><h2 id="Different-Colors-in-a-Surface">Different Colors in a Surface</h2><table><thead><tr><th style="text-align:left"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left"><img src="https://imgur.com/tTOeKDl.png" alt="Settings in Geomitry"></td><td style="text-align:left">The key idea for given different color is by rendern multiple layers of color on the surface. By reverse select residues , we could delete the colors from selected layer and expose the color from inner layer.</td></tr><tr><td style="text-align:left"><img src="https://imgur.com/MGA9Mqk.png" alt="Results"></td><td style="text-align:left">Final resutls show</td></tr></tbody></table><h2 id="Multiple-Style-in-One-Object">Multiple Style in One Object</h2><table><thead><tr><th style="text-align:left"><img src="https://imgur.com/TVEMkDe.png" alt="Blender Join Geometry"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left"><img src="https://imgur.com/AOUUkWK.png" alt=""></td><td style="text-align:left">Like the example in the picture, it rendered both surface model and the stick model in one object. This is achieved by <code>Join Geometry</code></td></tr></tbody></table><h2 id="Customize-the-Color-From-The-Surface">Customize the Color From The Surface</h2><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/QU1xa7K.png" alt=""></th><th style="text-align:center"><img src="https://imgur.com/7rdCxfl.png" alt=""></th></tr></thead><tbody></tbody></table><p>For Customizing the surface color, there are 2 ways to do it.</p><ol><li>using <code>pLDDT</code> nodes from <code>Color</code></li><li>using the <code>Color Attribute Map</code> nodes from <code>Color</code>.</li></ol><p>In both case, they are actually using the same set of value stored in <code>pdb</code> or <code>cif</code> file.<br>In the pdb format show below, the 11th column marked as white is the value for <code>pLDDT</code>. If you want to manage it with <code>Color Attribute Map</code>, the name of it is <code>b_factor</code></p><pre><font size=1>ATOM   4365  C   ASP C 150      17.854  27.766  83.090  1.00 <font color=white>99.42</font>      C    C  ATOM   4366  O   ASP C 150      17.369  28.239  82.038  1.00 <font color=white>95.32</font>      C    O  ATOM   4367  CB  ASP C 150      19.712  26.091  82.521  1.00 <font color=white>98.18</font>      C    C  ATOM   4368  CG  ASP C 150      20.447  24.817  82.987  1.00 <font color=white>96.59</font>      C    C  ATOM   4369  OD1 ASP C 150      20.121  24.255  84.056  1.00 <font color=white>96.78</font>      C    O  ATOM   4370  OD2 ASP C 150      21.402  24.406  82.277  1.00 <font color=white>96.06</font>      C    O1-ATOM   4371  OXT ASP C 150      18.041  28.393  84.184  1.00 <font color=white>95.18</font>      C    O1-</font></pre><h2 id="Watching-List">Watching List</h2><ul class="task-list"><li class="task-list-item"><input type="checkbox" id="cbx_0" disabled="true"><label for="cbx_0"> <a href="https://www.youtube.com/watch?v=sIblmWV0NuM">Select color pallet</a></label></li></ul><h2 id="Trouble-Shoot">Trouble Shoot</h2><h3 id="Dead-Black-in-Transparent">Dead Black in Transparent</h3><table><thead><tr><th>Dead Black</th><th>Change Setting</th><th>After Change</th></tr></thead><tbody><tr><td><img src="https://imgur.com/jCo5bO3.png" alt="Befor Change"></td><td><img src="https://imgur.com/IaT6UlB.png" alt="Change Setting"></td><td><img src="https://imgur.com/Zb3XWZz.png" alt="After Correction"></td></tr></tbody></table><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Render Your Protein in Blender with Molecular Nodes</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Protein Structure" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Protein-Structure/"/>
    
    
    <category term="3D" scheme="https://karobben.github.io/tags/3D/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Biochmistry" scheme="https://karobben.github.io/tags/Biochmistry/"/>
    
    <category term="Plot" scheme="https://karobben.github.io/tags/Plot/"/>
    
  </entry>
  
  <entry>
    <title>NCBI Data Submit with FTP/ASCP</title>
    <link href="https://karobben.github.io/2024/10/17/Bioinfor/ncbisubmit/"/>
    <id>https://karobben.github.io/2024/10/17/Bioinfor/ncbisubmit/</id>
    <published>2024-10-18T04:36:10.000Z</published>
    <updated>2024-10-19T21:44:30.634Z</updated>
    
    <content type="html"><![CDATA[<p>This post only talks about how to use the <code>ascp</code> to upload your sequencing data into NCBI.</p><h2 id="3-Different-Ways-of-Submit-Your-Data">3 Different Ways of Submit Your Data</h2><ul><li>Cloud from <strong>Amazon S3</strong> or <strong>Google Cloud</strong><br>If your data was stored in Amazon/Google Cloud at the beginning, you can easily and safely transfer them into NCBI. (I think so though I’ve never tried).</li><li><strong>FTP</strong> or <strong>ASCP</strong><br>I would recommend the second approach since our data was mostly stored in a Linux server. FTP and ASCP are very reliable. Especially for <code>ftp</code>, it is a very popular protocol. You could find a bunch of software like ‘<a href="https://filezilla-project.org/">FileZilla</a>’ to upload through ftp. The best feature of ‘FileZilla’ is it supports <strong>resume interrupted transfer</strong> and lists the fail-up loaded files so you can upload them again with one click. So, no matter how many and how big the files are, it can help you upload them safely. The main <strong>limitation</strong> for <strong>FileZilla</strong> is that it can’t used in command form and so, is not suitable for the server.</li><li><strong>Web Browser</strong><br>Unless your data are very small, you would never want to try uploading them online.</li></ul><h2 id="When-to-Upload-Your-Data">When to Upload Your Data</h2><p>You can upload your data whenever you want. It is better to upload your file before you start to fill the submission tables. In step 7, you could find the data/director you submitted and include them in the submission. But it seems like the NCBI would delete an inactivated file within 30 days. It is long enough to finish the submission.</p><p>I prefer to use <code>FileZilla</code> to upload my data. But since now, all of my data are on the server and I don’t want to download them again, I give the <code>ftp</code> and use <code>ascp</code>.</p><h2 id="How-to-use-the-ASCP">How to use the ASCP</h2><p><img src="https://imgur.com/Wik8zeG.png" alt=""></p><p>ASCP is very easy and works similarly to <code>scp</code>. In the Submission home page, select <mark>My submissions</mark> → <mark>Upload via Aspera command line or FTP</mark> → <mark>Aspera command line instructions</mark> to find the instructions. It would give you the download link and key for connecting to the NCBI server. After that, use it just like the <code>scp</code>.</p><p>Shortcomings or suggestions for <code>ascp</code></p><ol><li><strong>Keep everything in 1 director</strong>: You’d like to upload all files into one directory. Because during the submission step <strong>7 FILES</strong>, you could only select 1 directory.</li><li><strong>No sub-directories</strong>: You may want to upload the directory without subdirectories. Because in the submission portal, you can’t check files in subdirectories. So, it is hard to track back which filed upload failed.</li><li><strong>Keep ascp log</strong>: In the submission portal, you got only first 5 lines of uploaded files. So, remember to keep the <code>ascp</code> log to record the fail uploaded data.</li><li><mark><strong>upload the data one by one</strong></mark>: Strongly recommend to upload each <code>fq</code> or compressed file one by one with scripts. When you try to upload the entire directory, it may fail (I never get it done when upload a directory)</li><li>After you upload your data, you can’t see them until you go to step <strong>7 FILES</strong> in the submission portal.</li><li>You can’t check them immediately even from the submission portal. It takes time for them to show in the <strong>Step 7</strong></li></ol><p><mark>The good thing is it is easy to write a script to upload your data automatically.</mark></p><div class="admonition note"><p class="admonition-title">In the instructions, it suggest you to use those parameters: `-QT -l100m -k1`</p><ol><li><strong><code>-Q</code></strong>: This option disables the real-time display of progress and transfer statistics during the transfer. Normally, ASCP displays ongoing statistics, such as speed and percentage of completion, but using <code>-Q</code> will suppress this output.</li><li><strong><code>-T</code></strong>: This option disables encryption of the data stream during transfer. ASCP by default uses encryption for data security, but <code>-T</code> turns this off, which might improve transfer speed but at the cost of security.</li><li><strong><code>-l100m</code></strong>: This sets the transfer speed limit to <strong>100 megabits per second</strong>. You can adjust the value (e.g., <code>100m</code>) to control how fast the transfer is allowed to go, helping to prevent network congestion or manage bandwidth usage.</li><li><strong><code>-k1</code></strong>: This option controls file resume behavior. The value <code>1</code> means that if a transfer is interrupted, ASCP will resume from the point where it left off (resumable transfer). The other possible values for <code>-k</code> are:</li></ol><ul><li><code>0</code>: No resume. The transfer restarts from the beginning.</li><li><code>2</code>: Sparse resume. ASCP resumes only the missing parts of the file.</li></ul></div><h2 id="Personal-Experience">Personal Experience</h2><h3 id="Upload-your-data-into-a-specific-directory">Upload your data into a specific directory</h3><p>Though we gave the argument <code>-k1</code>, it could still fail. In the log, it says:</p><pre>Partial Completion: 19711732K bytes transferred in 3695 seconds(43691K bits/sec), in 6 files, 4 directories; 3 files failed.Session Stop  (Error: Disk write failed (server))</pre><p>After you went to the step 7, you could see:<br><img src="https://imgur.com/xxbYXVE.png" alt=""></p><p>Which means 2 directories are empty. In this case, you don’t need to worry too much. You can change the code a little bit and continue to upload could solve this problem.<br>For example, you uploaded a directory named <code>ALL_RNA</code> with code <code>ascp -i $key_file -QT -l100m -k1 -d ALL_RNA $AddressFromInstruction</code>, the data in the directory <code>ALL_RNA/SAMPLEX</code> was failed to upload, you can use the code <code>ascp -i $key_file -QT -l100m -k1 -d ALL_RNA/SAMPLEX $AddressFromInstruction/ALL_RNA</code> to continue upload the directory <code>SAMPLEX</code> into the <code>ALL_RNA</code> in NCBI server</p><pre> ascp -i $key_file -QT -l100m -k1 -d ALL_RNA $AddressFromInstructionascp -i $key_file -QT -l100m -k1 -d ALL_RNA<font color=red>/SAMPLEX</font> $AddressFromInstruction<font color=red>/ALL_RNA</font></pre><h3 id="Upload-your-date-in-the-script">Upload your date in the script</h3><p>When you have lots of data, one of the convenient ways I found is we could <code>ascp</code> each data independently. With a for loop, we could generate all codes into a script. When there are failed uploads, we just need to copy and paste the failed codes and run them again. Or we could also delete the code for the successfully uploaded one and run the entire script again.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> $(find YourDirectories -name <span class="hljs-string">&quot;.fastq.gz&quot;</span>); <span class="hljs-keyword">do</span><br>    <span class="hljs-built_in">echo</span> ascp -i <span class="hljs-variable">$key_file</span> -QT -l100m -k1 -d <span class="hljs-variable">$i</span> <span class="hljs-variable">$AddressFromInstruction</span><br><span class="hljs-keyword">done</span> &gt;&gt; ascp.sh<br><br>bash ascp.sh<br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">NCBI Data Submit</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Database" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Database/"/>
    
    
    <category term="Database" scheme="https://karobben.github.io/tags/Database/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
  </entry>
  
  <entry>
    <title>Protein Dock Overview</title>
    <link href="https://karobben.github.io/2024/10/15/AI/proteindock/"/>
    <id>https://karobben.github.io/2024/10/15/AI/proteindock/</id>
    <published>2024-10-15T20:51:32.000Z</published>
    <updated>2025-04-07T02:59:59.489Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Physical-Based-Docking">Physical Based Docking</h2><h3 id="1982-Dock-Kuntz-Irwin-D-et-al-Rigid-body-shape-based">1982: Dock; Kuntz, Irwin D., et al.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> (Rigid body-shape based)</h3><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/lsob6Ob.png" alt="Dock; Kuntz, Irwin D., et al. 1982"></th></tr></thead><tbody><tr><td style="text-align:center">© Kuntz, Irwin D., et al. 1982<sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1:1]</a></sup></td></tr></tbody></table><p>In this paper, Kuntz present a way of docking prediction by searching the steric overlap based on the knowing surface structure of 2 proteins. It originally developed by Irwin “Tack” Kuntz and colleagues at the University of California, San Francisco (<strong>UCSF</strong>), DOCK was initially used for small-molecule docking. However, it laid the foundation for the development of more advanced docking algorithms and software that could handle macromolecular docking.</p><p>In the first generation of the Dock, it focus on 2 rigid bodies. It treat 2 proteins as one object. The goal of this program is to <mark>fix the 6 degree of freedom (3 transitions and 3 orientations) that determine the best relative position</mark>. For achieving this goal, three rules are followed:</p><ol><li>No overlap between 2 proteins</li><li>all hydrogen are pared with N or O within 3.5 Å.</li><li>all ligand atoms within the receptor binding cite.</li></ol><p><strong>Dock families:</strong></p><ol><li>1994: Firstly extend the DOCK into DNA-protein Docking and by screening the Cambridge Crystallographic Database, they find that the protein CC-1065 has high score.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup><ul><li>1999: DREAM++<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>: It is a extent package for Dock. It use Dock to predict binding and evaluated the interaction and predicts the product, finally search to find the prohibits.</li></ul></li><li>2001: <strong>DOCK 4.0</strong><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>: It added incremental construction (to sample the internal degrees of freedom of the ligand) and random search. In the Dock4, the ligand is not rigid anymore. Ligands with rotatable-bonds generated multiple conformation by other model.</li><li>2006: <strong>DOCK 5.0</strong><sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>:<ul><li>anchoring: new scoring functions, sampling methods and analysis tools; energy minimizing was mentioned during the.</li><li>scoring: energy scoring function based on the AMBERL: only <strong>intermolecular</strong> van der Waals (VDW) and electrostatic components in the function.</li><li>main limitation: Ligands has lots of rotatable-bonds would cause lots of resource. During the test set, ligands with &gt; 7 rotatable bonds were removed.</li><li>Some test data correction: using “Compute” and “Biopolymer” from <strong>Sybyl</strong><sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> to calculate the Gasteiger–Hückel partial electrostatic charges and add hydrogen for residues.</li></ul></li><li>2009: <strong>DOCK 6</strong><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>: In this version, it extents it’s abilities in RNA-ligands. But the rotatable-bonds from the ligands are still limited into 7~13. With the increasing of the RNA, the accuracy are decreased.<ul><li>update scoring in <strong>solvation energy</strong>:<ul><li>Hawkins–Cramer–Truhlar (HCT) generalized Born with solvent-accessible surface area (GB/SA) solvation scoring with optional salt screening</li><li>Poisson–Boltzmann with solvent-accessible surface area (PB/SA) solvation scoring</li><li>AMBER molecular mechanics with GB/SA solvation scoring and optional receptor flexibility</li></ul></li><li>other scoring:<ul><li>VDW: grid-based form of the Lennard-Jones potential</li><li>electrostatic: Zap Tool Kit from OpenEye</li></ul></li></ul></li><li>2013: <strong>DOCK3.7</strong><sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>:</li></ol><table><thead><tr><th style="text-align:center">DOCK4</th><th style="text-align:center">DOCK5</th><th style="text-align:center">DOCK6</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/SJJfgKt.png" alt="DOCK4"></td><td style="text-align:center"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10822-006-9060-4/MediaObjects/10822_2006_9060_Fig1_HTML.gif" alt="DOCK5"></td><td style="text-align:center"><img src="https://rnajournal.cshlp.org/content/15/6/1219/F1.large.jpg" alt="DOCK6"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center">incremental: anchor-and-grow</td><td style="text-align:center">The number of<br>rotatable-bonds hashuge<br>effects on success rate</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">anchor-and-grow</p><p>The “anchor-and-grow” conformational search algorithm. The algorithm performs the following steps: (1) DOCK perceives the molecule’s rotatable bonds, which it uses to identify an anchor segment and overlapping rigid layer segments. (2) Rigid docking is used to generate multiple poses of the anchor within the receptor. (3) The first layer atoms are added to each anchor pose, and multiple conformations of the layer 1 atoms are generated. An energy score within the context of the receptor is computed for each conformation. (4) The partially grown conformations are ranked by their score and are spatially clustered. The least energetically favorable and spatially diverse conformations are discarded. (5) The next rigid layer is added to each remaining conformation, generating a new set of conformations. (6) Once all layers have been added, the set of completely grown conformations and orientations is returned</p></div><h4 id="Compare-to-Other-Related-Tools">Compare to Other Related Tools</h4><div class="c-article-table-container"><div class="c-article-table-border c-table-scroll-wrapper"><div class="c-table-scroll-wrapper__content c-table-scroll-wrapper__fade--transparent" data-component-scroll-wrapper=""><table class="data last-table"><thead class="c-article-table-head"><tr><th class="u-text-left "><p>Method</p></th><th class="u-text-left "><p>Ligand sampling method<sup>a</sup>                                          </p></th><th class="u-text-left "><p>Receptor sampling method<sup>a</sup>                                          </p></th><th class="u-text-left "><p>Scoring function<sup>b</sup>                                          </p></th><th class="u-text-left "><p>Solvation scoring<sup>c,d</sup>                                          </p></th></tr></thead><tbody><tr><td class="u-text-left "><p>DOCK 4/5 </p></td><td class="u-text-left "><p>IC</p></td><td class="u-text-left "><p>SE</p></td><td class="u-text-left "><p>MM</p></td><td class="u-text-left "><p>DDD, GB, PB</p></td></tr><tr><td class="u-text-left "><p>FlexX/FlexE </p></td><td class="u-text-left "><p>IC</p></td><td class="u-text-left "><p>SE</p></td><td class="u-text-left "><p>ED</p></td><td class="u-text-left "><p>NA</p></td></tr><tr><td class="u-text-left "><p>Glide</p></td><td class="u-text-left "><p>CE&nbsp;+&nbsp;MC</p></td><td class="u-text-left "><p>TS</p></td><td class="u-text-left "><p>MM&nbsp;+&nbsp;ED</p></td><td class="u-text-left "><p>DS</p></td></tr><tr><td class="u-text-left "><p>GOLD </p></td><td class="u-text-left "><p>GA</p></td><td class="u-text-left "><p>GA</p></td><td class="u-text-left "><p>MM&nbsp;+&nbsp;ED</p></td><td class="u-text-left "><p>NA</p></td></tr></tbody></table></div></div><div class="c-article-table-footer"><ol>                      <li>                                    <sup>a</sup>Sampling methods are defined as Genetic Algorithm (GA), Conformational Expansion (CE), Monte Carlo (MC), incremental construction (IC), merged target structure ensemble (SE), torsional search (TS)</li>                      <li>                                    <sup>b</sup>Scoring functions are defined as either empirically derived (ED) or based on molecule mechanics (MM)</li>                      <li>                                    <sup>c</sup>If the package does not accommodate this option, the symbol NA (Not Available) is used</li>                      <li>                                    <sup>d</sup>Additional accuracy can be added to the scoring function using implicit solvent models. The most commonly used options are distance dependent dielectric (DDD), a parameterized desolvation term (DS), generalized Born (GB) and linearized Poisson Boltzmann (PB)</li>                    </ol></div></div><hr><h3 id="2003-ZDock">2003: ZDock</h3><p>Version iteration:</p><ul><li>ZDOCK 2.3/2.3.2 Scoring Function: Chen R, Li L, Weng Z. (2003) ZDOCK<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup></li><li>ZDOCK 3.0/3.0.2 Scoring Function: Mintseris J, Pierce B, Wiehe K, Anderson R, Chen R, Weng Z. (2007)<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup></li><li>M-ZDOCK: Pierce B, Tong W, Weng Z. (2005) M-ZDOCK<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup></li><li>ZDOCK 3.0.2/2.3.2: Pierce BG, Hourai Y, Weng Z. (2011)<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup></li><li>Online Server: Pierce BG, Wiehe K, Hwang H, Kim BH, Vreven T, Weng Z. (2014) ZDOCK Server<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup></li></ul><h4 id="Abstract">Abstract</h4><p>ZDock was developed for ubbound docking. It is based on pairwise shape complementarity (Docking) with desolvation and electrostatics (Scoring). In there test, it shows high success rate in the <strong>antibody-antigen</strong> docking test case. It is especially helpful in <strong>“large concave binding pocket”</strong>.</p><p>Before the ZDock, there are:</p><ul><li><strong>FTDOck</strong>: gird-based shape complementarity (GSC) and electrostatic using a Fast Fourier Transform (FFT)</li><li><strong>DOT</strong>: FFT-based computes Poission-Bolzmann electrostatics.</li><li><strong>HEX</strong>: evaluates overlapping surface skins and electrostatic complementarity with Fourier coorelation.</li><li><strong>GRAMM</strong>: low-resolutoin docking with the similar scoring as FTDOck;</li><li><strong>PPD</strong>: matches critial poitns by using geometric hashing.</li><li><strong>GIGGER</strong>: maximal surface mapping and favorable amino acid contacts by bit-mapping.</li><li><strong>DARWIN</strong>: molecular mechanics energy defined according to CHARMM.</li></ul><p>For <strong>ZDock</strong>:</p><ul><li>Optimizes desolvation (<strong>GSC</strong>), <mark>key scoring function</mark>.<ul><li>GSC = grid points surrounding the receptor corresponding to ligand atoms - clash penalty</li></ul></li><li>*<em>FFT</em> for electrostatics</li><li>Novel pairwise shape complementarity function (<strong>PSC</strong>) by distance cut-off of receptor-ligand atom minus clash penalty.<ul><li>Favorable: Number of pair within cutoff</li><li>Penalty: The clash penalty for core-core, surface-core, and surface-surface (9<sup>9</sup>, 9<sup>3</sup>, 9)</li></ul></li><li><strong>DE</strong>: desolvation, estimated by atomic contact energy (<strong>ACE</strong>), which is a free energy change of breaking two protein atom-water contacts and forming a protein atom-protein atom contact and water-water contact. The sum of <strong>ACE</strong> is <strong>DE</strong></li></ul><p>Version for scoring functions:</p><ul><li><strong>ZDOCK1.3</strong><sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup>: GSC+DE+ELEC</li><li><strong>ZDOCK2.1</strong><sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup>: PSC</li><li><strong>ZDOCK2.2</strong><sup class="footnote-ref"><a href="#fn9" id="fnref9:1">[9:1]</a></sup>: PSC+DE</li><li><strong>ZDOCK2.3</strong><sup class="footnote-ref"><a href="#fn9" id="fnref9:2">[9:2]</a></sup>: PSC+DE+ELEC</li></ul><h3 id="2004-ClusPro">2004: ClusPro</h3><p><a href="https://academic.oup.com/nar/article/32/suppl_2/W96/1040440">ClusPro: a fully automated algorithm for protein–protein docking</a></p><h3 id="2010-Hex">2010: Hex</h3><p><a href="https://academic.oup.com/bioinformatics/article/26/19/2398/229220">Ultra-fast FFT protein docking on graphics processors</a></p><p><a href="https://hex.loria.fr/">Home page</a>, <a href="https://hex.loria.fr/manual800/hex_manual.html">Documentation</a></p><p>Hex is extremely fast but lack of accuracy. I tried to sampling over 100,1000 but results even close to native structure.<br>On the other hand, I didn’t find a way to mark the surface residues so we could focus on specific area. Although, GhatGPT said it could do constrained docking, but it seems we could only constrain the range angles of the receptor and the ligand.</p><table><thead><tr><th style="text-align:center"><img src="https://documentation.samson-connect.net/tutorials/hex/images/hex-results-animation.gif" alt="Hex Dock in SAMSON"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://documentation.samson-connect.net/tutorials/hex/protein-docking-with-hex/">© SAMSON</a></td></tr></tbody></table><h3 id="2014-rDock">2014: rDock</h3><p><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003571">rDock: a fast, versatile and open source program for docking ligands to proteins and nucleic acids</a></p><h3 id="2018-InterEvDock">2018: InterEvDock</h3><p><a href="https://link.springer.com/protocol/10.1007/978-1-4939-7759-8_28">Protein-Protein Docking Using Evolutionary Information</a></p><h2 id="Machine-Learning-Based-Docking">Machine Learning Based Docking</h2><h3 id="2021-DeepRank">2021: DeepRank</h3><table><thead><tr><th style="text-align:center">Model Grpah Abstract</th><th style="text-align:left">Model Name</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-021-27396-0/MediaObjects/41467_2021_27396_Fig1_HTML.png" alt="DeepRank"><br>© Chen, M., &amp; Zhou, X</td><td style="text-align:left">DeepRank</td></tr><tr><td style="text-align:center"><img src="https://raw.githubusercontent.com/DeepRank/Deeprank-GNN/master/deeprank_gnn.png" alt="DeepRank-GNN"><br>© Réau, M.</td><td style="text-align:left">DeepRank-GNN</td></tr><tr><td style="text-align:center"><img src="https://github.com/DeepRank/deeprank2/raw/main/deeprank2.png" alt="DeepRank2"><br>© Crocioni, G.</td><td style="text-align:left">Deeprank2</td></tr></tbody></table><p><strong>DeepRank</strong><sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup> is a <a href="https://github.com/DeepRank/deeprank">open source</a> framework designed to analyze 3D protein-protein interfaces by using deep learning to capture spatial and biochemical features. The paper presents DeepRank’s approach to transforming 3D structural data into 3D grids that a neural network can process. This setup allows DeepRank to identify interaction patterns, rank docking models, and predict binding affinities with high accuracy. It’s especially useful for discovering patterns in protein interfaces that might be overlooked with traditional scoring functions.</p><p>In this model, it turn the <strong>pdb into sql</strong> for efficient processing. The interfacing residues <strong>cut-off is 5.5 Å</strong>. When find all interfacing atoms, they would be mapped into **3D grid using a <strong>Gaussian mapping</strong>. The target value is very flexible, too. You can using any kind of values, iRMSD, FNAT, or DockQ score for instance, as the target values (Predicted value). The data was stored as <strong>hdf5</strong> format which keep the efficiency and small storage size.</p><p>DeepRank family:</p><ul><li><strong>DeepRank</strong><sup class="footnote-ref"><a href="#fn16" id="fnref16:1">[16:1]</a></sup>: 2021, Chen, M., et al.; It mapped the protein interfacing into a 3D grid and using CNN to train the regression model. It established the foundation of the architectural of DeepRank.<ul><li>In the DeepRank, it use information both from atom-level and residue-level. From the atom level, it calculates the atom density, charges, electrostatic energy, and VDW contacts. In residue-level, it included number of residue-residue contacts, buried surface area, and Position specific scoring matrix (PSSM)</li></ul></li><li><strong>DeepRank-GNN</strong><sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup>: 2023, Réau, M. et al.; from the same team replace the 3D grid based CNN into GNN which could avoid rotation challenge in 3D grid.<ul><li>The input information is very similar to the DeepRank. Instead of 3D grid, it relies on the adjacent matrix to build the network. In this time, the cut-off became 8.5 Å.</li><li>It has more rich features like Distance, residue half sphere exposure, Residue depth (from biopython, MSMS)</li></ul></li><li><strong>Deeprank_GNN_ESM</strong><sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup>: 2024, Xu, X., et al.; The <strong>PSSM</strong> calculating requires sequence alignment which consumes lots of time. For generate the graph efficiently, they replaced the <strong>PSSM</strong> with <strong>ESM</strong> embedding vectors.</li><li><strong>DeepRank2</strong><sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup>: 2024, Crocioni, G., et al.; In the DeepRank2., it supports both 3D grid and graph network as inputs. It also integrated the <a href="https://github.com/DeepRank/DeepRank-Mut">Deep-Mut</a> to do in silicon mutation screening.</li></ul><h3 id="DockQ">DockQ</h3><ul><li><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0161879">DockQ: a quality measure for protein–protein docking models</a></li><li><a href="https://academic.oup.com/bioinformatics/article/40/10/btae586/7796530">DockQ v2: improved automatic quality measure for protein multimers, nucleic acids, and small molecules</a></li></ul><p>DockQ is a machine learning based docking evaluation tool. It divided the docking results into 4 categories: Incorrect, Acceptable, Medium, or High quality. The score it uses is: F<sub>nat</sub>, LRMS, and iRMS as proposed and standardized by CAPRI. The training set are extremely in balanced. It has over 56,000 incorrect docking, 760 acceptable, 850 mdeium, and 74 high quality. w</p><h2 id="Online-Tools">Online Tools</h2><ul><li><a href="https://abemap.cluspro.org/cobemap/index.php">ClusPro AbEMap</a>: The ClusPro AbEMap web server for the prediction of antibody epitopes</li><li><a href="https://cluspro.bu.edu">ClusterPro?</a></li><li><a href="https://life.bsc.es/pid/ccharppi">CCharPPI</a></li><li><a href="http://www.zzdlab.com/AbAgIntPre/">AbAgIntPre</a>: No structure, binary output results only</li><li><a href="https://biosig.lab.uq.edu.au/csm_ab/prediction">CSM-AB</a>: CSM-AB: graph-based antibody–antigen binding affinity prediction and docking scoring function</li></ul><h2 id="Other-tools">Other tools</h2><ul><li>[ZRANK2]</li></ul><h2 id="Other-Infor">Other Infor</h2><ul><li><a href="https://piercelab.ibbr.umd.edu/antibody_benchmark/">Antibody-Antigen Structures and affinities</a></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p><a href="https://www.sciencedirect.com/science/article/pii/002228368290153X">Kuntz I D, Blaney J M, Oatley S J, et al. A geometric approach to macromolecule-ligand interactions[J]. Journal of molecular biology, 1982, 161(2): 269-288.</a> <a href="#fnref1" class="footnote-backref">↩︎</a> <a href="#fnref1:1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p><a href="https://link.springer.com/article/10.1007/BF00124018">Grootenhuis P D J, Roe D C, Kollman P A, et al. Finding potential DNA-binding compounds by using molecular shape[J]. Journal of Computer-Aided Molecular Design, 1994, 8: 731-750.</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p><a href="https://link.springer.com/article/10.1023/A:1008066310669">Makino S, Ewing T J A, Kuntz I D. DREAM++: flexible docking program for virtual combinatorial libraries[J]. Journal of computer-aided molecular design, 1999, 13: 513-532.</a> <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p><a href="https://link.springer.com/article/10.1023/a:1011115820450">Ewing T J A, Makino S, Skillman A G, et al. DOCK 4.0: search strategies for automated molecular docking of flexible molecule databases[J]. Journal of computer-aided molecular design, 2001, 15: 411-428.</a> <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p><a href="https://link.springer.com/article/10.1007/s10822-006-9060-4">Moustakas D T, Lang P T, Pegg S, et al. Development and validation of a modular, extensible docking program: DOCK 5[J]. Journal of computer-aided molecular design, 2006, 20: 601-619.</a> <a href="#fnref5" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p><a href="https://bcrf.biochem.wisc.edu/all-tutorials/tutorial-materials-guests/185-2/">S. Pérez, C. Meyer, A. Imberty. “Practical tools for accurate modeling of complex carbohydrates and their interactions with proteins” A. Pullman, J. Jortner, B. Pullman (Eds.), Modelling of Biomolecular Structures and Mechanisms, Kluwer Academic Publishers, Dordrecht (1996), pp. 425-454.</a> <a href="#fnref6" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p><a href="https://rnajournal.cshlp.org/content/15/6/1219.short">Lang P T, Brozell S R, Mukherjee S, et al. DOCK 6: Combining techniques to model RNA–small molecule complexes[J]. Rna, 2009, 15(6): 1219-1230.</a> <a href="#fnref7" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0075992">Coleman R G, Carchia M, Sterling T, et al. Ligand pose and orientational sampling in molecular docking[J]. PloS one, 2013, 8(10): e75992.</a> <a href="#fnref8" class="footnote-backref">↩︎</a></p></li><li id="fn9" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/12784371">Chen, R., Li, L., &amp; Weng, Z. (2003). ZDOCK: an initial‐stage protein‐docking algorithm. Proteins: Structure, Function, and Bioinformatics, 52(1), 80-87.</a> <a href="#fnref9" class="footnote-backref">↩︎</a> <a href="#fnref9:1" class="footnote-backref">↩︎</a> <a href="#fnref9:2" class="footnote-backref">↩︎</a></p></li><li id="fn10" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/17623839">Mintseris, J., Pierce, B., Wiehe, K., Anderson, R., Chen, R., &amp; Weng, Z. (2007). Integrating statistical pair potentials into protein complex prediction. Proteins: Structure, Function, and Bioinformatics, 69(3), 511-520.</a> <a href="#fnref10" class="footnote-backref">↩︎</a></p></li><li id="fn11" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/15613396">Pierce, B., Tong, W., &amp; Weng, Z. (2005). M-ZDOCK: a grid-based approach for C n symmetric multimer docking. Bioinformatics, 21(8), 1472-1478.</a> <a href="#fnref11" class="footnote-backref">↩︎</a></p></li><li id="fn12" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/21949741">Pierce, B. G., Hourai, Y., &amp; Weng, Z. (2011). Accelerating protein docking in ZDOCK using an advanced 3D convolution library. PloS one, 6(9), e24657.</a> <a href="#fnref12" class="footnote-backref">↩︎</a></p></li><li id="fn13" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/24532726">Pierce, B. G., Wiehe, K., Hwang, H., Kim, B. H., Vreven, T., &amp; Weng, Z. (2014). ZDOCK server: interactive docking prediction of protein–protein complexes and symmetric multimers. Bioinformatics, 30(12), 1771-1773.</a> <a href="#fnref13" class="footnote-backref">↩︎</a></p></li><li id="fn14" class="footnote-item"><p>Chen R, Weng Z. Docking unbound proteins using shape complementarity, desolvation, and electrostatics. Proteins 2002; 47: 281–294. <a href="#fnref14" class="footnote-backref">↩︎</a></p></li><li id="fn15" class="footnote-item"><p>Chen R, Weng Z. A novel shape complementarity scoring function for protein-protein docking. Proteins 2003; 51: 397–408. <a href="#fnref15" class="footnote-backref">↩︎</a></p></li><li id="fn16" class="footnote-item"><p><a href="https://www.nature.com/articles/s41467-021-27396-0">Renaud, N., Geng, C., Georgievska, S., Ambrosetti, F., Ridder, L., Marzella, D. F., … &amp; Xue, L. C. (2021). DeepRank: a deep learning framework for data mining 3D protein-protein interfaces. Nature communications, 12(1), 7068.</a> <a href="#fnref16" class="footnote-backref">↩︎</a> <a href="#fnref16:1" class="footnote-backref">↩︎</a></p></li><li id="fn17" class="footnote-item"><p><a href="https://academic.oup.com/bioinformatics/article/39/1/btac759/6845451">Réau, M., Renaud, N., Xue, L. C., &amp; Bonvin, A. M. (2023). DeepRank-GNN: a graph neural network framework to learn patterns in protein–protein interfaces. Bioinformatics, 39(1), btac759.</a> <a href="#fnref17" class="footnote-backref">↩︎</a></p></li><li id="fn18" class="footnote-item"><p>Xu, X., &amp; Bonvin, A. M. (2024). DeepRank-GNN-esm: a graph neural network for scoring protein–protein models using protein language model. Bioinformatics advances, 4(1), vbad191. <a href="#fnref18" class="footnote-backref">↩︎</a></p></li><li id="fn19" class="footnote-item"><p><a href="https://joss.theoj.org/papers/10.21105/joss.05983.pdf">Crocioni, G., Bodor, D. L., Baakman, C., Parizi, F. M., Rademaker, D. T., Ramakrishnan, G., … &amp; Xue, L. C. (2024). DeepRank2: Mining 3D Protein Structures with Geometric Deep Learning. Journal of Open Source Software, 9(94), 5983.</a> <a href="#fnref19" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">Protein Dock Tools and algorithm Overview</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Protein Structure" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Protein-Structure/"/>
    
    
    <category term="3D" scheme="https://karobben.github.io/tags/3D/"/>
    
    <category term="protein" scheme="https://karobben.github.io/tags/protein/"/>
    
    <category term="dock" scheme="https://karobben.github.io/tags/dock/"/>
    
  </entry>
  
  <entry>
    <title>Softmax</title>
    <link href="https://karobben.github.io/2024/10/09/AI/softmax/"/>
    <id>https://karobben.github.io/2024/10/09/AI/softmax/</id>
    <published>2024-10-09T22:46:15.000Z</published>
    <updated>2024-10-09T22:52:46.766Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Softmax">Softmax</h2><p>Key idea: $f_c(x) =$ posterior probability of cass $c$</p><ul><li>A perceptron has a one-hot output vector, in which $f_c(x) = 1$ if the<br>neural net thinks $c$ is the most likely value of $y$, and 0 otherwise</li><li>A softmax computes $f_c(x) \approx Pr(Y =c |x)$. The conditions for this to be true are:<ul><li>It needs to satisfy the axioms of probability:<br>$$ 0 \leq f_c(x) \leq 1, \quad \sum_{c=1}^{v} f_c(x) = 1$$</li><li>The weight matrix, $W$, is trained using a loss function that encourages $f(x)$ to approximate posterior probability of the labels on some training dataset:<br>$$f_c(x) \approx \Pr(Y = c|x)$$</li></ul></li></ul><h3 id="Softmax-satisfies-the-axioms-of-probability">Softmax satisfies the axioms of probability</h3><ul><li><p>Axiom #1, probabilities are non-negative $(f_k(x) \geq 0)$. There are many ways to do this, but one way that works is to choose:</p><p>$$<br>f_c(x) \propto \exp(w_c^T x + b_c)<br>$$</p></li><li><p>Axiom #2, probabilities should sum to one $(\sum_{k=1}^{v} f_k(x) = 1)$. This can be done by normalizing:</p></li></ul><p>$$<br>f(x) = [f_1(x), …, f_v(x)]^T<br>$$<br>$$<br>f_c(x) = \frac{\exp(w_c^T x + b_c)}{\sum_{k=0}^{v-1} \exp(w_k^T x + b_k)}<br>$$</p><p>where $w_k^T$ is the $k^{th}$ row of the matrix $W$.</p><h3 id="The-logistic-sigmoid-function">The logistic sigmoid function</h3><p>For a two-class classifier, we don’t really need the vector label. If we define $w = w_2 - w_1$ and $b = b_2 - b_1$, then the softmax simplifies to:</p><p>$$<br>f(Wx + b) =<br>\begin{bmatrix}<br>\text{Pr}(Y = 1|x) \\<br>\text{Pr}(Y = 2|x)<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\frac{1}{1+e^ {-(w^ Tx+b)}} \\<br>\frac{e^ {-(w^ Tx+b)}}{1+e^ {-(w^ Tx+b)}}<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\sigma(w^Tx + b) \\<br>1 - \sigma(w^Tx + b)<br>\end{bmatrix}<br>$$</p><p>… so instead of the softmax, we use a scalar function called the logistic sigmoid function:</p><p>$$<br>\sigma(z) = \frac{1}{1+e^{-z}}<br>$$</p><p>This function is called sigmoid because it is S-shaped.</p><p>For $z \to -\infty$, $\sigma(z) \to 0$</p><p>For $z \to +\infty$, $\sigma(z) \to 1$</p><h3 id="Gradient-descent">Gradient descent</h3><p>Suppose we have training tokens $(x_i, y_i)$, and we have some initial class vectors $w_1$ and $w_2$. We want to update them as</p><p>$$<br>w_1 \leftarrow w_1 - \eta \frac{\partial \mathcal{L}}{\partial w_1}<br>$$</p><p>$$<br>w_2 \leftarrow w_2 - \eta \frac{\partial \mathcal{L}}{\partial w_2}<br>$$</p><p>…where $\mathcal{L}$ is some loss function. What loss function makes sense?</p><h3 id="Zero-one-loss-function">Zero-one loss function</h3><p>The most obvious loss function for a classifier is its classification error rate,</p><p>$$<br>\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \ell(\hat{f}(x_i), y_i)<br>$$</p><p>Where $\ell(\hat{y}, y)$ is the zero-one loss function,</p><p>$$<br>\ell(f(x), y) =<br>\begin{cases}<br>0 &amp; \text{if } f(x) = y \\<br>1 &amp; \text{if } f(x) \neq y<br>\end{cases}<br>$$</p><p>The problem with zero-one loss is that it’s not differentiable.</p><h3 id="A-loss-function-that-learns-probabilities">A loss function that learns probabilities</h3><p>Suppose we have a softmax output, so we want $f_c(x) \approx \Pr(Y = c|x)$. We can train this by learning $W$ and $b$ to maximize the probability of the training corpus. If we assume all training tokens are independent, we get:</p><p>$$<br>W, b = \underset{W,b}{\text{argmax}} \prod_{i=1}^{n} \Pr(Y = y_i|x_i) = \underset{W,b}{\text{argmax}} \sum_{i=1}^{n} \ln \Pr(Y = y_i|x_i)<br>$$</p><p>But remember that $f_c(x) \approx \Pr(Y = c|x)$! Therefore, maximizing the log probability of training data is the same as minimizing the cross entropy between the neural net and the ground truth:</p><p>$$<br>W, b = \underset{W,b}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} \mathcal{L}_ i, \quad \mathcal{L}_ i = - \log f_ {y_ i}(x_ i)<br>$$</p><h3 id="Cross-entropy">Cross-entropy</h3><p>This loss function:</p><p>$$<br>\mathcal{L} = - \ln f_{y}(x)<br>$$</p><p>is called cross-entropy. It measures the difference in randomness between:</p><ul><li>Truth: $Y = y$ with probability 1.0, $\ln(1.0) = 0$, minus the</li><li>Neural net estimate: $Y = y$ with probability $f_{y}(x)$.</li></ul><p>Thus</p><p>$$<br>\mathcal{L} = 0 - \ln f_{y}(x)<br>$$</p><h3 id="Gradient-of-the-cross-entropy-of-the-softmax">Gradient of the cross-entropy of the softmax</h3><p>Since we have these definitions:</p><p>$$<br>\mathcal{L} = - \ln f_{y}(x), \quad f_{y}(x) = \frac{\exp(z_{y})}{\sum_{k=1}^{v} \exp(z_{k})}, \quad z_{c} = w_c^T x + b_c<br>$$</p><p>Then:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w_c} = \left( \frac{\partial \mathcal{L}}{\partial z_c} \right) \left( \frac{\partial z_c}{\partial w_c} \right) = \left( \frac{\partial \mathcal{L}}{\partial z_c} \right) x<br>$$</p><p>…where:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial z_c} =<br>\begin{cases}<br>f_{c}(x_i) - 1 &amp; c = y \\<br>f_{c}(x_i) &amp; c \neq y<br>\end{cases}<br>$$</p><h3 id="Similarity-to-linear-regression">Similarity to linear regression</h3><p>For linear regression, we had:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w} = \epsilon x, \quad \epsilon = f(x) - y<br>$$</p><p>For the softmax classifier with cross-entropy loss, we have</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w_c} = \epsilon_c x<br>$$</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y \text{ (output should be 1)} \\<br>f_c(x_i) &amp; \text{otherwise (output should be 0)}<br>\end{cases}<br>$$</p><h3 id="Similarity-to-perceptron">Similarity to perceptron</h3><p>Suppose we have a training token $(x, y)$, and we have some initial class vectors $w_c$. Using softmax and cross-entropy loss, we can update the weight vectors as</p><p>$$<br>w_c \leftarrow w_c - \eta \epsilon_c x<br>$$</p><p>…where</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y_i \\<br>f_c(x_i) &amp; \text{otherwise}<br>\end{cases}<br>$$</p><p>In other words, like a perceptron,</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>\epsilon_c &lt; 0 &amp; c = y_i \\<br>\epsilon_c &gt; 0 &amp; \text{otherwise}<br>\end{cases}<br>$$</p><h3 id="Outline">Outline</h3><ul><li><p>Softmax:<br>$$ f_c(x) = \frac{\exp(w_c^T x + b_c)}{\sum_{k=1}^{v} \exp(w_k^T x + b_k)} \approx \Pr(Y = c|x) $$</p></li><li><p>Cross-entropy:<br>$$ \mathcal{L} = - \ln f_{y}(x) $$</p></li><li><p>Derivative of the cross-entropy of a softmax:<br>$$ \frac{\partial \mathcal{L}}{\partial w_c} = \epsilon_c x, \quad \epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y \text{ (output should be 1)} \\<br>f_c(x_i) &amp; \text{otherwise (output should be 0)}<br>\end{cases} $$</p></li><li><p>Gradient descent:<br>$$ w_c \leftarrow w_c - \eta \epsilon_c x $$</p></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Softmax</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Support Vector Machine</title>
    <link href="https://karobben.github.io/2024/09/29/AI/supportvectormachine/"/>
    <id>https://karobben.github.io/2024/09/29/AI/supportvectormachine/</id>
    <published>2024-09-30T02:41:26.000Z</published>
    <updated>2024-10-09T22:55:34.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-Support-Vector-Machine">What is Support Vector Machine</h2><p>SVM was developed in the 1990s by Vladimir Vapnik and his colleagues. The development of SVM was rooted in statistical learning theory. It introduced the concept of finding the maximum margin hyperplane to separate classes effectively, with extensions to handle non-linear data through kernel functions. SVM gained popularity due to its ability to create powerful classifiers, especially in high-dimensional feature spaces.</p><h3 id="Compare-to-Random-Forest">Compare to Random Forest</h3><p>Random Forest is an ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting, handling both linear and non-linear data well. It’s good for large datasets and provides feature importance but is less interpretable.</p><p>SVM finds the optimal hyperplane to separate classes by maximizing the margin. It works well for smaller, high-dimensional datasets but is computationally expensive for large datasets and harder to interpret.</p><h3 id="Compare-to-Linear-Regression">Compare to Linear Regression</h3><p>The decision function of <strong>Support Vector Machine (SVM)</strong> looks very similar to a <strong>linear function</strong>—and indeed, it shares common elements with <strong>linear regression</strong>. However, the main differences lie in their objectives and the way they handle data:</p><h4 id="Similarities">Similarities</h4><ul><li><strong>Linear Function Form</strong>: Both SVM and Linear Regression use a linear function of the form:<br>$$<br>f(x) = w_1 x_1 + w_2 x_2 + \cdots + w_k x_k + b<br>$$<br>Where $ w_i $ are the weights, $ x_i $ are the features, and $ b $ is the bias term.</li><li><strong>Weight Optimization</strong>: Both models optimize the weights ($ w $) to achieve their goals.</li></ul><h4 id="Key-Differences">Key Differences</h4><ol><li><p><strong>Objective Function</strong>:</p><ul><li><strong>Linear Regression</strong>: The goal is to <strong>minimize the error</strong> (typically the mean squared error) between predicted and actual values. It aims to find the line (or hyperplane) that best fits the data points by minimizing the difference between predictions and true values.</li><li><strong>SVM</strong>: The goal is to <strong>maximize the margin</strong> between different classes. SVM seeks to find a hyperplane that not only separates the classes but does so with the largest possible gap between the nearest points of each class (called <strong>support vectors</strong>). This makes the decision boundary as robust as possible against errors or noise.</li></ul></li><li><p><strong>Loss Function</strong>:</p><ul><li><strong>Linear Regression</strong>: Uses <strong>squared loss</strong> to penalize errors, which means that even small deviations contribute to the overall loss.</li><li><strong>SVM</strong>: Uses a <strong>hinge loss function</strong> for classification, which penalizes misclassifications and ensures a margin of separation. The loss function focuses more on correctly classifying data points with maximum confidence.</li></ul></li><li><p><strong>Problem Type</strong>:</p><ul><li><strong>Linear Regression</strong>: Primarily used for <strong>regression</strong> problems, where the goal is to predict a continuous output.</li><li><strong>SVM</strong>: Primarily used for <strong>classification</strong> (though it can be adapted for regression as <strong>SVR</strong>), where the goal is to classify data points into different categories. In SVM, the function output is interpreted using a sign function, where:<br>$$<br>f(x) = w^T x + b \Rightarrow \text{classify as } \begin{cases}<br>+1, &amp; \text{if } f(x) &gt; 0 \\<br>-1, &amp; \text{if } f(x) &lt; 0<br>\end{cases}<br>$$</li></ul></li><li><p><strong>Margin and Support Vectors</strong>:</p><ul><li><strong>Linear Regression</strong>: There is no concept of a <strong>margin</strong> or <strong>support vectors</strong> in linear regression. It simply finds the line of best fit for all data points.</li><li><strong>SVM</strong>: Introduces the concept of <strong>margin</strong>, which is the distance between the hyperplane and the closest data points from each class. These closest points are called <strong>support vectors</strong>, and they are crucial to defining the decision boundary.</li></ul></li><li><p><strong>Use of Kernels (Non-linearity)</strong>:</p><ul><li><strong>Linear Regression</strong>: Strictly a linear model. To handle non-linearity, you would have to explicitly add polynomial features or transform the features.</li><li><strong>SVM</strong>: Supports <strong>kernel tricks</strong> (such as polynomial or radial basis function kernels) to project data into higher dimensions, allowing it to separate data that isn’t linearly separable in its original space. This feature makes SVM more powerful for complex, non-linear classification problems.</li></ul></li></ol><h4 id="Summary">Summary</h4><ul><li><strong>Linear Regression</strong>: Minimizes prediction error for a best-fit line, used for regression.</li><li><strong>SVM</strong>: Maximizes the margin to find an optimal separating hyperplane, used for classification.</li><li>While both use linear functions, SVM is fundamentally about <strong>classification and margin maximization</strong>, whereas linear regression focuses on <strong>minimizing the difference between predicted and actual continuous values</strong>. SVM also handles non-linearity more effectively through kernels, making it more versatile for complex datasets.</li></ul><h2 id="Overview-of-SVM">Overview of SVM</h2><ul><li>Decision Boundary: $w^T x + b$.</li><li>Classification: $f(x) = sign(w^T x + b)$</li><li>Cost function: Training error cost + $\lambda$ penalty</li></ul><table><thead><tr><th><strong>Number of Features</strong></th><th><strong>Decision Boundary Equation</strong></th><th><strong>Classification Equation</strong></th></tr></thead><tbody><tr><td>1 Feature</td><td>$ w_1 x_1 + b = 0 $</td><td>$ f(x) = \text{sign}(w_1 x_1 + b) $</td></tr><tr><td>2 Features</td><td>$ w_1 x_1 + w_2 x_2 + b = 0 $</td><td>$ f(x) = \text{sign}(w_1 x_1 + w_2 x_2 + b) $</td></tr><tr><td>$ k $ Features</td><td>$ w_1 x_1 + w_2 x_2 + \cdots + w_k x_k + b = 0 $</td><td>$ f(x) = \text{sign}(w_1 x_1 + w_2 x_2 + \cdots + w_k x_k + b) $</td></tr></tbody></table><div class="admonition question"><p class="admonition-title">What does $w^T$ mean</p><p><strong>Explanation</strong>:</p><ul><li>A vector $ w $ is typically represented as a column vector, meaning it has multiple rows and a single column.</li><li>$ w^T $ is the <strong>transpose</strong> of $ w $, which means converting a column vector into a row vector, or vice versa.</li></ul><p><strong>Mathematical Notation</strong>:</p><ul><li>If $ w $ is a column vector with elements:$$w = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}$$</li><li>Then the <strong>transpose</strong> $ w^T $ is w row vector:$$w^T = \begin{bmatrix} w_1 &amp; w_2 &amp; \cdots &amp; w_n \end{bmatrix}$$In <strong>SVM</strong> or <strong>machine learning</strong>, the transpose is often used to indicate a <strong>dot product</strong> operation when combined with another vector or matrix. For example, if you have: $w^T x $, it means you're calculating the <strong>dot product</strong> of vector $ w $ and vector $ x $, which is a scalar value used in calculating distances, projections, or in constructing decision boundaries in algorithms like SVM.</li></ul></div><h3 id="Features">Features</h3><p>$$<br>f(\mathbf{x}) = \text{sign}(\mathbf{w}^T \mathbf{x} + b)<br>$$</p><p>where</p><p>$$<br>\mathbf{x} = \begin{bmatrix} x_0 \ x_1 \end{bmatrix}, \quad \mathbf{w} = \begin{bmatrix} w_0 \ w_1 \end{bmatrix}, \quad \text{and} \quad b \text{ is a scalar.}<br>$$</p><p><strong>Boundary</strong></p><p>The boundary condition is given by:</p><p>$$<br>\begin{bmatrix} w_0 &amp; w_1 \end{bmatrix} \begin{bmatrix} x_0 \ x_1 \end{bmatrix} + b = 0<br>$$</p><p>Solving for $ x_1 $:</p><p>$$<br>w_0 x_0 + w_1 x_1 + b = 0<br>$$</p><p>$$<br>x_1 = -\frac{w_0}{w_1} x_0 - \frac{b}{w_1}<br>$$</p><p><strong>Classification</strong></p><p>The classification function is:</p><p>$$<br>y = \begin{cases}<br>1 &amp; \text{if } x_1 \geq -\frac{w_0}{w_1}x_0 - \frac{b}{w_1} \\<br>-1 &amp; \text{if } x_1 &lt; -\frac{w_0}{w_1}x_0 - \frac{b}{w_1}<br>\end{cases}<br>$$</p><h2 id="Training-Cost">Training Cost</h2><p>The training cost in SVM refers to the computational and resource-related costs involved in training the model, which is an important consideration when choosing an algorithm, especially for larger datasets. SVM’s training cost is influenced by its optimization problem, which involves finding the hyperplane that maximizes the margin while correctly classifying the training data (or with minimal misclassification for soft margins).</p><h3 id="Training-Cost-in-SVM">Training Cost in SVM</h3><ol><li><p><strong>Optimization Complexity</strong>:</p><ul><li>SVM training involves <strong>solving a quadratic optimization problem</strong> to find the best hyperplane.</li><li>This process is complex and takes more computation, especially with <strong>non-linear kernels</strong>.</li></ul></li><li><p><strong>Time Complexity</strong>:</p><ul><li><strong>Linear SVM</strong>: Training time is between $O(n * d)$ and $O(n^2 * d)$, where $ n $ is the number of data points and $ d $ is the number of features.</li><li><strong>Non-linear Kernel SVM</strong>: Training complexity is approximately $O(n^2)$ to $O(n^3)$, making it very expensive for large datasets.</li></ul></li><li><p><strong>Memory Usage</strong>:</p><ul><li>With kernels, SVM stores a <strong>kernel matrix</strong> of size $ n \times n $, which uses a lot of memory if $ n $ is large.</li></ul></li><li><p><strong>Support Vectors</strong>:</p><ul><li>More <strong>support vectors</strong> means more computation during both training and prediction. Complex datasets often need more support vectors.</li></ul></li></ol><h3 id="Why-Care-About-Training-Cost">Why Care About Training Cost?</h3><ul><li><strong>Scalability</strong>: SVM can become impractical for <strong>large datasets</strong> due to the high cost in terms of time and memory.</li><li><strong>Resources</strong>: It requires substantial <strong>CPU and memory</strong>, limiting its use on resource-constrained systems.</li><li><strong>Algorithm Selection</strong>: For small to medium datasets, SVM works well. For large datasets, other methods like <strong>Random Forest</strong> or <strong>SGD</strong> may be better.</li></ul><h3 id="Reducing-Training-Cost">Reducing Training Cost</h3><ol><li><strong>Linear SVM</strong>: Use for linearly separable data—it has lower complexity.</li><li><strong>Approximations</strong>: Use <strong>SGDClassifier</strong> or <strong>kernel approximations</strong> for faster training.</li><li><strong>Data Subset</strong>: Train on a <strong>smaller subset</strong> of data to speed up training.</li></ol><h3 id="Hinge-Loss">Hinge Loss</h3><table><thead><tr><th>Condition</th><th>Cost Function</th><th>Description</th></tr></thead><tbody><tr><td>$y_i \neq \text{sign}(\hat{y}_i)$</td><td>$C(y_ i, \hat{y}_ i) = |y_ i| + 1$</td><td>Large</td></tr><tr><td>$y_i = \text{sign}(\hat{y}_i)$ close</td><td>$C(y_ i, \hat{y}_ i) = |y_ i| + 1$</td><td>Medium</td></tr><tr><td>$y_i = \text{sign}(\hat{y}_i)$ away</td><td>$C(y_ i, \hat{y}_ i) = 0$</td><td>No cost</td></tr><tr><td>General Cost Function</td><td>$C(y_ i, \hat{y}_ i) = \max(0, 1 - y_ i \cdot \hat{y}_ i)$</td><td>-</td></tr></tbody></table><h2 id="Train-a-SVM">Train a SVM</h2><h3 id="Training-Error">Training Error</h3><ul><li>$ \frac{1}{N} \sum_{i=1}^N C(y_i, \hat{y}_ i)$<ul><li>$ = \frac{1}{N} \sum_{i=1}^N \max(0, 1 - y_ i \cdot \hat{y}_ i) $</li><li>$ = \frac{1}{N} \sum_{i=1}^N \max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b)) $</li></ul></li></ul><h3 id="Cost-Function">Cost Function</h3><p>$$ S(\mathbf{w}, b; \lambda) = \frac{1}{N} \sum_{i=1}^N [\max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b))] + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w} $$</p><h3 id="Stochastic-Gradient-Descent">Stochastic Gradient Descent</h3><p>In training a Support Vector Machine (SVM), the primary objective is to minimize the cost function. This cost function often includes terms that measure the classification error and possibly a regularization term. The minimization of the cost function aims to find the best hyperplane that separates the classes while also considering the margin maximization between different classes and controlling model complexity to prevent overfitting.</p><p>$$<br>\mathbf{u} = \begin{bmatrix} \mathbf{w} \ b \end{bmatrix}<br>$$</p><p><strong>Minimize cost function:</strong></p><p>$$<br>g(\mathbf{u}) = \left[ \frac{1}{N} \sum_{i=1}^N g_i(\mathbf{u}) \right] + g_0(\mathbf{u})<br>$$</p><p>where:</p><p>$$<br>g_i(\mathbf{u}) = \max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b))<br>$$</p><p>and:</p><p>$$<br>g_0(\mathbf{u}) = \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}<br>$$</p><p><strong>Iteratively, at step $(n)$:</strong></p><ul><li>Compute descent direction $p^{(n)}$ and step size $\eta$</li><li>Ensure that $g(u^{(n)} + \eta p^{(n)}) \leq g(u^{(n)})$</li><li>Update $u^{(n+1)} = u^{(n)} + \eta p^{(n)}$</li></ul><p><strong>Descent direction:</strong></p><p>$$<br>p^{(n)} = -\nabla g(\mathbf{u}^{(n)})<br>$$</p><p>$$<br>= -\left( \frac{1}{N} \sum_{i=1}^N \nabla g_i(\mathbf{u}) + \nabla g_0(\mathbf{u}) \right)<br>$$</p><p><strong>Estimation through mean of batch:</strong></p><p>$$<br>p^{(n)}_ {N_ b} = -\left( \frac{1}{N_b} \sum_ {j \in \text{batch}} \nabla g_ j(\mathbf{u}) + \nabla g_ 0(\mathbf{u}) \right)<br>$$</p><p><strong>Epoch</strong></p><ul><li>One pass on training set of size $N$</li><li>Each step sees a batch of $N_b$ items</li><li>The dataset is covered in $\frac{N}{N_b}$ steps</li><li>Step size in epoch $e$: $\eta^{(e)} = \frac{m}{e + l}$<ul><li>Constants $m$ and $l$: tune on small subsets</li></ul></li></ul><p><strong>Season</strong></p><ul><li>Constant number of iterations, much smaller than epochs</li><li>Each step sees a batch of $N_b$ items</li><li>Step size in season $s$: $\eta^{(s)} = \frac{m}{s + l}$<ul><li>Constants $m$ and $l$: tune on small subsets</li></ul></li></ul><h3 id="Full-SGD">Full SGD</h3><ul><li><p><strong>Vector u and its gradient:</strong><br>$$ \mathbf{u} = \begin{bmatrix} u_1 \\ \vdots \\ u_d \end{bmatrix}, \quad \nabla g = \begin{bmatrix} \frac{\partial g}{\partial u_1} \\ \vdots \\ \frac{\partial g}{\partial u_d} \end{bmatrix} $$</p></li><li><p><strong>Batches of 1 sample at each training step:</strong><br>$$ N_b = 1 $$</p></li><li><p><strong>Gradient of g(u):</strong><br>$$ \nabla g(\mathbf{u}) = \nabla \left( \max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b)) + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w} \right) $$</p></li><li><p><strong>Update rules for a and b:</strong><br>$$ \begin{bmatrix} \mathbf{w}^{(n+1)} \ b^{(n+1)} \end{bmatrix} = \begin{bmatrix} \mathbf{w}^{(n)} \ b^{(n)} \end{bmatrix} - \eta \begin{bmatrix} \nabla_{\mathbf{w}} \ \nabla_{b} \end{bmatrix} $$</p></li><li><p><strong>Condition for correct classification away from the boundary:</strong><br>$$ y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) \geq 1. \quad \text{Correct, away from boundary} $$<br>$$ \nabla_ {\mathbf{w}} (0 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = \lambda \mathbf{w}, \quad \nabla_ {b}(0 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = 0 $$</p></li><li><p><strong>Condition for classification close to the boundary or incorrect:</strong><br>$$ y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) &lt; 1. \quad \text{Correct, close to boundary, or incorrect} $$<br>$$ \nabla_ {\mathbf{w}} (1 - y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = -y_i \mathbf{x}_ i + \lambda \mathbf{w} $$<br>$$ \nabla_ {b} (1 - y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = -y_i $$</p></li></ul><h3 id="Stops">Stops</h3><p>Stop when</p><ul><li>predefined number of seasons or epochs</li><li>error on held-out data items is smaller than some threshold</li><li>other criteria</li></ul><p><strong>Regularization Constant $ \lambda $</strong></p><ul><li><p>Regularization constant $ \lambda $ in $ g(\mathbf{u}) = \frac{1}{2} \lambda \mathbf{w}^T \mathbf{w} $. Try at different scales (e.g., $ \lambda \in {10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1} $)</p></li><li><p><strong>Procedure for Cross-Validation:</strong></p><ul><li>Split dataset into Test Set and Train Set for cross-validation.</li><li>For each $ \lambda_i $ in set to try, iteratively:<ul><li>Generate a new Fold from Train Set with a Cross-Validation Train Set and Validation Set.</li><li>Using testing $ \lambda_i $, apply Stochastic Gradient Descent (SGD) on Cross-Validation Train Set to find $ \mathbf{w} $ and $ b $.</li><li>Evaluate $ \mathbf{w} $, $ b $, $ \lambda_i $ on Validation Set and record error for current Fold.</li><li>Cross-validation error for chosen $ \lambda_i $ is average error over all the Folds.</li></ul></li><li>Using $ \lambda $ with the lowest cross-validation error, apply SGD on whole training set to get final $ \mathbf{w} $ and $ b $.</li></ul></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Support vector machien is a very commonly used in machine learning</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
</feed>
