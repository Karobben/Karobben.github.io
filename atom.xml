<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Karobben</title>
  
  <subtitle>Engjoy~</subtitle>
  <link href="https://karobben.github.io/atom.xml" rel="self"/>
  
  <link href="https://karobben.github.io/"/>
  <updated>2024-02-14T06:22:54.030Z</updated>
  <id>https://karobben.github.io/</id>
  
  <author>
    <name>Karobben</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hidden Markov Model</title>
    <link href="https://karobben.github.io/2024/02/12/LearnNotes/ai-hmm/"/>
    <id>https://karobben.github.io/2024/02/12/LearnNotes/ai-hmm/</id>
    <published>2024-02-12T06:59:06.000Z</published>
    <updated>2024-02-14T06:22:54.030Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://imgur.com/oMmL1Ln.png" alt=""></p><p>A Hidden Markov Model is a Bayes Network with these assumptions:<br>‚Ä¢ <em>Y<sub>t</sub></em> depends only on <em>Y<sub>t-1</sub></em><br>‚Ä¢ <em>X<sub>t</sub></em> depends only on <em>Y<sub>t</sub></em></p><p>The belief network conveys the independence assumption:<br>$$<br>for\ all\ i \geq 0, P(S_{i+1}|S_i) = P (S_1|S_0)<br>$$</p><p>$$<br>P(S_i = s) = \sum_{s‚Äô} P(S_{i+1} = s \mid S_i = s‚Äô) * P(S_i = s‚Äô)<br>$$</p><p>In the context of the equation you‚Äôre referring to, ( s ) and ( s‚Äô ) represent states in a Markov chain. Typically, ( s ) is used to denote the current state, while ( s‚Äô ) (read as ‚Äús prime‚Äù) denotes a subsequent or different state that the system can transition into from the current state ( s ).</p><p>The summation over ( s‚Äô ) in the equation indicates that you‚Äôre summing over all possible subsequent states that the system can transition to from the current state ( s ). This is part of the definition of a stationary distribution for a Markov chain, where the probability of being in any given state ( s ) is equal to the sum of the probabilities of transitioning to state ( s ) from all possible previous states ( s‚Äô ), weighted by the probability of being in state ( s‚Äô ) at the previous time step.</p><h3 id="Key-advantage-of-a-hidden-Markov-model-Polynomial-time-complexity">Key advantage of a hidden Markov model: Polynomial-time complexity</h3><ul><li>Suppose there are <em>|y|</em> different speech sounds in English, and the length of the utterance is <em>d</em> centiseconds (<em>|y| ‚âà 50, d ‚âà 100</em>)</li><li>Without the HMM assumptions, to compute <em>f(x)= argmaxP(y<sub>1</sub>, ‚Ä¶ , y<sub>d</sub>|x<sub>1</sub>, ‚Ä¶ , x<sub>d</sub>)</em> requires a time complexity of<br><em>O{|y|<sup>d</sup>} ‚âà 50<sup>100</sup></em></li><li>With an HMM, each variable has only one parent, so inference is <em>O{|y|<sup>d</sup>} ‚âà 50<sup>2</sup></em></li><li>The computationally efficient algorithm that we use to compute <em>f(x)= argmaxP(y<sub>1</sub>, ‚Ä¶ , y<sub>d</sub>|x<sub>1</sub>, ‚Ä¶ , x<sub>d</sub>)</em> is called the Viterbi algorithm, named after the electrical engineer who first applied it to error correction coding.</li></ul><div class="admonition note"><p class="admonition-title">it works much better than bayes</p><p>Text generated by a na√Øve Bayes model (unigram model):</p><ul><li>Representing and speedily is an good apt or come can different natural here he the a in came the to of to expert gray come to furnishes the line message had be these‚Ä¶</li></ul><p>Text generated by a HMM (bigram model):</p><ul><li>The head and in frontal attack on an English writer that the character of this point is therefore another for the letters that the time of who ever told the problem for an unexpected‚Ä¶</li></ul></div><h3 id="Applications-of-HMMs">Applications of HMMs</h3><ul><li>Speech recognition HMMs:<ul><li>Observations are acoustic signals (continuous valued)</li><li>States are specific positions in specific words (so, tens of thousands)</li></ul></li><li>Machine translation HMMs:<ul><li>Observations are words (tens of thousands)</li><li>States are cross-lingual alignments</li></ul></li><li>Robot tracking:<ul><li>Observations are range readings (continuous)</li><li>States are positions on a map</li></ul></li></ul><h3 id="Viterbi-Algorithm">Viterbi Algorithm</h3><p>The Viterbi algorithm is a computationally efficient algorithm for computing the maximum a posteriori (MAP) state sequence<br>$$<br>f(x)= argmax_{y_1, ‚Ä¶ , y_d}P(y_1, ‚Ä¶ , y_d|x_1, ‚Ä¶ , x_d)<br>$$</p><style>pre {  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Hidden Markov Model</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Basic Mathematics Calculating</title>
    <link href="https://karobben.github.io/2024/02/11/LearnNotes/math-cal/"/>
    <id>https://karobben.github.io/2024/02/11/LearnNotes/math-cal/</id>
    <published>2024-02-12T04:58:46.000Z</published>
    <updated>2024-02-14T06:22:54.034Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sum">Sum</h2><p>The sum symbol, represented by the Greek letter sigma (Œ£), is widely used in mathematics to denote the summation of a sequence of numbers or expressions. When you see this symbol, it means you should add up a series of numbers according to the specified rule. Here‚Äôs a breakdown of how it‚Äôs typically used:</p><h3 id="Basic-Structure">Basic Structure</h3><p>The summation symbol is written as:</p><p>$$<br>\sum_{i=a}^{b} f(i)<br>$$</p><p>where:</p><ul><li>$i$ is the index of summation, which takes on each integer value from $a$ to $b$, inclusive.</li><li>$a$ is the lower limit of summation, the starting value of $i$.</li><li>$b$ is the upper limit of summation, the ending value of $i$.</li><li>$f(i)$ is the function of $i$ to be summed over the range from $a$ to $b$.</li></ul><h3 id="Examples">Examples</h3><ol><li><p><strong>Sum of the first 5 natural numbers</strong>:<br>$$<br>\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15<br>$$<br>Here, $f(i) = i$, and you sum the values of $i$ from 1 to 5.</p></li><li><p><strong>Sum of the squares of the first 3 positive integers</strong>:<br>$$<br>\sum_{i=1}^{3} i^2 = 1^2 + 2^2 + 3^2 = 1 + 4 + 9 = 14<br>$$<br>In this example, $f(i) = i^2$, so you square each $i$ from 1 to 3 and then add them together.</p></li><li><p><strong>Sum of a constant over a range</strong>:<br>Suppose you want to add the number 4, five times. The expression would be:<br>$$<br>\sum_{i=1}^{5} 4 = 4 + 4 + 4 + 4 + 4 = 20<br>$$</p><p>Here, $f(i) = 4$, which doesn‚Äôt depend on $i$. You‚Äôre essentially multiplying 4 by the number of terms (5 in this case).</p></li><li><p><strong>Two sums</strong><br>$$<br>\sum_{i=1}^ {5}\sum_{j=2}^ {6} ij<br>$$<br>For this, you sum over $j$ from 2 to 6 for each value of $i$ from 1 to 5, and then sum those results. It‚Äôs like computing a series within another series. The operation proceeds as follows:</p><ol><li>First, fix $i$ at its starting value, 1.</li><li>Then, for $i = 1$, sum over $j$ from 2 to 6, calculating $1 \cdot j$ for each $j$ and adding them together.</li><li>Repeat this process for each value of $i$ up to 5.</li><li>Finally, sum all the results from the inner summations together.</li></ol><p>Let‚Äôs compute this step-by-step to see the result.<br>The result of the double summation $\sum_{i=1}^ {5}\sum_{j=2}^ {6} ij$ is 300. This means that when you sum the product of $i$ and $j$ for each $i$ from 1 to 5 and each $j$ from 2 to 6, the total sum is 300.<br>PS: in python:</p> <div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">N= <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>):<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>,<span class="hljs-number">7</span>):<br>        N += i*j<br></code></pre></td></tr></table></figure></div></li></ol><h3 id="How-to-Use">How to Use</h3><ul><li><strong>Identify the sequence</strong> you need to sum. This could be a series of numbers, functions of an index, or even a constant value repeated several times.</li><li><strong>Determine the starting and ending indices</strong> ($a$ and $b$, respectively) for your summation.</li><li><strong>Write down the function or value</strong> to be summed as $f(i)$ for each $i$ in the range from $a$ to $b$.</li><li><strong>Compute each term</strong> in the series and <strong>add them together</strong> to find the total sum.</li></ul><p>Summation notation is a powerful tool in mathematics, especially for dealing with sequences and series, and it‚Äôs widely used in various fields such as statistics, physics, and finance.</p><h2 id="Product-Notation">Product Notation</h2><p>Similarly, we have <mark>product notation</mark>, too. The product symbol is represented by the Greek letter pi (Œ†), not to be confused with the mathematical constant $\pi$ (pi) used for the ratio of a circle‚Äôs circumference to its diameter. The product symbol is used to denote the multiplication of a sequence of numbers or expressions, just like the sum symbol is used for addition.</p><p>$$<br>\prod_{i=a}^{b} f(i)<br>$$</p><p>where:</p><ul><li>$i$ is the index of multiplication, taking on each integer value from $a$ to $b$, inclusive.</li><li>$a$ is the lower limit of the product, the starting value of $i$.</li><li>$b$ is the upper limit of the product, the ending value of $i$.</li><li>$f(i)$ is the function of $i$ to be multiplied over the range from $a$ to $b$.</li></ul><h3 id="Examples-v2">Examples</h3><ol><li><p><strong>Product of the first 5 natural numbers</strong> (also known as $5!$, factorial of 5):</p><p>$$<br>\prod_{i=1}^{5} i = 1 \times 2 \times 3 \times 4 \times 5 = 120<br>$$</p><p>This multiplies the values of $i$ from 1 to 5.</p></li></ol><p>In mathematics and particularly in machine learning, besides the summation (Œ£) and product (Œ†) notations, another frequently used notation is the integral symbol (‚à´). While the summation and product notations deal with discrete sequences, the integral symbol is used for continuous functions and is fundamental in calculus. Integrals play a crucial role in various aspects of machine learning, especially in optimization, probability distributions, and understanding the area under curves (such as ROC curves).</p><h2 id="Integral-Notation">Integral Notation</h2><p>The basic structure of an integral is:</p><p>$$<br>\int_{a}^{b} f(x) , dx<br>$$</p><p>where:</p><ul><li>$a$ and $b$ are the lower and upper limits of integration, respectively, defining the interval over which the function $f(x)$ is integrated.</li><li>$f(x)$ is the function to be integrated over $x$.</li><li>$dx$ represents an infinitesimally small increment of $x$, indicating that the integration is performed with respect to $x$.</li></ul><h3 id="Importance-in-Machine-Learning">Importance in Machine Learning</h3><ol><li><p><strong>Optimization</strong>: Many machine learning models involve optimization problems where the goal is to minimize or maximize some function (e.g., a loss function in neural networks or a cost function in logistic regression). Integrals are essential in solving continuous optimization problems, especially when calculating gradients or understanding the behavior of functions over continuous intervals.</p></li><li><p><strong>Probability Distributions</strong>: In the context of probabilistic models and statistics, integrals are used to calculate probabilities, expected values, and variances of continuous random variables. For example, the area under the probability density function (PDF) of a continuous random variable over an interval gives the probability of the variable falling within that interval.</p></li><li><p><strong>Feature Extraction and Signal Processing</strong>: In machine learning applications involving signal processing or feature extraction from continuous data, integrals are used to calculate various features and transform signals into more useful forms.</p></li><li><p><strong>Kernel Methods</strong>: In machine learning, kernel methods (e.g., support vector machines) utilize integrals in the formulation of kernel functions, which are essential in mapping input data into higher-dimensional spaces for classification or regression tasks.</p></li><li><p><strong>Deep Learning</strong>: In the training of deep neural networks, integrals may not be explicitly visible but are conceptually present in the form of continuous optimization and in the calculation of gradients during backpropagation.</p></li></ol><h3 id="Example">Example</h3><p>Consider the problem of finding the area under a curve, which is a fundamental concept in machine learning for evaluating model performance (e.g., calculating the area under the ROC curve (AUC) for classification problems). If $f(x)$ represents the curve, the area under $f(x)$ from $a$ to $b$ can be computed by the integral:</p><p>$$<br>\text{Area} = \int_{a}^{b} f(x) , dx<br>$$</p><h2 id="Other-Frequently-Used-Notations">Other Frequently Used Notations</h2><p>This integral computes the total area under $f(x)$ between $a$ and $b$, providing a measure of the model‚Äôs performance over that interval.</p><p>Integrals, along with summation and product notations, form the backbone of many mathematical operations in machine learning, from theoretical underpinnings to practical applications in data analysis, model evaluation, and optimization strategies.</p><p>Beyond summation (Œ£), product (Œ†), and integral (‚à´) notations, there are several other mathematical symbols and concepts that are frequently used in machine learning and statistics. These include:</p><h3 id="Gradient-‚àá">Gradient (‚àá)</h3><p>The gradient is a vector operation that represents the direction and rate of the fastest increase of a scalar function. In machine learning, the gradient is crucial for optimization algorithms like gradient descent, which is used to minimize loss functions. The gradient of a function $f(x_1, x_2, \ldots, x_n)$ with respect to its variables is denoted by:</p><p>$$<br>\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)<br>$$</p><h3 id="Partial-Derivative-‚àÇ">Partial Derivative (‚àÇ)</h3><p>The partial derivative represents the rate of change of a function of multiple variables with respect to one of those variables, keeping the others constant. It‚Äôs denoted by the symbol ‚àÇ. Partial derivatives are essential in the calculation of gradients and in the optimization of machine learning models.</p><h3 id="Expectation-E">Expectation (E)</h3><p>The expectation or expected value of a random variable is a fundamental concept in probability and statistics, denoted by $E[X]$ for a random variable $X$. It represents the average or mean value that $X$ takes over its probability distribution and is crucial in understanding the behavior of models, especially in probabilistic settings.</p><h3 id="Variance-Var-and-Standard-Deviation-œÉ">Variance (Var) and Standard Deviation (œÉ)</h3><p>Variance measures the spread of a random variable‚Äôs values and is denoted by $Var(X)$ or $\sigma^2$ for a random variable $X$. The standard deviation, $\sigma$, is the square root of the variance and provides a measure of the dispersion of data points around their mean value. These concepts are vital in assessing the reliability and performance of models.</p><h3 id="Covariance-and-Correlation">Covariance and Correlation</h3><p>Covariance and correlation measure the relationship between two random variables. Covariance indicates the direction of the linear relationship between variables, while correlation measures both the strength and direction of this linear relationship. Understanding these relationships is essential in features selection and in modeling the interactions between variables.</p><h3 id="Big-O-Notation-O">Big O Notation (O)</h3><p>Big O notation is used to describe the computational complexity of algorithms, which is crucial in machine learning for understanding the scalability and efficiency of models and algorithms. For example, an algorithm with a complexity of $O(n^2)$ means its execution time or space requirements increase quadratically as the input size $n$ increases.</p><h3 id="Matrix-Notations-and-Operations">Matrix Notations and Operations</h3><p>Matrices and vectors are fundamental in machine learning for representing and manipulating data. Operations such as matrix multiplication, transpose, and inversion are essential for linear algebra, which underpins many machine learning algorithms, including neural networks, PCA (Principal Component Analysis), and SVMs (Support Vector Machines).</p><p>Each of these mathematical concepts plays a crucial role in the formulation, analysis, and implementation of machine learning algorithms. They provide the theoretical foundation for understanding model behavior, optimizing performance, and evaluating outcomes in a wide range of applications.</p><h2 id="Matrix-Calculating">Matrix Calculating</h2><p>Matrix multiplication is a fundamental operation in linear algebra with extensive applications in mathematics, physics, engineering, computer science, and particularly in machine learning and data analysis. The way matrix multiplication is defined‚Äîby taking the dot product of rows and columns‚Äîmight seem arbitrary at first, but it‚Äôs designed to capture several important mathematical and practical concepts.</p><p>Understanding how to perform basic operations with matrices‚Äîaddition, subtraction, multiplication, and division (in a sense)‚Äîis crucial in linear algebra, which is foundational for many areas of mathematics, physics, engineering, and especially machine learning. Here‚Äôs a brief overview of each operation:</p><p>$$<br>\begin{pmatrix}<br>a_{11} &amp; \cdots &amp; a_{1j} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>a_{i1} &amp; \cdots &amp; a_{ij}<br>\end{pmatrix}<br>$$</p><p>Each element within the matrix is a pair $(i,j)$, where $i$ is the row index and $j$ is the column index.</p><h3 id="Matrix-Addition-and-Subtraction">Matrix Addition and Subtraction</h3><p>Matrix addition and subtraction are straightforward operations that are performed element-wise. This means you add or subtract the corresponding elements of the matrices. For these operations to be defined, the matrices must be of the same dimensions.</p><ul><li><p><strong>Addition</strong>: If $A$ and $B$ are matrices of the same size, their sum $C = A + B$ is a matrix where each element $c_{ij}$ is the sum of $a_{ij} + b_{ij}$.</p></li><li><p><strong>Subtraction</strong>: Similarly, the difference $C = A - B$ is a matrix where each element $c_{ij}$ is the difference $a_{ij} - b_{ij}$.</p></li></ul><h4 id="Example-v2">Example:</h4><p>If $A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}$ and $B = \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix}$, then</p><ul><li>$A + B = \begin{pmatrix} 1+5 &amp; 2+6 \\ 3+7 &amp; 4+8 \end{pmatrix} = \begin{pmatrix} 6 &amp; 8 \\ 10 &amp; 12 \end{pmatrix}$</li><li>$A - B = \begin{pmatrix} 1-5 &amp; 2-6 \\ 3-7 &amp; 4-8 \end{pmatrix} = \begin{pmatrix} -4 &amp; -4 \\ -4 &amp; -4 \end{pmatrix}$</li></ul><h3 id="Matrix-Multiplication">Matrix Multiplication</h3><p>Matrix multiplication is more complex and involves a dot product of rows and columns. For two matrices $A$ and $B$ to be multiplied, the number of columns in $A$ must equal the number of rows in $B$. If $A$ is an $m \times n$ matrix and $B$ is an $n \times p$ matrix, the resulting matrix $C = AB$ will be an $m \times p$ matrix where each element $c_{ij}$ is computed as the dot product of the $i$th row of $A$ and the $j$th column of $B$.</p><h4 id="Example-v3">Example:</h4><p>If $A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}$ and $B = \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix}$, then</p><ul><li>$AB = \begin{pmatrix} (1√ó 5 + 2√ó 7) &amp; (1√ó 6 + 2√ó 8) \\ (3√ó 5 + 4√ó 7) &amp; (3√ó 6 + 4√ó 8) \end{pmatrix} = \begin{pmatrix} 19 &amp; 22 \\ 43 &amp; 50 \end{pmatrix}$</li></ul><h3 id="Matrix-Division">Matrix Division</h3><p>Matrix division as such doesn‚Äôt exist in the way we think of division for real numbers. Instead, we talk about the inverse of a matrix. For matrix $A$ to ‚Äúdivide‚Äù another matrix $B$, you would multiply $B$ by the inverse of $A$, denoted as $A^{-1}$. This operation is only defined for square matrices (same number of rows and columns), and not all square matrices have an inverse.</p><ul><li><strong>Multiplying by the Inverse</strong>: If you want to solve for $X$ in $AX = B$, you can multiply both sides by $A^{-1}$, assuming $A^{-1}$ exists, to get $X = A^{-1}B$.</li></ul><h4 id="Example-v4">Example:</h4><p>If $A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}$, and its inverse $A^{-1} = \begin{pmatrix} -2 &amp; 1 \\ 1.5 &amp; -0.5 \end{pmatrix}$, and you want to ‚Äúdivide‚Äù $B = \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix}$ by $A$, you would compute $A^{-1}B$.</p><h3 id="Key-Points">Key Points</h3><ul><li><strong>Addition/Subtraction</strong>: Element-wise operation requiring matrices of the same dimensions.</li><li><strong>Multiplication</strong>: Involves the dot product of rows and columns, requiring the number of columns in the first matrix to equal the number of rows in the second.</li><li><strong>Division</strong>: Not directly defined, but involves multiplying by the inverse of a matrix.</li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Basic Mathematics Calculating</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Perceptron</title>
    <link href="https://karobben.github.io/2024/02/07/LearnNotes/ai-perceptron/"/>
    <id>https://karobben.github.io/2024/02/07/LearnNotes/ai-perceptron/</id>
    <published>2024-02-07T19:03:23.000Z</published>
    <updated>2024-02-14T06:22:54.030Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Perceptron">Perceptron</h2><p>Perceptron is invented before the loss function</p><p>Linear classifier: Notation<br>‚Ä¢ The observation x<sup>T</sup> = [x<sub>1</sub>, ‚Ä¶ , x<sub>n</sub>] is a real-valued vector (d is the number of feature dimensions)<br>‚Ä¢ The class label y ‚àà Y is drawn from some finite set of class labels.<br>‚Ä¢ Usually the output vocabulary, Y, is some set of strings. For<br>convenience, though, we usually map the class labels to a sequence<br>of integers, Y = [1, ‚Ä¶ , v} , where ÔøΩ is the vocabulary size</p><h2 id="Linear-classifier-Definition">Linear classifier: Definition</h2><p>A linear classifier is defined by</p><p>$$<br>f(x) = \text{argmax } Wx + b<br>$$</p><p>where:</p><p><img src="https://imgur.com/zuycCx8.png" alt=""></p><p>$w_k, b_k$ are the weight vector and bias corresponding to class $k$, and the argmax function finds the element of the vector $wx$ with the largest value.</p><p>There are a total of $v(d + 1)$ trainable parameters: the elements of the matrix $w$.</p><p><img src="https://imgur.com/undefined.png" alt=""></p><h1>Example</h1><p>Notice that in the two-class case, the equation</p><p>$$<br>f(x) = \text{argmax } Wx + b<br>$$</p><p>Simplifies to</p><p><img src="https://imgur.com/QAGB3Ur.png" alt=""></p><p>The class boundary is the line whose equation is</p><p>$$<br>(w_2 - w_1)^T x + (b_2 - b_1) = 0<br>$$</p><h2 id="Gradient-descent">Gradient descent</h2><h1>Gradient descent</h1><p>Suppose we have training tokens $(x_i, y_i)$, and we have some initial class vectors $w_1$ and $w_2$. We want to update them as</p><p>$$<br>w_1 \leftarrow w_1 - \eta \frac{\partial \mathcal{L}}{\partial w_1}<br>$$</p><p>$$<br>w_2 \leftarrow w_2 - \eta \frac{\partial \mathcal{L}}{\partial w_2}<br>$$</p><p>‚Ä¶where $\mathcal{L}$ is some loss function. What loss function makes sense?</p><p><img src="https://imgur.com/YaSOBI6.png" alt=""></p><h3 id="Zero-one-loss-function">Zero-one loss function</h3><h1>Zero-one loss function</h1><p>The most obvious loss function for a classifier is its classification error rate,</p><p>$$<br>\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \ell(f(x_i), y_i)<br>$$</p><p>Where $\ell(\hat{y}, y)$ is the zero-one loss function,</p><p>$$<br>\ell(f(x), y) =<br>\begin{cases}<br>0 &amp; \text{if } f(x) = y \<br>1 &amp; \text{if } f(x) \neq y<br>\end{cases}</p><h3 id="Non-differentiable">Non-differentiable!</h3><p>The problem with the zero -one loss function is that it‚Äôs not differentiable:<br><img src="https://imgur.com/tuIgHI9.png" alt=""></p><h3 id=""></h3><p>Integer vectors: One-hot vectors, A one-hot vector is a binary vector in which all elements are 0 except for a single element that‚Äôs equal to 1.</p><p>Drive the perceptron</p><h1>The perceptron learning algorithm</h1><p>a mistake happens here (function)</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Perceptron</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>FLUORESCENCE SPECTROSCOPY</title>
    <link href="https://karobben.github.io/2024/02/06/LearnNotes/fluorescence/"/>
    <id>https://karobben.github.io/2024/02/06/LearnNotes/fluorescence/</id>
    <published>2024-02-06T07:22:42.000Z</published>
    <updated>2024-02-08T18:05:42.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="FLUORESCENCE-SPECTROSCOPY">FLUORESCENCE SPECTROSCOPY</h2><div class="admonition note"><p class="admonition-title">What happens after the molecule is excited?</p><p><img src="https://imgur.com/nD2T2sw.png" alt="" />Fluorescence properties depend on what happens to the molecule during the ~10-8 sec during which it is excited. The decay after absorption includes 1. radiative decay ($K_f$) 2. Non-radiative decay($k_NR$)<br>Fluorescence happens very fast because it back to the ground state very fast. In general, the decay brings the electron from excited state to the ground state (decay defines as events per sec ($k_f$))<br>Nan-radiative decay (exp: form of heat), the decay faster and does not generate photon. The energy transfer into solid molecules and spreed away. They don't went to exited state and generate photons.</p></div><ul><li>the quantum yields for phosphorescence are usually very low because <mark>the radiative decay rates are slow compared to typical nonradiative rates</mark> and quenching processes<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</li></ul><h3 id="What-are-the-processes-of-non-radiative-decay">What are the processes of non-radiative decay?</h3><table><thead><tr><th style="text-align:center"></th><th style="text-align:left">Unit: sec<sup>-1</sup></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/uYOhyLM.png" alt=""></td><td style="text-align:left">Black = Non-radiative<br> Red = Radiative (photon)<br> ABS = absorption (10<sup>15</sup>)<br><strong>IC</strong> = internal conversion <br> (k<sub>IC</sub> ‚âà 10<sup>11~12</sup>)<br> <strong>Q</strong> = quenching<br><strong>IX</strong>=intersystem crossing<br>S<sub>1</sub>‚ÜíT<sub>0</sub>: 10<sup>8</sup>; T<sub>1</sub>‚ÜíS<sub>0</sub>: 10<sup>2</sup><br> <strong>Chem</strong>=photochemistry<br>k<sub>f</sub> ‚âà 10<sup>8</sup>; k<sub>p</sub> ‚âà 10<sup>2</sup><br> <strong>F</strong> = fluorescence<br> <strong>P</strong> = phosphorescence<br> Trans = energy transfer<br>k<sub>collision</sub> ‚âà 10<sup>10</sup> M<sup>-1</sup>sec<sup>-1</sup></td></tr></tbody></table><p>Only apart of electron went to the S<sub>1</sub> and they decay back to the ground state to generate fluorescence. Most of them when to S<sub>2</sub> and decay faster., In this case, less energy lost through fluorescence. Those change are internal Change. When they when to T<sub>1</sub> it transferred to other states (intersystem crossing) and generate phosphorescence. This state state decay very slow (phosphorescence decays for a few seconds or even more slow)</p><ul><li>Internal Conversion: energy loss due to collisions with solvent molecules<ul><li>collision rate = k<sub>coll</sub> [solvent]<ul><li>k<sub>coll</sub> ~10<sup>10</sup>M<sup>-1</sup>sec<sup>-1</sup></li><li>[slovent]: 55M for water</li><li>The rate of collision of a single molecule is ‚âà 10<sup>11</sup>-10<sup>12</sup>sec<sup>-1</sup></li></ul></li><li><mark>S1-S2</mark>: Heat. Fast IC (10<sup>-11</sup>sec): heat loss to solvent, <strong>all excited molecules are in the lowest vibrational state</strong> of S<sub>1</sub></li><li><mark>S0-S1</mark>: Heat. Slow IC (10<sup>-8</sup>sec): due to larger energy gap, therefore fluorescence is possible.</li></ul></li></ul><div class="admonition note"><p class="admonition-title">What is the concentration of pure water?</p><li> 1 L water = 1000 g<li> water molecule = 18 g/mol<li> So 1 L water = 1000/18 =55 mol<li> [M] = 55 mol/1 L = 55 M</div><p>The processes from the S<sub>2</sub> to S<sub>1</sub> is very fast and easily be absorbed and stored in the solvent.<br>The processes from the S<sub>1</sub> to the S<sub>0</sub>, the processes relatively slow.<br>Concentration of bonds for solvent like water is very high, so it could store lots of energy.</p><h4 id="Solvent-reorganization-and-the-Stokes-Shift">Solvent reorganization and the Stokes Shift</h4><p>Measure fluorescence at fixed Œª<sub>ex</sub> as a function of <em><strong>Œª<sub>em</sub></strong></em><br><strong>Stokes shift</strong>: Emission spectrum is always red-shifted (lower energy) compared to the absorption spectrum</p><table><thead><tr><th style="text-align:center">Vibrational relaxation</th><th style="text-align:center">Solvent reorganization</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://chem.libretexts.org/@api/deki/files/79075/%253DScreen_shot_2011-03-14_at_11.08.58_AM.png?revision=1&amp;size=bestfit&amp;width=224&amp;height=274" alt=""></td><td style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/f/fc/Stokes_shift_diagram.svg" alt=""></td></tr><tr><td style="text-align:center"><a href="https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Supplemental_Modules_(Physical_and_Theoretical_Chemistry)/Spectroscopy/Electronic_Spectroscopy/Jablonski_diagram">¬© libretexts.org</a></td><td style="text-align:center"><a href="https://en.wikipedia.org/wiki/Stokes_shift">¬© wikipedia</a></td></tr><tr><td style="text-align:center"></td><td style="text-align:center">Franck-Condon Overlap Factors<br>Prob (0‚Äô‚Üí2‚Äô) ‚âÖ Prob (2‚Äô‚Üê0‚Äô) etc</td></tr></tbody></table><p>The vibrational relaxation looks almost symitry. So, the peaks from the solvent reorganization should corresponded the states change in the vibrational relaxation.<br>The emission shift (Stokes Shift) always as red-shifted (into right). So, the fluorescence always as less energy as the excited state.</p><table><thead><tr><th style="text-align:center">Solvent Effect</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/3c7E6y8.png" alt=""></td></tr></tbody></table><p>The larger, the more effects (?)</p><ol><li>Dipole changes after absorption. But the solvent dipole doesn‚Äôt change. But this is not the favorite result, the dipole of the molecule changes</li><li>(right) the solvent changes with the molecule. When the molecule back to the ground state and emitting fluorescence. But the solvent delay and change the ground the state of the molecule. It means the path from the S<sub>1</sub> to S<sub>0</sub> became shorter and the energy would used lesser. This phenomenon could be intensify by using more polar solvent.</li></ol><p>Absorption: ~ 10<sup>-15</sup> sec<br>Solvent reorganization (relaxation): ~ 10<sup>-10</sup> sec<br>Fluorescence: ~ 10<sup>-8</sup> sec<br>Step 1: Permanent Dipoles of solvent re-orient to adjust to the altered dipole of the excited fluorophore.<br>Step 2: The dipole-dipole interaction in turn stabilizes S1 and destabilizes S0.<br><strong>Requires</strong>:<br>1. solvent polarity (dielectric constant, Œµ)<br>2. mobility of solvent (reorientation of solvent dipoles)</p><h3 id="How-long-can-a-molecule-stay-in-its-excited-state">How long can a molecule stay in its excited state?</h3><p>Excite some molecules to $ S_1 $ with a brief pulse of light at $ t = 0 $, $ N_0^ * $ excited state molecules<br>Decay of the excited state population is exponential:<br>$$ \frac{dN^ *(t)}{dt} = -(k_ f + k_{NR})N^ *(t) $$</p><p>left: ?. right: Chemical rate total decay rate * total concentration</p><p>So: $ N ^ * (t) = N_ 0 ^ * e ^ {-(k_f+k_ {NR})t} = N_0 ^ * e^ {-t/\tau} \quad$<br>where $N ^ * (t)$ is the number of excited molecules at time <em><strong>t</strong></em>.</p><p>Define: fluorescence lifetime $ \tau $<br>$ \tau = \frac{1}{k_f + k_{NR}} $<br>the processes when 1/e ?</p><p>Hence, the equation for the decay of the excited state population:<br>$ N^ *(t) = N_0^ * e^ {-t/\tau} $</p><div class="admonition note"><p class="admonition-title">The meaning of the fluorescence lifetime</p><p><em><strong>œÑ</strong></em> has units of time (seconds) <br>$[N_ {t=\tau}^ *] = \frac{N_ 0^ *}{e} \approx 0/37N_ 0^ *$<br>After one lifetime following excitation, the probability of a molecule still being in the excited state is about 37%. The shorter the <em><strong>œÑ</strong></em> the faster the decay.<br>Commonly used fluorescence in biological system, the  <em><strong>œÑ</strong></em> ~ 1-10 ns</p></div><h4 id="How-bright-can-a-molecule-be">How bright can a molecule be?</h4><p><strong>Rate up</strong>: $S_ 0 \rightarrow S_ 1 = I_ 0$, unit: [# of photons absorbed/sec]<br><strong>Rate down</strong>: $S_ 1 \rightarrow S_ 0 = -(k_ f + k_ {NR}) \cdot N ^*(t)$<br><em>Unit</em>: [1/sec][# of photons]</p><p>$N^ *(t)$ = concentration of excited state molecules at any time, $t$<br>$k_  {NR}$ = sum of all non-radiative rate constants<br>$N^ *(t) = [S_1(t)]$, [# of molecules]</p><p>Steady state: rate up = rate down<br>$0 = \frac{dN ^ *(t)}{dt} = I_ 0 - (k_ f + k_ {NR}) N ^ *(t)$</p><p>In the steady state $N ^ *(t)$ is constant $= N ^ * _ {SS}$<br>$I_0 = (k_f + k_{NR}) N ^ *_{SS}$</p><h4 id="Fluorescence-quantum-yield-QY">Fluorescence quantum yield (QY)</h4><p>$Q_f$ (QY): fraction of excited-state molecules that relax to the ground state by emitting a photon.</p><p>photons/sec emitted in steady state</p><p>$$ Q_f = \frac{k_ f N^ *_ {SS}}{I_ 0} = \frac{k_ f N^ *_ {SS}}{(k_ f + k_{NR})N^ *_ {SS}} $$</p><p>Since $I_0 = (k_f + k_{NR})N^*_{SS}$</p><p>photons/sec absorbed in steady state</p><p><mark>Quantum yield</mark>: $Q_f = \frac{k_f}{(k_f + k_{NR})} = k_f \times \tau$</p><p>Recall $\tau = \frac{1}{(k_f + k_{NR})}$</p><h3 id="What-are-the-fluorophores-in-biological-systems">What are the fluorophores in biological systems?</h3><ul><li>Intrinsic Fluorescence of Proteins</li></ul><p>Absorption spectra Fluorescence spectra of amino acids in water ‚ÄúAbout 300 papers per year abstracted in Biological Abstracts report work that exploits or studies tryptophan (Trp) fluorescence in proteins‚Ä¶‚Äù<br>Vivian et al. Biophysical Journal 2001</p><table><thead><tr><th></th><th>Lifetime (nsec)</th><th>Absorption</th><th></th><th>Fluorescence</th><th></th></tr></thead><tbody><tr><td></td><td></td><td>Wavelength (nm)</td><td>Absorptivity (Œµ, M<sup>-1cm</sup>-1)</td><td>Wavelength (Œªmax, nm)</td><td>Quantum Yield (25¬∞C)</td></tr><tr><td>Tryptophan</td><td>2.6</td><td>280</td><td>5,600</td><td>348</td><td>0.20</td></tr><tr><td>Tyrosine</td><td>3.6</td><td>274</td><td>1,400</td><td>303</td><td>0.14</td></tr><tr><td>Phenylalanine</td><td>6.4</td><td>257</td><td>200</td><td>282</td><td>0.04</td></tr></tbody></table><h2 id="Maturation-of-GFP">Maturation of GFP</h2><table><thead><tr><th style="text-align:center"><img src="https://zeiss-campus.magnet.fsu.edu/articles/probes/images/aequoreafpintrofigure1.jpg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://zeiss-campus.magnet.fsu.edu/articles/probes/images/aequoreafpintrofigure2.jpg" alt=""></td></tr><tr><td style="text-align:center"><img src="https://zeiss-campus.magnet.fsu.edu/articles/probes/images/aequoreafpintrofigure4.jpg" alt=""></td></tr><tr><td style="text-align:center"><a href="https://zeiss-campus.magnet.fsu.edu/print/probes/jellyfishfps-print.html">¬© zeiss</a></td></tr></tbody></table><table><thead><tr><th>Compound</th><th>Lifetime (nsec)</th><th>Wavelength (nm)(Absorption)</th><th>Absorptivity (Œµ, M<sup>-1</sup>cm<sup>-1</sup>)</th><th>Wavelength (Œª<sub>max</sub>, nm) (Emission)</th><th>Quantum Yield (25¬∞C)    (Emission)</th></tr></thead><tbody><tr><td>Tryptophan</td><td>2.6</td><td>280</td><td>5,600</td><td>348</td><td>0.20</td></tr><tr><td>Tyrosine</td><td>3.6</td><td>274</td><td>1,400</td><td>303</td><td>0.14</td></tr><tr><td>Phenylalanine</td><td>6.4</td><td>257</td><td>200</td><td>282</td><td>0.04</td></tr><tr><td>wtGFP</td><td>3.3/2.8</td><td>395/475</td><td>21,000</td><td>509</td><td>0.77</td></tr><tr><td>(Enhanced)</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>EGFP (F64L, S65T)</td><td>2.7</td><td>484</td><td>56,000</td><td>507</td><td>0.60</td></tr></tbody></table><h3 id="Fluorescence-Quenching">Fluorescence Quenching</h3><p>Quenching: reduce the fluorescent signal. Static and dynamic quenching causing the similar result. But the processes are totally different.</p><ol><li>Static quenching:</li></ol><ul><li>Formation of a <strong>‚Äúdark‚Äù complex of the ground state</strong> of the fluorophore and another molecule.</li></ul><ol start="2"><li>Dynamic quenching:</li></ol><ul><li>Collision between the <strong>excited state of the fluorophore</strong> and another molecule</li><li><strong>Enhancing the non-radiative decay</strong> to the ground state</li></ul><p>$$F = \sigma √ó ùêº √ó ùëÑY$$</p><p>$F$: fluorescence intensity (photons/sec)<br>$\sigma$: absorption cross-section (cm<sup>2</sup>)<br>$I$: excitation light flux - photons/(cm<sup>2</sup> sec)<br>$QY$: Quantum yield (unitless)</p><p>Mirror-image rule: between the citation and the shifting spectrum, they are symmetry.</p><h3 id="Static-quenching-ground-state">Static quenching (ground state)</h3><p>Fluorescent species $A$ can associate with <strong>quencher</strong> $Q$ to form a non-fluorescent complex $AQ$:</p><p>$$<br>A + Q \leftrightarrow AQ<br>$$</p><p>The association constant $K_a$ is defined as:</p><p>$$<br>K_a = \frac{[AQ]}{[A][Q]}<br>$$</p><p>The ratio of the fluorescence intensities without and with the quencher present is given by:</p><p>$$<br>\frac{F_0}{F} = \frac{A_{tot}}{A} = \frac{[A] + [AQ]}{[A]}  = \frac{[A] + [A][Q]K_a}{[A]} \\ = 1 + [Q]K_a<br>$$</p><ul><li>Fluorescence depends on the concentration of the quencher, $[Q]$</li><li>Data analysis yields the association constant</li></ul><p>F is after quenching<br>It could quenching black radioactive object.</p><h3 id="Dynamic-quenching-collision-with-the-excited-state">Dynamic quenching: collision with the excited state</h3><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left"><li> $A + hv \rightarrow A^ *$ (excitation) <li> $ A^ * + Q \xrightarrow{k_Q} A + Q + \text{heat}$ (quenching) <li> $A ^ * \xrightarrow{k_f} A + hv‚Äô$ (fluorescence) <li> $k_Q$: second order rate constant for collisional quenching</td><td style="text-align:center"><img src="https://imgur.com/4rrQGDW.png" alt=""></td></tr></tbody></table><p>The diagram illustrates the energy levels $S_1$ and $S_0$, with $k_f$ representing the rate of fluorescence, $k_{NR}$ the non-radiative decay, and $[Q]k_Q$ the rate of quenching by the quencher $Q$. There‚Äôs also an illustrative depiction of a molecule $A^*$ being quenched by $Q$ within a radius of 50√Ö.</p><p>Only quenching excited molecule</p><p><img src="https://imgur.com/lcjPXNW.png" alt=""></p><p>The energy levels $S_1$ and $S_0$ are shown with $k_f$ representing the rate of fluorescence, $k_{NR}$ the non-radiative decay, and $[Q]k_Q$ the rate of quenching by the quencher $Q$.</p><ul><li><p>Rate of decay due to collision:<br>$$<br>\frac{d[S_1]}{dt} = -k_Q[Q][S_1]<br>$$</p></li><li><p>Total rate of decay: $S_1 \rightarrow S_0$:</p><ul><li>$ \frac{d[S_1]}{dt} = -k_f[S_1] - k_{NR}[S_1] - k_Q[Q][S_1] $</li><li>$ \frac{d[S_1]}{dt} = -(k_f + k_{NR} + k_Q[Q])[S_1] $</li></ul></li></ul><div class="admonition note"><p class="admonition-title">The quantum yield in the presence of a quencher:</p><p>$$ Q_f^{\theta} = \frac{k_f}{k_f + k_{NR} + k_Q[Q]} $$</p></div><p><strong>No Quencher:</strong>$Q_f^0 = \frac{k_f}{k_f + k_{NR}}$</p><p><strong>Plus quencher:</strong>$Q_f^{\theta} = \frac{k_f}{k_f + k_{NR} + k_Q[Q]}$</p><p>The ratio of fluorescence intensities without and with the quencher is given by:</p><p>$$<br>\frac{F_0}{F} = \frac{Q_f^ 0}{Q_f^ 0 + Q_f} \\ =  \frac{k_ f}{k_ f + k_ {NR}} \times \frac{k_ f + k_ {NR} + k_ Q[Q]}{k_ f}  \\<br>= 1 + \frac{k_Q[Q]}{k_f + k_{NR}} \\ = 1 + \tau_0 k_Q[Q]<br>$$</p><p>Where $\tau_0 = \frac{1}{k_f + k_{NR}}$ is the fluorescence lifetime (without quencher).</p><div class="admonition note"><p class="admonition-title">Define: the Stern-Volmer constant</p><p>$K_{SV} = k_Q \tau_0$<br>$ \frac{F_0}{F} = 1 + K_{SV} [Q] $</p></div><p>K<sub>SV</sub> measures <mark>the rate of quencher colliding</mark> into fluorophores at the excited state.<br>The more the fluorophore is protected from solvent, the smaller the value of K<sub>SV</sub>.</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/HOnMo3v.png" alt=""></td><td style="text-align:left"><li>Slope: $K_{SV} = 8 M^{-1}$ <br> (It descripts how strong the quencher it is. The larger (sharper), the stronger.)<li>Separately measure $\tau_0 = 4 ns$                           <li>Calculate $k_Q = \frac{K_{SV}}{\tau_0} = 2 \times 10^9 M^{-1} sec^{-1}$</td></tr></tbody></table><h3 id="Dynamic-and-Static-quenching-have-the-same-dependence-on-Q">Dynamic and Static quenching have the same dependence on [Q]</h3><p>Dynamic quenching: $\frac{F_0}{F} = 1 + K_{SV} [Q]$<br>Static quenching: $\frac{F_0}{F} = 1 + K_a [Q]$</p><p>In each case, you will get a straight line if you plot $\frac{F_0}{F}$ vs $[Q]$</p><h3 id="How-can-one-distinguish-between-static-quenching-and-dynamic-quenching">How can one distinguish between static quenching and dynamic quenching?</h3><p>The differential equation for the decay of excited state molecules $N^*$ is given by:</p><p>$$<br>\frac{dN^ * (t)}{dt} = -(k_f + k_{NR} + k_Q[Q])N^ * (t)<br>$$</p><p>This leads to the solution:</p><p>$$<br>N^ * (t) = N ^  * _ 0 e ^ {-(k_f+k_{NR}+k_Q[Q])t}<br>$$</p><p>And equivalently:</p><p>$$<br>N^ * (t) = N ^ * _0 e^ {-\frac{t}{\tau}}<br>$$</p><p><strong>DYNAMIC QUENCHING:</strong> The <strong>lifetime of the excited state decreases</strong> as the concentration of the quencher is increased.</p><p>In the presence of a quencher, the lifetime $\tau$ is given by:</p><p>$$<br>\tau = \frac{1}{(k_f + k_{NR} + k_Q[Q])}<br>$$</p><h3 id="Dynamic-quenching">Dynamic quenching</h3><table><thead><tr><th style="text-align:center">Plus quencher</th><th style="text-align:center"><strong>No quencher</strong></th></tr></thead><tbody><tr><td style="text-align:center">$\tau = \frac{1}{(k_f + k_{NR} + k_Q[Q])}$</td><td style="text-align:center">$ \tau_0 = \frac{1}{(k_f + k_{NR})} $</td></tr><tr><td style="text-align:center">$Q_f^{+Q} = \frac{k_f}{k_f + k_{NR} + k_Q[Q]} $</td><td style="text-align:center">$ Q_f^{0} = \frac{k_f}{k_f + k_{NR}} $</td></tr></tbody></table><p>hence</p><p>$$<br>\frac{\tau_0}{\tau} = \frac{Q_f^ {0}}{Q_f^ {+Q}} \approx \frac{F_0}{F}<br>$$</p><p>Stern-Volmer plot will be the same if you plot lifetimes or fluorescence intensity</p><h3 id="Static-quenching-DOES-NOT-affect-the-lifetime-of-the-excited-state">Static quenching DOES NOT affect the lifetime of the excited state</h3><p>Static quenching (ground state)</p><p>$$<br>A + Q \leftrightarrow AQ<br>$$</p><p>$$<br>K_a = \frac{[AQ]}{[A][Q]}<br>$$</p><p>Fluorescent species $A$ can form a non-fluorescent complex $AQ$ with quencher $Q$. The association constant $K_a$ is defined as the ratio of the concentration of the complex to the product of the concentrations of $A$ and $Q$.</p><p>The ratio of the fluorescence intensities without and with the quencher is given by:</p><p>$$<br>\frac{F_0}{F} = \frac{A_{tot}}{A} = \frac{[A] + [AQ]}{[A]} = \frac{[A] + [A][Q]K_a}{[A]} = 1 + [Q]K_a<br>$$</p><p>The excited state species $A^*$ has the same properties in the presence of the static quencher. But there is less of it, so the fluorescence intensity decreases.</p><h3 id="If-both-static-and-dynamic-quenching-are-occurring-in-the-same-sample">If both static and dynamic quenching are occurring in the same sample</h3><p><img src="https://imgur.com/26OTvpP.png" alt=""></p><p>$$<br>\frac{F_0}{F} = (1 + k_Q \tau_0 [Q])(1 + K_a [Q]) = (1 + K_{SV} [Q])(1 + K_a [Q])<br>$$</p><h3 id="Trp94-H¬±His18-form-a-dark-complex-no">Trp94-(H¬±His18) form a dark complex: no</h3><p>fluorescence</p><p><strong>Trp94 + His18 ‚áå Trp94¬∑(H+His18) DARK</strong></p><p>At acidic pH:</p><ul><li>Quantum yield of W94 (Qf) is <strong>decreased</strong>;</li><li>Fluorescence lifetime of W94 (œÑ‚ÇÄ) is <strong>unchanged</strong></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Hurtubise RJ (1990) Phosphorimetry: Theory, Instrumentation, and Applications, VCH, New York. <a href="#fnref1" class="footnote-backref">‚Ü©Ô∏é</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">FLUORESCENCE SPECTROSCOPY</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>SeqKit: A Cross-Platform and Ultrafast Toolkit for FASTA/Q File Manipulation</title>
    <link href="https://karobben.github.io/2024/02/05/Bioinfor/seqkit/"/>
    <id>https://karobben.github.io/2024/02/05/Bioinfor/seqkit/</id>
    <published>2024-02-05T21:00:06.000Z</published>
    <updated>2024-02-14T06:22:54.030Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Install">Install</h2><p>GitHub: <a href="https://github.com/shenwei356/seqkit">shenwei356/seqkit</a></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">wget https://github.com/shenwei356/seqkit/releases/download/v2.7.0/seqkit_linux_amd64.tar.gz<br>tar -zxvf seqkit_linux_amd64.tar.gz<br></code></pre></td></tr></table></figure></div><h3 id="Convert-the-Fastq-to-Fasta">Convert the Fastq to Fasta</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">seqkit fq2fa output_directory/output_prefix.extendedFrags.fastq -o output_directory/output_prefix.merged.fasta<br></code></pre></td></tr></table></figure></div><h3 id="Remove-Duplicated-Sequence">Remove Duplicated Sequence</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">seqkit rmdup -s sequences.fasta -o unique_sequences.fasta -D counts.tsv<br></code></pre></td></tr></table></figure></div><ul><li><code>-s</code>: Specifies that duplicates should be identified based on sequence content.</li><li><code>[input_file]</code>: Replace this with the path to your input FASTA or FASTQ file.</li><li><code>-o [output_file]</code>: Specifies the output file. Replace <code>[output_file]</code> with the desired path for the file containing the sequences after duplicate removal.</li><li><code>-D</code>: write all removed duplicates (and counts) to this specified file.</li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">SeqKit provides a comprehensive suite of utilities for the efficient and high-throughput processing of FASTA/Q files. This toolkit allows for format conversion, subsequence extraction, quality control, and much more.</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/"/>
    
    <category term="Fasta/q" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/Fasta-q/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="Fasta" scheme="https://karobben.github.io/tags/Fasta/"/>
    
    <category term="Sequencing" scheme="https://karobben.github.io/tags/Sequencing/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression</title>
    <link href="https://karobben.github.io/2024/02/05/LearnNotes/ai-linear/"/>
    <id>https://karobben.github.io/2024/02/05/LearnNotes/ai-linear/</id>
    <published>2024-02-05T18:26:13.000Z</published>
    <updated>2024-02-14T06:22:54.030Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Linear-Regression">Linear Regression</h2><h3 id="Vectors-and-Matrix">Vectors and Matrix</h3><p>In numpy, the dot product can be written np.dot(w,x) or w@x.<br>Vectors will always be column vectors. Thus:</p><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/Zq0RMO8.png" alt=""></th><th style="text-align:center"><img src="https://imgur.com/TF5NMbz.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">Vectors are lowercase bold letters</td><td style="text-align:center">Matrices are uppercase bold letters</td></tr></tbody></table><p>Vector and Matrix Gradients<br>The gradient of a scalar function with respect to a vector or matrix is:<br>The symbol $\frac{\sigma f}{\sigma x_ 1}$ means ‚Äúpartial derivative of f with respect to <em>x<sub>1</sub></em>‚Äù</p><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/2b5hSHK.png" alt=""></th></tr></thead><tbody></tbody></table><table><thead><tr><th style="text-align:center"><img src="https://www.researchgate.net/profile/Vladimir-Nasteski/publication/328146111/figure/fig4/AS:702757891751937@1544561946700/Visual-representation-of-the-linear-regression-22.ppm" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.researchgate.net/publication/328146111_An_overview_of_the_supervised_machine_learning_methods?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">¬© Vladimir Nasteski</a></td></tr></tbody></table><p>$$ f(x) = w^ T x + b = \sum_{j=0} ^{D-1} w_ j x_ j + b $$</p><ul><li>$f(x) = y$</li><li>Generally, we want to choose the weights and bias, <em>w</em> and <em>b</em>, in order to minimize the errors.</li><li>The errors are the vertical green bars in the figure at right, <em>Œµ = f(x) ‚àí y</em>.</li><li>Some of them are positive, some are negative. What does it mean to ‚Äúminimize‚Äù them?<ul><li>$ f(x) = w^ T x + b = \sum_{j=0} ^{D-1} w_ j x_ j + b $</li></ul></li><li>Training token errors Using that notation, we can define a signed error term for every training token: <em>Œµ = f(x<sub>i</sub>) - y<sub>i</sub></em></li><li>The error term is positive for some tokens, negative for other tokens. What does it mean to minimize it?</li></ul><h3 id="Mean-squared-error">Mean-squared error</h3><p>Squared: tends to notice the big values and trying ignor small values.</p><p>One useful criterion (not the only useful criterion, but perhaps the most common) of ‚Äúminimizing the error‚Äù is to minimize the mean squared error:<br>$$  \mathcal{L} = \frac{1}{2n} \sum_{i=1}^ {n} \varepsilon_i^ 2 = \frac{1}{2n} \sum_{i=1}^ {n} (f(x_ i) - y_ i)^ 2  $$<br>The factor $\frac{1}{2}$ is included so that, so that when you differentiate ‚Ñí , the 2 and the $\frac{1}{2}$ can cancel each other.</p><div class="admonition note"><p class="admonition-title">MSE = Parabola </p><p>Notice that MSE is a non -negative quadratic function of <em>f(<strong>x</strong>~i~) = <strong>w</strong>^T^ x~i~ + b</em>, therefore it‚Äôs a non negative quadratic function of <em><strong>w</strong></em> . Since it‚Äôs a non -negative quadratic function of <em><strong>w</strong></em>, it has a unique minimum that you can compute in closed form! We won‚Äôt do that today.</p></div><p>$\mathcal{L} = \frac{1}{2n} \sum_{i=1}^ {n} (f(x_ i) - y_ i)^ 2$</p><p><strong>The iterative solution to linear regression</strong> (gradient descent):</p><ul><li>Instead of minimizing MSE in closed form, we‚Äôre going to use an iterative algorithm called gradient descent. It works like this:<ul><li>Start: random initial <em><strong>w</strong></em> and <em>b</em> (at <em>t=0</em>)</li><li>Adjust <em><strong>w</strong></em> and <em>b</em> to reduce MSE (<em>t=1</em>)</li><li>Repeat until you reach the optimum (<em>t = ‚àû</em>).</li></ul></li></ul><p>$ w \leftarrow w - \eta \frac{\partial \mathcal{L}}{\partial w} $<br>$ b \leftarrow b - \eta \frac{\partial \mathcal{L}}{\partial b} $</p><h3 id="Finding-the-gradient">Finding the gradient</h3><p>The loss function ( \mathcal{L} ) is defined as:</p><p>[ \mathcal{L} = \frac{1}{2n} \sum_{i=1}^{n} L_i, \quad L_i = \varepsilon_i^2, \quad \varepsilon_i = w^T x_i + b - y_i ]</p><p>To find the gradient, we use the chain rule of calculus:</p><p>[ \frac{\partial \mathcal{L}}{\partial w} = \frac{1}{2n} \sum_{i=1}^{n} \frac{\partial L_i}{\partial w}, \quad \frac{\partial L_i}{\partial w} = 2\varepsilon_i \frac{\partial \varepsilon_i}{\partial w}, \quad \frac{\partial \varepsilon_i}{\partial w} = x_i ]</p><p>Putting it all together,</p><p>[ \frac{\partial \mathcal{L}}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} \varepsilon_i x_i ]</p><h3 id="The-iterative-solution-to-linear-regression">The iterative solution to linear regression</h3><p>‚Ä¢ Start from random initial values of<br>ÔøΩ and ÔøΩ (at ÔøΩ = 0).<br>‚Ä¢ Adjust ÔøΩ and ÔøΩ according to:</p><p>[ w \leftarrow w - \frac{\eta}{n} \sum_{i=1}^{n} \varepsilon_i x_i ]<br>[ b \leftarrow b - \frac{\eta}{n} \sum_{i=1}^{n} \varepsilon_i ]</p><ul><li>Intuition:</li><li>Notice the sign:<br>[ w \leftarrow w - \frac{\eta}{n} \sum_{i=1}^{n} \varepsilon_i x_i ]</li><li>If ( \varepsilon_i ) is positive (( f(x_i) &gt; y_i )), then we want to <em>reduce</em> ( f(x_i) ), so we make ( w ) less like ( x_i )</li><li>If ( \varepsilon_i ) is negative (( f(x_i) &lt; y_i )), then we want to <em>increase</em> ( f(x_i) ), so we make ( w ) more like ( x_i )</li></ul><h2 id="Perceptron">Perceptron</h2><p>Perceptron is invented before the loss function</p><p>Linear classifier: Notation<br>‚Ä¢ The observation x<sup>T</sup> = [x<sub>1</sub>, ‚Ä¶ , x<sub>n</sub>] is a real-valued vector (d is the number of feature dimensions)<br>‚Ä¢ The class label y ‚àà Y is drawn from some finite set of class labels.<br>‚Ä¢ Usually the output vocabulary, Y, is some set of strings. For<br>convenience, though, we usually map the class labels to a sequence<br>of integers, Y = [1, ‚Ä¶ , v} , where ÔøΩ is the vocabulary size</p><h2 id="Linear-classifier-Definition">Linear classifier: Definition</h2><p>A linear classifier is defined by</p><p>$$<br>f(x) = \text{argmax } Wx + b<br>$$</p><p>where:</p><p><img src="https://imgur.com/zuycCx8.png" alt=""></p><p>$w_k, b_k$ are the weight vector and bias corresponding to class $k$, and the argmax function finds the element of the vector $wx$ with the largest value.</p><p>There are a total of $v(d + 1)$ trainable parameters: the elements of the matrix $w$.</p><p><img src="https://imgur.com/undefined.png" alt=""></p><h1>Example</h1><p>Notice that in the two-class case, the equation</p><p>$$<br>f(x) = \text{argmax } Wx + b<br>$$</p><p>Simplifies to</p><p><img src="https://imgur.com/QAGB3Ur.png" alt=""></p><p>The class boundary is the line whose equation is</p><p>$$<br>(w_2 - w_1)^T x + (b_2 - b_1) = 0<br>$$</p><h2 id="Gradient-descent">Gradient descent</h2><h1>Gradient descent</h1><p>Suppose we have training tokens $(x_i, y_i)$, and we have some initial class vectors $w_1$ and $w_2$. We want to update them as</p><p>$$<br>w_1 \leftarrow w_1 - \eta \frac{\partial \mathcal{L}}{\partial w_1}<br>$$</p><p>$$<br>w_2 \leftarrow w_2 - \eta \frac{\partial \mathcal{L}}{\partial w_2}<br>$$</p><p>‚Ä¶where $\mathcal{L}$ is some loss function. What loss function makes sense?</p><p><img src="https://imgur.com/YaSOBI6.png" alt=""></p><h3 id="Zero-one-loss-function">Zero-one loss function</h3><h1>Zero-one loss function</h1><p>The most obvious loss function for a classifier is its classification error rate,</p><p>$$<br>\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \ell(f(x_i), y_i)<br>$$</p><p>Where $\ell(\hat{y}, y)$ is the zero-one loss function,</p><p>$$<br>\ell(f(x), y) =<br>\begin{cases}<br>0 &amp; \text{if } f(x) = y \<br>1 &amp; \text{if } f(x) \neq y<br>\end{cases}</p><h3 id="Non-differentiable">Non-differentiable!</h3><p>The problem with the zero -one loss function is that it‚Äôs not differentiable:<br><img src="https://imgur.com/tuIgHI9.png" alt=""></p><h3 id=""></h3><p>Integer vectors: One-hot vectors, A one-hot vector is a binary vector in which all elements are 0 except for a single element that‚Äôs equal to 1.</p><p>Drive the perceptron</p><h3 id="The-perceptron-learning-algorithm">The perceptron learning algorithm</h3><p>a mistake happens here (function)</p><h2 id="Softmax">Softmax</h2><h3 id="The-perceptron-learning-algorithm-v2">The perceptron learning algorithm</h3><ol><li><p>Compute the classifier output $\hat{y} = \text{argmax}_k (w_k^T x + b_k)$</p></li><li><p>Update the weight vectors as:</p></li></ol><p>$$<br>w_k \leftarrow<br>\begin{cases}<br>w_k - \eta x &amp; \text{if } k = \hat{y} \\<br>w_k + \eta x &amp; \text{if } k = y \\<br>w_k &amp; \text{otherwise}<br>\end{cases}<br>$$</p><p>where $\eta \approx 0.01$ is the learning rate.</p><p>Key idea: $f_c(x) =$ posterior probability of cass $c$</p><ul><li>A perceptron has a one-hot output vector, in which $f_c(x) = 1$ if the<br>neural net thinks $c$ is the most likely value of $y$, and 0 otherwise</li><li>A softmax computes $f_c(x) \approx Pr(Y =c |x)$. The conditions for this to be true are:<ul><li>It needs to satisfy the axioms of probability:<br>$$ 0 \leq f_c(x) \leq 1, \quad \sum_{c=1}^{v} f_c(x) = 1$$</li><li>The weight matrix, $W$, is trained using a loss function that encourages $f(x)$ to approximate posterior probability of the labels on some training dataset:<br>$$f_c(x) \approx \Pr(Y = c|x)$$</li></ul></li></ul><h3 id="Softmax-satisfies-the-axioms-of-probability">Softmax satisfies the axioms of probability</h3><ul><li><p>Axiom #1, probabilities are non-negative $(f_k(x) \geq 0)$. There are many ways to do this, but one way that works is to choose:</p><p>$$<br>f_c(x) \propto \exp(w_c^T x + b_c)<br>$$</p></li><li><p>Axiom #2, probabilities should sum to one $(\sum_{k=1}^{v} f_k(x) = 1)$. This can be done by normalizing:</p></li></ul><p>$$<br>f(x) = [f_1(x), ‚Ä¶, f_v(x)]^T<br>$$<br>$$<br>f_c(x) = \frac{\exp(w_c^T x + b_c)}{\sum_{k=0}^{v-1} \exp(w_k^T x + b_k)}<br>$$</p><p>where $w_k^T$ is the $k^{th}$ row of the matrix $W$.</p><h3 id="The-logistic-sigmoid-function">The logistic sigmoid function</h3><p>For a two-class classifier, we don‚Äôt really need the vector label. If we define $w = w_2 - w_1$ and $b = b_2 - b_1$, then the softmax simplifies to:</p><p>$$<br>f(Wx + b) =<br>\begin{bmatrix}<br>\text{Pr}(Y = 1|x) \\<br>\text{Pr}(Y = 2|x)<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\frac{1}{1+e^ {-(w^ Tx+b)}} \\<br>\frac{e^ {-(w^ Tx+b)}}{1+e^ {-(w^ Tx+b)}}<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\sigma(w^Tx + b) \\<br>1 - \sigma(w^Tx + b)<br>\end{bmatrix}<br>$$</p><p>‚Ä¶ so instead of the softmax, we use a scalar function called the logistic sigmoid function:</p><p>$$<br>\sigma(z) = \frac{1}{1+e^{-z}}<br>$$</p><p>This function is called sigmoid because it is S-shaped.</p><p>For $z \to -\infty$, $\sigma(z) \to 0$</p><p>For $z \to +\infty$, $\sigma(z) \to 1$</p><h3 id="Gradient-descent-v2">Gradient descent</h3><p>Suppose we have training tokens $(x_i, y_i)$, and we have some initial class vectors $w_1$ and $w_2$. We want to update them as</p><p>$$<br>w_1 \leftarrow w_1 - \eta \frac{\partial \mathcal{L}}{\partial w_1}<br>$$</p><p>$$<br>w_2 \leftarrow w_2 - \eta \frac{\partial \mathcal{L}}{\partial w_2}<br>$$</p><p>‚Ä¶where $\mathcal{L}$ is some loss function. What loss function makes sense?</p><h3 id="Zero-one-loss-function-v2">Zero-one loss function</h3><p>The most obvious loss function for a classifier is its classification error rate,</p><p>$$<br>\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \ell(\hat{f}(x_i), y_i)<br>$$</p><p>Where $\ell(\hat{y}, y)$ is the zero-one loss function,</p><p>$$<br>\ell(f(x), y) =<br>\begin{cases}<br>0 &amp; \text{if } f(x) = y \\<br>1 &amp; \text{if } f(x) \neq y<br>\end{cases}<br>$$</p><p>The problem with zero-one loss is that it‚Äôs not differentiable.</p><h3 id="A-loss-function-that-learns-probabilities">A loss function that learns probabilities</h3><p>Suppose we have a softmax output, so we want $f_c(x) \approx \Pr(Y = c|x)$. We can train this by learning $W$ and $b$ to maximize the probability of the training corpus. If we assume all training tokens are independent, we get:</p><p>$$<br>W, b = \underset{W,b}{\text{argmax}} \prod_{i=1}^{n} \Pr(Y = y_i|x_i) = \underset{W,b}{\text{argmax}} \sum_{i=1}^{n} \ln \Pr(Y = y_i|x_i)<br>$$</p><p>But remember that $f_c(x) \approx \Pr(Y = c|x)$! Therefore, maximizing the log probability of training data is the same as minimizing the cross entropy between the neural net and the ground truth:</p><p>$$<br>W, b = \underset{W,b}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} \mathcal{L}_ i, \quad \mathcal{L}_ i = - \log f_ {y_ i}(x_ i)<br>$$</p><h3 id="Cross-entropy">Cross-entropy</h3><p>This loss function:</p><p>$$<br>\mathcal{L} = - \ln f_{y}(x)<br>$$</p><p>is called cross-entropy. It measures the difference in randomness between:</p><ul><li>Truth: $Y = y$ with probability 1.0, $\ln(1.0) = 0$, minus the</li><li>Neural net estimate: $Y = y$ with probability $f_{y}(x)$.</li></ul><p>Thus</p><p>$$<br>\mathcal{L} = 0 - \ln f_{y}(x)<br>$$</p><h3 id="Gradient-of-the-cross-entropy-of-the-softmax">Gradient of the cross-entropy of the softmax</h3><p>Since we have these definitions:</p><p>$$<br>\mathcal{L} = - \ln f_{y}(x), \quad f_{y}(x) = \frac{\exp(z_{y})}{\sum_{k=1}^{v} \exp(z_{k})}, \quad z_{c} = w_c^T x + b_c<br>$$</p><p>Then:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w_c} = \left( \frac{\partial \mathcal{L}}{\partial z_c} \right) \left( \frac{\partial z_c}{\partial w_c} \right) = \left( \frac{\partial \mathcal{L}}{\partial z_c} \right) x<br>$$</p><p>‚Ä¶where:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial z_c} =<br>\begin{cases}<br>f_{c}(x_i) - 1 &amp; c = y \\<br>f_{c}(x_i) &amp; c \neq y<br>\end{cases}<br>$$</p><h3 id="Similarity-to-linear-regression">Similarity to linear regression</h3><p>For linear regression, we had:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w} = \epsilon x, \quad \epsilon = f(x) - y<br>$$</p><p>For the softmax classifier with cross-entropy loss, we have</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w_c} = \epsilon_c x<br>$$</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y \text{ (output should be 1)} \\<br>f_c(x_i) &amp; \text{otherwise (output should be 0)}<br>\end{cases}<br>$$</p><h3 id="Similarity-to-perceptron">Similarity to perceptron</h3><p>Suppose we have a training token $(x, y)$, and we have some initial class vectors $w_c$. Using softmax and cross-entropy loss, we can update the weight vectors as</p><p>$$<br>w_c \leftarrow w_c - \eta \epsilon_c x<br>$$</p><p>‚Ä¶where</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y_i \\<br>f_c(x_i) &amp; \text{otherwise}<br>\end{cases}<br>$$</p><p>In other words, like a perceptron,</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>\epsilon_c &lt; 0 &amp; c = y_i \\<br>\epsilon_c &gt; 0 &amp; \text{otherwise}<br>\end{cases}<br>$$</p><h3 id="Outline">Outline</h3><ul><li><p>Softmax:<br>$$ f_c(x) = \frac{\exp(w_c^T x + b_c)}{\sum_{k=1}^{v} \exp(w_k^T x + b_k)} \approx \Pr(Y = c|x) $$</p></li><li><p>Cross-entropy:<br>$$ \mathcal{L} = - \ln f_{y}(x) $$</p></li><li><p>Derivative of the cross-entropy of a softmax:<br>$$ \frac{\partial \mathcal{L}}{\partial w_c} = \epsilon_c x, \quad \epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y \text{ (output should be 1)} \\<br>f_c(x_i) &amp; \text{otherwise (output should be 0)}<br>\end{cases} $$</p></li><li><p>Gradient descent:<br>$$ w_c \leftarrow w_c - \eta \epsilon_c x $$</p></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Linear Regression</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Learning Progress</title>
    <link href="https://karobben.github.io/2024/02/02/LearnNotes/ai-learning/"/>
    <id>https://karobben.github.io/2024/02/02/LearnNotes/ai-learning/</id>
    <published>2024-02-02T18:58:50.000Z</published>
    <updated>2024-02-14T06:22:54.030Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Learning">Learning</h2><div class="admonition note"><p class="admonition-title">Biological inspiration: Long-term potentiation</p><ol><li>A synapse is repeatedly stimulated<br></li><li>More dendritic receptors<br></li><li>More neurotransmitters<br></li><li>A stronger link between neurons</li></ol></div><ul><li>Mathematical Model this Biological Learning Model:<ul><li><em><strong>X</strong></em> = input signal; <em><strong>f(X)</strong></em> = output signal</li><li><strong>Learning</strong> = adjust the parameters of the learning machine so that <em><strong>f(x)</strong></em> becomes the function we want</li></ul></li><li>Mathematical Model of Supervised Learning<ul><li><em><strong>D = {(x<sub>1</sub>, y<sub>1</sub>), ‚Ä¶, (x<sub>n</sub>, y<sub>n</sub>)}</strong></em> = training dataset containing pairs of (example signal <em><strong>x<sub>i</sub></strong></em>, desired system output <em><strong>y<sub>i</sub></strong></em>)</li><li><strong>Supervised Learning</strong> = adjust parameters of the learner to minimize <em><strong>E[‚Ñì(Y, f(X))]</strong></em></li><li><em><strong>‚Ñì</strong></em>: loss function</li></ul></li></ul><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/228ikan.png" alt="loss-function"></th></tr></thead><tbody></tbody></table><h3 id="Decision-tree-learning-An-example">Decision tree learning: An example</h3><p>The Titanic sank. You were rescued. You want to know if your friend was also rescued. You can‚Äôt find them. Can you use machine learning methods to estimate the probability that your friend survived? (Calculate the possibility of your friend also be rescued)</p><ol><li>Gather data about as many of the passengers as you can.<ul><li>X = variables that describe the passenger, e.g., age, gender, number of siblings on board.</li><li>Y = 1 if the person is known to have survived</li></ul></li><li>Learn a function, f(X), that matches the known data as well as possible</li><li>Apply f(x) to your friend‚Äôs facts, to estimate their probability of survival</li></ol><p><strong>Decision-tree learning</strong>*:</p><ul><li>1st branch = variable that best distinguishes between groups with higher vs. lower survival rates (e.g., gender)</li><li>2nd branch = variable that best subdivides the remaining group</li><li>Quit when all people in a group have the same outcome, or when the group is too small to be reliably subdivided.</li></ul><table><thead><tr><th style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/e/eb/Decision_Tree.jpg" alt="Decision-tree for Titanic"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://en.m.wikipedia.org/wiki/File:Decision_Tree.jpg">¬© wikipedia</a></td></tr></tbody></table><p>In each leaf node of this tree:</p><ul><li><p>Number on the left = probability of survival</p></li><li><p>Number on the right = percentage of all known cases that are explained by this node</p></li><li><p>A decision tree is an example of a parametric learner</p></li><li><p>The function <em><strong>f(x)</strong></em> is determined by some learned parameters</p></li><li><p>In this case, the parameters are:</p></li><li><p>Should this node split, or not?</p></li><li><p>If so, which tokens go to the right-hand child?</p></li><li><p>If not, what is <em><strong>f(x)</strong></em> at the current node?</p></li><li><p>Titanic shipwreck example:</p><ul><li>Œò = [Y, female, Y, age ‚â§ 9.5, N, f(x) = 0.73, ‚Ä¶]</li></ul></li></ul><div class="admonition note"><p class="admonition-title">A mathematical definition of learning</p><ul><li>Environment: there are two random variables, X and Y, that are jointly distributed according to</li><li><em><strong>P(X,Y)</strong></em></li><li>Data: <em><strong>P(X, Y)</strong></em> is unknown, but we have a sample of training data</li><li><em><strong>D = {(x~1~, y~1~), ..., (x~n~, y~n~)}</strong></em></li><li>Objective: We would like a function ÔøΩ that minimizes the expected value of some loss function, <em><strong>‚Ñì(Y , f(x))</strong></em> :</li><li><em><strong>‚Ñõ = E[‚Ñì(Y, f(x))]</strong></em></li><li>Definition of learning: Learning is the task of estimating the function <em><strong>f</strong></em>, given knowledge of <em><strong>D</strong></em>.</li></ul></div><h3 id="Training-vs-Test-Corpora">Training vs. Test Corpora</h3><ul><li><strong>Training Corpus</strong> = a set of data that you use in order to optimize the parameters of your classifier (for example, optimize which features you measure, how you use those features to make decisions, and so on).<ul><li>Measuring the training corpus accuracy is important for debugging: if your training algorithm is working, then training corpus error rate should always go down.</li></ul></li><li><strong>Test Corpus</strong> = a set of data that is non-overlapping with the training set (none of the test tokens are also in the training dataset) that you can use to measure the error rate.<ul><li>Measuring the test corpus error rate is the only way to estimate how your classifier will work on new data (data that you‚Äôve never yet seen)</li></ul></li><li><strong>Training error</strong> is sometimes called ‚Äúoptimization error‚Äù. It happens because you haven‚Äôt finished optimizing your parameters.</li><li><strong>Test error</strong> = <mark>optimization error + generalization error</mark></li><li><strong>Evaluation Test Corpus</strong> = a dataset that is used only to test the ONE classifier that does best on DevTest. From this corpus, you learn how well your classifier will perform in the real world.</li></ul><h3 id="Early-stopping">Early stopping</h3><ul><li><p><strong>Learning</strong>: Given $\mathcal{D} = {(x_1, y_1), \ldots, (x_n, y_n)}$, find the function $f(X)$ that minimizes some measure of risk.</p></li><li><p><strong>Empirical risk</strong>: a.k.a. training corpus error:</p><ul><li>$R_{\text{emp}} = \frac{1}{n} \sum_{i=1}^{n} \ell(y_i, f(x_i))$</li></ul></li><li><p><strong>True risk</strong>, a.k.a. expected test corpus error:</p><ul><li>$R = \mathbb{E}[\ell(Y, f(X))] = R_{\text{emp}} + R_{\text{generalization}}$</li></ul></li><li><p>Usually, minimum test error and minimum dev error don‚Äôt occur at the same time</p></li><li><p>‚Ä¶ but early stopping based on the test set is cheating,</p></li><li><p>‚Ä¶ so early stopping based on the dev set is the best we can do w/o cheating.</p></li></ul><h2 id="Summary">Summary</h2><ul><li><strong>Biological inspiration</strong>: Neurons that fire together wire together. Given enough training examples <em><strong>(x<sub>i</sub>, y<sub>i</sub>)</strong></em>, can we learn a desired function so that <em><strong>f(x) ‚âà y</strong></em>?</li><li><strong>Classification tree</strong>: Learn a sequence of if-then statements that computes <em><strong>f(x) ‚âà y</strong></em></li><li><strong>Mathematical definition of supervised learning</strong>: Given a training dataset, <em><strong>D = {(x<sub>1</sub>, y<sub>1</sub>), ‚Ä¶, (x<sub>n</sub>, y<sub>n</sub>)}</strong></em> , find a function <em><strong>f</strong></em> that minimizes the risk, <em><strong>‚Ñõ = E[‚Ñì(Y, f(x))]</strong></em>.</li><li><strong>Overtraining</strong>: $‚Ñõ_ {emp} = \frac{1}{n} \sum^n_{i=1} ‚Ñì*y_i, f(x_i))$ reaches zero if you train long enough.</li><li><strong>Early Stopping</strong>: Stop when error rate on the dev set reaches a minimum</li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Learning Progress</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Naive Bayes and Bayes NetWork</title>
    <link href="https://karobben.github.io/2024/02/01/LearnNotes/ai-bayes/"/>
    <id>https://karobben.github.io/2024/02/01/LearnNotes/ai-bayes/</id>
    <published>2024-02-02T05:20:50.000Z</published>
    <updated>2024-02-14T06:22:54.030Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Naive-Bayes">Na√Øve Bayes</h2><div class="admonition note"><p class="admonition-title">The problem with likelihood: Too many words</p><p>What does it mean to say that the words, x, have a particular probability?<br>Suppose our training corpus contains two sample emails:<br></p><ul><li>Email1: Y = spam, X =&quot;Hi there man ‚Äì feel the vitality! Nice meeting you‚Ä¶&quot;<br></li><li>Email2: Y = ham, X =&quot;This needs to be in production by early afternoon‚Ä¶&quot;<br></li></ul><p>Our test corpus is just one email:<br></p><ul><li>Email1: X=&quot;Hi! You can receive within days an approved prescription for increased vitality and stamina&quot;<br>How can we estimate P(X=&quot;Hi! You can receive within days an approved prescription for increased vitality and stamina&quot;|Y = spam)?<br></li></ul></div><p>One thing we could do is:</p><ol><li>$P(W = \text{‚Äúhi‚Äù} | Y = \text{spam}), P(W = \text{‚Äúhi‚Äù} | Y = \text{ham})$</li><li>$P(W = \text{‚Äúvitality‚Äù} | Y = \text{spam}), P(W = \text{‚Äúvitality‚Äù} | Y = \text{ham})$</li><li>$P(W = \text{‚Äúproduction‚Äù} | Y = \text{spam}), P(W = \text{‚Äúproduction‚Äù} | Y = \text{ham})$</li></ol><p>Then the approximation formula for $P(X | Y)$ is given by:</p><p>$$ P(X = x | Y = y) \approx \prod_{i=1}^{n} P(W = w_i | Y = y) $$</p><p>In this context, $W$ represents a word in a document, $X$ represents the document itself, $Y$ represents the class (spam or ham), $w_i$ represents the $i$-th word in the document, and $n$ is the total number of words in the document. The product is taken over all words in the document, assuming that the words are conditionally independent of each other given the class label $Y$.</p><div class="admonition question"><p class="admonition-title">Why na√Øve Bayes is na√Øve?</p><p>We call this model &quot;na√Øve Bayes&quot; because the words aren't really conditionally independent given the label. For example, the sequence &quot;for you&quot; is more common in spam emails than it would be if the words &quot;for&quot; and &quot;you&quot; were conditionally independent.<br><strong>True Statement</strong>:<br></p><ul><li><em><strong>P(X = for you|Y= Spam &gt; P( W = for |Y = Spam)(P = you |= Spam)</strong></em><br>The na√Øve Bayes approximation simply says: estimating the likelihood of every word sequence is too hard, so for computational reasons, we'll pretend that sequence probability doesn't matter.<br></li></ul><p><strong>Na√Øve Bayes Approximation</strong>:<br></p><ul><li><em><strong>P(X = for you |Y = Spam) ‚âà P(W = for |Y = Spam)P( W= you |Y= Spam)</strong></em><br>We use na√Øve Bayes a lot because, even though we know it's wrong, it gives us computationally efficient algorithms that work remarkably well in practice.</li></ul></div><h3 id="Floating-point-underflow">Floating-point underflow</h3><p>That equation has a computational issue. Suppose that the probability of any given word is roughly <em><strong>P(W = W<sub>i</sub>|Y = y) ‚âà 10<sup>-3</sup></strong></em>, and suppose that there are 103 words in an email. Then <em><strong>‚àè<sup>n</sup><sub>i=1</sub> P(W = W<sub>i</sub>|Y = y) = 10<sup>-309</sup></strong></em>,which gets rounded off to zero. This phenomenon is called ‚Äúfloating-point underflow‚Äù.</p><div class="admonition note"><p class="admonition-title">Solution</p><p>$$f(x) = \underset{y}{\mathrm{argmax}} \left( \ln P(Y = y) + \sum^n_{i=1} \ln P(W = w_i | Y = y) \right)$$</p></div><h3 id="Reducing-the-naivety-of-naive-Bayes">Reducing the naivety of na√Øve Bayes</h3><p>Remember that the bag-of-words model is unable to represent this fact:</p><ul><li><p><strong>True Statement</strong>:</p><ul><li><em><strong>P(X = for you|Y= Spam &gt; P( W = for |Y = Spam)(P = you |= Spam)</strong></em><br>Though the bag-of-words model can‚Äôt represent that fact, we can represent it using a slightly more sophisticated na√Øve Bayes model, called a ‚Äúbigram‚Äù model.</li></ul></li><li><p>N-Grams:</p><ul><li>Unigram: a unigram (1-gram) is an isolated word, e.g., ‚Äúyou‚Äù</li><li>Bigram: a bigram (2-gram) is a pair of words, e.g., ‚Äúfor you‚Äù</li><li>Trigram: a trigram (3-gram) is a triplet of words, e.g., ‚Äúprescription for you‚Äù</li><li>4-gram: a 4-gram is a 4-tuple of words, e.g., ‚Äúapproved prescription for you‚Äù</li></ul></li></ul><div class="admonition note"><p class="admonition-title">Bigram na√Øve Bayes</p><p>A bigram na√Øve Bayes model approximates the bigrams as conditionally independent, instead of the unigrams. For example,<br><em><strong>P(X = ‚Äúapproved prescription for you‚Äù | Y= Spam) ‚âà</strong></em><br><em><strong>P(B = ‚Äúapproved prescription‚Äù |Y = Spam) √ó</strong></em><br><em><strong>P(B = ‚Äúprescription for‚Äù | Y= Spam) √ó</strong></em><br><em><strong>P(B = ‚Äúfor you‚Äù |Y = Spam)</strong></em></p></div><ul><li>The na√Øve Bayes model has two types of parameters:<ul><li>The a priori parameters: <em><strong>P(Y = y)</strong></em></li><li>The likelihood parameters: <em><strong>P(W = w<sub>i</sub>| Y = y)</strong></em></li></ul></li><li>In order to create a na√Øve Bayes classifiers, we must somehow estimate the numerical values of those parameters.</li><li>Model parameters: feature likelihoods <em><strong>P(Word | Class)</strong></em> and priors <em><strong>P(Class)</strong></em></li></ul><h3 id="Parameter-estimation-Prior">Parameter estimation: Prior</h3><p>The prior, <em><strong>P(x)</strong></em>, is usually estimated in one of two ways.</p><ul><li>If we believe that the test corpus is like the training corpus, then we just use frequencies in the training corpus:<br>$$<br>P(Y = Spam) = \frac{Docs(Y=Spam)}{Docs(Y=Spam) + Docs(Y \neq Spam)}<br>$$<br>where <em><strong>Docs(Y=Spam)</strong></em> means the number of documents in the training corpus that have the label Y=Spam.</li><li>If we believe that the test corpus is different from the training corpus, then we set <em><strong>P(Y = Spam)</strong></em> = the frequency with which we believe spam will occur in the test corpus.</li></ul><h3 id="Parameter-estimation-Likelihood">Parameter estimation: Likelihood</h3><p>The likelihood, ***P(W = w<sub>i</sub>|Y = y), is also estimated by counting. The ‚Äúmaximum likelihood estimate of the likelihood parameter‚Äù is the most intuitively obvious estimate:<br>$$<br>P(W=w_i| Y = Spam) = \frac{Count(W=w_i, Y = Spam)}{Count(Y = Spam)}<br>$$<br>where <em><strong>Count(W=w<sub>i</sub>, Y = Spam)</strong></em> means the number of times that the word <em><strong>w<sub>i</sub></strong></em> occurs in the Spam portion of the training corpus, and <em><strong>Count(Y = Spam)</strong></em> is the total number of words in the Spam portion.</p><h3 id="Laplace-Smoothing-for-Naive-Bayes">Laplace Smoothing for Na√Øve Bayes</h3><p>One of the biggest challenge for Bayes is it can‚Äôt handle unobserved situation.</p><ul><li><p>The basic idea: add $k$ ‚Äúunobserved observations‚Äù to the count of every unigram</p><ul><li>If a word occurs 2000 times in the training data, Count = 2000+k</li><li>If a word occur once in training data, Count = 1+k</li><li>If a word never occurs in the training data, then it gets a pseudo-Count of $k$</li></ul></li><li><p>Estimated probability of a word that occurred Count(w) times in the training data:<br>$$ P(W = w) = \frac{k + \text{Count}(W = w)}{k + \sum_v (k + \text{Count}(W = v))} $$</p></li><li><p>Estimated probability of a word that never occurred in the training data (an ‚Äúout of vocabulary‚Äù or OOV word):<br>$$ P(W = \text{OOV}) = \frac{k}{k + \sum_v (k + \text{Count}(W = v))} $$</p></li><li><p>Notice that<br>$$ P(W = \text{OOV}) + \sum_w P(W = w) = 1 $$</p></li></ul><h2 id="Bayesian-Networks">Bayesian Networks</h2><div class="admonition question"><p class="admonition-title">Why Network?</p><ul><li>Example: $Y$ is a scalar, but $X = [X_1, ‚Ä¶ , X_{100}]^T$ is a vector</li><li>Then, even if every variable is binary, $P(Y = y|X = x)$ is a table with $2^{101}$ numbers. Hard to learn from data; hard to use.</li></ul></div><p>A better way to represent knowledge: Bayesian network</p><ul><li>Each variable is a node.</li><li>An arrow between two nodes means that the child depends on the parent.</li><li>If the child has no direct dependence on the parent, then there is no arrow.</li></ul><h3 id="Space-Complexity-of-Bayesian-Network">Space Complexity of Bayesian Network</h3><pre class="mermaid">graph LR    B --> A    E --> A    A --> J    A --> M</pre><ul><li>Without the Bayes network: I have 5 variables, each is binary, so the probability distribution $P(B, E, A, M, J)$ is a table with $2^5 = 32$ entries.</li><li>With the Bayes network:<ul><li>Two of the variables, B and E, depend on nothing else, so I just need to know $P(B = ‚ä§)$ and $P(E = ‚ä§)$ ‚Äî 1 number for each of them.</li><li>M and J depend on A, so I need to know $P(M = ‚ä§|A = ‚ä§)$ and $P(M = ‚ä§|A = ‚ä•)$ ‚Äì 2 numbers for each of them.</li><li>A depends on both B and E, so I need to know $P(A = ‚ä§|B = b, E =e)$ for all 4 combinations of $(b, e)$</li><li>Total: 1+1+2+2+4 = 10 numbers to represent the whole distribution!</li></ul></li></ul><p>$$<br>P(B = T \mid J = T, M = T) = \frac{P(B = T, J = T, M = T)}{P(J = T, M = T)} \\<br>= \frac{P(B = T, J = T, M = T)}{P(J = T, M = T)} + \frac{P(B = L, J = T, M = T)}{P(J = T, M = T)} \\<br>= \sum_{e=T}^{L} \sum_{a=T}^{L} P(B = T, E = e, A = a, J = T, M = T) \\<br>= \sum_{e=T}^{L} \sum_{a=T}^{L} P(B = T) P(E = e) \ √ó \\<br>P(A = a \mid B = T, E = e) P(J = T \mid A = a) P(M = T \mid A = a)<br>$$</p><h3 id="Variables-are-independent-and-or-conditionally-independent">Variables are independent and/or conditionally independent</h3><h4 id="Independence">Independence</h4><pre class="mermaid">graph TDB --> AE --> A</pre><ul><li>Variables are independent if they have no common ancestors<br>$ P(B = \top, E = \top) = P(B = \top)P(E = \top)= P(B = \top) $</li></ul><p>!!! Independent variables may not be conditionally independent</p><ul><li>The variables B and E are not conditionally independent of one another given knowledge of A</li><li>If your alarm is ringing, then you probably have an earthquake OR a burglary. If there is an earthquake, then the conditional probability of a burglary goes down:<ul><li>$P(B = \top| E = \top, A = \top) \neq P(B = \top| E = \bot, A = \top)$</li></ul></li><li>This is called the ‚Äúexplaining away‚Äù effect. The earthquake ‚Äúexplains away‚Äù the alarm, so you become less worried about a burglary.</li></ul><h4 id="Conditional-Independence">Conditional Independence</h4><pre class="mermaid">graph TDA --> JA --> M</pre><ul><li>The variables J and M are conditionally independent of one another given knowledge of A</li><li>If you know that there was an alarm, then knowing that John texted gives no extra knowledge about whether Mary will text:<br>$P(M = \top | J = \top , A = \top)=P(M = \top | J = \bot, A = \top)= P(M = \top| A = \top)$</li></ul><div class="admonition note"><p class="admonition-title">Conditionally Independent variables may not be independent</p><ul><li>The variables J and M are not independent!</li><li>If you know that John texted, that tells you that there was probably an alarm. Knowing that there was an alarm tells you that Mary will probably text you too:</li><li>$P(M = \top| J = \top) \neq P(M= \top| J = \bot)$</li></ul></div><ul><li>Variables are conditionally independent of one another, given their common ancestors, if (1) they have no common descendants, and (2) none of the descendants of one are ancestors of the other<br>$ P(U = T, M = T \mid A = T) = P(U = T \mid A = T)P(M = T \mid A = T) $</li></ul><h4 id="How-to-tell-at-a-glance-if-variables-are-independent-and-or-conditionally-independent">How to tell at a glance if variables are independent and/or conditionally independent</h4><pre class="mermaid">graph LR    B --> A    E --> A    A --> J    A --> M</pre><ul><li>Variables are independent if they have no common ancestors<ul><li>$P(B = \top, E = \top) = P(B=\top)P(E =\top)$</li></ul></li><li>Variables are conditionally independent of one another, given their common ancestors, if:<ol><li>they have no common descendants, and</li><li>none of the descendants of one are ancestors of the other</li></ol><ul><li>$P(J = \top, M = \top| A = \top) = P(J = \top| A = \top) P (M = \top| A = \top)$</li></ul></li></ul><style>pre {  background-color:#EEFFEF ;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Naive Bayes and Bayes NetWork</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>OPTICAL SPECTROSCOPY ‚Äì THE ABSORPTION PROCESS</title>
    <link href="https://karobben.github.io/2024/01/30/LearnNotes/absorption/"/>
    <id>https://karobben.github.io/2024/01/30/LearnNotes/absorption/</id>
    <published>2024-01-31T05:37:02.000Z</published>
    <updated>2024-02-05T16:04:10.670Z</updated>
    
    <content type="html"><![CDATA[<h2 id="OPTICAL-SPECTROSCOPY-‚Äì-THE-ABSORPTION-PROCESS">OPTICAL SPECTROSCOPY ‚Äì THE ABSORPTION PROCESS</h2><table><thead><tr><th style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/1/14/EM_Spectrum_Properties_%28Amplitude_Corrected%2C_Bitmap%29.png" alt="Eelctromagenetic spectrum"></th></tr></thead><tbody><tr><td style="text-align:center">¬© wiki</td></tr></tbody></table><ul><li>Small wavelength; High frequency; Blue end of the visible spectrum</li><li>Long wavelength; Low frequency; Red end of the visible spectrum</li><li>Energy of visible light is about 100-500 kJ/mol</li></ul><h2 id="Beer‚Äôs-Law-and-Absorbance">Beer‚Äôs Law and Absorbance</h2><p>$$<br>\frac{I}{I_ 0} = 10^ {-\frac{kc(\Delta y)}{2.303}} = 10^ {- \epsilon c (\Delta y)} = 10^ {-A}<br>$$</p><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/66fOitE.png" alt="Beer‚Äôs Law"></th><th style="text-align:left"><mark>Define of absorbance</mark>: <li> $ A = \epsilon c (\Delta y)$ or $A=\epsilon c l $<li> <em><strong>Œµ</strong></em>: Molar extinction coefficient<li> <em><strong>c</strong></em>: concentration in <em><strong>M</strong></em><li> <em><strong>Œîy</strong></em>: path length in <em><strong>cm</strong></em> (some place use <em><strong>l</strong></em> as <em><strong>Œîy</strong></em>)</th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://imamagnets.com/en/blog/the-beer-lambert-law/">¬© imamagnets</a></td><td style="text-align:left"></td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Application of Absorbance</p><p>Use UV-Vis absorbance to calculate the concentration of the molecules like DNA, protein, etc.</p><p>$A=\epsilon (\lambda) c l$</p></div><table><thead><tr><th>Molecule</th><th>Œª (nm)</th><th>Œµ (√ó10<sup>-3</sup>) (M<sup>-1</sup>cm<sup>-1</sup>)</th></tr></thead><tbody><tr><td>Adenine</td><td>260.5</td><td>13.4</td></tr><tr><td>Adenosine</td><td>259.5</td><td>14.9</td></tr><tr><td>NADH</td><td>340, 259</td><td>6.23, 14.4</td></tr><tr><td>NAD+</td><td>260</td><td>18</td></tr><tr><td>FAD</td><td>450</td><td>11.2</td></tr><tr><td>Tryptophan</td><td>280, 219</td><td>5.6, 47</td></tr><tr><td>Tyrosine</td><td>274,222,193</td><td>1.4, 8, 48</td></tr><tr><td>Phenylalanine</td><td>257, 206, 188</td><td>0.2, 9.3, 60</td></tr><tr><td>Histidine</td><td>211</td><td>5.9</td></tr><tr><td>Cysteine</td><td>250</td><td>0.3</td></tr></tbody></table><h2 id="Quantum-Mechanical-Transition-Probability">Quantum Mechanical Transition Probability</h2><p><img src="https://imgur.com/63nZqbL.png" alt="Transition Probability"></p><p>The probability per unit time that a molecule in state 1 will end up in state 2 in the presence of an oscillating electromagnetic field at the resonance frequency<br><mark>Energy of the light = Difference between energy levels</mark></p><p>$$<br>Rate_ {1 ‚Üí 2} = B_ {12} \rho (\nu)[S_ 1]<br>$$</p><ul><li><em><strong>B<sub>12</sub></strong></em>: rate constant</li><li><em><strong>œÅ(ŒΩ)</strong></em>: radiation field density</li><li><em><strong>S<sub>1</sub></strong></em>: Concentration of molecules in the ground state</li></ul><div class="admonition note"><p class="admonition-title">Rate constant dependents on the transition dipole moment </p><p>$B_{12} \propto \langle \mu \rangle^2$<br>$\langle \mu \rangle = \int \Psi_2 (q_e\vec{r})\Psi dV$<br>Transition dipole moment:<br>$\langle \mu \rangle \propto$ overlap between $\Psi_1$ and $\Psi_2$<br></p></div><h3 id="Dipole-approximation">Dipole approximation</h3><p>When a light wave hit hydrogen atom, the <em><strong>r</strong></em> from the atom into electron is far smaller than the <em><strong>Œª</strong></em>.</p><ul><li>$\because r &lt;&lt; \lambda$</li><li>$\vec{\mu} = - q\vec{r}$</li><li>$Energy = -\vec{\mu}\cdot\vec{E}$<ul><li>$\vec{\mu}$: matter</li><li>$\vec{E}$: Electric Field (amplitude of light)</li></ul></li></ul><table><thead><tr><th style="text-align:center"><img src="https://i.stack.imgur.com/Dmk1Z.png" alt="Transition of the Dipole"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://chemistry.stackexchange.com/questions/90956/are-there-any-simple-molecules-with-very-different-absorption-and-emission-dip">¬© Sentry</a></td></tr><tr><td style="text-align:center">The transition dipole reflects the change of electron distribution by excitation</td></tr></tbody></table><table><thead><tr><th style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/e/e0/StationaryStatesAnimation.gif" alt="Transition of the Dipole"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://en.wikipedia.org/wiki/Transition_dipole_moment">¬© wikipedia</a></td></tr><tr><td style="text-align:center">Transition dipole moment</td></tr></tbody></table><h3 id="Overlap-of-wavefunctions-and-transition-probability">Overlap of wavefunctions and transition probability</h3><p>$$\mu_{mn} = \int_{-\infty}^{\infty} \Psi_n^* \left( \sum_{i=1}^{N} q_e \vec{r}_i \right) \Psi_m , dr $$</p><p>Where:</p><ul><li>$N$: Number of electrons in a molecule</li><li>$\vec{r}_i$: Position of each electron</li><li>$q_e$: electron charge</li><li>$\Psi_n$: Excited state (or final state) molecular wave function</li><li>$\Psi_m$: Ground state (or initial state) molecular wave function</li></ul><p>Note: each molecular wavefunction depends on the position of BOTH Nucleus AND electron but for now, let‚Äôs focus on electron wavefunction (i.e. no structural change of molecular structure or atom position).<br>Larger overlap of initial and final state wavefunctions means higher transition probability, which generates higher extinction coefficient (Fermi‚Äôs golden rule).</p><p><strong>Wavefunction overlap:</strong> Larger wavefunction overlap of initial and final state means higher transition probability, which generates higher extinction coefficient (Fermi‚Äôs golden rule).</p><p><strong>Orbital Symmetry:</strong></p><p>$\int_{-\infty}^{\infty} f(r ) , dr = 0 \quad \text{if} \quad f(r )$ is an odd function; i.e., $f(-x) = -f(x)$<br>$\int_{-\infty}^{\infty} f(r ) , dr \neq 0 \quad \text{if} \quad f(r )$ is an even function; i.e., $f(-x) = f(x)$</p><p>So: $\Psi_n^* \tilde{\Psi}_m$ must be an even function;<br>or: $\Psi_n^* \Psi_m$ must be an odd function (e.g., } $\pi$ and $\pi^ *$ state</p><h3 id="Dipole-strength-and-Oscillator-strength">Dipole strength and Oscillator strength</h3><p>Traditional ways to quantify the ‚Äústrength of a transition‚Äù</p><p><strong>Dipole strength</strong>: $D_{mn} = |\mu_{mn}|^2 = 9.18 \times 10^{-3} \int \left( \frac{\varepsilon}{\nu} \right) d\nu $<br><strong>Oscillator strength</strong>: $f_{mn} = 4.315 \times 10^{-9} \int \varepsilon(\nu) d\nu $<br>Area under the spectrum associated with the <em><strong>m‚Üín</strong></em> transition</p><p>$ f_{mn} \approx 0.1-1 $ Strong absorption (heme, chlorophyll, organic dyes)<br>$ f_{mn} \approx 10^{-5} $ Weak absorption</p><h3 id="Kuhn-Thomas-sum-rule-for-oscillator-strengths">Kuhn-Thomas sum rule for oscillator strengths</h3><p>In any molecule with N electrons the sum of the oscillator strengths from any one state to all of the other states is equal to the sum of the electrons<br>$$<br>\sum_j f_{ij} = N<br>$$</p><p>This means that <strong>the area underneath the absorption spectrum is a constant</strong> (ground state is the initial state).<br>If a molecule is perturbed (change environments) then if one transition goes down, another must go up.</p><div class="admonition note"><p class="admonition-title">Every transition is associated with a transition dipole</p><p>The transition dipole is a vector:</p><ul><li>Direction: point in the direction of the electron displacement</li><li>Amplitude: Strength of the absorption.</li></ul></div><h3 id="Possible-transitions-in-UV-Vis-light-range">Possible transitions in UV-Vis light range</h3><table><thead><tr><th style="text-align:center"><img src="http://www.chem.ucla.edu/~bacher/UV-vis/electronic_energy_diagram.jpg" alt=""></th><th style="text-align:center"><img src="http://www.chem.ucla.edu/~bacher/UV-vis/UV_vis_tetracyclone.jpg" alt=""></th></tr></thead><tbody></tbody></table><p><a href="http://www.chem.ucla.edu/~bacher/UV-vis/uv_vis_tetracyclone.html.html">¬© ucla.edu</a></p><p><em><strong>œÉ‚ÜíœÉ<sup>*</sup></strong></em> often requires absorption of photons higher than the UV- vis range (200-700 nm).</p><h3 id="What-determines-the-probability-of-a-transition-strength-of-the-absorption-band">What determines the probability of a transition? (strength of the absorption band)</h3><ol><li>Orbital overlap (wavefunction)<br>œÄ ‚ÜíœÄ * Large overlap, more likely to happen, strong absorption<br>n ‚ÜíœÄ * Small overlap, weak absorption</li><li>Spin multiplicity<br>Electrons prefer not to change its intrinsic spin direction after absorption.</li></ol><p><img src="https://imgur.com/zDYufIq.png" alt=""></p><h3 id="Effective-light-matter-interaction">Effective light-matter interaction</h3><p><img src="https://imgur.com/xIpNElQ.png" alt=""></p><ul><li><p><strong>Transition Rate:</strong><br>$$ Rate_{1 \rightarrow 2} = B_{12} \rho(\nu) [S_1]$$<br>Radiation field density</p></li><li><p><strong>Component of the Electric Field:</strong><br>$$ E_{\parallel} = |\vec{E}| \cos \theta$$</p></li><li><p><strong>Density of States:</strong><br>$$ \rho(\nu) \propto |E_{\parallel}|^2 = |\vec{E}|^2 \cos^2 \theta$$</p></li></ul><h3 id="Effective-light-matter-interaction-v2">Effective light-matter interaction</h3><p>The density of states $\rho(\nu)$ is proportional to the square of the parallel component of the electric field:</p><p>$$ \rho(\nu) \propto |E_{\parallel}|^2 = |\vec{E}|^2 \cos^2 \theta$$</p><p>This relationship is depicted through diagrams that illustrate the electric field vector $\vec{E}$ relative to the molecular transition dipole moment $\vec{\mu}$. The alignment of $\vec{E}$ with $\vec{\mu}$ affects the absorption, with maximum absorption when they are parallel and zero absorption when they are perpendicular. This is exemplified by the molecular orientations of adenine shown in the image.</p><h3 id="Absorption-emission-and-stimulated-emission">Absorption, emission, and stimulated emission</h3><p>The rates of absorption, emission, and stimulated emission can be described by the following equations:</p><table><thead><tr><th style="text-align:left">Rates</th><th style="text-align:left">Equetion</th></tr></thead><tbody><tr><td style="text-align:left"><strong>Absorption Rate</strong></td><td style="text-align:left">$ Rate_{abs} = B_{12} \rho(\nu) [S_1] $</td></tr><tr><td style="text-align:left"><strong>Emission Rate</strong></td><td style="text-align:left">$ Rate_{emi} = -A_{21} [S_2] $</td></tr><tr><td style="text-align:left"><strong>Stimulated Emission Rate</strong></td><td style="text-align:left">$ Rate_{se} = -B_{21} \rho(\nu) [S_2] $</td></tr></tbody></table><p>At steady state, the rate of upward transitions (absorption) equals the rate of downward transitions (emission and stimulated emission):<br>$$ B_{12} \rho(\nu) [S_1] = A_{21} [S_2] + B_{21} \rho(\nu) [S_2] $$</p><p><mark>A<sub>21</sub>, B<sub>12</sub>, and B<sub>21</sub> are called Einstein coefficients.</mark></p><p><img src="https://imgur.com/xLU05Eh.png" alt=""></p><p>It can be shown that:</p><ol><li>B<sub>12</sub>=B<sub>21</sub></li><li>$\frac{A_ {21}}{B_ {21}} = \frac{16\pi^ 2 \hbar \nu^ 3}{c^ 3}$</li></ol><p>Faster spontaneous emission at higher Frequency</p><p>In a typical UV-Vis spectroscopy (electronic transitions)</p><p><strong>Conditions:</strong> $ A \gg B \rho(\nu) $<br><strong>Einstein coefficients relationships:</strong> $ B_{12} \rho(\nu) [S_1] = A_{21} [S_2] + B_{21} \rho(\nu) [S_2] \approx A_{21} [S_2] $<br><strong>Approximations:</strong> $ \frac{B_{12} \rho(\nu)}{A_{21}} \frac{[S_2]}{[S_1]} \ll 1 $<br><strong>Population of states:</strong> $ [S_2] \ll [S_1] $<br>The population of the excited state never builds up to a significant amount.</p><p>laser requires stimulated emission rate constant, i.e. B21, to be much larger than the spontaneous emission rate constant, i.e. A21.<br>So, UV laser is harder to make than visible light laser</p><h2 id="Boltzmann-Distribution">Boltzmann Distribution</h2><p>The probability $P_i$ of a system being in a state i with energy $E_i$ at temperature T is given by:</p><p>$$ P_i = \frac{e^{-\frac{E_i}{k_B T}}}{\sum_{i=0}^{M} e^{-\frac{E_i}{k_B T}}} = \frac{e^{-\frac{E_i}{k_B T}}}{Q} $$</p><p>Where:</p><ul><li>$Q$: Partition function</li><li>$E_i$: Energy of the ith state</li><li>$k_B$: Boltzmann constant $= 1.38 \times 10^{-23} J/K$</li><li>$T$: Temperature (K)</li></ul><p>The ratio of probabilities between two states i and j is given by:</p><p>$$ \frac{P_i}{P_j} = e^{-\frac{(E_i - E_j)}{k_B T}} = e^{-\frac{\Delta E}{k_B T}} $$</p><h3 id="Quantum-Mechanical-Harmonic-Oscillator">Quantum Mechanical Harmonic Oscillator</h3><table><thead><tr><th style="text-align:center"><img src="https://www.researchgate.net/profile/Mauricio-Palafox/publication/277716088/figure/fig6/AS:651496681140224@1532340321204/Light-behaves-in-exactly-the-same-way-as-a-quantum-harmonic-oscillator-and-has-the-same.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.researchgate.net/publication/277716088_Spectra_and_structure_of_benzonitriles_and_some_of_its_simple_derivatives_Spectra_and_structure_of_benzonitriles_and_some_of_its_simple_derivatives?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">¬© Mauricio Alcolea Palafox</a></td></tr></tbody></table><table><thead><tr><th style="text-align:center"><img src="https://www.researchgate.net/profile/Anas-Al-Rabadi/publication/228529042/figure/fig3/AS:393722123571205@1470882077644/Harmonic-oscillator-HO-potential-and-wavefunctions-a-wavefunctions-for-various.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.researchgate.net/figure/Harmonic-oscillator-HO-potential-and-wavefunctions-a-wavefunctions-for-various_fig3_228529042">¬© Anas Al-Rabadi</a> The panel left: wave funciton, the panel right: porbability of finding a nuclei</td></tr></tbody></table><ul><li>Solve the time-independent Schr√∂dinger Equation (for the nucleus) with<ul><li>$ V(x) = \frac{1}{2}kx^2 $</li></ul></li><li>The time-independent Schr√∂dinger Equation is:<ul><li>$ -\frac{\hbar^2}{2m} \frac{d^ 2\Psi(x)}{dx^2} + V(x)\Psi(x) = E\Psi(x) $</li></ul></li><li>We get a set of wave functions and a set of energies:<ul><li>$ \Psi_n(x) \quad$ (set of wave functions)</li><li>$ E_n \quad $ (set of energies)</li></ul></li><li>For a <mark>harmonic oscillator potential</mark>, the energy levels are given by:<ul><li>$ E_n = \left(n + \frac{1}{2}\right)\hbar\omega $</li><li>$ \omega_0 = \sqrt{\frac{k}{m_r}} = 2\pi\nu_0 $<br>where $ n = 0,1,2,3,\ldots $</li></ul></li></ul><table><thead><tr><th style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Franck-Condon-diagram.png/800px-Franck-Condon-diagram.png" alt=""></th><th style="text-align:center"><img src="https://api.oe1.com/upload/2023/06/1_638205815647121689-20230606104546661.jpg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://en.wikipedia.org/wiki/Deslandres_table">¬© wikipedia</a> Vibrational energy of the nuclei on top of electronic energy</td><td style="text-align:center"><a href="https://www.oe1.com/article/7071678131478081536.html">¬© oe1.com</a> Jablonski energy diagram</td></tr></tbody></table><p><mark>VERTICAL TRANSITION</mark>: consider the nuclei to remain in the same place during an electronic transition.<br>At thermal equilibrium, most molecules will be in the lowest vibrational state</p><h3 id="Franck-Condon-factor-nuclear">Franck-Condon factor (nuclear)</h3><p>The total wavefunction $\Psi(r, R)$ is a product of the electronic $\Psi_{el}(r, R)$ and nuclear $\Psi_{nuc}¬Æ$ wavefunctions:</p><p>$$ \Psi(r, R) = \Psi_{el}(r, R)\Psi_{nuc}( R) $$</p><ul><li><code>electrons</code> refers to $\Psi_{el}(r, R)$</li><li><code>nuclei</code> refers to $\Psi_{nuc}(R )$</li></ul><p>Transition from vibrational level i of the ground electronic state to the vibrational level j of the exited electronic state is given by:</p><p>$$ \vec{\mu}_ {g \rightarrow ex,j} = \left( \vec{\mu}_ {g \rightarrow ex} \right) \int \Psi_ {nuc(j)}^* \Psi_ {nuc(i)} dR $$</p><ul><li>The electron transition dipole moment $\vec{\mu}_{g \rightarrow ex}$ represents the <code>Electron transition dipole moment</code>.</li><li>Rest of the integral represents the <code>Nuclear overlap Factor</code>, also known as the Franck-Condon factor.</li></ul><table><thead><tr><th style="text-align:center">Vibrational structure and the Franck- Condon principle: vertical transitions</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/tykLmAg.png" alt=""></td></tr></tbody></table><h3 id="Spectroscopic-broadening">Spectroscopic broadening</h3><h4 id="Intrinsic">Intrinsic</h4><table><thead><tr><th>Column1</th><th>Column2</th><th>Column3</th></tr></thead><tbody><tr><td>Vibrational Structure</td><td><img src="https://imgur.com/pmgO3tQ.png" alt=""></td><td><img src="https://imgur.com/eEq28Js.png" alt=""></td></tr><tr><td>Overlapping Electronic Bands</td><td><img src="https://imgur.com/g55mveQ.png" alt=""></td><td><img src="https://imgur.com/WxAS0gX.png" alt=""></td></tr></tbody></table><h4 id="Environment-solvent-effect">Environment: solvent effect</h4><p><img src="https://imgur.com/UzP9Z1j.png" alt=""></p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center"><li>25% Œª1 (state 1)<li> 50% Œª2 (state 2)<li> 25% Œª3 (state 3)</td><td style="text-align:center"><img src="https://imgur.com/FIuq6xu.png" alt=""></td></tr></tbody></table><h3 id="Solvent-effects-on-the-absorption-spectrum-of-anisole">Solvent effects on the absorption spectrum of anisole</h3><table><thead><tr><th style="text-align:center"><img src="https://psiberg.com/wp-content/uploads/2021/09/chromophoric-shift-path.svg" alt="Hypochromic shift"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://psiberg.com/uv-vis-spectroscopy">¬© PSIBERG Team</a></td></tr></tbody></table><p><img src="https://imgur.com/eU4ljWs.png" alt=""></p><p>to left: Bathochromic or <mark>Red shift</mark><br>to right: Hypsochromic or <mark>Blue shift</mark><br>The nature of the changes are not always simple to predict.</p><div class="admonition note"><p class="admonition-title">How does the solvent influence the ground and excited states?</p><ol><li>SOLVENT POLARITY: permanent dipole of the solvent molecules measured by the static dielectric constant. <em><strong>Œµ~r~</strong></em></li><li>SOLVENT POLARIZABILITY: electron polarizability measured by index of refraction. <em><strong>n</strong></em></li><li>Hydrogen bonding (protic vs. aprotic solvent)</li></ol></div><p><img src="https://imgur.com/q6g0Azf.png" alt=""></p><div class="admonition note"><p class="admonition-title">More on the dielectric constant</p></div><p><strong>Material 1</strong>: High <em><strong>Œµ<sub>r</sub></strong></em>, therefore higher ability to cancel out (stabilize) the original source charge<br><strong>Material 2</strong>: Low <em><strong>Œµ<sub>r</sub></strong></em>, The ability to insulate charge or The ability to stabilize charges</p><p>The relative permittivity $\varepsilon_r$ as a function of frequency $\omega$ is given by:</p><p>$$ \varepsilon_r(\omega) = \frac{\varepsilon(\omega)}{\varepsilon_0} $$</p><p>Where:</p><ul><li>$\varepsilon_0$: vacuum permittivity (= 1.0)</li><li>$\varepsilon$: material‚Äôs absolute permittivity</li><li>$\varepsilon_r$: relative permittivity or dielectric constant</li></ul><p>Examples of relative permittivity for different materials:</p><ul><li>$\varepsilon_r$ (styrofoam) = 1.03</li><li>$\varepsilon_r$ (dry wood) = 1.4 - 2.9</li><li>$\varepsilon_r$ = 20</li></ul><p>Relative permittivity values for various solvents:</p><table><thead><tr><th>Solvent</th><th>Hexane</th><th>Ether</th><th>Ethanol</th><th>Methanol</th><th>Water</th></tr></thead><tbody><tr><td>$\varepsilon_r$</td><td>2</td><td>4.3</td><td>25.8</td><td>31</td><td>81</td></tr></tbody></table><p>Small value means it is non-polar solvent. Large value means it is a polar solvent.</p><h3 id="Polar-effects-on-transitions-between-molecular-orbitals">Polar effects on transitions between molecular orbitals</h3><table><thead><tr><th style="text-align:center"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0030402617306472-gr9_lrg.jpg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.sciencedirect.com/science/article/pii/S0030402617306472#fig0045">¬© Vaishali Gupta</a></td></tr></tbody></table><table><thead><tr><th>Compound</th><th>Œª(nm)</th><th>Intensity/Œµ</th><th>Transition with lowest energy</th></tr></thead><tbody><tr><td>CH‚ÇÑ</td><td>122</td><td>intense</td><td>œÉ‚ÜíœÉ* (C-H)</td></tr><tr><td>CH‚ÇÉCH‚ÇÉ</td><td>130</td><td>intense</td><td>œÉ‚ÜíœÉ* (C-C)</td></tr><tr><td>CH‚ÇÉOH</td><td>183</td><td>200</td><td>n‚ÜíœÉ* (C-O)</td></tr><tr><td>CH‚ÇÉSH</td><td>235</td><td>180</td><td>n‚ÜíœÉ* (C-S)</td></tr><tr><td>CH‚ÇÉNH‚ÇÇ</td><td>210</td><td>800</td><td>n‚ÜíœÉ* (C-N)</td></tr><tr><td>CH‚ÇÉCl</td><td>173</td><td>200</td><td>n‚ÜíœÉ* (C-Cl)</td></tr><tr><td>CH‚ÇÉI</td><td>258</td><td>380</td><td>n‚ÜíœÉ* (C-I)</td></tr><tr><td>CH‚ÇÇ=CH‚ÇÇ</td><td>165</td><td>16000</td><td>œÄ‚ÜíœÄ* (C=C)</td></tr><tr><td>CH‚ÇÉCOCH‚ÇÉ</td><td>187</td><td>950</td><td>œÄ‚ÜíœÄ* (C=O)</td></tr><tr><td>CH‚ÇÉCOCH‚ÇÉ</td><td>273</td><td>14</td><td>n‚ÜíœÄ* (C=O)</td></tr><tr><td>CH‚ÇÉCSCl‚ÇÉ</td><td>460</td><td>weak</td><td>n‚ÜíœÄ* (C=S)</td></tr><tr><td>CH‚ÇÉN=NCCH‚ÇÉ</td><td>347</td><td>15</td><td>n‚ÜíœÄ* (N=N)</td></tr></tbody></table><h3 id="Polarity-effect-on-œÄ-œÄ-transitions">Polarity effect on œÄ-œÄ* transitions</h3><p><strong>Typical œÄ-œÄ<sup>*</sup> transitions</strong>: the dipole gets larger in the same direction<br>More stabilization, energy increases; <em><strong>high solvent polarity results in a RED SHIFT (of the absorption peak)</strong></em>.<br><img src="https://imgur.com/gvx01OG.png" alt=""><br>In a more polar state: More stabilization, energy increases; high solvent polarity results in a RED SHIFT (of the absorption peak).</p><p><strong>Typical n-œÄ<sup>*</sup> transitions</strong>: the dipole of the chromophore <strong>gets smaller</strong> or shifts direction after excitation.<br><img src="https://imgur.com/OIis4vS.png" alt=""><br>Less stabilization, energy increases; <strong>high solvent polarity</strong> results in a <strong>BLUE SHIFT (of the absorption peak)</strong></p><h3 id="Example-spectral-shifts-of-mesityl-oxide">Example: spectral shifts of mesityl oxide</h3><table><thead><tr><th style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/d/d8/Mesityl_oxide.png" alt="Mesityl oxide"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://en.wikipedia.org/wiki/Mesityl_oxide">¬© wikipedia</a></td></tr></tbody></table><p>The solvent effects on the absorption maxima (Œªmax) for the œÄ‚ÜíœÄ* and n‚ÜíœÄ* transitions in acetone:</p><table><thead><tr><th>solvent</th><th>Static dielectric constant</th><th>Œªmax (nm) œÄ‚ÜíœÄ* (Red shift)</th><th>Œªmax (nm) n‚ÜíœÄ* (Blue shift)</th></tr></thead><tbody><tr><td>Hexane</td><td>2</td><td>229.5</td><td>327</td></tr><tr><td>Ether</td><td>4.3</td><td>230</td><td>326</td></tr><tr><td>Ethanol</td><td>25.8</td><td>237</td><td>315</td></tr><tr><td>Methanol</td><td>31</td><td>238</td><td>312</td></tr><tr><td>Water</td><td>81</td><td>244.5</td><td>305</td></tr></tbody></table><ul><li><mark>Red shift</mark> indicates a lower energy transition as the dielectric constant increases.</li><li><mark>Blue shift</mark> indicates a higher energy transition as the dielectric constant decreases.</li></ul><p>The transitions are characterized by their molar absorptivities (Œµ):</p><ul><li>n‚ÜíœÄ*: Œµ = 40M‚Åª¬πcm‚Åª¬π</li><li>œÄ‚ÜíœÄ*: Œµ = 12,600M‚Åª¬πcm‚Åª¬π</li></ul><h3 id="Indol-Tryptophan-œÄ-œÄ-transitions">Indol (Tryptophan) œÄ-œÄ* transitions</h3><table><thead><tr><th style="text-align:center"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1386142506001314-gr1.jpg" alt=""></th><th style="text-align:center"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1386142506001314-gr3.gif" alt=""></th></tr></thead><tbody></tbody></table><p><a href="https://www.sciencedirect.com/science/article/pii/S1386142506001314?via%3Dihub">¬© Neera Sharma</a><br><em><strong>L<sub>a</sub></strong></em>: large excited state dipole (lower energy state)<br><em><strong>L<sub>b</sub></strong></em>: Smaller excited state dipole</p><h3 id="Solvent-polarizability-measured-by-the-index-of-refraction">Solvent polarizability measured by the index of refraction</h3><p>dipole is induced in the solvent by the dipole of the chromophore (ground state and excited state)<br>No nuclear movement involved. Purely due to electrons</p><p><img src="https://imgur.com/qK3vQLw.png" alt=""><br>ground state dipole ( ‚Üê )<br>excited state (‚Üë)<br>induced dipoles in solvent ( ‚Üê )</p><h3 id="Some-values-of-solvent-index-of-refraction">Some values of solvent index of refraction</h3><table><thead><tr><th style="text-align:left">solvent</th><th style="text-align:center">Index of refraction</th></tr></thead><tbody><tr><td style="text-align:left">Perfluoropentane</td><td style="text-align:center">1.239</td></tr><tr><td style="text-align:left">Water</td><td style="text-align:center">1.333</td></tr><tr><td style="text-align:left">Ethanol</td><td style="text-align:center">1.362</td></tr><tr><td style="text-align:left">Iso-octane</td><td style="text-align:center">1.392</td></tr><tr><td style="text-align:left">Chloroform</td><td style="text-align:center">1.446</td></tr><tr><td style="text-align:left">Carbontetrachloride</td><td style="text-align:center">1.463</td></tr></tbody></table><p>Note that water is less polarizable than iso-octane although clearly water is a much more polar solvent (larger dielectric constant)</p><h3 id="Influence-on-the-energy-levels-of-œÄ-n-œÄ-orbitals-by-solvent-polarizability">Influence on the energy levels of œÄ, n, œÄ* orbitals by solvent polarizability</h3><p><img src="https://imgur.com/g0UTFzH.png" alt=""></p><ul><li>œÄ - œÄ* transitions: red shift in more polarizable solvent<br>œÄ* interacts with solvent dipoles more strongly than œÄ</li><li>n - œÄ* transitions: blue shift in more polarizable solvent<br>n interacts with solvent dipoles more strongly than œÄ*</li></ul><h3 id="Solvent-can-influence-the-energy-of-both-the-ground-and-the-excited-states">Solvent can influence the energy of both the ground and the excited states</h3><p><img src="https://imgur.com/WPrGRUi.png" alt=""></p><h3 id="Rhodopsin-a-protein-‚Äúsolvent‚Äù-effect-on-the-absorption-spectrum-of-retinal">Rhodopsin: a protein ‚Äúsolvent‚Äù effect on the absorption spectrum of retinal</h3><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/fwb7LLC.png" alt=""></td><td style="text-align:left">vision is due to same pigment proteins in <strong><mark>rod</mark></strong> and <strong><mark>cone</mark></strong> cells:<li> Œªmax = 500 nm (rod cell)<li> Œªmax = 414 nm (blue cone)<li> Œªmax = 533 nm (green cone)<li> Œªmax = 560 nm (red cone) <br><br> Same chromophore: 11-cis retinal <br> ‚Äúspectral tuning‚Äù by interaction with amino acid residues nearby</td></tr></tbody></table><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/zpUcJAg.png" alt=""></td><td style="text-align:left"><li>WT: 500 nm<li> G90S: 487 nm<li> T118A: 484 nm<li> E122D: 477 nm<li> A292S: 489 nm<li> A295S: 498 nm<li> T/E/A triple mutant: 453 nm</td></tr></tbody></table><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">OPTICAL SPECTROSCOPY ‚Äì THE ABSORPTION PROCESS</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Artificial Intelligent 1</title>
    <link href="https://karobben.github.io/2024/01/26/LearnNotes/ai-probability/"/>
    <id>https://karobben.github.io/2024/01/26/LearnNotes/ai-probability/</id>
    <published>2024-01-26T07:22:21.000Z</published>
    <updated>2024-02-14T06:22:54.030Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Random-Variable">Random Variable</h2><p>Probability:</p><ul><li>Exp1: $Pr(A) &gt; 0$ which means that A has non-negative probability.</li><li>Exp2: $Pr(A) = 1$ means when event A occurs, the probability is 1.</li><li>Exp3: $Pr(A \cap B) = Pr(A) √ó Pr(B)$ when events A and B are independent.</li></ul><div class="admonition question"><p class="admonition-title">What is <b>Random Variable</b>?</p><ul><li>We use <b>capital letters</b> to denote the random variable</li><li>We use a small letter to denote a particular outcome of the experiment</li></ul></div><p>$P(X=x)$ means the probability of the occurs for value x. Here $P(X=x)$ is a <mark>number</mark>, the $P(X)$ is a distribution.</p><div class="admonition note"><p class="admonition-title">Example </p><p>Event = [Cloud, Cloud, Rain]In this Weather event P(X), it the probability of:</p><ul><li>Cloud: $P(X=Cloud) = \frac{2}{3}$.</li><li>Rain: $P(X=Rain) = \frac{1}{3}$</li><li>Sun:  $P(X=sun) = 0$</li></ul></div><p>The random variable we used in the example above is <mark>Discrete</mark> random variable, but sometimes we have to use continues random variable. For example: $X \in R$ (the set of all positive real numbers)</p><p>Because we have two types of random variable, the function for calculating the sun of all possible variables are different:</p><ul><li><strong>Probability Mass Function</strong> (pmf): For discrete random variable<ul><li>If <em>X</em> is a <strong>discrete random variable</strong>, then <em>P(X)</em> is its <strong>probability mass function (pmf)</strong>.</li><li>A probability mass is just a probability. <em>P(X=x) = Pr(X=x)</em> is the just the probability of the outcome <em>X = x</em> Thus:</li><li>$0 \leqslant P(X=x)$</li><li>$ 1 = \sum _x{P(X=x)}$</li></ul></li><li><strong>Probability Density Function</strong> (pdf):<ul><li>If <em>X</em> is a <strong>density random variable</strong>, then <em>P(x)</em> is its <strong>probability density function (pdf)</strong>.</li><li>A probability density is NOT a probability. Instead, we define it as a density $P(X=x) = \frac{d}{dx} Pr(X \leqslant x)$</li><li>$0 \leqslant P(X=x)$</li><li>$ 1 = \int_{-\infty}^\infty {P (X=x)dx}$</li></ul></li></ul><h3 id="Jointly-Random-Variables">Jointly Random Variables</h3><ul><li>Two or three random variables are ‚Äújointly random‚Äù if they are both outcomes of the same experiment.</li><li>For example, here are the temperature (Y, in ¬∞C), and precipitation (X, symbolic) for six days in Urbana:</li></ul><table><thead><tr><th>Date</th><th>X=Temperature (¬∞C)</th><th>Y=Precipitation</th></tr></thead><tbody><tr><td>January 11</td><td>4</td><td>cloud</td></tr><tr><td>January 12</td><td>1</td><td>cloud</td></tr><tr><td>January 13</td><td>-2</td><td>snow</td></tr><tr><td>January 14</td><td>-3</td><td>cloud</td></tr><tr><td>January 15</td><td>-3</td><td>clear</td></tr><tr><td>January 16</td><td>4</td><td>rain</td></tr></tbody></table><p>For this table, we could have joint random variables <em>P(X=x, Y=y)</em>:</p><table><thead><tr><th>P(X=x,Y=y)</th><th>snow</th><th>rain</th><th>cloud</th><th>clear</th></tr></thead><tbody><tr><td>-3</td><td>0</td><td>0</td><td>1/6</td><td>1/6</td></tr><tr><td>-2</td><td>1/6</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1/6</td><td>0</td></tr><tr><td>4</td><td>0</td><td>1/6</td><td>1/6</td><td>0</td></tr></tbody></table><h4 id="Notation-Vectors-and-Matrices">Notation: Vectors and Matrices</h4><ul><li>A normal-font capital letter (<em>X</em>) is a random variable, which is a function mapping from the outcome of an experiment to a measurement</li><li>A normal-font small letter (<em>x</em>) is a scalar instance</li><li>A boldface small letter (<em><strong>x</strong></em>) is a vector instance</li><li>A boldface capital letter (<em><strong>X</strong></em>) is a matrix instance</li></ul><p><em>P(X=<strong>x</strong>)</em> is the probability that random variable <em>X</em> takes the value of the vector <em><strong>x</strong></em>. This is just a shorthand for the joint distribution of <em>x<sub>1</sub>, x<sub>2</sub>, ‚Ä¶, x<sub>n</sub></em></p><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/FRHL0fI.png" alt="vector x"></th></tr></thead><tbody></tbody></table><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/ihgeuXc.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">When <em><strong>X</strong></em> is a random matrix</td></tr></tbody></table><h3 id="Marginal-Distributions">Marginal Distributions</h3><p>Suppose we know the joint distribution <em>P(X,Y)</em>. We want to find the two <strong>marginal distributions</strong> <em>P(X)</em>:</p><ul><li>If the unwanted variable is discrete, we marginalize by adding:<ul><li>$P(X) = \sum_ y P(X,Y=y)$</li></ul></li><li>If the unwanted variable is continuous, we marginalize by integrating:<ul><li>$P(X) = \int P(X,Y=y)$</li></ul></li></ul><p>Backing the table above, we could know that the marginal distributions of:</p><ul><li><em>P(X)</em> = 1/6 + 1/6 = 1/3; 1/6; 1/6; 1/6 + 1/6 = 1/3</li><li><em>P(Y)</em> = 1/6; 1/6; 1/6 + 1/6 + 1/6 = 1/2; 1/6</li></ul><p>PS: Some place also write <em>P(X)</em> as <em>P<sub>X</sub>(X)</em> or <em>P<sub>X</sub>(i)</em> and <em>P(Y)</em> as <em>P<sub>Y</sub>(Y)</em> or <em>P<sub>Y</sub>(j)</em>.</p><h3 id="Joint-and-Conditional-distributions">Joint and Conditional distributions</h3><p>With the joint possibility and marginal possibility, we could now calculating the Joint and Conditional distributions, which is <em>P(Y|X)</em></p><ul><li><em>P(Y|X)</em> is the probability (or pdf) that <em>Y = y</em> happens, given that <em>X = x</em> happens, over all <em>x</em> and <em>y</em>. This is called the <strong>conditional distribution</strong> of <em>Y</em> given <em>X</em>.</li><li><em>P(X|Y)</em> is the conditional probability distribution of outcomes <em>P(X=x|Y=y)</em></li><li>The conditional is the joint divided by the marginal:<ul><li>$P(X=x|Y=y) = \frac{P(X = x, Y = y)}{P(Y= y)}$</li></ul></li></ul><div class="admonition note"><p class="admonition-title">Exp of Joint and Conditional Distribution *P(X|Y = cloud)*</p><p>$P(X|Y=could) = \frac{P(X, Y = y)}{P(Y = cloud)}$</p><p>$=\frac{\frac{1}{6}\ \ 0\ \ \frac{1}{6}\ \ \frac{1}{6}}{\frac{1}{2}}$</p><p>So, the result is a vector = {1/3, 0, 1/3, 1/3}</p></div><p>According to the example, we could know that: <mark>Joint = Conditional√óMarginal</mark>; which is:<br>$$<br>P(X,Y) = P(X|Y)P(Y)<br>$$</p><h3 id="Independent-Random-Variables">Independent Random Variables</h3><ul><li>Two random variables are said to be independent, which means <em>P(X|Y) = P(X)</em><br>In other words, knowing the value of <em>Y</em> tells you nothing about the value of <em>X</em>.</li><li>According to this, we can also know:<ul><li>$\because$ <em>P(X,Y) = P(X|Y)P(Y)</em></li><li>$\therefore$ <em>P(X,Y) = P(X)P(Y)</em></li></ul></li><li><em>Pr(A‚ãÄB) = Pr(A)Pr(B)</em></li></ul><h3 id="Expectation">Expectation</h3><p>The expected value of a function is its weighted average, weighted by its pmf or pdf.</p><ul><li>For discrete X and Y:<br>$ E[f(X, Y)] = \sum_{x,y} f(x, y)P(X = x, Y = y) $</li><li>If X is continuous:<br>$ E[f(X, Y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y)P(X = x, Y = y) ,dx,dy $</li></ul><h3 id="Covariance">Covariance</h3><p>The covariance of two random variables is the expected product of their deviations:</p><p>$$<br>Covar(X,Y) = E[(X- E[X])(Y-E[Y])]<br>$$</p><ul><li>( E[X] ) is the expected value (or mean) of the random variable X.</li><li>( E[Y] ) is the expected value (or mean) of the random variable Y.</li><li>( X - E[X] ) is the deviation of X from its mean (how far X is from its mean).</li><li>( Y - E[Y] ) is the deviation of Y from its mean (how far Y is from its mean).</li><li>( E\left[(X - E[X])(Y - E[Y])\right] ) is the expected value of the product of these deviations.</li></ul><div class="admonition note"><p class="admonition-title">Example</p><p>Suppose we have two random variables, X and Y, with the following values:</p><ul><li>X: 1, 2, 3</li><li>Y: 2, 3, 4First, we calculate the $E[X]$ and $E(Y)$:</li><li>$E[X] = \sum_{i=i}^3 f(x_i)P(x_i) = 1√ó\frac{1}{3}+ 2√ó\frac{1}{3}+3√ó\frac{1}{3} = 2$</li><li>$E[Y] = \sum_{i=i}^3 f(y_i)P(y_i) = 2√ó\frac{1}{3}+ 3√ó\frac{1}{3}+4√ó\frac{1}{3} = 3$</li><li><strong>PS</strong>: $E[X] = mean(X)$ because when we calculating them through the whole list, we would count them one by one even though they are duplicated. In this case, if one element for example, the frequent of the x in X is 50% and the lenghth of the X is 10, we list x 5 times as the frequent of 1/10 equals doing one time by $x * \frac{1}{2}$Next, we calculate the deviations of each value from their means:</li><li>Deviations for $X-E[X] = [1-2, 2-2, 3-2] = [-1, 0, 1]$</li><li>Deviations for $Y-E[Y] = [2-3, 3-3, 4-3] = [-1, 0, 1]$</li></ul><p>Now we multiply these deviations pairwise and sum them up:</p><ul><li>$ (-1 \times -1) + (0 \times 0) + (1 \times 1) = 1 + 0 + 1 = 2 $</li></ul><p>Since we have three observations, we divide the sum by 3-1 (in the case of sample covariance) or simply by 3 (if we are dealing with a population).So, if we treat these as a population, the covariance is:</p><ul><li>Covar$(X, Y) = \frac{2}{3}$This positive value suggests that X and Y tend to increase together.</li></ul></div><p>for python code:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><br>X = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>])<br>Y = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br>(X - X.mean()) @ (Y - Y.mean()) / (<span class="hljs-built_in">len</span>(X))<br><br>X = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>])<br>Y = np.array([<span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])<br>(X - X.mean()) @ (Y - Y.mean()) / (<span class="hljs-built_in">len</span>(X))<br></code></pre></td></tr></table></figure></div><pre>2.5556-2.6665</pre><p>In this case, the covariance &gt; 0 means it is positively associated, covariance &lt; 0 means X and Y are negative-associated. Covariance = 0 means they are not associated at all.</p><p>Covariance Matrix:<br>Suppose <em>X = [X<sub>1</sub>, ‚Ä¶ , X<sub>n</sub>]</em> is a random vector. Its matrix of variances and covariances (a.k.a. covariance matrix) is</p><p>In other places, the covariance equation are mostly write as:<br>$$<br>Cov(X,Y) = \frac{\sum(X_i-\bar{X})(Y_j-\bar{Y})}{n}<br>$$</p><p>We can expected that they are the same because mostly, we expected the mean value is the expected value for a random variable.<br>\ tokens\ correctly\ classified}{n\ tokens\ total}$</p><ul><li>Error Rate<br>Equivalently, we could report error rate, which is just 1-accuracy:<br>$Error Rate = \frac{n\ tokens\ incorrectly\ classified}{n\ tokens\ total}$</li><li>Bayes Error Rate<br>The ‚ÄúBayes Error Rate‚Äù is the smallest possible error rate of any classifier with labels ‚Äúy‚Äù and features ‚Äúx‚Äù:<br>$Error Rate = \sum_x P(X=x)\underset{y}{min} P(Y \neq y |x=x)$<br>It‚Äôs called the ‚ÄúBayes error rate‚Äù because it‚Äôs the error rate of the Bayesian classifier</li></ul><h2 id="The-problem-with-accuracy">The problem with accuracy</h2><ul><li>In most real-world problems, there is one class label that is much more frequent than all others.<ul><li>Words: most words are nouns</li><li>Animals: most animals are insects</li></ul></li><li>Disease: most people are healthy</li><li>It is therefore easy to get a very high accuracy. All you need to do is write a program that completely ignores its input, and always guesses the majority class. The accuracy of this classifier is called the ‚Äúchance accuracy.‚Äù</li><li>It is sometimes very hard to beat the chance accuracy. If chance=90%, and your classifier gets 89% accuracy, is that good, or bad?</li></ul><p>The solution: <mark>Confusion Matrix</mark>:<br>Confusion Matrix =<br>‚Ä¢ <em><strong>(m, n)<sup>th</sup></strong></em> element is the number of tokens of the <em><strong>m<sup>th</sup></strong></em> class that were labeled, by the classifier, as belonging to the <em><strong>n<sup>th</sup></strong></em> class.</p><table><thead><tr><th style="text-align:center"><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/Basic-Confusion-matrix.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/">¬© Aniruddha Bhandari</a></td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Confusion matrix for a binary classifier</p><ul><li>Suppose that the correct label is either 0 or 1. Then the confusion matrix is just 2x2.<br></li><li>For example, in this box, you would write the # tokens of class 1 that were misclassified as class 0<br></li><li>Than, you got TP (True Positives), FN (False Negatives), FP (False Positives), and TN (True Negative)<br></li><li>The binary confusion matrix is standard in many fields, but different fields summarize its content in different ways.<br></li><li>In medicine, it is summarized using Sensitivity and Specificity.</li><li>In information retrieval (IR) and AI, we usually summarize it using Recall and Precision.</li></ul></div><h4 id="Specificity-and-Sensitivity">Specificity and Sensitivity</h4><ul><li>Specificity = True Negative Rate (TNR):<ul><li>$TNR = P(f(X) =0|Y=0) = \frac{TN}{TN+FP}$</li></ul></li><li>Sensitivity = True Positive Rate (TPR):<ul><li>$TRP = P(f(X) =1|Y=1) = \frac{TP}{TP+FN}$</li></ul></li><li>Precision:<ul><li>$P=P(Y =1|f(x)=1)=\frac{TP}{TP+FP}$</li></ul></li><li>Recall:<ul><li>Recall = Sensitivity = TPR:</li><li>$R = TRP = P(f(X) =1|Y=1) = \frac{TP}{TP+FN}$</li></ul></li></ul><h4 id="Training-Corpora">Training Corpora</h4><ul><li>Training vs. Test Corpora<br><strong>Training Corpus</strong>: a set of data that you use in order to optimize the parameters of your classifier (for example, optimize which features you measure, how you use those features to make decisions, and so on).<br><strong>Test Corpus</strong>: a set of data that is non-overlapping with the training set (none of the test tokens are also in the training dataset) that you can use to measure the accuracy.<ul><li>Measuring the training corpus accuracy is useful for debugging: if your training algorithm is working, then training corpus accuracy should always go up.</li><li>Measuring the test corpus accuracy is the only way to estimate how your classifier will work on new data (data that you‚Äôve never yet seen).</li></ul></li></ul><div class="admonition note"><p class="admonition-title">Accuracy on which corpus?</p><ul><li>Large Scale Visual Recognition Challenge 2015: Each competing institution was allowed to test up to 2 different fully-trained classifiers per week.<br></li><li>One institution used 30 different e-mail addresses so that they could test a lot more classifiers (200, total). One of their systems achieved &lt;46% error rate ‚Äì the competition‚Äôs best, at that time.<br></li><li>Is it correct to say that that institution‚Äôs algorithm was the best?</li></ul></div><ul><li>Training vs. development test vs. evaluation test corpora<ul><li><strong>Training Corpus</strong>: a set of data that you use in order to optimize the parameters of your classifier (for example, optimize which features you measure, what are the weights of those features, what are the thresholds, and so on).</li><li><strong>Development Test (DevTest or Validation) Corpus</strong>: a dataset, separate from the training dataset, on which you test 200 different fully-trained classifiers (trained, e.g., using different training algorithms, or different features) to find the best.</li><li><strong>Evaluation Test Corpus</strong>: a dataset that is used only to test the ONE classifier that does best on DevTest. From this corpus, you learn how well your classifier will perform in the real world.</li></ul></li></ul><h3 id="Summary">Summary</h3><ol><li><p>Bayes Error Rate:<br>$$<br>Error Rate = \sum_x P(X=x)\underset{y}{min} P(Y \neq y |x=x)<br>$$</p></li><li><p>Confusion Matrix, Precision &amp; Recall (Sensitivity)<br>$$P=P(Y =1|f(x)=1)=\frac{TP}{TP+FP}$$<br>$$R = TRP = P(f(X) =1|Y=1) = \frac{TP}{TP+FN}$$</p></li></ol><h4 id="Training-Corpora-v2">Training Corpora</h4><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Artificial Intelligent</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Schrodinger Function</title>
    <link href="https://karobben.github.io/2024/01/25/LearnNotes/schrodinger/"/>
    <id>https://karobben.github.io/2024/01/25/LearnNotes/schrodinger/</id>
    <published>2024-01-25T21:15:01.000Z</published>
    <updated>2024-01-31T05:30:45.767Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Energy-Momentum-Relation">Energy Momentum Relation</h2><p>$E^2 = (m_ 0 C ^2) ^2 + (pc)^ 2$</p><p>For Energy with rest mess: $E = m_0c^2$<br>For Energy with no rest mess: $E = pc$</p><p>Introducing Bacground:</p><ul><li>Light is a ruler; molecules are objects to be measured.</li><li>Visible light is a coarse ruler (300 nm resolution)</li><li>Molecules, on the other hand, are fine objects (0.1 nm)</li><li>How can we measure such a fine object with a coarse ruler?<ul><li>Use a finer ruler (electron microscopy)</li><li>Use indirect evidence to infer the information (today‚Äôs focus)</li></ul></li></ul><p>How to infer? What indirect evidence?</p><ul><li>Use a language to describe the matter (molecule).</li><li>Find information that is related to the size of a molecule.</li><li>Hopefully that information can be obtained by measurement (using light).</li><li>Infer the information about the molecule to be studied.</li><li>Here is one way to do this:<ul><li>Use the Schr√∂dinger equation to describe the system (molecule, atoms, electrons).</li><li>Find out that energy of electrons is related to the size of the molecule.</li><li>Measure the energy of the system using light.</li><li>Infer the size of the molecule by interpreting the energy information.</li></ul></li></ul><h2 id="Use-the-Schrodinger-Equation-to-Describe-the-System">Use the Schr√∂dinger Equation to Describe the System</h2><p>$$<br>-\frac{\hbar}{2m} \frac{d^2 \Psi (x)}{dx^ 2} + V(x)\Psi(x) = E\Psi(x)<br>$$</p><ul><li>V(x): Potential energy provides the constraints</li><li>E: Solve for Energy(also called eigenvalues)</li><li>$\Psi(x)$: Solve for wavefunctions (also called eigenvectors or eigenfunctions)</li></ul><p>We will end up with a series of wavefunctions with associated energies:<br>$$\Psi(x) \leftrightarrow E_ n $$</p><p>The Schr√∂dinger Equation is a fundamental equation in quantum mechanics that describes how the quantum state of a physical system changes over time. It was formulated by Erwin Schr√∂dinger in 1925. There are two forms of the Schr√∂dinger Equation: the time-dependent and the time-independent forms.</p><h3 id="The-Meaning-of-Œ¶-x">The Meaning of Œ¶(x)</h3><p>$$<br>P(x_0, t_0)dx = \Psi^ * (x_0, t_0)\Psi(x_0, t_0)dx = |\Psi (x_0, t_0)|^2 dx<br>$$</p><p><em><strong>P(x<sub>0</sub>, t<sub>0</sub>)</strong></em>: <em>Probability</em>* of finding the particle (e.g. electron) within an interval<br>of <em>dx</em> of position <em>x<sub>0</sub></em> at time <em>t<sub>0</sub></em></p><h3 id="Properties-of-Œ¶-x">Properties of Œ¶(x);</h3><p>$$<br>\int_{-\infty}^{\infty} \Psi^* (x, t) \Psi (x, t)dx = 1<br>$$</p><p>Average value of f(x) over all space</p><p>$$<br>\left \langle  f(x) \right \rangle = \frac{ \int_{-\infty}^{\infty} \Psi^* (x) \Psi (x)f(x)dx }{\int_{-\infty}^{\infty} \Psi^* (x) \Psi (x)dx}<br>$$</p><h2 id="Find-Out-That-Energy-of-Electrons-Is-Related-to-the-Size-of-the-Molecule">Find Out That Energy of Electrons Is Related to the Size of the Molecule.</h2><h3 id="1-d-particle-in-a-box">1-d particle in a box</h3><p>Here <strong>Particle = electrons in a molecule</strong><br><strong>Layman language</strong>: Potential energy outside the box is infinite.<br><strong>Mathematical language</strong>: Boundary condition</p><p><img src="https://bouman.chem.georgetown.edu/S02/lect13/piab.gif" alt=""></p><p><em><strong>V(x) = 0 for L &gt; x &gt; 0</strong></em><br><em><strong>V(x) = ‚àû for x ‚â• L, x ‚â§ 0</strong></em></p><p>$$<br>\frac{d^ 2 \Psi(x)}{dx^ 2} = \frac{2m} {\hbar^ 2} [V(x)-E]\Psi(x)<br>$$</p><ol><li><p>Since <em><strong>V(x)</strong></em> is infinite outside the box, <em><strong>Œ®(x)</strong></em> must be zero outside the box<br>(otherwise $\frac{d^2 Œ®(x)}{dx^2}$ would be infinite: not allowed)</p></li><li><p>Since <em><strong>Œ®(x)</strong></em> must be continous, <em><strong>Œ®(x)</strong></em> inside the box must connect smoothly to <em><strong>Œ®(x)</strong></em> outside the box<br>Hence, $Œ®(0)= Œ®(L) = 0$</p></li></ol><p>$$ \Psi_ n (x) = \sqrt{\frac{2}{L}}  sin(\frac{n\pi x}{L}) $$</p><ul><li>$ E_ n= \frac{h^2 n ^2}{8ma ^ 2} $</li><li>$ where n = 1, 2, 3, 4‚Ä¶ $</li><li>Energy of the electron (<em><strong>E</strong></em>) is related to the size of the molecule (<em><strong>L</strong></em>).</li></ul><p>With this theory, we could measure the energy of the system using light</p><ul><li>Pi-electrons behave as a ‚Äúparticle in a box: Molecules has different absorb peak when they as different number of conjugation bounds.</li></ul><div class="admonition note"><p class="admonition-title">The electrons and its orbitals</p><p>4 pi electrons =&gt; 2 orbitals8 pi electrons =&gt; 4 orbitals</p></div><h2 id="Estimate-bond-length-from-the-transition-energy">Estimate bond length from the transition energy</h2><p>$$<br>\Delta E = \frac{(n_ 3 ^ 2 - n_ 2^ 2) h^ 2 }{8mL^ 2}<br>$$</p><p>For this compound, There are <strong>4 pi electrons</strong>. Two each in the n=1 and n=2 orbitals. (This is due to electron spin, which we will see later).<br>The absorption is due to promoting an electron from the n=2 to the n=3 orbital.</p><ul><li>Infinite potential: infinite number of solutions</li><li>Finite potential:<ol><li>A finite number of solutions (5 in this example)</li><li>Wavefunctions are not zero at the boundary of the box</li><li>The wavefunctions have finite amplitude outside the box.<br>Finite chance the particle can be outside the box even though <em><strong>E&lt;V<sub>0</sub></strong></em></li></ol></li></ul><div class="admonition note"><p class="admonition-title">TUNNELING effect in Quantum Mechanics</p><p><img src="https://www.ntmdt-si.com/data/media/images/spm_basics/scanning_tunnel_microscopy_stm/stm_physical_backgrounds/tunneling_effect/img04.gif" alt="Quantum Tunneling" /> <a href="https://www.ntmdt-si.com/resources/spm-theory/theoretical-background-of-spm/1-scanning-tunnel-microscopy-%28stm%29/11-stm-physical-backgrounds/111-tunneling-effect">¬© ntmdt-si</a></p><p>a particle can go &quot;through&quot; an energy barrier instead of needing to have sufficient energy to go over the barrierWhen the wave go through the energy barrier, the exponential decay occurred inside of the energy barrier.</p></div><h2 id="Predict-the-Energy-and-Optical-Property">Predict the Energy and Optical Property</h2><p>If we know the structure of the molecule, can we predict the energy and optical property of the molecule?<br>Take the <strong>hydrogen atom</strong> as example.</p><ul><li>$V( r ) = \frac{e^ 2}{4\pi \epsilon_ 0 r}$</li><li><em><strong>e</strong></em> = electron charge</li><li><em><strong>Œµ<sub>0</sub></strong></em> = permittivity of free space</li></ul><h3 id="Predicted-the-Energy">Predicted the Energy</h3><p>Here we only need 3 quantum number: <em><strong>n</strong></em>, <em><strong>l</strong></em>, and <em><strong>m<sub>l</sub></strong></em></p><ul><li>the principal quantum number: <em><strong>n</strong></em>  = 1, 2, 3, 4‚Ä¶</li><li>the angular momentum quantum number: <em><strong>l</strong></em> = 0, 1, 2, ‚Ä¶ (n -1)</li><li>the magnetic quantum number: <em><strong>m<sub>l</sub></strong></em> = 0, ¬±1, ¬±2, ‚Ä¶ ¬± <em><strong>l</strong></em></li></ul><p>As you can see, the energy only depends on <em><strong>n</strong></em>:<br>$E_ n = - \frac{m_ e e^ 4}{8 \epsilon_ 0^ 2 h^2 n^ 2} = - \frac{2.179 √ó 10 ^ {-18}}{n^ 2}Joule = -\frac{13.6}{n^ 2}eV\ \ \ n = 1, 2, 3‚Ä¶ $</p><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/Kwx6VEm.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">¬© Atkins, The Elements of Physical Chemistry</td></tr></tbody></table><h2 id="Cheat-Sheet">Cheat Sheet</h2><ul><li>Energy: kcal/mol, kJ/mol</li><li>Wavelength: nm</li><li>Frequency: Hz</li><li>$E = h\nu = \frac{hc}{\lambda}$</li><li>Planck constant (<em><strong>h</strong></em>) = 6.62607015 √ó 10<sup>-34</sup> J‚àôs</li><li>Speed of light (<em><strong>c</strong></em>) = 299 792 458 m/s ~ 3 √ó 10<sup>8</sup> m/s</li></ul><div class="admonition note"><p class="admonition-title">Calculate the energy of one photon of green light (532 nm)</p><p>$ E = h\nu = \frac{hc}{\lambda} = \frac{hc}{532 √ó 10^ {-9}m} = 3.823√ó10^ {-19}J$</p></div><div class="admonition note"><p class="admonition-title">Convert J to Hz</p><p>$ \nu = \frac{E}{h} = \frac{3.823√ó10^ {-19}J}{h} = 5.8 √ó 10^ {14} Hz$</p></div><div class="admonition note"><p class="admonition-title">Convert kJ/mol and kcal/mol</p><p>$E_ {total} = N_ A √ó E = 6.02 √ó 10^ {23} \frac{1}{mol} √ó 3.823 √ó 10^ {-19} J$</p><p>$= 230226 J/mol ~ 230 kJ/mol $</p><p>$= 55 kcal/mol$</p></div><br><br><br><br><br><br><br><h2 id="Extra-Reading">Extra Reading</h2><ol><li><p><strong>Time-Dependent Schr√∂dinger Equation:</strong><br>$$<br>i\hbar\frac{\partial}{\partial t}\Psi(\mathbf{r}, t) = \hat{H}\Psi(\mathbf{r}, t)<br>$$<br>Here, $\Psi(\mathbf{r}, t)$ is the wave function of the system, $i$ is the imaginary unit, $\hbar$ is the reduced Planck constant, $t$ represents time, $\mathbf{r}$ is the position vector, and $\hat{H}$ is the Hamiltonian operator which represents the total energy of the system.</p></li><li><p><strong>Time-Independent Schr√∂dinger Equation:</strong><br>$$<br>\hat{H}\psi(\mathbf{r}) = E\psi(\mathbf{r})<br>$$<br>In this form, $\psi(\mathbf{r})$ is the time-independent wave function, $E$ represents the energy of the system, and other symbols have the same meaning as in the time-dependent equation.</p></li></ol><p>The Schr√∂dinger Equation is a cornerstone of quantum mechanics, providing a mathematical framework for understanding and predicting the behavior of quantum systems. It‚Äôs important to note that these equations are usually accompanied by specific boundary conditions or potentials, depending on the physical situation being modeled.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">To develop a quantitative understanding of macromolecules in biological systems.</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>PARTICLE-WAVE DUALITY</title>
    <link href="https://karobben.github.io/2024/01/23/LearnNotes/par-wave-dual/"/>
    <id>https://karobben.github.io/2024/01/23/LearnNotes/par-wave-dual/</id>
    <published>2024-01-23T15:57:26.000Z</published>
    <updated>2024-01-31T05:31:36.820Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Failure-of-Classical-Mechanics">Failure of Classical Mechanics</h2><h3 id="Black-Body-Radiation">Black Body Radiation</h3><ul><li><strong>Experiment</strong>: measure the radiation intensity as a function of frequency of the radiation emitted from a black body (a physical body that absorbs all incident electromagnetic radiation) at thermal equilibrium</li><li><strong>Model</strong>: Emitted radiation is classically due to oscillating electric dipoles within the material, acting like broadcast antennas.</li><li><strong>Classical expectation</strong>: higher temperature should result in a large increase in the radiation emitted at high frequencies (fast oscillation at high T)</li></ul><div class="admonition note"><p class="admonition-title">Gustav Kirchhoff</p><p>The blackbody is an idealized physical body that absorbs all incident electromagnetic radiation (such as light), regardless of frequency or angle of incidence.</p><p>A black body can also emit black-body radiation, which is solely determined by its temperature.</p><p><a href="https://en.wikipedia.org/wiki/Gustav_Kirchhoff"><img src="https://upload.wikimedia.org/wikipedia/commons/f/fe/Gustav_Robert_Kirchhoff.jpg" alt="Gustav Robert Kirchhoff" /></a></p></div><h3 id="Black-body-Radiation-Spectrum-Intensity-vs-Wavelength">Black-body Radiation Spectrum: Intensity vs. Wavelength</h3><table><thead><tr><th style="text-align:center"><img src="https://i.stack.imgur.com/f4xki.gif" alt="Relationship between radiational intensity vs. wavelength"></th></tr></thead><tbody></tbody></table><p>Notic: the ‚ÄúCold‚Äù object also emit ‚Äúlight‚Äù as long as its above the absolute zero (-273.15 ¬∞C). And it would appear ‚Äúblakc‚Äù because the peak wavelength is in infrared range, which human eye cannot recognize.</p><p>According to this theory, we could astimate the temperature of stars based on its color.</p><table><thead><tr><th style="text-align:center"><img src="https://stsci-opo.org/STScI-01G7RQ0CRCSNKZNX18HNNAQRV0.jpg" alt=""></th><th style="text-align:center"><img src="https://www.astronomy.com/wp-content/uploads/sites/2/2023/02/ScreenShot20210420at3.19.16PM.jpg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://webbtelescope.org/contents/media/images/01F8GF8WYBCQVKTGPX3MA58182?Type=Infographics&amp;Tag=Spectroscopy">webbtelescope</a></td><td style="text-align:center"><a href="https://www.astronomy.com/astronomy-for-beginners/color-coding-stars/">astronomy.com</a></td></tr></tbody></table><h3 id="Wien‚Äôs-Displacement-Law">Wien‚Äôs Displacement Law</h3><p>$ \lambda _{max} = \frac{W}{T} $</p><ul><li>$  \lambda _{max} $: Peak wavelength</li><li>W: Wien‚Äôs constant = 2.9 e<sup>-3</sup> m Kelvin</li><li>T: Surfave temperature</li></ul><p>Exp:</p><p>When Œª<sub>max</sub> = 500nm:<br>$T = \frac{W}{\lambda_ {max}} = \frac{2.9 e^ {-3} mK}{500e^ {-9}m } = 5800K$</p><h3 id="Rayleigh-Jeans-law-Classical-description-of-light">Rayleigh-Jeans law (Classical description of light)</h3><ul><li>Predict the spectral irradiance (or spectral density) of a black body radiation as a function of wavelength or frequency for a fixed temperature.</li><li>Spectrum Density (ŒΩ, T) = $\frac{8\pi\nu^2}{ c^3 }k_BT$<ul><li>c: speed of light</li><li>k<sub>B</sub> = Boltzmann constant = 1.380649 √ó 10<sup>-23</sup> J/K</li><li>T: temperature</li><li>ŒΩ: frequency</li></ul></li></ul><h3 id="Spectral-density">Spectral density</h3><ul><li>Density = radiance<br>The power of radiation (W) passing a unit area (m<sup>2</sup>) within a unit solid angle (sr), within a unit time<br>(s). It has the unit of W/(m<sup>2</sup> √ó sr √ó s)</li><li>Spectral density = Spectral radiance<br>The radiance per unit wavelength (um) or per unit frequency (Hz), depending on if the power is measured over wavelength or frequency. Therefore, spectral density has the unit of W/(m<sup>2</sup> √ó sr √ó s √ó um) or W/(m<sup>2</sup> √ó sr √ó s √ó Hz).</li></ul><h3 id="Unexpected-Black-Body-Radiation-UV-Catastrophe">Unexpected Black Body Radiation: UV Catastrophe</h3><div class="admonition note"><p class="admonition-title">Conflict between observation and expectation</p><ul><li><strong>Observation</strong> :Spectral density does not monotonically increase as the frequency increases.</li><li><strong>Classical expectation</strong>:According to the Rayleigh-Jean law, spectral density should increase as the frequency increases</li></ul></div><p>Reason: <mark>The classic oscillating field from electromagnetic radiation drives the motion of a spring</mark><br>- faster oscillation ‚Üí higher frequency ‚Üí higher energy</p><p>For fix this conflict, a propportional model was given: E = nhŒΩ (<mark>Max Planck</mark>)</p><ul><li>n = 1, 2, 3‚Ä¶</li><li>Planck‚Äôs constant (h) = 6.626√ó10<sup>‚àí34</sup> Js</li></ul><p>$$<br>\rho (\nu, T) = \frac{8\pi\nu^2 h \nu}{ c^3 } \frac{1}{e^{\frac{hv}{k_ BT}} - 1 }<br>$$</p><p>Explained:<br>- energy of photon (with frequent ŒΩ): hŒΩ<br>- weight if photon population (ŒΩ): g(ŒΩ) = $\frac{8\pi \nu ^2 }{c^ 3}$<br>- averagy number of photon (ŒΩ): n(#, T) = $\frac{1}{exp(\frac{h\nu}{k_BT})-1}$<br>- œÅ (ŒΩT)dŒΩ = hŒΩ √ó g(ŒΩ) √ó n(#, T)<br>= $h\nu √ó \frac{8\pi \nu ^2 }{c^ 3} √ó \frac{1}{exp(\frac{h\nu}{k_BT})-1}$<br>= $\frac{8\pi h \nu^3}{ c^3 } \frac{1}{e^{\frac{hv}{k_ BT}} - 1 }$</p><p>In Planck Low, it show either high temperature or low frequency would case the ‚ÄúUltralviolet catastrophe‚Äù:</p><p><img src="https://imgur.com/rWBTB6C.png" alt="planck Law"></p><ul><li>From the left to the right:<ul><li>$\frac{8\pi \nu ^2 }{c^ 3}$</li><li>$\frac{1}{exp(\frac{h\nu}{k_BT})-1}$</li><li>$\frac{8\pi h \nu^3}{ c^3 } \frac{1}{e^{\frac{hv}{k_ BT}} - 1 }$</li></ul></li></ul><details><summary>Plot Codes</summary><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Rayleigh_Jeans</span>(<span class="hljs-params">v, T</span>):</span><br>    kB = <span class="hljs-number">1.380649</span> * <span class="hljs-number">10</span>**(<span class="hljs-number">0</span>-<span class="hljs-number">23</span>)<br>    c = <span class="hljs-number">3</span> * <span class="hljs-number">10</span>**<span class="hljs-number">8</span><br>    <span class="hljs-keyword">return</span> (v**<span class="hljs-number">2</span>) * <span class="hljs-number">8</span> * np.pi * kB * T / (c**<span class="hljs-number">3</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Planck</span>(<span class="hljs-params">v, T</span>):</span><br>    h = <span class="hljs-number">6.626</span>*<span class="hljs-number">10</span>**(<span class="hljs-number">0</span>-<span class="hljs-number">34</span>)<br>    kB = <span class="hljs-number">1.380649</span> * <span class="hljs-number">10</span>**(<span class="hljs-number">0</span>-<span class="hljs-number">23</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> /( np.e ** (h*v / (kB * T) ) - <span class="hljs-number">1</span> )<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Merged</span>(<span class="hljs-params">v, T</span>):</span><br>    h = <span class="hljs-number">6.626e-34</span><br>    kB = <span class="hljs-number">1.380649e-23</span><br>    c = <span class="hljs-number">3</span> * <span class="hljs-number">10</span> **<span class="hljs-number">8</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">8</span> * np.pi * h * v**<span class="hljs-number">3</span> / c**<span class="hljs-number">3</span> / np.e**(h *v /(kB * T))<br><br><span class="hljs-comment"># Constants</span><br>h = <span class="hljs-number">6.62607015e-34</span>  <span class="hljs-comment"># Planck&#x27;s constant (m^2 kg / s)</span><br>c = <span class="hljs-number">299792458</span>       <span class="hljs-comment"># Speed of light in vacuum (m / s)</span><br>k_B = <span class="hljs-number">1.380649e-23</span>  <span class="hljs-comment"># Boltzmann constant (J / K)</span><br><br><span class="hljs-comment"># Planck&#x27;s law function</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">planck_law</span>(<span class="hljs-params">frequency, temperature</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Calculate the spectral radiance of a black body at thermal equilibrium.  </span><br><span class="hljs-string">    Parameters:</span><br><span class="hljs-string">    frequency (float): frequency of the electromagnetic radiation (Hz)</span><br><span class="hljs-string">    temperature (float): absolute temperature of the body (K)</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    float: spectral radiance (W / (m^2 * sr * Hz))</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    exponent = (h * frequency) / (k_B * temperature)<br>    spectral_radiance = (<span class="hljs-number">8</span> * np.pi * h * frequency**<span class="hljs-number">3</span>) / (c**<span class="hljs-number">3</span>) * (<span class="hljs-number">1</span> / (np.exp(exponent) - <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">return</span> spectral_radiance<br><br><span class="hljs-comment"># Example usage: Calculate the spectral radiance for a frequency of 1e14 Hz at 5000 K</span><br>frequency_example = <span class="hljs-number">1e14</span>  <span class="hljs-comment"># Hz</span><br>temperature_example = <span class="hljs-number">5000</span>  <span class="hljs-comment"># K</span><br>radiance_example = planck_law(frequency_example, temperature_example)<br><br><span class="hljs-comment"># Plot the graphs above</span><br>X = [i /<span class="hljs-number">10</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span>  <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">100</span>)]<br>fig, axs = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br>axs[<span class="hljs-number">0</span>].plot(X, [Planck(i * <span class="hljs-number">1e14</span>, <span class="hljs-number">3000</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> X])<br>axs[<span class="hljs-number">1</span>].plot(X, [Rayleigh_Jeans(i * <span class="hljs-number">1e14</span>, <span class="hljs-number">3000</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> X])<br>axs[<span class="hljs-number">2</span>].plot(X, [h *(i *<span class="hljs-number">1e14</span>)* Rayleigh_Jeans(i*<span class="hljs-number">1e14</span>, <span class="hljs-number">3000</span>) * Planck(i*<span class="hljs-number">1e14</span>, <span class="hljs-number">3000</span>)  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> X])<br>plt.tight_layout()<br>plt.show()<br><br><span class="hljs-comment"># plot the mergerd function</span><br>plt.plot(X, [Merged(i*<span class="hljs-number">1e14</span>/<span class="hljs-number">10</span>, <span class="hljs-number">3000</span>)  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> X])<br>plt.show()<br></code></pre></td></tr></table></figure></div></details><p>So, both Rayleigh-Jean law and Planck law agree with the observation very well in very low frequency.<br>But only Plank law could fit the decreasing of the intensity in high frequency situation.</p><h2 id="Light-is-Particle">Light is Particle</h2><h3 id="Einstein‚Äôs-Contribution-Photoelectric-Effect">Einstein‚Äôs Contribution: Photoelectric Effect</h3><p>$$<br>E = nhŒΩ<br>$$</p><p>By using the light to eject electrons from a copper, they found the <mark>kinetic energy of ejected electrons depends on light frequency</mark></p><p>$$<br>E_{light} = \beta\nu_{light}<br>$$</p><h3 id="The-Photoelectric-Effect">The Photoelectric Effect</h3><p>Conservation of energy</p><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/DSOpstj.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">‚Ä¶?unknow</td></tr></tbody></table><p>$$<br>E_{elec} + \Phi= \beta \nu<br>$$</p><ul><li>E<sub>elec</sub>: (Measured) Electron kinetic energy</li><li>&amp;\Phi; Work to remove electron from target (independently determined)</li><li>ŒΩ: Determine the value of &amp;\beta;</li></ul><div class="admonition note"><p class="admonition-title">Einstein concluded that light must be behaving like a particle in this experiment: PHOTON</p><p>E = <em>hŒΩ</em></p></div><h3 id="Duality-in-partical-with-mass">Duality in partical with mass</h3><ul><li><strong>Electron diffraction</strong>: This is a typical diffraction pattern of a beam of electrons diffracted by a crystalline solid</li></ul><p>from Planck‚Äôs equation to Einstein‚Äôs equestion:</p><ul><li><em>E = h&amp;nu</em> ‚Üí <em>E = mc<sup>2</sup></em></li></ul><p>$v = h\nu$<br>$mv = P = m\lambda\nu$<br>$v = \frac{P}{m\lambda}$<br>$E = h\nu = \frac(hP){m\lambda} ‚Üí E = \frac{p^2 }{m}$</p><p>De Broglie Equation:<br>$\lambda = \frac{h}{p} = \frac{h}{m\nu}$</p><h3 id="About-Weight-Terms-in-Spectral-Density">About Weight Terms in Spectral Density</h3><p>$g(\nu) = \frac{N(E)}{V} = \frac{Number of States (E)}{Volumne}$</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">PARTICLE-WAVE DUALITY</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Light in Physics</title>
    <link href="https://karobben.github.io/2024/01/18/LearnNotes/light/"/>
    <id>https://karobben.github.io/2024/01/18/LearnNotes/light/</id>
    <published>2024-01-19T03:30:37.000Z</published>
    <updated>2024-02-02T01:28:32.399Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Mathmatic-Description-of-the-Light">Mathmatic Description of the Light</h2><p>In the equation you‚Äôve provided:</p><p>$$<br>E(x,t) = E^0 \sin\left[2\pi \left(\frac{x}{\lambda} - \frac{t}{T}\right)\right]<br>$$</p><p>This represents a sinusoidal wave function, where $E(x,t)$ is the electric field of the light wave at a position $x$ and at time $t$. Here‚Äôs what the terms mean:</p><ul><li>$E^0$ is the amplitude of the wave, which indicates the maximum strength of the electric field.</li><li>$\lambda$ (lambda) is the wavelength of the light, which is the distance over which the wave‚Äôs shape repeats.</li><li>$T$ is the period of the wave, which is the time it takes for one complete cycle of the wave to pass a point. ($\frac{1}{T} = \nu$)</li></ul><p>The reason both $\lambda$ and $T$ are present in the equation is because they describe different aspects of the wave:</p><ul><li>$\lambda$ describes the spatial repetition of the wave along the $x$-axis.</li><li>$T$ describes the temporal repetition of the wave along the $t$-axis (time).</li></ul><p>The term $\frac{x}{\lambda} - \frac{t}{T}$ is the phase of the wave, which determines the position of the peaks and troughs of the wave at any given time $t$ and position $x$. It‚Äôs not meant to equal zero; instead, it changes with time and position to represent the propagation of the wave through space and time.</p><p>The phase changes as time goes by, indicating that the peaks and troughs of the wave are moving. If $\frac{x}{\lambda} - \frac{t}{T}$ were always zero, it would imply a stationary wave, not a propagating one.</p><p>The product $2\pi$ times the phase gives you the argument of the sine function in radians, which is necessary because the sine function is periodic with a period of $2\pi$. This means that the wave repeats itself every $2\pi$ radians, which corresponds to one wavelength in space and one period in time.</p><p><img src="https://imgur.com/D0t8uGs.png" alt="Wave Function"></p><details><summary>Plot codes</summary><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">waveFun</span>(<span class="hljs-params">x, t = <span class="hljs-number">0</span>, lamb = <span class="hljs-number">1</span>, pi = np.pi, T = <span class="hljs-number">1</span>, E0 = <span class="hljs-number">1</span></span>):</span><br>    E = E0 * np.sin(<span class="hljs-number">2</span> * pi * (x/lamb - t/T))<br>    <span class="hljs-keyword">return</span> E     <br><br>Y = <span class="hljs-built_in">range</span>(-<span class="hljs-number">50</span>,<span class="hljs-number">50</span>)<br>X = <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">100</span>)<br>Points = []<br><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X:<br>    <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> Y:<br>        Dis = np.sqrt((x/<span class="hljs-number">10</span>)**<span class="hljs-number">2</span> + (y/<span class="hljs-number">10</span>)**<span class="hljs-number">2</span>)<br>        Points += [[x, y, waveFun(Dis)]]<br><br>Points = np.array(Points)<br><br>plt.figure(figsize=(<span class="hljs-number">7</span>, <span class="hljs-number">5</span>))<br>plt.scatter(Points[:, <span class="hljs-number">0</span>], Points[:, <span class="hljs-number">1</span>] + <span class="hljs-number">0.5</span>, c= Points[:, <span class="hljs-number">2</span>], marker=<span class="hljs-string">&#x27;o&#x27;</span>, cmap=plt.cm.coolwarm,)<br>plt.show()<br></code></pre></td></tr></table></figure></div></details><h3 id="Example-1-Single-Location-Position">Example 1: Single Location Position</h3><p>Let‚Äôs set the $\lambda$ as 1, T as 10, and x = 0. Then, the equation could be simplified as $E(0, t) = sin[2\pi(0 - \frac{t}{10})]$.<br>And the change of the E<sup>0</sup> with T could be show as the animation below.</p><table><thead><tr><th style="text-align:center">The E<sup>0</sup> corresponded with the t</th><th style="text-align:center">Static View by using x axis as t</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/WvAIY5p.gif" alt="Observing the wave in single position"></td><td style="text-align:center"><img src="https://imgur.com/Ct71NCk.png" alt="Spread the observed points"></td></tr></tbody></table><h3 id="Example-2-Multiple-Observation-Locations">Example 2: Multiple Observation Locations</h3><p>If we observe more positions, let‚Äôs say, 0 to 10, and using the full function, we could get an animation like below. This is the propagating wave we could observe in 10 different locations.</p><p><img src="https://imgur.com/KdWk5PU.gif" alt="The propagating wave"></p><table><thead><tr><th style="text-align:center"><img src="https://www.acs.psu.edu/drussell/Demos/wave-x-t/wave-x-t.gif" alt="Wave Motion in Time and Space"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.acs.psu.edu/drussell/Demos/wave-x-t/wave-x-t.html">¬© psu</a></td></tr></tbody></table><h2 id="Direction-of-the-Wave">Direction of the Wave</h2><p>When t increases, x increases =&gt; ‚Äú+‚Äù direction on x;<br>When t increases, x decreases =&gt; ‚Äú-‚Äù direction on x.</p><h2 id="Speed-of-the-Wave">Speed of the Wave</h2><p>$velocity = \frac{\lambda}{T} = \lambda \nu$</p><ul><li>$\nu$ id frequency ($\frac{1}{T}$) of the wave.</li></ul><p>So, the light of the wave is:</p><ul><li>$c = \lambda \nu$</li></ul><h2 id="Energy-of-the-Wave">Energy of the Wave</h2><p>Energy of a wave is proportional to the square of the amplitude (in classical mechanics)</p><p>$\rho(\nu)$: energy per unit volume</p><p>$$<br>\rho(\nu) = constant\ x (E^ o)^ 2<br>$$</p><ul><li><mark>Classic</mark>: Energy is dependent on amplitude</li><li><mark>QM</mark>: Energy is dependent on frequency</li></ul><h2 id="Commonly-used-notation">Commonly used notation</h2><ol><li><p><strong>Wave Vector ($k$)</strong>: The wave vector is defined as $\frac{2\pi}{\lambda}$, where $\lambda$ is the wavelength of the wave. The wave vector points in the direction of the wave‚Äôs propagation and has a magnitude equal to the number of wave cycles per unit distance. The notation $\hat{k}$ represents a unit vector in the direction of $k$, so the wave vector $k$ is sometimes written as $\frac{2\pi}{\lambda} \hat{k}$, emphasizing its direction.</p></li><li><p><strong>Angular Frequency ($\omega$)</strong>: This is defined as $2\pi\nu$, where $\nu$ is the frequency of the wave. It represents how many radians the wave cycles through per unit time.</p></li><li><p><strong>Phase ($\phi$)</strong>: The phase is a term that allows us to specify where in its cycle the wave is at $t = 0$ and $x = 0$. It lets us define <strong>the ‚Äúzero point‚Äù or starting point</strong> of the wave at a place other than the origin of our coordinate system.</p></li></ol><p>$$E(x,t) = E^0 \sin(kx - \omega t + \phi)$$<br>$$H(x,t) = H^0 \sin(kx - \omega t + \phi)$$</p><div class="admonition question"><p class="admonition-title">Why Standard Form?</p><p>The first function form expresses the wave in terms of its wavelength Œª and period T, which are perhaps more intuitive when you're first learning about waves.  It makes it very clear that the wave repeats itself every wavelength Œª in space and every period T in time.</p><p>The second function is the standard form. It's particularly useful in more advanced topics like wave interference, diffraction, and quantum mechanics, where the concept of phase space and the relationship between position and momentum (or wavelength and frequency) are crucial.</p><p>For example, when you want to mimic the interference of the wave, the previous function could became extreamly complcated because the only difference between two wave is phase.</p></div><details> <summary>More descriptions</summary>These equations describe how the electric and magnetic fields oscillate as a function of space and time, which is characteristic of electromagnetic waves such as light. The quantities $E^0$ and $H^0$ are the maximum strengths of the electric and magnetic fields, respectively.<p>In an electromagnetic wave, the electric and magnetic fields are perpendicular to each other and to the direction of wave propagation. The equations show that both fields oscillate in sync (they have the same phase $\phi$) but are described by separate equations since they are perpendicular components.</p><p>The term $kx - \omega t$ indicates that the wave is moving in the positive $x$-direction. If the wave were moving in the negative $x$-direction, the sign in front of $\omega t$ would be positive.</p><p>The factor $\sin(kx - \omega t + \phi)$ varies between (-1) and (1), causing the electric and magnetic field strengths to oscillate between $-E^0$ to $E^0$ and $-H^0$ to $H^0$, respectively. The wave thus carries energy and, if it is light, can be observed as it interacts with matter.</p></details><h2 id="Huygen‚Äôs-Principle-1678">Huygen‚Äôs Principle (1678)</h2><p>Waves spread as if each region of space is behaving as a source of new waves of the <mark>same frequency and phase</mark>.<br>So, Huygen‚Äôs principle applied to light showing a wave front.</p><h2 id="Diffraction">Diffraction</h2><p><img src="https://imgur.com/vtuefhO.png" alt="Wave Diffraction"></p><p>When we konw d, D, X, and Œ∏ we could calcualate the Œª (wave length):</p><ul><li>When the light patten shows the dark point, we know that the phase between two waves is $n\frac{\lambda}{2}$</li><li>So the $\Delta \phi$ of the first dark spot would be $\frac{\lambda}{2}$</li><li>According to the plot, the Œª would be (path4 - path3) * 2 $ = \frac{d}{2}sin\theta$</li><li>When the angle is very small, we have $sin\theta \approx tan\theta = \frac{X}{2}\frac{1}{D}$</li><li>So finally, we could get: $\frac{d}{2}\frac{X}{2D} = \frac{\lambda}{2}$<ul><li>$\lambda = \frac{dX}{2D}$</li></ul></li></ul><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/qiLpQ7c.png" alt="Two wave interference"></th><th style="text-align:center"><img src="https://imgur.com/1Cqo89a.png" alt="Three wave interference"></th></tr></thead><tbody><tr><td style="text-align:center">Two wave interference (center: y<sub>1</sub> = -15, y<sub>2</sub> = 15)</td><td style="text-align:center">Three wave interference (y<sub>1</sub> = 10, y<sub>2</sub> = 0, y<sub>3</sub> = -10)</td></tr></tbody></table><table><thead><tr><th style="text-align:center"><img src="http://hyperphysics.phy-astr.gsu.edu/hbase/phyopt/imgpho/muls2.png" alt="Double Slit"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="http://hyperphysics.phy-astr.gsu.edu/hbase/phyopt/mulslid.html">¬© gsu</a></td></tr><tr><td style="text-align:center"><img src="https://myslu.stlawu.edu/~jmil/physics/labs/152_lab/setup_manual/blackboard/img/double_slit.gif" alt="Double Slit"></td></tr><tr><td style="text-align:center"><a href="http://stlawu.edu">stlawu.edu</a></td></tr></tbody></table><p>$$<br>n\lambda = d sin\theta_n \approx d tan\theta_n = d \frac{x_ n }{D}<br>$$</p><ul><li>Each photon is represented as a plane wave at the slits.</li><li>The square of the amplitude of the recombined wave is proportional to the probability of finding the photon at this point</li></ul><details><summary>Plot code</summary><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>Y = <span class="hljs-built_in">range</span>(-<span class="hljs-number">50</span>,<span class="hljs-number">50</span>)<br>X = <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">100</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">waveFun</span>(<span class="hljs-params">x, t = <span class="hljs-number">0</span>, lamb = <span class="hljs-number">1</span>, pi = np.pi, T = <span class="hljs-number">1</span>, E0 = <span class="hljs-number">1</span></span>):</span><br>    E = E0 * np.sin(<span class="hljs-number">2</span> * pi * (x/lamb - t/T))<br>    <span class="hljs-keyword">return</span> E     <br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">WaveE</span>(<span class="hljs-params">X, Y, dy = <span class="hljs-number">0</span></span>):</span><br>    Points = []<br>    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X:<br>        <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> Y:<br>            Dis = np.sqrt((x/<span class="hljs-number">10</span>)**<span class="hljs-number">2</span> + (y/<span class="hljs-number">10</span>)**<span class="hljs-number">2</span>)<br>            Points += [[x, y + dy, waveFun(Dis)]]<br>    Points = np.array(Points)<br>    <span class="hljs-keyword">return</span> Points<br><br>P1 = WaveE(X, Y, <span class="hljs-number">0</span>)<br>P2 = WaveE(X, Y, <span class="hljs-number">10</span>)<br>P3 = WaveE(X, Y, -<span class="hljs-number">10</span>)<br><br>P1 = pd.DataFrame(P1, columns = [<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&quot;E1&quot;</span>])<br>P2 = pd.DataFrame(P2, columns = [<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&quot;E2&quot;</span>])<br>P3 = pd.DataFrame(P3, columns = [<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&quot;E3&quot;</span>])<br><br>TB = pd.merge(P1, P2)<br>TB = pd.merge(TB, P3)<br>TB[<span class="hljs-string">&#x27;E&#x27;</span>] = TB.E1 + TB.E2  + TB.E3<br><br>fig, ax = plt.subplots(subplot_kw=&#123;<span class="hljs-string">&quot;projection&quot;</span>: <span class="hljs-string">&quot;3d&quot;</span>&#125;, figsize=(<span class="hljs-number">6</span>,<span class="hljs-number">6</span>))<br>ax.plot_trisurf(TB.x, TB.y, TB.E, vmin=TB.E.<span class="hljs-built_in">min</span>() * <span class="hljs-number">2</span>, cmap=cm.coolwarm)<br>plt.show()<br><br><span class="hljs-comment">#plt.figure(figsize=(5, 5))</span><br><span class="hljs-comment">#plt.scatter(TB.x, TB.y, c= TB.E, marker=&#x27;o&#x27;, cmap=plt.cm.coolwarm,)</span><br><span class="hljs-comment">#plt.show()</span><br></code></pre></td></tr></table></figure></div></details><h2 id="Principle-of-Superposition">Principle of Superposition</h2><p>At the beginning of the story, let‚Äôs say there are two same waves with different $\phi$ which: $\phi_2 - \phi_1 = \pi$</p><p>In this case, the Intensity of this two wave is:</p><ul><li>$E^2_{sum} = (E_1 + E_2 )^2 = 0$</li><li>ps: <mark>Not</mark> $E^2_{sum} = E_1^2 + E_2^2 \neq 0$</li></ul><p>(Destructive interference)</p><h3 id="For-Two-Traveling-Waves">For Two Traveling Waves</h3><ul><li>Two sine waves with the same amplitude but slightly different frequencies traveling at the same velocity in the same direction</li></ul><p>$ E(x,t) = E^o sin(k_1 x - \omega _1 t) +E^o sin(k_2 x - \omega _2 t) $<br>$ = 2 E^o cos [\frac{[k_1 - k_2]}{2}x - \frac{\omega _1 - \omega _2}{2}t] sin[\frac{[k_1 - k_2]}{2}x - \frac{\omega _1 - \omega _2}{2}t] $</p><h2 id="Direction">Direction</h2><p>$$ ùëõ_1 \cdot sin ùúÉ_1 = ùëõ_2 \cdot sin ùúÉ_2 $$</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style><h2 id="Extra-Explore">Extra Explore</h2><p>Becuase we konw:</p><ul><li>$E(x,t) = E^0 \sin\left[2\pi \left(\frac{x}{\lambda} - \frac{t}{T}\right)\right]$</li></ul><p>So, when we move to the 2D wave, we could have function:</p><ul><li>$E(x, y, t) = E^0 \sin\left[2\pi \left(\frac{\sqrt{x^2 - y^2}}{\lambda} - \frac{t}{T}\right)\right]$</li></ul><p>In order to calcualte the 2D wave from the different emission location, we need to introduce the initial point b=(b<sub>x</sub>, b<sub>y</sub>)</p><ul><li>$E(x, y, b_x, b_y, t) = E^0 \sin\left[2\pi \left(\frac{\sqrt{(x-b_x)^2 - (y-b_y)^2}}{\lambda} - \frac{t}{T}\right)\right]$</li></ul><details><summary>Plot codes</details><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> NearestNeighbors<br><span class="hljs-keyword">import</span> pyvista <span class="hljs-keyword">as</span> pv<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">waveFun</span>(<span class="hljs-params">x, y, bx =<span class="hljs-number">0</span>, by = <span class="hljs-number">0</span>, t = <span class="hljs-number">0</span>, lamb = <span class="hljs-number">1</span>, pi = np.pi, T = <span class="hljs-number">1</span>, E0 = <span class="hljs-number">1</span></span>):</span><br>    E = E0 * np.sin(<span class="hljs-number">2</span> * pi * ((np.sqrt((x-bx)**<span class="hljs-number">2</span>+(y-by)**<span class="hljs-number">2</span>))/lamb - t/T))<br>    <span class="hljs-keyword">return</span> E     <br><br><br>X = np.arange(<span class="hljs-number">100</span>, step = <span class="hljs-number">.1</span>)<br>Y = np.arange(-<span class="hljs-number">50</span>,<span class="hljs-number">50</span>, step = <span class="hljs-number">.1</span>)<br>X, Y = np.meshgrid(X, Y)<br>Points = waveFun(X,X)<br><br>X = np.arange(-<span class="hljs-number">50</span>, <span class="hljs-number">50</span>, step = <span class="hljs-number">.1</span>)<br>Y = np.arange(-<span class="hljs-number">50</span>, <span class="hljs-number">50</span>, step = <span class="hljs-number">.1</span>)<br>X, Y = np.meshgrid(X, Y)<br>Points += waveFun(X,Y, lamb = <span class="hljs-number">.7</span>, T = <span class="hljs-number">.7</span>)<br><br>plt.imshow(Points)<br>plt.show()<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">200</span>):<br>    i = (i-<span class="hljs-number">100</span>)/<span class="hljs-number">20</span>  <br>    X = np.arange(<span class="hljs-number">100</span>, step = <span class="hljs-number">.1</span>)<br>    Y = np.arange(-<span class="hljs-number">50</span> + i,<span class="hljs-number">50</span> + i, step = <span class="hljs-number">.1</span>)<br>    X, Y = np.meshgrid(X, Y)<br>    Points += waveFun(X,Y)<br><br><br><span class="hljs-comment">#plt.scatter(points_array[:, 0], points_array[:, 1] + 0.5, c= points_array[:, 2], marker=&#x27;o&#x27;, cmap=plt.cm.coolwarm,)</span><br><span class="hljs-comment">#plt.show()</span><br><span class="hljs-comment"># Create a PyVista point cloud</span><br>point_cloud = pv.PolyData(points_array)<br>point_cloud[<span class="hljs-string">&#x27;point_color&#x27;</span>] = point_cloud.points[:, <span class="hljs-number">2</span>]<br>point_cloud.plot(point_size=<span class="hljs-number">5</span>,scalars=<span class="hljs-string">&#x27;point_color&#x27;</span>, cmap=<span class="hljs-string">&quot;jet&quot;</span>, show_bounds=<span class="hljs-literal">True</span>)<br><br><br><br><br><br><br>Y = <span class="hljs-built_in">range</span>(-<span class="hljs-number">100</span>,<span class="hljs-number">100</span>)<br>X = <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">200</span>)<br>Points = []<br><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X:<br>    x /=<span class="hljs-number">10</span><br>    <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> Y:<br>        y /=<span class="hljs-number">10</span><br>        E = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> by <span class="hljs-keyword">in</span> np.arange(-<span class="hljs-number">10</span>,<span class="hljs-number">11</span>, <span class="hljs-number">.1</span>):<br>            E += waveFun(x, y, <span class="hljs-number">0</span>, by)<br>        Points += [[x, y, E] ]<br><br>points_array = np.array(Points)<br><span class="hljs-comment">#plt.scatter(points_array[:, 0], points_array[:, 1] + 0.5, c= points_array[:, 2], marker=&#x27;o&#x27;, cmap=plt.cm.coolwarm,)</span><br><span class="hljs-comment">#plt.show()</span><br><span class="hljs-comment"># Create a PyVista point cloud</span><br>point_cloud = pv.PolyData(points_array)<br>point_cloud[<span class="hljs-string">&#x27;point_color&#x27;</span>] = point_cloud.points[:, <span class="hljs-number">2</span>]<br>point_cloud.plot(point_size=<span class="hljs-number">5</span>,scalars=<span class="hljs-string">&#x27;point_color&#x27;</span>, cmap=<span class="hljs-string">&quot;jet&quot;</span>, show_bounds=<span class="hljs-literal">True</span>)<br><br><br><br>point_cloud = pv.PolyData(Points)<br>volume = point_cloud.delaunay_3d(alpha = <span class="hljs-number">5</span>)<br>shell = volume.extract_geometry()<br>shell.plot(show_edges=<span class="hljs-literal">True</span>)<br><br>TB = pd.DataFrame(Points, columns = [<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&#x27;E&#x27;</span>])<br><br>fig, ax = plt.subplots(subplot_kw=&#123;<span class="hljs-string">&quot;projection&quot;</span>: <span class="hljs-string">&quot;3d&quot;</span>&#125;, figsize=(<span class="hljs-number">6</span>,<span class="hljs-number">6</span>))<br>P = ax.plot_trisurf(TB.x, TB.y, TB.E, vmin=TB.E.<span class="hljs-built_in">min</span>() * <span class="hljs-number">2</span>, cmap=plt.cm.coolwarm)<br>plt.show()<br><br><br><span class="hljs-keyword">from</span> scipy.interpolate <span class="hljs-keyword">import</span> griddata<br><br>df =TB<br>x1 = np.linspace(df[<span class="hljs-string">&#x27;x&#x27;</span>].<span class="hljs-built_in">min</span>(), df[<span class="hljs-string">&#x27;x&#x27;</span>].<span class="hljs-built_in">max</span>(), <span class="hljs-built_in">len</span>(df[<span class="hljs-string">&#x27;x&#x27;</span>].unique()))<br>y1 = np.linspace(df[<span class="hljs-string">&#x27;y&#x27;</span>].<span class="hljs-built_in">min</span>(), df[<span class="hljs-string">&#x27;y&#x27;</span>].<span class="hljs-built_in">max</span>(), <span class="hljs-built_in">len</span>(df[<span class="hljs-string">&#x27;y&#x27;</span>].unique()))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">x, y via meshgrid for vectorized evaluation of</span><br><span class="hljs-string">2 scalar/vector fields over 2-D grids, given</span><br><span class="hljs-string">one-dimensional coordinate arrays x1, x2,..., xn.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>x2, y2 = np.meshgrid(x1, y1)<br><span class="hljs-comment"># Interpolate unstructured D-dimensional data.</span><br>z2 = griddata((df[<span class="hljs-string">&#x27;x&#x27;</span>], df[<span class="hljs-string">&#x27;y&#x27;</span>]), df[<span class="hljs-string">&#x27;E&#x27;</span>], (x2, y2), method=<span class="hljs-string">&#x27;cubic&#x27;</span>)<br><br><br><br><span class="hljs-comment"># Ready to plot</span><br>fig = plt.figure()<br>ax = fig.subplots(subplot_kw = &#123;<span class="hljs-string">&quot;projection&quot;</span>:<span class="hljs-string">&#x27;3d&#x27;</span>&#125;)<br><br>surf = ax.plot_surface(x2, y2, z2, rstride=<span class="hljs-number">1</span>, cstride=<span class="hljs-number">1</span>, cmap=plt.cm.coolwarm,<br>                       linewidth=<span class="hljs-number">0</span>, antialiased=<span class="hljs-literal">False</span>)<br>ax.set_zlim(-<span class="hljs-number">1.01</span>, <span class="hljs-number">1.01</span>)<br><br>ax.zaxis.set_major_locator(LinearLocator(<span class="hljs-number">10</span>))<br>ax.zaxis.set_major_formatter(FormatStrFormatter(<span class="hljs-string">&#x27;%.02f&#x27;</span>))<br>fig.colorbar(surf, shrink=<span class="hljs-number">0.5</span>, aspect=<span class="hljs-number">5</span>)<br>plt.title(<span class="hljs-string">&#x27;Meshgrid Created from 3 1D Arrays&#x27;</span>)<br><br>plt.show()<br><br></code></pre></td></tr></table></figure></div></details>]]></content>
    
    
    <summary type="html">Light in Physics</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Using Vim as Python and R IDE</title>
    <link href="https://karobben.github.io/2024/01/03/Linux/nvimr/"/>
    <id>https://karobben.github.io/2024/01/03/Linux/nvimr/</id>
    <published>2024-01-03T20:42:34.000Z</published>
    <updated>2024-01-20T23:06:45.609Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Using-Vim-as-IDE">Using Vim as IDE</h2><p>Vim is a classic text editor known for efficiency, while NeoVim is its modernized fork with improvements like better plugin support. LunarVim, built on NeoVim, offers a pre-configured setup, making it easier for users to get a powerful, feature-rich environment without the hassle of individual configurations. Ideal for those new to Vim/NeoVim or seeking a ready-to-use development setup, LunarVim combines ease of setup with customizability. It‚Äôs particularly appealing for its integrated toolset, active community support, and a balance between functionality and performance, making it a great choice for a streamlined coding experience.</p><p>Plug: <a href="https://github.com/jamespeapen/Nvim-R/wiki">Nvim-R</a><br>Video Tutorial: <a href="https://www.youtube.com/watch?v=nm45WagtV3w">Rohit Farmer</a><br>Instruction following the video: <a href="https://gist.github.com/rohitfarmer/68cdadeaeeb196e8a6ecdebdee6e76a5">rohitfarmer</a></p><p>Final work:</p><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/Pdzomq1.png" alt="Using vim as R IDE"></th></tr></thead><tbody><tr><td style="text-align:center">¬© Karobben</td></tr></tbody></table><h2 id="Installation">Installation</h2><p>Please install the latest NeoVim by following the <a href="https://github.com/neovim/neovim/blob/master/INSTALL.md">Neovim document</a> and <a href="https://www.lunarvim.org/docs/installation">LunarVim Document</a></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># install neovim</span><br><span class="hljs-comment"># sudo apt install neovim</span><br><span class="hljs-comment"># install vim-plug</span><br>curl -fLo ~/.<span class="hljs-built_in">local</span>/share/nvim/site/<span class="hljs-built_in">autoload</span>/plug.vim --create-dirs \<br>    https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim<br><br><span class="hljs-comment"># for storing the config file</span><br>mkdir ~/.config/nvim<br>touch ~/.config/nvim/init.vim<br><span class="hljs-comment"># for storing plug</span><br><span class="hljs-comment"># because all plug would be installed by here, you could just delete the directories here to delete plugs.</span><br>mkdir ~/.vim/plugged<br><span class="hljs-comment"># open and edit the config file</span><br>vim ~/.config/nvim/init.vim<br></code></pre></td></tr></table></figure></div><p>Save the lines below in the <code>~/.config/nvim/init.vim</code> file to install the pluges.</p><pre>" Specify a directory for plugins" - Avoid using standard Vim directory names like 'plugin'call plug#begin('~/.vim/plugged')" List of plugins." Make sure you use single quotes" Shorthand notationPlug 'jalvesaq/Nvim-R', { 'branch' : 'stable' }Plug 'ncm2/ncm2'Plug 'roxma/nvim-yarp'Plug 'gaalcaras/ncm-R'Plug 'preservim/nerdtree'Plug 'Raimondi/delimitMate'Plug 'patstockwell/vim-monokai-tasty'Plug 'itchyny/lightline.vim'" Initialize plugin systemcall plug#end()</pre><div class="admonition note"><p class="admonition-title">How to instsall the plunges</p><p>After stored the change, you need to open it again by using <code>nvim ~/.config/nvim/init.vim</code>. And then, under the command model (which is triggered by <code>:</code>), input <code>PlugInstall</code> (<code>PlugUpdate</code> if you want to update them). After you see the picture below which means you installed it successfully:<img src="https://imgur.com/SMB1YM3.png" alt="" /></p></div><p>By following the instruction from <a href="https://gist.github.com/rohitfarmer/68cdadeaeeb196e8a6ecdebdee6e76a5">rohitfarmer</a>‚Äôs post, we could add more things at the end of the <code>init.vim</code> file:</p><div class="admonition note"><p class="admonition-title">folding behavior</p><pre>" Set foldbehavior<p>set tabstop=2        &quot; Number of spaces that a <Tab> in the file counts for<br>set shiftwidth=2     &quot; Number of spaces to use for each step of (auto)indent<br>set softtabstop=2    &quot; Number of spaces that a <Tab> counts for while performing editing operations<br>set expandtab        &quot; Use spaces instead of tabs</p><p>set foldmethod=indent<br>set foldlevelstart=2    &quot; Start folding at an indent level greater than 2<br></pre></p><p>For quick unfold all codes:</p><pre>:set nofoldenable</pre></div><pre>" Set a Local Leader" With a map leader it's possible to do extra key combinations" like <leader>w saves the current filelet mapleader = ","let g:mapleader = ","" Plugin Related Settings" NCM2autocmd BufEnter * call ncm2#enable_for_buffer()    " To enable ncm2 for all buffers.set completeopt=noinsert,menuone,noselect           " :help Ncm2PopupOpen for more                                                    " information." NERD Treemap <leader>nn :NERDTreeToggle<CR>                  " Toggle NERD tree." Monokai-tastylet g:vim_monokai_tasty_italic = 1                  " Allow italics.colorscheme vim-monokai-tasty                       " Enable monokai theme." LightLine.vim set laststatus=2              " To tell Vim we want to see the statusline.let g:lightline = {   \ 'colorscheme':'monokai_tasty',   \ }" General NVIM/VIM Settings" Mouse Integrationset mouse=i                   " Enable mouse support in insert mode." Tabs & Navigationmap <leader>nt :tabnew<cr>    " To create a new tab.map <leader>to :tabonly<cr>     " To close all other tabs (show only the current tab).map <leader>tc :tabclose<cr>    " To close the current tab.map <leader>tm :tabmove<cr>     " To move the current tab to next position.map <leader>tn :tabn<cr>        " To swtich to next tab.map <leader>tp :tabp<cr>        " To switch to previous tab." Line Numbers & Indentationset backspace=indent,eol,start  " To make backscape work in all conditions.set ma                          " To set mark a at current cursor location.set number                      " To switch the line numbers on.set expandtab                   " To enter spaces when tab is pressed.set smarttab                    " To use smart tabs.set autoindent                  " To copy indentation from current line                                 " when starting a new line.set si                          " To switch on smart indentation." Searchset ignorecase                  " To ignore case when searching.set smartcase                   " When searching try to be smart about cases.set hlsearch                    " To highlight search results.set incsearch                   " To make search act like search in modern browsers.set magic                       " For regular expressions turn magic on." Bracketsset showmatch                   " To show matching brackets when text indicator                                 " is over them.set mat=2                       " How many tenths of a second to blink                                 " when matching brackets." Errorsset noerrorbells                " No annoying sound on errors." Color & Fontssyntax enable                   " Enable syntax highlighting.set encoding=utf8                " Set utf8 as standard encoding and                                  " en_US as the standard language." Enable 256 colors palette in Gnome Terminal.if $COLORTERM == 'gnome-terminal'    set t_Co=256endiftry    colorscheme desertcatchendtry" Files & Backupset nobackup                     " Turn off backup.set nowb                         " Don't backup before overwriting a file.set noswapfile                   " Don't create a swap file.set ffs=unix,dos,mac             " Use Unix as the standard file type." Return to last edit position when opening filesau BufReadPost * if line("'\"") > 1 && line("'\"") <= line("$") | exe "normal! g'\"" | endif</pre><h2 id="Basic-Usage-of-Nvim-R">Basic Usage of Nvim-R</h2><pre>Ctrl + W + HJKL   " Remove the cursor from window to window,nt               " Open a new tab,tn               " Move to the next tab,tp               " Back to the previous tab# code fold behaviorzc - Close (fold) the current fold under the cursor.zo - Open (unfold) the current fold under the cursor.za - Toggle between closing and opening the fold under the cursor.zR - Open all folds in the current buffer.zM - Close all folds in the current buffer.# Nvim-RCtrl + x + o      " Access the help information (auto fill)\rf               " Connect to R console.\rq               " Quit R console.\ro               " Open object bowser.\d                " Execute current line of code and move to the next line.\ss               " Execute a block of selected code.\aa               " Execute the entire script. This is equivalent to source().\xx               " Toggle comment in an R script.# NERDTree,nn               " Toggle NERDTree.</pre><h2 id="Basic-codes-for-nvim">Basic codes for nvim</h2><p>You could also included them into the <code>vim.init</code> file</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># wrap the words in it is too long</span><br>:<span class="hljs-built_in">set</span> wrap <br>:<span class="hljs-built_in">set</span> nowrap<br><span class="hljs-comment"># set the wrap behavior</span><br>:<span class="hljs-built_in">set</span> showbreak=‚Ü™\ <br></code></pre></td></tr></table></figure></div><h2 id="Bugs">Bugs</h2><p>After installed the Nvim-R (them master branch), you‚Äôll have the error code below whenever you open nvim. Just ignore it and it would be fine.</p><pre>Error detected while processing function ROnJobStdout[40]..UpdateSynRhlist[11]..FunHiOtherBf:line   10:E117: Unknown function: nvim_set_option_valuePress ENTER or type command to continue</pre><h2 id="Configure-for-LuanrVim">Configure for LuanrVim</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">curl --proto <span class="hljs-string">&#x27;=https&#x27;</span> --tlsv1.2 -sSf https://sh.rustup.rs | sh<br>cat <span class="hljs-string">&quot;<span class="hljs-variable">$HOME</span>/.cargo/env&quot;</span> &gt;&gt; ~/.zshrc<br><span class="hljs-built_in">source</span> ~/.zshrc<br><br><span class="hljs-comment"># install LunarVim</span><br>LV_BRANCH=<span class="hljs-string">&#x27;release-1.3/neovim-0.9&#x27;</span> bash &lt;(curl -s https://raw.githubusercontent.com/LunarVim/LunarVim/release-1.3/neovim-0.9/utils/installer/install.sh)<br><br>rm -rf ~/.config/lvim/<br>git <span class="hljs-built_in">clone</span> https://github.com/Karobben/kickstart.nvim.git ~/.config/lvim<br></code></pre></td></tr></table></figure></div><h3 id="Nerd-font">Nerd font</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># curl -fsSL https://raw.githubusercontent.com/ronniedroid/getnf/master/install.sh | bash</span><br>mkdir -p ~/.<span class="hljs-built_in">local</span>/share/fonts<br><span class="hljs-built_in">cd</span> ~/.<span class="hljs-built_in">local</span>/share/fonts &amp;&amp; curl -fLO https://github.com/ryanoasis/nerd-fonts/raw/HEAD/patched-fonts/DroidSansMono/DroidSansMNerdFont-Regular.otf<br>fc-cache -f -v<br></code></pre></td></tr></table></figure></div><h3 id="Words-Editing">Words Editing</h3><p>For editing the word, we need to switch the model of read, visual, and editing. Press <code>i</code> enable the editing mode. Type <code>Esc</code> or <code>Ctrl + c</code> exist the editing mode and back to the reading mode. <code>v</code> enable selection model so you could select words.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="LIVECODESERVER"><figure class="iseeu highlight /livecodeserver"><table><tr><td class="code"><pre><code class="hljs livecodeserver">i                   enter editing mode<br>dd                  cut <span class="hljs-keyword">the</span> selected <span class="hljs-built_in">line</span> <span class="hljs-keyword">into</span> paste board<br>p                   paste <span class="hljs-keyword">the</span> coppied contents<br>v                   selecte mode <span class="hljs-built_in">to</span> selecte multiple <span class="hljs-keyword">words</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">lines</span><br>    wd              <span class="hljs-built_in">delete</span> <span class="hljs-keyword">the</span> selected <span class="hljs-keyword">words</span>/<span class="hljs-keyword">lines</span><br>    y               copy <span class="hljs-keyword">the</span> selected <span class="hljs-keyword">words</span>/<span class="hljs-keyword">lines</span><br>    p               repalce <span class="hljs-keyword">the</span> selected workds/<span class="hljs-keyword">lines</span> <span class="hljs-keyword">with</span> coppied contents<br>o                   Start <span class="hljs-keyword">a</span> <span class="hljs-built_in">new</span> <span class="hljs-built_in">line</span><br>:&gt;                  Intend <span class="hljs-keyword">the</span> selected <span class="hljs-built_in">line</span> <br>:&gt;&gt;                 Intend tiwce<br>:&lt;                  Undo <span class="hljs-keyword">the</span> intend<br>: m <span class="hljs-number">10</span>              Move <span class="hljs-keyword">the</span> selected <span class="hljs-keyword">words</span>/<span class="hljs-built_in">line</span> <span class="hljs-keyword">into</span> <span class="hljs-built_in">line</span> <span class="hljs-number">10</span><br>Alt+ j/k        Move selected <span class="hljs-keyword">words</span>/<span class="hljs-keyword">lines</span> up/donw<br></code></pre></td></tr></table></figure></div><h3 id="Cursor-Related">Cursor Related</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="LIVECODESERVER"><figure class="iseeu highlight /livecodeserver"><table><tr><td class="code"><pre><code class="hljs livecodeserver">h                   moving left<br>j                   moving down<br>k                   moving up<br>l                   moving <span class="hljs-literal">right</span><br>:<span class="hljs-number">10</span>                 moving <span class="hljs-built_in">to</span> <span class="hljs-built_in">line</span> <span class="hljs-number">10</span><br>w                   moving <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> head <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> next <span class="hljs-built_in">word</span><br>e                   moving <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> <span class="hljs-function"><span class="hljs-keyword">end</span> <span class="hljs-title">of</span> <span class="hljs-title">the</span> <span class="hljs-title">word</span></span><br>b                   moving <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> head <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> previous <span class="hljs-built_in">word</span><br>ge                  moving back <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> <span class="hljs-function"><span class="hljs-keyword">end</span> <span class="hljs-title">of</span> <span class="hljs-title">the</span> <span class="hljs-title">word</span></span><br><br>gg                  moving <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> top <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> <span class="hljs-built_in">file</span><br>G                   moving <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> <span class="hljs-function"><span class="hljs-keyword">end</span> <span class="hljs-title">of</span> <span class="hljs-title">the</span> <span class="hljs-title">file</span></span><br>Ctrl + f            page foward (donw)<br>Ctrl + b            page back (up)<br></code></pre></td></tr></table></figure></div><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="VIM"><figure class="iseeu highlight /vim"><table><tr><td class="code"><pre><code class="hljs vim"><span class="hljs-symbol">&lt;leader&gt;</span> <span class="hljs-keyword">e</span>          <span class="hljs-keyword">open</span> directory exploer<br><span class="hljs-symbol">&lt;leader&gt;</span> <span class="hljs-keyword">w</span>          save the <span class="hljs-keyword">edit</span> <span class="hljs-keyword">file</span> (:<span class="hljs-keyword">w</span>)<br><br># about <span class="hljs-keyword">tab</span><br><span class="hljs-symbol">&lt;leader&gt;</span> bb         back <span class="hljs-keyword">to</span> the <span class="hljs-keyword">tab</span> <span class="hljs-keyword">left</span><br><span class="hljs-symbol">&lt;leader&gt;</span> <span class="hljs-keyword">bn</span>         <span class="hljs-keyword">go</span> <span class="hljs-keyword">to</span> the <span class="hljs-keyword">next</span> <span class="hljs-keyword">tab</span> (<span class="hljs-keyword">right</span>)<br>:<span class="hljs-keyword">bd</span>                 <span class="hljs-keyword">close</span> the current <span class="hljs-keyword">tab</span><br><br><span class="hljs-symbol">&lt;Alt&gt;</span> <span class="hljs-number">1</span>/<span class="hljs-number">2</span>/<span class="hljs-number">3</span>         <span class="hljs-keyword">open</span> <span class="hljs-keyword">a</span> terminal (<span class="hljs-keyword">only</span> when you installed the pluges)<br><span class="hljs-symbol">&lt;Ctrl&gt;</span> <span class="hljs-keyword">w</span>            swtich between windows<br><br>:<span class="hljs-keyword">set</span> mouse-=<span class="hljs-keyword">a</span>       Disable the mouse selection<br>:<span class="hljs-keyword">set</span> mouse=<span class="hljs-keyword">a</span>        Inable the mouse <br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Using Vim as Python and R IDE, easy configure</summary>
    
    
    
    <category term="Linux" scheme="https://karobben.github.io/categories/Linux/"/>
    
    
    <category term="IDE" scheme="https://karobben.github.io/tags/IDE/"/>
    
    <category term="vim" scheme="https://karobben.github.io/tags/vim/"/>
    
  </entry>
  
  <entry>
    <title>Structure of the Immunoglobulin</title>
    <link href="https://karobben.github.io/2024/01/02/LearnNotes/igstructure/"/>
    <id>https://karobben.github.io/2024/01/02/LearnNotes/igstructure/</id>
    <published>2024-01-02T16:53:01.000Z</published>
    <updated>2024-01-12T00:29:13.787Z</updated>
    
    <content type="html"><![CDATA[<h2 id="The-Structure-of-the-Immunoglobulin">The Structure of the Immunoglobulin</h2><table><thead><tr><th style="text-align:center"><img src="https://www.researchgate.net/publication/337733518/figure/fig2/AS:832370903097347@1575464096611/Basic-structure-of-an-IgG-antibody-The-IgG-antibody-is-made-out-of-variable-V-and.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.researchgate.net/publication/337733518_Quantitative_Mass_Spectrometric_Analysis_of_Autoantibodies_as_a_Paradigm_Shift_in_Autoimmune_Serology">¬© Adrian Y. S. Lee</a></td></tr></tbody></table><p>Immunoglobulins, commonly known as antibodies, are crucial proteins in the immune system that recognize and bind to specific antigens, such as bacteria and viruses, to help protect the body. Their structure is both unique and complex, consisting of several key components:</p><ol><li><p><strong>Basic Structure</strong>: Immunoglobulins are Y-shaped molecules made up of four polypeptide chains - two identical heavy (H) chains and two identical light (L) chains. These chains are held together by disulfide bonds.</p></li><li><p><strong>Variable (V) and Constant ¬© Regions</strong>:</p><ul><li><strong>Variable Regions</strong>: The tips of the ‚ÄòY‚Äô shape consist of the variable regions of the light and heavy chains. These regions are highly diverse and are responsible for the antigen-binding specificity of the antibody.</li><li><strong>Constant Regions</strong>: The rest of the molecule forms the constant region, which is relatively conserved across different antibodies. The constant region of the heavy chains determines the class or isotype (e.g., IgG, IgM, IgA, IgE, IgD) of the antibody and mediates effector functions.</li></ul></li><li><p><strong>Isotypes</strong>: Mammals have several classes of immunoglobulins (IgG, IgA, IgM, IgE, IgD), each with different roles in the immune response. These isotypes differ mainly in their heavy chain constant regions.</p></li><li><p><strong>Glycosylation</strong>: Many antibodies are glycosylated, meaning they have carbohydrate groups attached. This glycosylation can affect the antibody‚Äôs stability, distribution, and activity.</p></li><li><p><strong>Light Chain Types</strong>: There are two types of light chains in antibodies - kappa (<mark>Œ∫</mark>) and lambda (<mark>Œª</mark>). An individual antibody will have two identical light chains of one type.</p></li></ol><h2 id="V-D-J">V(D)J</h2><table><thead><tr><th style="text-align:center"><img src="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5089068/bin/nihms-673138-f0001.jpg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5089068/">¬© David B. Roth</a></td></tr></tbody></table><p>V(D)J recombination is a mechanism in the immune system that generates the immense diversity of antibodies (immunoglobulins) and T cell receptors necessary for the adaptive immune response. This process is named for the three gene segments involved in the recombination: Variable (V), Diversity (D), and Joining (J). So, V(D)J is the recombination unit.</p><h3 id="V-D-J-in-3D-Structure">V(D)J in 3D Structure</h3><p>We take <code>5wl2</code> as example:</p><table><thead><tr><th style="text-align:center"><img src="https://cdn.rcsb.org/images/structures/5wl2_assembly-1.jpeg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.rcsb.org/structure/5wl2">¬©PDB 5wl2</a></td></tr></tbody></table><h2 id="Kabat-Number">Kabat Number</h2><table><thead><tr><th style="text-align:center"><img src="https://assets-global.website-files.com/628cfd01406f3f5bb9c8477d/63821c6283c0cdf36d5289da_Antibody-numbering-IMGT-Kabat-Chothia.png" alt="Kabat number"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://pipebio.com/blog/antibody-numbering">¬© pipebio</a></td></tr></tbody></table><p>How to make it in python:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> abnumber <span class="hljs-keyword">import</span> Chain<br><br>Fasta = <span class="hljs-string">&quot;xxx.fa&quot;</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(Fasta, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> F:<br>    Seq = F.read()<br><br>KABAT = pd.DataFrame([<span class="hljs-built_in">dict</span>(Chain(i, scheme=<span class="hljs-string">&#x27;kabat&#x27;</span>)) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span>  Seq.split(<span class="hljs-string">&#x27;\n&#x27;</span>)[:-<span class="hljs-number">2</span>] <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;&gt;&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> i])<br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Structure of the Immunoglobulin</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>ImageMagick: convert</title>
    <link href="https://karobben.github.io/2023/12/25/Linux/convert/"/>
    <id>https://karobben.github.io/2023/12/25/Linux/convert/</id>
    <published>2023-12-25T23:16:09.000Z</published>
    <updated>2023-12-26T01:02:50.382Z</updated>
    
    <content type="html"><![CDATA[<p>Documentation: <a href="https://imagemagick.org/script/command-line-tools.php">imagemagick.org</a></p><p>The <code>convert</code> command in Linux is a part of the ImageMagick suite, a powerful toolset for image manipulation. This command allows you to convert between different image formats, resize images, change image quality, and perform a wide variety of other image transformations.</p><p>Here are a few basic examples of what you can do with the <code>convert</code> command:</p><ol><li><strong>Converting an Image Format:</strong><br>To convert a JPEG image to a PNG, you would use:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert image.jpg image.png<br></code></pre></td></tr></table></figure></div><ol start="2"><li><strong>Resizing an Image:</strong><br>To resize an image to a specific width and height (e.g., 100x100 pixels), you can use:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># based on the pixel of x*y </span><br>convert original.jpg -resize 100x100 resized.jpg<br><span class="hljs-comment"># based on the ratio </span><br>convert original.jpg -resize 30% reduced.jpg<br><span class="hljs-comment"># based on the ratio of x*y</span><br>convert original.jpg -resize 50%x30% reduced.jpg<br></code></pre></td></tr></table></figure></div><ol start="3"><li><strong>Changing Image Quality:</strong><br>To change the quality of a JPEG image, useful for reducing file size, use:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -quality 85 compressed.jpg<br></code></pre></td></tr></table></figure></div><ol start="4"><li><strong>Combining Multiple Images:</strong><br>To combine multiple images into one, for instance, side by side:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert image1.jpg image2.jpg +append combined.jpg<br></code></pre></td></tr></table></figure></div><ol start="5"><li><strong>Converting a PDF to an Image:</strong><br>To convert a PDF file to a series of images, you can use:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert document.pdf document.png<br></code></pre></td></tr></table></figure></div><ol start="6"><li><strong>Creating an Animated GIF:</strong><br>To create an animated GIF from a series of images:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert -delay 20 -loop 0 frame1.png frame2.png frame3.png animated.gif<br></code></pre></td></tr></table></figure></div><p>These examples are just the tip of the iceberg in terms of what ImageMagick‚Äôs <code>convert</code> command can do. It‚Äôs a very powerful tool with a wide array of options and capabilities. For more detailed information, you can check the manual page (<code>man convert</code>) or the official ImageMagick documentation.</p><h2 id="Other-Functions-You-May-Want-to-Know">Other Functions You May Want to Know</h2><p>Certainly! Here are examples demonstrating various capabilities of ImageMagick:</p><ol><li><strong>Image Composition:</strong><br>Overlay one image on top of another (watermark):</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert background.jpg watermark.png -gravity center -composite output.jpg<br></code></pre></td></tr></table></figure></div><ol start="2"><li><strong>Color Manipulation:</strong><br>Convert an image to grayscale:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -colorspace Gray grayscale.jpg<br></code></pre></td></tr></table></figure></div><ol start="3"><li><strong>Cropping:</strong><br>Crop an image to a 100x100 pixel square starting at (50,50):</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -crop 100x100+50+50 cropped.jpg<br></code></pre></td></tr></table></figure></div><ol start="4"><li><strong>Rotating and Flipping:</strong><br>Rotate an image by 90 degrees:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -rotate 90 rotated.jpg<br></code></pre></td></tr></table></figure></div><ol start="5"><li><strong>Blurring and Sharpening:</strong><br>Apply a Gaussian blur:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -blur 0x8 blurred.jpg<br></code></pre></td></tr></table></figure></div><ol start="6"><li><strong>Drawing:</strong><br>Draw a red rectangle:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -fill none -stroke red -draw <span class="hljs-string">&quot;rectangle 10,10 50,50&quot;</span> output.jpg<br></code></pre></td></tr></table></figure></div><ol start="7"><li><strong>Format Conversion:</strong><br>Convert an image to a different format (e.g., PNG to GIF):</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert image.png image.gif<br></code></pre></td></tr></table></figure></div><ol start="8"><li><strong>Handling Transparency:</strong><br>Make the white background of an image transparent:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -transparent white output.png<br></code></pre></td></tr></table></figure></div><ol start="9"><li><strong>Image Annotation:</strong><br>Add text to an image:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -pointsize 24 -fill black -annotate +50+50 <span class="hljs-string">&#x27;Sample Text&#x27;</span> output.jpg<br></code></pre></td></tr></table></figure></div><ol start="10"><li><strong>Special Effects:</strong><br>Add a shadow to an image:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg \( +<span class="hljs-built_in">clone</span> -background black -shadow 60x5+10+10 \) +swap -background none -layers merge +repage shadow.jpg<br></code></pre></td></tr></table></figure></div><ol start="11"><li><strong>Image Optimization:</strong><br>Optimize an image for the web (reduce file size):</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -strip -interlace Plane -gaussian-blur 0.05 -quality 85% optimized.jpg<br></code></pre></td></tr></table></figure></div><ol start="12"><li><strong>Batch Processing:</strong><br>Resize all PNG images in a directory (example in a bash loop):</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> *.png; <span class="hljs-keyword">do</span> convert <span class="hljs-string">&quot;<span class="hljs-variable">$img</span>&quot;</span> -resize 50% <span class="hljs-string">&quot;resized_<span class="hljs-variable">$img</span>&quot;</span>; <span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure></div><ol start="13"><li><strong>Image Analysis:</strong><br>Get image information (format, dimensions, etc.):</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">identify -verbose image.jpg<br></code></pre></td></tr></table></figure></div><ol start="14"><li><strong>Creating Thumbnails:</strong><br>Generate a thumbnail:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert original.jpg -thumbnail 100x100 thumbnail.jpg<br></code></pre></td></tr></table></figure></div><ol start="15"><li><strong>Morphing and Transforming:</strong><br>Morph between two images:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">convert image1.jpg image2.jpg -morph 10 morph_output.jpg<br></code></pre></td></tr></table></figure></div><p>These commands showcase the versatility of ImageMagick. Remember to adjust the file names and parameters according to your specific needs. The ImageMagick documentation provides more detailed information and examples for these and other features.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">ImageMagick: convert</summary>
    
    
    
    <category term="Linux" scheme="https://karobben.github.io/categories/Linux/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Linux/Software/"/>
    
    
    <category term="Image" scheme="https://karobben.github.io/tags/Image/"/>
    
    <category term="Linux" scheme="https://karobben.github.io/tags/Linux/"/>
    
    <category term="bash" scheme="https://karobben.github.io/tags/bash/"/>
    
    <category term="Scripting" scheme="https://karobben.github.io/tags/Scripting/"/>
    
    <category term="CLI Tools" scheme="https://karobben.github.io/tags/CLI-Tools/"/>
    
  </entry>
  
  <entry>
    <title>IMGT¬Æ: the international ImMunoGeneTics information system¬Æ</title>
    <link href="https://karobben.github.io/2023/12/25/Bioinfor/imgt/"/>
    <id>https://karobben.github.io/2023/12/25/Bioinfor/imgt/</id>
    <published>2023-12-25T17:06:14.000Z</published>
    <updated>2023-12-25T19:08:52.156Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-IMGT">What is IMGT</h2><p>IMGT¬Æ provides a range of databases, tools, and web resources focused on the immune system, particularly on the genetic and structural aspects of immunoglobulins (IG), T cell receptors (TCR), major histocompatibility complex (MHC) of all vertebrate species, and related proteins of the immune system (RPI) of any species. These resources are crucial for research in various fields, including immunology, genetics, bioinformatics, drug design, and personalized medicine.</p><p>Key features and offerings of IMGT¬Æ include:</p><ol><li><p><strong>Databases:</strong> IMGT¬Æ offers several databases containing detailed information on IG, TCR, and MHC sequences and structures, along with RPI. These databases are meticulously curated and regularly updated.</p></li><li><p><strong>Analysis Tools:</strong> IMGT¬Æ provides tools for sequence analysis, gene identification, and 3D structure determination. IMGT/V-QUEST, for instance, is a widely used tool for the analysis of IG and TCR sequences.</p></li><li><p><strong>Standardized Nomenclature:</strong> IMGT¬Æ has established a standardized nomenclature for the description of IG and TCR genetic components, which is essential for consistent communication and research in the field.</p></li><li><p><strong>Educational Resources:</strong> The system also offers educational resources for those new to the field of immunogenetics, including tutorials, glossaries, and comprehensive descriptions of the molecular components of the immune system.</p></li><li><p><strong>Research and Clinical Applications:</strong> The information and tools provided by IMGT¬Æ are invaluable for various applications, including research in immunology, genetics, and autoimmunity, as well as in clinical settings for antibody engineering, diagnosis, and understanding of immune disorders.</p></li></ol><h2 id="Main-Service-of-the-IMGT">Main Service of the IMGT</h2><table><thead><tr><th><strong>Category</strong></th><th><strong>Name</strong></th><th><strong>Description/Focus Area</strong></th></tr></thead><tbody><tr><td><strong>Databases</strong></td><td>IMGT/GENE-DB</td><td>Database for immunoglobulin (IG) and T cell receptor (TCR) genes of all vertebrate species.</td></tr><tr><td></td><td>IMGT/3Dstructure-DB</td><td>Database for 3D structures of IG, TCR, MHC, and RPI (related proteins of the immune system).</td></tr><tr><td></td><td>IMGT/LIGM-DB</td><td>A comprehensive database of IG and TCR nucleotide sequences from various species.</td></tr><tr><td></td><td>IMGT/PRIMER-DB</td><td>Database of primers and probes for IG and TCR gene sequences.</td></tr><tr><td></td><td>IMGT/PROTEIN-DB</td><td>Database for IG, TCR, MHC, and RPI protein sequences and structures.</td></tr><tr><td><strong>Analysis Tools</strong></td><td><mark>IMGT/V-QUEST</mark></td><td>Tool for the analysis of IG and TCR nucleotide sequences. Identifies V, D, and J gene segments and alleles.</td></tr><tr><td></td><td><mark>IMGT/JunctionAnalysis</mark></td><td>Tool focused on detailed analysis of the V-J and V-D-J junctions of IG and TCR sequences.</td></tr><tr><td></td><td>IMGT/HighV-QUEST</td><td>High-throughput version of IMGT/V-QUEST for next-generation sequencing (NGS) data.</td></tr><tr><td></td><td>IMGT/DomainGapAlign</td><td>Tool for the analysis of IG, TCR, and RPI domain sequences and comparison with IMGT reference directory.</td></tr><tr><td></td><td>IMGT/Collier-de-Perles</td><td>Tool for two-dimensional (2D) graphical representation of IG and TCR variable domains.</td></tr><tr><td><strong>Resources</strong></td><td>IMGT Education</td><td>Educational resources, including tutorials, glossaries, and comprehensive descriptions of immunogenetics.</td></tr><tr><td></td><td>IMGT Scientific chart</td><td>Standardized nomenclature and classification for IG, TCR, and MHC of humans and other vertebrates.</td></tr><tr><td></td><td>IMGT Repertoire</td><td>Compilation of allelic polymorphisms and protein displays for variable (V), diversity (D), and joining (J) genes.</td></tr><tr><td><strong>Other Services</strong></td><td>IMGT/Therapeutic</td><td>Information on therapeutic antibodies and fusion proteins for immune applications.</td></tr><tr><td></td><td>IMGT/mAb-DB</td><td>Specific database for monoclonal antibodies (mAbs).</td></tr></tbody></table><h2 id="What-is-IMGT-V-QUEST-database">What is IMGT/V-QUEST database?</h2><p>IMGT/V-QUEST is a specialized database and analysis tool that is part of the broader IMGT¬Æ, the international ImMunoGeneTics information system¬Æ. This system is a high-quality integrated knowledge resource specializing in immunoglobulins (IG), T cell receptors (TCR), major histocompatibility complex (MHC) of all vertebrate species, and related proteins of the immune system (RPI) of any species.</p><p>IMGT/V-QUEST specifically provides detailed analysis of nucleotide sequences for immunoglobulins (IG) and T cell receptors (TCR). It is widely used in immunology and related research fields for tasks such as:</p><ol><li><p><strong>Sequence Analysis:</strong> It allows for the identification and delimitation of V, D, and J genes and alleles in the input sequences. This is crucial for understanding the genetic basis of the immune response.</p></li><li><p><strong>Clonotype Characterization:</strong> Researchers use it to characterize the clonotypes (unique T cell or B cell receptor sequences) in an individual‚Äôs immune repertoire, which is important in studies of immune system diversity and response.</p></li><li><p><strong>Somatic Hypermutation Studies:</strong> It helps in the analysis of somatic hypermutations, which are critical for understanding adaptive immunity and processes like affinity maturation.</p></li><li><p><strong>Comparative Immunology:</strong> By providing a comprehensive and curated database of IG and TCR sequences across different species, it aids in comparative immunology studies.</p></li></ol><p>As of my last update in April 2023, IMGT/V-QUEST continues to be a valuable resource for immunologists, molecular biologists, and other researchers studying the adaptive immune system. The database is regularly updated to include new findings and sequences, ensuring its relevance and usefulness in the field.</p><div class="admonition note"><p class="admonition-title">Something You May Want to Know</p><p>c16 &gt; g, Q6 &gt; E (++‚àí) means that the nt mutation (c &gt; g) leads to an AA change at codon 6 with the same hydropathy (+) and volume (+) but with different physicochemical properties (‚àí) classes ( 12 )[^Pommi√©_04].</p></div><h2 id="What-is-IMGT-Ontology">What is IMGT-Ontology?</h2><p>IMGT/Ontology is a key component of IMGT¬Æ, the international ImMunoGeneTics information system¬Æ. It represents the first ontology for immunogenetics and immunoinformatics and is a foundational aspect of the IMGT¬Æ information system. Developed by Marie-Paule Lefranc and her team, IMGT/Ontology provides a standardized vocabulary and a set of concepts that are essential for the consistent annotation, description, and comparison of the immune system‚Äôs components across different species. So, <strong>IMGT/Ontology is more of a set of criteria or a framework rather than a specific tool or software.</strong></p><p>Key features and aspects of IMGT/Ontology include:</p><details><summary>1. Standardized Vocabulary</summary>IMGT/Ontology provides a controlled vocabulary for the description of immunogenetic sequences. This includes terms for genes, alleles, and other genetic elements relevant to immunogenetics.</details><details><summary>2. Classification Criteria</summary>It establishes clear and standardized criteria for the classification of immunoglobulins (IG), T cell receptors (TCR), and major histocompatibility complex (MHC) molecules. This classification is crucial for the accurate comparison and analysis of these molecules.</details><details><summary>3. Identification of Key Concepts</summary>The ontology identifies and defines key concepts in immunogenetics, such as gene segment functionality, recombination features, and junctional diversity. These concepts are critical for understanding the adaptive immune response.</details><details><summary>4. IMGT-ONTOLOGY Axioms</summary>These are the rules that define the relations between the concepts and are used for the annotation and description of the IG, TCR, and MHC sequences in the IMGT¬Æ databases and tools.</details><details><summary>5. Facilitating Data Integration and Analysis</summary>By providing a common framework and language, IMGT/Ontology enables the integration, analysis, and sharing of immunogenetic data across different research projects and databases.</details><details><summary>6. Support for Research and Clinical Applications</summary>The standardized approach of IMGT/Ontology supports various research and clinical applications, including antibody engineering, vaccine development, and the study of immune responses.</details><br><div class="admonition note"><p class="admonition-title">IMGT-ONTOLOGY axioms and concepts</p></div><blockquote><p>Seven IMGT-ONTOLOGY axioms have been defined: ‚ÄòIDENTIFICATION‚Äô <sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>, ‚ÄòDESCRIPTION‚Äô <sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>, ‚ÄòCLASSIFICATION‚Äô <sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>, ‚ÄòNUMEROTATION‚Äô <sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup><sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>, ‚ÄòLOCALIZATION‚Äô, ‚ÄòORIENTATION‚Äô, and ‚ÄòOBTENTION‚Äô. They constitute the Formal IMGT-ONTOLOGY or IMGT-Kaleidoscope <sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>.</p></blockquote><table><thead><tr><th style="text-align:center"><img src="https://www.imgt.org/IMGTeducation/Tutorials/Ontology/UK/Figure3.png" alt="IMGT-ONTOLOGY"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.imgt.org/IMGTeducation/Tutorials/index.php?article=Ontology&amp;lang=UK&amp;nbr=all">¬© IMGT Education</a></td></tr></tbody></table><p>IMGT/Ontology is a critical resource for researchers and professionals in immunology, genetics, bioinformatics, and related fields. It ensures that data and analyses are consistent, reproducible, and interoperable, which is vital in advancing our understanding of the immune system and in developing immunotherapy and other medical applications.</p><p>The oldest paper about ontology:</p><ul><li>1999: <a href="https://academic.oup.com/bioinformatics/article/15/12/1047/248534">Ontology for immunogenetics: the IMGT-ONTOLOGY</a> <sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup><ul><li>The first ontology to be developed for immunogenetics. ‚ÄòAn ontology is a specification of conceptualization‚Äô (Gruber, 1993).</li><li>Covers four main concepts: ‚ÄòIDENTIFICATION‚Äô, ‚ÄòDESCRIPTION‚Äô, ‚ÄòCLASSIFICATION‚Äô and ‚ÄòOBTENTION‚Äô.</li></ul></li><li>2008: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0300908407002453">IMGT-Kaleidoscope, the formal IMGT-ONTOLOGY paradigm</a><sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup><ul><li>seven axioms, ‚ÄòIDENTIFICATION‚Äô, ‚ÄòDESCRIPTION‚Äô, ‚ÄòCLASSIFICATION‚Äô, ‚ÄòNUMEROTATION‚Äô, ‚ÄòLOCALIZATION‚Äô, ‚ÄòORIENTATION‚Äô and ‚ÄòOBTENTION‚Äô</li></ul></li></ul><h3 id="References">References</h3><ol><li><a href="https://www.imgt.org/IMGTindex/ontology.php">IMGT¬Æ: Ontology (IMGT-ONTOLOGY)</a></li><li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0300908407002453">Duroux P, Kaas Q, Brochet X, et al. IMGT-Kaleidoscope, the formal IMGT-ONTOLOGY paradigm[J]. Biochimie, 2008, 90(4): 570-583.</a></li><li><a href="https://www.frontiersin.org/articles/10.3389/fgene.2012.00079/full">Giudicelli V, Lefranc M P. Imgt-ontology 2012[J]. Frontiers in genetics, 2012, 3: 79.</a></li><li><a href="https://www.frontiersin.org/articles/10.3389/fimmu.2014.00022/full">Lefranc M P. Immunoglobulin and T cell receptor genes: IMGT¬Æ and the birth and rise of immunoinformatics[J]. Frontiers in immunology, 2014, 5: 22.</a></li></ol><h2 id="Other-References">Other References</h2><ol><li><a href="https://academic.oup.com/nar/article/36/suppl_2/W503/2506795?login=false">Brochet X, Lefranc M P, Giudicelli V. IMGT/V-QUEST: the highly customized and integrated system for IG and TR standardized VJ and VDJ sequence analysis[J]. Nucleic acids research, 2008, 36(suppl_2): W503-W508.</a></li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Lefranc, M.-P. From IMGT-ONTOLOGY IDENTIFICATION Axiom to IMGT Standardized Keywords: For Immunoglobulins (IG), T Cell Receptors (TR), and Conventional Genes. Cold Spring Harb Protoc., 1;2011(6): 604-613. pii: pdb.ip82. doi: 10.1101/pdb.ip82(2011) PMID:21632792. <a href="#fnref1" class="footnote-backref">‚Ü©Ô∏é</a></p></li><li id="fn2" class="footnote-item"><p>Lefranc, M.-P. From IMGT-ONTOLOGY DESCRIPTION Axiom to IMGT Standardized Labels: For Immunoglobulin (IG) and T Cell Receptor (TR) Sequences and Structures. Cold Spring Harb Protoc., 1;2011(6): 614-626. pii: pdb.ip83. doi: 10.1101/pdb.ip83 (2011) PMID:21632791. <a href="#fnref2" class="footnote-backref">‚Ü©Ô∏é</a></p></li><li id="fn3" class="footnote-item"><p>Lefranc, M.-P. From IMGT-ONTOLOGY CLASSIFICATION Axiom to IMGT Standardized Gene and Allele Nomenclature: For Immunoglobulins (IG) and T Cell Receptors (TR). Cold Spring Harb Protoc., 1;2011(6): 627-632. pii: pdb.ip84. doi: 10.1101/pdb.ip84 (2011) PMID:21632790. <a href="#fnref3" class="footnote-backref">‚Ü©Ô∏é</a></p></li><li id="fn4" class="footnote-item"><p>Lefranc, M.-P. ‚ÄúIMGT Collier de Perles for the Variable (V), Constant ¬©, and Groove (G) Domains of IG, TR, MH, IgSF, and MhSF‚Äù Cold Spring Harb Protoc. 2011 Jun 1;2011(6). pii: pdb.ip86. doi: 10.1101/pdb.ip86. PMID: 21632788 <a href="#fnref4" class="footnote-backref">‚Ü©Ô∏é</a></p></li><li id="fn5" class="footnote-item"><p>Lefranc, M.-P. IMGT Unique Numbering for the Variable (V), Constant ¬©, and Groove (G) Domains of IG, TR, MH, IgSF, and MhSF. Cold Spring Harb Protoc., 1;2011(6). pii: pdb.ip85. doi: 10.1101/pdb.ip85 (2011) PMID: 21632789 <a href="#fnref5" class="footnote-backref">‚Ü©Ô∏é</a></p></li><li id="fn6" class="footnote-item"><p>Duroux P et al., IMGT-Kaleidoscope, the Formal IMGT-ONTOLOGY paradigm. Biochimie, 90:570-83. Epub 2007 Sep 11 (2008) PMID:17949886 <a href="#fnref6" class="footnote-backref">‚Ü©Ô∏é</a></p></li><li id="fn7" class="footnote-item"><p>Giudicelli V, Lefranc M P. Ontology for immunogenetics: the IMGT-ONTOLOGY[J]. Bioinformatics, 1999, 15(12): 1047-1054. <a href="#fnref7" class="footnote-backref">‚Ü©Ô∏é</a></p></li><li id="fn8" class="footnote-item"><p>Duroux P, Kaas Q, Brochet X, et al. IMGT-Kaleidoscope, the formal IMGT-ONTOLOGY paradigm[J]. Biochimie, 2008, 90(4): 570-583. <a href="#fnref8" class="footnote-backref">‚Ü©Ô∏é</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">IMGT¬Æ: the international ImMunoGeneTics information system¬Æ</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Database" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Database/"/>
    
    
    <category term="Database" scheme="https://karobben.github.io/tags/Database/"/>
    
    <category term="Bioinformatic" scheme="https://karobben.github.io/tags/Bioinformatic/"/>
    
    <category term="Immunology" scheme="https://karobben.github.io/tags/Immunology/"/>
    
    <category term="Protein" scheme="https://karobben.github.io/tags/Protein/"/>
    
  </entry>
  
  <entry>
    <title>Immunoglobulin BLAST (Igblast), a Blast Tool Specific for Antibodies</title>
    <link href="https://karobben.github.io/2023/12/21/Bioinfor/igblast/"/>
    <id>https://karobben.github.io/2023/12/21/Bioinfor/igblast/</id>
    <published>2023-12-22T02:21:23.000Z</published>
    <updated>2023-12-26T01:34:43.074Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Key-Features-of-IgBLAST">Key Features of IgBLAST</h2><ol><li><p><strong>Identification of V(D)J segments</strong>: IgBLAST can identify variable (V), diversity (D), and joining (J) gene segments in IG or TCR sequences.</p></li><li><p><strong>Clonotype Analysis</strong>: It helps in determining clonotypes based on V(D)J segment usage, providing insights into the diversity and clonality of IG or TCR repertoires.</p></li><li><p><strong>Somatic Hypermutation Analysis</strong>: It identifies somatic hypermutations in IG sequences and can compare these to germline sequences, which is critical in understanding adaptive immune responses.</p></li><li><p><strong>Flexible Input Options</strong>: IgBLAST can process both nucleotide and protein sequences, and it supports various input formats.</p></li><li><p><strong>Detailed Alignment Information</strong>: It provides detailed alignment results that include information about gene segments, framework regions, complementarity-determining regions (CDRs), and mutations.</p></li><li><p><strong>Integration with Other Databases</strong>: The results can be linked to other NCBI databases for additional information and analysis.</p></li></ol><p>IgBLAST is widely used in immunology and related fields for studying B cell and T cell receptor repertoire, which is crucial for understanding immune responses, vaccine development, and in the study of autoimmune diseases and cancer.</p><h2 id="Local-Set-Up">Local Set Up</h2><ul><li>Basically, you can use the online service: <a href="https://www.ncbi.nlm.nih.gov/igblast/">NCBI igblast</a></li><li>Set up by following the official documentation: <a href="https://ncbi.github.io/igblast/cook/How-to-set-up.html">NCBI igblast set up</a></li></ul><p>Here is an example of set up by using conda from <a href="https://github.com/nicwulab/SARS-CoV-2_Abs#local-igblast-setup">nicwulab/SARS-CoV-2_Abs</a></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">conda create -n Abs -c bioconda \<br>  python=3.9 \<br>  igblast<br><br>conda activate Abs<br><span class="hljs-comment"># install pyir and use it to set up the blast database</span><br>pip3 install crowelab_pyir<br>pyir setup<br></code></pre></td></tr></table></figure></div><p>In the <code>pyir</code>, it is using ‚Äòhttp‚Äô and download the data failed. By following the error code, we could find the script and alter the ‚Äòhttp‚Äô to ‚Äòhttps‚Äô. It should solving the problem.</p><p>An example of running the program:</p><ol><li><p>prepare the DataBase</p></li><li><p>run the blast</p></li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">igblastn -query result/test.fasta \<br>  -germline_db_V imgt_database/human_nuc/IGV.fasta \<br>  -germline_db_J imgt_database/human_nuc/IGJ.fasta \<br>  -germline_db_D imgt_database/human_nuc/IGD.fasta \<br>  -organism human -domain_system kabat \<br>  -auxiliary_data imgt_database/optional_file/human_gl.aux \<br>  -out result/igblast_output<br></code></pre></td></tr></table></figure></div><pre>-germline_db_V <String> Germline database name-organism <String> The organism for your query sequence. Supported organisms include human, mouse, rat, rabbit and rhesus_monkey for Ig and human and mouse for TCR. Custom organism is also supported but you need to supply your own germline annotations (see IgBLAST web site for details) Default = `human'</pre><h2 id="pyir">pyir</h2><p>If you installed pyir, we could use the <a href="https://github.com/crowelab/PyIR">pyir</a> to do the igblast with less parameters.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">pyir -m 60  result/clean_split.fa --outfmt tsv -o result/clean<br></code></pre></td></tr></table></figure></div><p>Key parameters:</p><pre>--sequence_type {nucl,prot}     default: nucl-m MULTI, --multi MULTI         Number of threads-o,  --out                      default: inputfile.json.gz--outfmt {tsv,lsjson,dict,json} suggest: tsv--igdata IGDATA                 Path to your IGDATA directory.-r, --receptor {Ig,TCR}         The receptor you are analyzing, immunoglobulin or t cell receptor-s, --species {human,mouse...}  The Species you are analyzing {human,mouse,rabbit,rat,rhesus_monkey}</pre><h2 id="Q-A">Q&amp;A</h2><div class="admonition question"><p class="admonition-title">Can I annotate the light chain and heavy chain simultaneously?</p><p>IgBLAST is designed to analyze immunoglobulin (IG) sequences, including both heavy and light chains. However, it typically processes and analyzes these chains separately. When you input a sequence that contains both heavy and light chains, IgBLAST might only process the first recognizable sequence, which in your case appears to be the heavy chain.</p><p>To analyze both heavy and light chains using IgBLAST, you generally need to input them as separate sequences. This means splitting your combined sequence into two parts - one for the heavy chain and the other for the light chain - and then running IgBLAST for each part individually.</p><p>There isn't a parameter in IgBLAST that allows for the simultaneous analysis of both heavy and light chains when they are combined into a single sequence. The tool's algorithm is designed to identify and annotate the V(D)J segments of a single chain at a time, as the structure and sequence features of heavy and light chains are distinct.</p><p>If you are consistently working with sequences that contain both chains, you may need to develop a preprocessing step in your workflow to separate these chains before analysis. Alternatively, if such a tool is essential for your work, you might need to look into other bioinformatics tools or custom scripting to first identify and separate the heavy and light chain sequences before feeding them into IgBLAST.</p></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Igblast, a Blast Tool Specific for Antibodies</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/"/>
    
    <category term="Align" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/Align/"/>
    
    
    <category term="Software" scheme="https://karobben.github.io/tags/Software/"/>
    
    <category term="Database" scheme="https://karobben.github.io/tags/Database/"/>
    
    <category term="Align" scheme="https://karobben.github.io/tags/Align/"/>
    
  </entry>
  
  <entry>
    <title>scVDJ-Seq Pipeline (CellRanger)</title>
    <link href="https://karobben.github.io/2023/12/21/Bioinfor/scvdj-seq/"/>
    <id>https://karobben.github.io/2023/12/21/Bioinfor/scvdj-seq/</id>
    <published>2023-12-21T20:52:46.000Z</published>
    <updated>2023-12-21T21:14:25.261Z</updated>
    
    <content type="html"><![CDATA[<h2 id="scVDJ-Seq">scVDJ-Seq</h2><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/PxRTphj.png" alt="scVDJ-Seq"></th></tr></thead><tbody><tr><td style="text-align:center">V(D)J Library Construction</td></tr></tbody></table><ol><li><p><font style="background-color: red">V</font> (Variable): These are gene segments that code for the variable region of an antibody or T-cell receptor. The variable region is responsible for binding to antigens.</p></li><li><p><font style="background-color: gold">D</font> (Diversity): These segments are found in some classes of antibodies and in T-cell receptors. They provide an additional level of diversity to the antigen-binding region.</p></li><li><p><font style="background-color: green">J</font> (Joining): These gene segments join with the V (and D, where present) segments to complete the variable region of the receptor.</p></li><li><p><font style="background-color: royalblue">C</font> (Constant): The constant region of the antibody or T-cell receptor is encoded by these segments. This region does not vary much between different antibodies and is responsible for the effector functions of the antibody, such as recruiting other parts of the immune system.</p></li></ol><h2 id="Pipeline">Pipeline</h2><table><thead><tr><th style="text-align:center"><img src="https://cdn.10xgenomics.com/image/upload/v1654273213/software-support/vdj/algorithms/algorithm-workflow.png" alt="Cell Ranger's V(D)J Algorithm"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.10xgenomics.com/cn/support/software/cell-ranger/algorithms-overview/cr-5p-vdj-algorithm">¬© 10X Genomics</a></td></tr></tbody></table><h2 id="Install-CellRanger">Install CellRanger</h2><p>Click the <a href="https://www.10xgenomics.com/support/software/cell-ranger/downloads">Link</a> and fill out the information and you could get the download page</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># download the software (expired)</span><br>curl -o cellranger-7.2.0.tar.gz <span class="hljs-string">&quot;https://cf.10xgenomics.com/releases/cell-exp/cellranger-7.2.0.tar.gz?Expires=1703232056&amp;Key-Pair-Id=APKAI7S6A5RYOXBWRPDA&amp;Signature=hm5oQoPrhuNznBtqCREVSaH34WF-Fute6XHYRDUIvIsajK~sFKYuonBEUQsxRJ1oKmxuuAhmtJg3N-mEQU2dr223oXTTr9e70gFlx9-3qgR7cvAhbZXMMGPMhOVVixoEF2GaE1~x0LA4KLXG3xu4mDGsBn4u870Ql~~OfhYBF5bHcqV6hf8X-YPXNG8TbRZMe-dqcogRTPYpeOpfKBtvPs63CDJ3YgC2Bahci4jYuo2v7MZDTR018C~X-3qwgRMIPKCMZFInEjpkfds34TJ0yP3uwAprvpR~S3WCngKKzSmAQszkDMqSB2eXZw6~FF~6oFIIYV~-DmPV~a7DO416nQ__&quot;</span><br><br><span class="hljs-comment"># decompress and install</span><br>tar -zxvf cellranger-7.2.0.tar.gz<br><br><span class="hljs-comment"># add this directory in your path</span><br><span class="hljs-built_in">export</span> PATH=$(<span class="hljs-built_in">pwd</span>):<span class="hljs-variable">$PATH</span><br><span class="hljs-comment"># You may wish to add this command to your ~/.bashrc or ~/.zshrc for convenience.</span><br><span class="hljs-comment"># for get the full command, you can run `echo export PATH=$(pwd):\$PATH` and add the print out result at the end of the ~/.bashrc or ~/.zshrc</span><br></code></pre></td></tr></table></figure></div><div class="admonition note"><p class="admonition-title">Reference</p><p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Download the reference</span><br><span class="hljs-comment">## Human reference (GRCh38) - 2020-A</span><br>curl -O <span class="hljs-string">&quot;https://cf.10xgenomics.com/supp/cell-exp/refdata-gex-GRCh38-2020-A.tar.gz&quot;</span><br><span class="hljs-comment">## Mouse reference (mm10) - 2020-A</span><br>curl -O <span class="hljs-string">&quot;https://cf.10xgenomics.com/supp/cell-exp/refdata-gex-mm10-2020-A.tar.gz&quot;</span><br><span class="hljs-comment">## Human (GRCh38) and mouse (mm10) reference - 2020-A</span><br>curl -O <span class="hljs-string">&quot;https://cf.10xgenomics.com/supp/cell-exp/refdata-gex-GRCh38-and-mm10-2020-A.tar.gz&quot;</span><br><br><span class="hljs-comment">## Human V(D)J reference (GRCh38)</span><br>curl -O <span class="hljs-string">&quot;https://cf.10xgenomics.com/supp/cell-vdj/refdata-cellranger-vdj-GRCh38-alts-ensembl-7.1.0.tar.gz&quot;</span><br><span class="hljs-comment">## Mouse V(D)J reference (GRCm38)</span><br>curl -O <span class="hljs-string">&quot;https://cf.10xgenomics.com/supp/cell-vdj/refdata-cellranger-vdj-GRCm38-alts-ensembl-7.0.0.tar.gz&quot;</span><br></code></pre></td></tr></table></figure></div></p></div><h3 id="Run">Run</h3><p>Documentation: <a href="https://www.10xgenomics.com/cn/support/software/cell-ranger/algorithms-overview/cr-5p-vdj-algorithm">10X Genomics</a></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">cellranger vdj --id=sample345 \<br>         --reference=/opt/refdata-cellranger-vdj-GRCh38-alts-ensembl-7.1.0 \<br>         --fastqs=/home/jdoe/runs/HAWT7ADXX/outs/fastq_path \<br>         --sample=mysample \<br>         --localcores=8 \<br>         --localmem=64<br></code></pre></td></tr></table></figure></div><ul><li><strong>Command and Arguments</strong>:</li></ul><pre>cellranger vdj:     This is the main command being run. `cellranger` is the software package, and `vdj` specifies that you are running the V(D)J analysis pipeline, which is used for assembling and annotating V(D)J sequences from single-cell RNA-Seq data.--id=sample345:     This sets the unique identifier for the run. Here, the identifier is `sample345`. This ID is used to name the output directory.--reference=...:    This specifies the reference dataset to be used for the analysis. The provided path (`/opt/refdata-cellranger-vdj-GRCh38-alts-ensembl-7.1.0`) points to a reference dataset for human V(D)J sequences.--fastqs=...:       This indicates the directory where the FASTQ files are located. FASTQ files are the input files for the Cell Ranger software, containing the sequenced reads.--sample=mysample:  This specifies the name of the sample to be analyzed. It should match the sample name in the FASTQ files.--localcores=8:     This parameter tells Cell Ranger to use 8 CPU cores for the computation. This setting helps to optimize the use of available computational resources.--localmem=64:      This allocates 64 GB of memory (RAM) for the run. This parameter is crucial for ensuring the software has enough memory to process the data without crashing.</pre><h2 id="When-the-Job-is-Done">When the Job is Done</h2><p>A successful <code>cellranger vdj</code> run should conclude with a message similar to this:</p><pre>Outputs:- Run summary HTML:                                 /home/jdoe/runs/sample345/outs/web_summary.html- Run summary CSV:                                  /home/jdoe/runs/sample345/outs/metrics_summary.csv- Clonotype info:                                   /home/jdoe/runs/sample345/outs/clonotypes.csv- Filtered contig sequences FASTA:                  /home/jdoe/runs/sample345/outs/filtered_contig.fasta- Filtered contig sequences FASTQ:                  /home/jdoe/runs/sample345/outs/filtered_contig.fastq- Filtered contigs (CSV):                           /home/jdoe/runs/sample345/outs/filtered_contig_annotations.csv- All-contig FASTA:                                 /home/jdoe/runs/sample345/outs/all_contig.fasta- All-contig FASTA index:                           /home/jdoe/runs/sample345/outs/all_contig.fasta.fai- All-contig FASTQ:                                 /home/jdoe/runs/sample345/outs/all_contig.fastq- Read-contig alignments:                           /home/jdoe/runs/sample345/outs/all_contig.bam- Read-contig alignment index:                      /home/jdoe/runs/sample345/outs/all_contig.bam.bai- All contig annotations (JSON):                    /home/jdoe/runs/sample345/outs/all_contig_annotations.json- All contig annotations (BED):                     /home/jdoe/runs/sample345/outs/all_contig_annotations.bed- All contig annotations (CSV):                     /home/jdoe/runs/sample345/outs/all_contig_annotations.csv- Barcodes that are declared to be targetted cells: /home/jdoe/runs/sample345/outs/cell_barcodes.json- Clonotype consensus FASTA:                        /home/jdoe/runs/sample345/outs/consensus.fasta- Clonotype consensus FASTA index:                  /home/jdoe/runs/sample345/outs/consensus.fasta.fai- Contig-consensus alignments:                      /home/jdoe/runs/sample345/outs/consensus.bam- Contig-consensus alignment index:                 /home/jdoe/runs/sample345/outs/consensus.bam.bai- Clonotype consensus annotations (CSV):            /home/jdoe/runs/sample345/outs/consensus_annotations.csv- Concatenated reference sequences:                 /home/jdoe/runs/sample345/outs/concat_ref.fasta- Concatenated reference index:                     /home/jdoe/runs/sample345/outs/concat_ref.fasta.fai- Contig-reference alignments:                      /home/jdoe/runs/sample345/outs/concat_ref.bam- Contig-reference alignment index:                 /home/jdoe/runs/sample345/outs/concat_ref.bam.bai- Loupe V(D)J Browser file:                         /home/jdoe/runs/sample345/outs/vloupe.vloupe- V(D)J reference:fasta:regions:       /home/jdoe/runs/sample345/outs/vdj_reference/fasta/regions.fadonor_regions: /home/jdoe/runs/sample345/outs/vdj_reference/fasta/donor_regions.fareference: /home/jdoe/runs/sample345/outs/vdj_reference/reference.json- AIRR Rearrangement TSV:                           /home/jdoe/runs/sample345/outs/airr_rearrangement.tsv- All contig info (ProtoBuf format):                /home/jdoe/runs/sample345/outs/vdj_contig_info.pbWaiting 6 seconds for UI to do final refresh.Pipestance completed successfully!</pre><p>Once <code>cellranger vdj</code> has successfully completed, you can browse the resulting summary HTML file in any supported web browser, open the <code>.vloupe</code> file in Loupe V(D)J Browser, or refer to the Understanding Output section to explore the data by hand.</p><h2 id="Trouble-Shoot">Trouble Shoot</h2><h3 id="Too-Low-to-Meet-the-Required-Threshold">Too Low to Meet the Required Threshold</h3><pre>[error] Pipestance failed. Error log at:MockC_cs/SC_VDJ_ASSEMBLER_CS/SC_MULTI_CORE/MULTI_CHEMISTRY_DETECTOR/VDJ_CHEMISTRY_DETECTOR/DETECT_VDJ_RECEPTOR/fork0/chnk0-u22ea849f77/_errorsLog message:V(D)J Chain detection failed for Sample VDJ-B-293-redo-1 in "/raid/home/wenkanl2/MokC_sc/1_primary_seq".Total Reads          = 1000000Reads mapped to TR   = 30Reads mapped to IG   = 28665In order to distinguish between the TR and the IG chain the following conditions need to be satisfied:- A minimum of 10000 total reads- A minimum of 5.0% of the total reads needs to map to TR or IG- The number of reads mapped to TR should be at least 3.0x compared to the number of reads mapped to IG or vice versaPlease check the input data and/or specify the chain via the --chain argument.</pre><p>The problem here is with the proportion of reads mapping to TR and IG. Even though you have a significant number of reads mapped to IG, the number of reads mapped to TR is <mark>too low to meet the required thresholds</mark>.</p><div class="admonition note"><p class="admonition-title">Resolution:</p><p>The message suggests checking the input data or specifying the chain via the --chain argument. Explicitly specify whether you are analyzing T-cell receptors (TR) or Immunoglobulins (IG) by using the --chain flag in your Cell Ranger command.For example, assume that it is B cell data, we could add <code>--chain IG</code> to solve this problem</p></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">scVDJ-Seq Pipeline</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Single Cell" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Single-Cell/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="RNA-Seq" scheme="https://karobben.github.io/tags/RNA-Seq/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="scRNA-Seq" scheme="https://karobben.github.io/tags/scRNA-Seq/"/>
    
  </entry>
  
  <entry>
    <title>Secure Shell (SSH)</title>
    <link href="https://karobben.github.io/2023/12/20/Linux/ssh/"/>
    <id>https://karobben.github.io/2023/12/20/Linux/ssh/</id>
    <published>2023-12-20T17:59:14.000Z</published>
    <updated>2024-01-14T08:37:53.151Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SSH">SSH</h2><p>SSH, short for Secure Shell, is a network protocol used to securely access and manage a computer over an unsecured network. It provides a secure channel over an unencrypted network, like the internet, allowing users to log into another computer, execute commands remotely, and move files. SSH uses strong encryption to protect the data being transmitted, ensuring confidentiality and integrity of the data against eavesdropping and interception. It‚Äôs commonly used by system administrators and IT professionals for managing systems and applications remotely.</p><h2 id="How-to-Use-It">How to Use It</h2><p>Using SSH typically involves two primary components: an SSH client and an SSH server. The server runs on the machine you want to connect to, while the client runs on the machine you‚Äôre connecting from. Here‚Äôs a basic guide on how to use SSH:</p><h3 id="Setting-Up-an-SSH-Server">Setting Up an SSH Server</h3><ol><li><p><strong>Install SSH Server</strong>: On the remote machine (the one you want to access), you need to install an SSH server. For Linux systems, this is often done using the <code>openssh-server</code> package.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">sudo apt update<br>sudo apt install openssh-server<br></code></pre></td></tr></table></figure></div><p>This example is for Debian-based systems (like Ubuntu). The commands might vary for other systems.</p></li><li><p><strong>Start and Enable SSH Service</strong>: Ensure that the SSH service is started and enabled to start on boot.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">sudo systemctl start ssh<br>sudo systemctl <span class="hljs-built_in">enable</span> ssh<br></code></pre></td></tr></table></figure></div></li><li><p><strong>Configure SSH Server (Optional)</strong>: You can configure your SSH server by editing the <code>/etc/ssh/sshd_config</code> file. This step is optional and typically only necessary for advanced configurations.</p></li></ol><h3 id="Connecting-Using-an-SSH-Client">Connecting Using an SSH Client</h3><ol><li><p><strong>Install SSH Client</strong>: Most Unix-like systems (Linux, macOS) come with an SSH client pre-installed. For Windows, you can use clients like PuTTY or use the built-in SSH client in Windows 10/11.</p></li><li><p><strong>Establish an SSH Connection</strong>: To connect to the SSH server, you need the IP address or hostname of the server and the username on that system. The basic command is:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">ssh [username]@[host]<br></code></pre></td></tr></table></figure></div><p>For example, if your username is <code>user</code> and the server‚Äôs IP address is <code>192.168.1.100</code>, you would use:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">ssh user@192.168.1.100<br></code></pre></td></tr></table></figure></div></li><li><p><strong>Authenticate</strong>: When you connect for the first time, you‚Äôll be asked to verify the identity of the server. After accepting, you‚Äôll be prompted for the password of the user account you are logging into on the remote machine.</p></li><li><p><strong>Using SSH</strong>: Once connected, you can execute commands on the remote machine as if you were physically present.</p></li><li><p><strong>Transferring Files (Optional)</strong>: SSH also allows for secure file transfer using SCP or SFTP.</p><ul><li>To copy a file from your local machine to the remote machine:<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">scp /path/to/<span class="hljs-built_in">local</span>/file user@192.168.1.100:/path/to/remote/directory<br></code></pre></td></tr></table></figure></div></li><li>To copy a file from the remote machine to your local machine:<div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">scp user@192.168.1.100:/path/to/remote/file /path/to/<span class="hljs-built_in">local</span>/directory<br></code></pre></td></tr></table></figure></div></li></ul></li><li><p><strong>Exiting SSH</strong>: To end your SSH session, simply type <code>exit</code> or press <code>Ctrl+D</code>.</p></li></ol><h2 id="Configure-file">Configure file</h2><p><code>vim ~/.ssh/config</code></p><pre>Host home_pc    HostName 192.168.3.1    User john    Port 2322</pre><p>How to login this host:<br><code>ssh home_pc</code></p><h3 id="Security-Considerations">Security Considerations</h3><ul><li><p><strong>SSH Keys</strong>: For better security, it‚Äôs recommended to use SSH keys instead of passwords. SSH keys are a pair of cryptographic keys that can be used to authenticate to an SSH server as an alternative to password-based logins.</p></li><li><p><strong>Firewall Settings</strong>: Make sure your firewall settings allow SSH connections (usually on port 22).</p></li><li><p><strong>Regular Updates</strong>: Keep the SSH server software up to date for security.</p></li></ul><p>SSH is a powerful tool for remote administration, but it‚Äôs important to use it securely to protect your systems and data.</p><h2 id="SSH-Key">SSH Key</h2><p>Generating an SSH key is a security practice for authenticating to an SSH server more securely than using just a password. Here‚Äôs why you should do it and how to generate an SSH key:</p><h3 id="Why-Use-SSH-Keys">Why Use SSH Keys?</h3><ol><li><p><strong>Enhanced Security</strong>: SSH keys are cryptographic keys that are much more secure than passwords. They are almost impossible to decipher using brute force methods.</p></li><li><p><strong>No Need for Passwords</strong>: When you use SSH keys, you don‚Äôt need to enter your password every time you connect, which reduces the risk of password theft.</p></li><li><p><strong>Automation Friendly</strong>: SSH keys are ideal for automated processes. Scripts and applications can authenticate without manual password entry.</p></li><li><p><strong>Access Control</strong>: SSH keys can be used to control who can access a server. Only users with the matching private key can access the server configured with the public key.</p></li></ol><h3 id="How-to-Generate-an-SSH-Key">How to Generate an SSH Key</h3><h4 id="On-Linux-or-macOS">On Linux or macOS:</h4><ol><li><p><strong>Open Terminal</strong>: Launch the terminal application.</p></li><li><p><strong>Generate Key Pair</strong>: Use the <code>ssh-keygen</code> command to generate a new SSH key pair.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">ssh-keygen -t rsa -b 4096<br></code></pre></td></tr></table></figure></div><p>This command creates a new SSH key using the RSA algorithm with a key size of 4096 bits, providing a good balance between security and compatibility. You can choose other algorithms like <code>ed25519</code> which is considered more secure but may not be compatible with older systems.</p></li><li><p><strong>Specify File to Save the Key</strong>: By default, <code>ssh-keygen</code> will save the key in the <code>~/.ssh/id_rsa</code> file. You can specify a different file if you want.</p></li><li><p><strong>Enter a Passphrase (Optional)</strong>: For additional security, you can enter a passphrase when prompted. This passphrase will be required whenever the private key is used.</p></li></ol><h3 id="Adding-the-SSH-Key-to-Your-SSH-Server">Adding the SSH Key to Your SSH Server</h3><ol><li><p><strong>Copy Public Key</strong>: After generating your SSH key, you need to add the public key to the <code>~/.ssh/authorized_keys</code> file on your SSH server.</p></li><li><p><strong>Use <code>ssh-copy-id</code> on Linux/macOS</strong>: If you‚Äôre using Linux or macOS, you can use <code>ssh-copy-id</code> to copy your public key to the server easily.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">ssh-copy-id user@server-address<br></code></pre></td></tr></table></figure></div></li><li><p><strong>Manual Copying</strong>: If <code>ssh-copy-id</code> isn‚Äôt available or you‚Äôre using Windows, you can manually copy the public key text and append it to <code>~/.ssh/authorized_keys</code> on the server.</p></li></ol><p>Remember, never share your private key. The public key is what you distribute or add to servers, while the private key should be securely stored and kept private.</p><h2 id="Trouble-Shooting-Public-Key-Doesn‚Äôt-Work">Trouble Shooting: Public Key Doesn‚Äôt Work</h2><p>If your SSH public key authentication isn‚Äôt working, there could be several reasons. Here‚Äôs a troubleshooting guide to help you resolve common issues:</p><h3 id="1-Check-File-Permissions">1. Check File Permissions</h3><ul><li><strong>On the Server</strong>: SSH is particular about file permissions for the <code>~/.ssh</code> directory and the <code>authorized_keys</code> file. Incorrect permissions can prevent SSH from authenticating using keys.<ul><li>The <code>~/.ssh</code> directory should have permissions set to <code>700</code> (drwx------).</li><li>The <code>authorized_keys</code> file should have permissions set to <code>600</code> (-rw-------).</li><li>Use the <code>chmod</code> command to set these permissions:</li></ul></li></ul><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">chmod 700 ~/.ssh<br>chmod 600 ~/.ssh/authorized_keys<br></code></pre></td></tr></table></figure></div><h3 id="2-Ensure-Correct-Ownership">2. Ensure Correct Ownership</h3><p>The <code>.ssh</code> directory and the <code>authorized_keys</code> file should be owned by the user, not root or any other user. Use the <code>chown</code> command to set the ownership:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">chown -R your_username:your_username ~/.ssh<br></code></pre></td></tr></table></figure></div><p>You can also make sure that the home directory permissions are restricted to the user. I think it could do the same thing.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">chmod -R u+rwX,go-rwx /home/your_username<br></code></pre></td></tr></table></figure></div><h3 id="3-Verify-the-Public-Key-Format">3. Verify the Public Key Format</h3><p>Ensure that the public key in <code>authorized_keys</code> is in the correct format. It should be a single line starting with <code>ssh-rsa</code> or <code>ssh-ed25519</code>, followed by the key, and optionally a comment.</p><h3 id="4-Check-the-SSH-Server-Configuration">4. Check the SSH Server Configuration</h3><p>The SSH server configuration file (<code>/etc/ssh/sshd_config</code>) on the server might restrict public key authentication. Check the following settings:</p><ul><li><code>PubkeyAuthentication yes</code> should be set to allow public key authentication.</li><li><code>AuthorizedKeysFile</code> should point to the correct path, typically <code>.ssh/authorized_keys</code>.</li><li>If <code>PasswordAuthentication</code> is set to <code>no</code>, the server will not fall back to password authentication if key authentication fails.</li><li>After making changes, restart the SSH service:</li></ul><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">sudo systemctl restart ssh<br></code></pre></td></tr></table></figure></div><h3 id="5-Check-SSH-Client-Configuration">5. Check SSH Client Configuration</h3><p>On your client machine, ensure you‚Äôre specifying the correct private key. If you‚Äôre using a key with a non-default name or location, specify it with the <code>-i</code> option:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">ssh -i /path/to/private_key user@server<br></code></pre></td></tr></table></figure></div><h3 id="6-Look-at-Server-Logs">6. Look at Server Logs</h3><p>SSH server logs can provide details on why the key authentication is failing. Check the logs for relevant error messages:</p><ul><li>On most Linux systems, SSH logs are located in <code>/var/log/auth.log</code> or <code>/var/log/secure</code>.</li></ul><h3 id="7-Validate-Key-Pair">7. Validate Key Pair</h3><p>Ensure that the public and private keys are a matching pair. If you have regenerated or changed keys, make sure the server has the corresponding public key.</p><h3 id="8-Check-Passphrase">8. Check Passphrase</h3><p>If your private key is protected with a passphrase, ensure you‚Äôre entering the correct passphrase when prompted.</p><h3 id="9-Network-Issues">9. Network Issues</h3><p>Confirm there are no network issues preventing SSH access. Firewall settings on either the client or server side can block SSH connections.</p><h3 id="10-Client-Side-Debugging">10. Client-Side Debugging</h3><p>Use SSH with the <code>-vvv</code> option for verbose output, which can give more insights:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">ssh -vvv -i /path/to/private_key user@server<br></code></pre></td></tr></table></figure></div><p>This will provide detailed debug information about each step of the SSH connection process, potentially highlighting where the issue lies.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Secure Shell (SSH) installation, usage, and trouble shooting</summary>
    
    
    
    <category term="Linux" scheme="https://karobben.github.io/categories/Linux/"/>
    
    <category term="System" scheme="https://karobben.github.io/categories/Linux/System/"/>
    
    
    <category term="Linux" scheme="https://karobben.github.io/tags/Linux/"/>
    
    <category term="bash" scheme="https://karobben.github.io/tags/bash/"/>
    
    <category term="CLI Tools" scheme="https://karobben.github.io/tags/CLI-Tools/"/>
    
  </entry>
  
</feed>
