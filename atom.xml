<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Karobben</title>
  
  <subtitle>Engjoy~</subtitle>
  <link href="https://karobben.github.io/atom.xml" rel="self"/>
  
  <link href="https://karobben.github.io/"/>
  <updated>2024-12-11T06:00:50.121Z</updated>
  <id>https://karobben.github.io/</id>
  
  <author>
    <name>Karobben</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>OpenMM, Molecular Dynamic Simulation</title>
    <link href="https://karobben.github.io/2024/12/08/Bioinfor/openMM/"/>
    <id>https://karobben.github.io/2024/12/08/Bioinfor/openMM/</id>
    <published>2024-12-08T17:48:03.000Z</published>
    <updated>2024-12-11T06:00:50.121Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Install">Install</h2><p>More detailed installations: <a href="http://docs.openmm.org/latest/userguide/application/01_getting_started.html">OpenMM User Guide</a></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">conda create -n openmm python=3.9 -y<br>conda activate openmm<br>conda install -c conda-forge openmm<br></code></pre></td></tr></table></figure></div><p><strong>Test the installation</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">python -m openmm.testInstallation<br></code></pre></td></tr></table></figure></div><pre>OpenMM Version: 8.2Git Revision: 53770948682c40bd460b39830d4e0f0fd3a4b868There are 4 Platforms available:1 Reference - Successfully computed forces2 CPU - Successfully computed forces3 CUDA - Successfully computed forces1 warning generated.1 warning generated.4 OpenCL - Successfully computed forcesMedian difference in forces between platforms:Reference vs. CPU: 6.29538e-06Reference vs. CUDA: 6.75176e-06CPU vs. CUDA: 7.49106e-07Reference vs. OpenCL: 6.75018e-06CPU vs. OpenCL: 7.64529e-07CUDA vs. OpenCL: 1.757e-07All differences are within tolerance.</pre><h2 id="Use-the-GPU-in-Simulation">Use the GPU in Simulation</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> openmm <span class="hljs-keyword">import</span> Platform<br><br><span class="hljs-comment"># define the platform</span><br>platform = Platform.getPlatformByName(<span class="hljs-string">&#x27;CUDA&#x27;</span>)<br>properties = &#123;<span class="hljs-string">&#x27;DeviceIndex&#x27;</span>: <span class="hljs-string">&#x27;1&#x27;</span>, <span class="hljs-string">&#x27;Precision&#x27;</span>: <span class="hljs-string">&#x27;mixed&#x27;</span>&#125; <br><br><span class="hljs-comment"># add the parameter in simulation</span><br>simulation = app.Simulation(pdb.topology, system, integrator, platform, properties)<br></code></pre></td></tr></table></figure></div><p>To be notice: Evene though, you implied that the GPU parameter in the code, it still heavily relies on the CPU.</p><h2 id="PDB-fix">PDB fix</h2><p>Before you run the simulation, you may need to fix the PDB first.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> openmm <span class="hljs-keyword">import</span> app<br><span class="hljs-keyword">from</span> openmm.unit <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> pdbfixer <span class="hljs-keyword">import</span> PDBFixer<br><span class="hljs-keyword">from</span> openmm.app <span class="hljs-keyword">import</span> PDBFile<br><br><br><span class="hljs-comment"># repair the PDB</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">PDB_fix</span>(<span class="hljs-params">INPUT, OUPUT</span>):</span><br>    fixer = PDBFixer(filename=INPUT)<br>    fixer.findMissingResidues()<br>    fixer.findNonstandardResidues()<br>    fixer.replaceNonstandardResidues()<br>    fixer.findMissingAtoms()<br>    fixer.addMissingAtoms()<br>    fixer.addMissingHydrogens(<span class="hljs-number">7.0</span>)<br>    <span class="hljs-comment">#fixer.addSolvent(fixer.topology.getUnitCellDimensions())</span><br>    <span class="hljs-comment"># Remove problematic water molecules and add correct TIP3P water</span><br>    fixer.removeHeterogens(keepWater=<span class="hljs-literal">True</span>)<br>    PDBFile.writeFile(fixer.topology, fixer.positions, <span class="hljs-built_in">open</span>(OUPUT, <span class="hljs-string">&#x27;w&#x27;</span>))<br><br>PDB_fix(<span class="hljs-string">&#x27;best.pdb&#x27;</span>, <span class="hljs-string">&#x27;best_fix.pdb&#x27;</span>)<br></code></pre></td></tr></table></figure></div><h2 id="Anchor-a-protein">Anchor a protein</h2><p><mark>Code was not tested because my protein has hydrophobic surface and it would crush in the water environment</mark></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Anchor protein 1 by restraining its atoms</span><br>anchor_force = mm.CustomExternalForce(<span class="hljs-string">&#x27;0.5 * k * (x^2 + y^2 + z^2)&#x27;</span>)<br>anchor_force.addPerParticleParameter(<span class="hljs-string">&#x27;k&#x27;</span>)<br><br><span class="hljs-comment"># Add position restraints to protein 1</span><br><span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> pdb.topology.atoms():<br>    <span class="hljs-keyword">if</span> atom.residue.chain.<span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>]:  <span class="hljs-comment"># Assume protein 1 is in chain A</span><br>        anchor_force.addParticle(atom.index, [<span class="hljs-number">1000</span>])  <span class="hljs-comment"># High force constant</span><br><br>system.addForce(anchor_force)<br></code></pre></td></tr></table></figure></div><h2 id="Pull-Protein-Force-Apply">Pull Protein Force Apply</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Apply a pulling force to protein 2</span><br>pulling_force = mm.CustomExternalForce(<span class="hljs-string">&#x27;k_pull * (x - x0)^2&#x27;</span>)<br>pulling_force.addPerParticleParameter(<span class="hljs-string">&#x27;k_pull&#x27;</span>)<br>pulling_force.addPerParticleParameter(<span class="hljs-string">&#x27;x0&#x27;</span>)<br><br><span class="hljs-comment"># Add pulling force to atoms of protein 2 (e.g., chain B)</span><br><span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> pdb.topology.atoms():<br>    <span class="hljs-keyword">if</span> atom.residue.chain.<span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>]:  <span class="hljs-comment"># Assume protein 2 is in chain B</span><br>        pulling_force.addParticle(atom.index, [-<span class="hljs-number">1</span>, <span class="hljs-number">1.0</span>])  <span class="hljs-comment"># Adjust constants</span><br><br>system.addForce(pulling_force)<br></code></pre></td></tr></table></figure></div><h2 id="Change-Record">Change Record</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">simulation.reporters.append(app.StateDataReporter(<br>    <span class="hljs-string">&#x27;best_fix_sld_tr.csv&#x27;</span>, <span class="hljs-number">10</span>, step=<span class="hljs-literal">True</span>, potentialEnergy=<span class="hljs-literal">True</span>, temperature=<span class="hljs-literal">True</span>))<br>simulation.reporters.append(app.PDBReporter(<span class="hljs-string">&#x27;best_fix_sld_tr.pdb&#x27;</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure></div><h2 id="Save-the-structure-as-cif">Save the structure as cif</h2><p>During the simulation, you may wants to add lots of wather molecular. When the number of molecular over than 100,000, pdb format can’t handle it anymore. So you want to save it as <code>cif</code> format.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;best_fix_sld.cif&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    app.PDBxFile.writeFile(modeller.topology, modeller.positions, f)<br></code></pre></td></tr></table></figure></div><h2 id="Save-the-structure-from-the-simulation">Save the structure from the simulation</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># After finishing your MD steps:</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;final_structure.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br></code></pre></td></tr></table></figure></div><h2 id="In-Action-Protein-in-Water">In Action: Protein in Water</h2><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/JnRSG6l.gif" alt=""></td><td style="text-align:left">With the help of openMM toolkit, you can simulate the protein in water easily. Here is the code from the <a href="https://openmm.github.io/openmm-cookbook/latest/notebooks/tutorials/protein_in_water.html">document</a>.</td></tr></tbody></table><p>In the simulation, the main codes is explained:</p><ol><li>read the pdb file with <code>PDBFile</code></li><li>Specify the force filed.</li><li>Clean water and add water as a period box. In this step, you can add the water in the force filed based on the size of the filed. Our you can test small filed. The filed is a period box (replicated infinitely in all directions) which means when the object moves to the end of one side, it will not run out of the box, but coming back from <strong>against face</strong>.</li><li>Setup the integrator:<ul><li><code>forcefield.createSystem</code>:<ul><li><code>modeller.topology</code>: you’ll add you molecular (protein)</li><li><code>nonbondedMethod=PME</code>: <font title='ChatGPT o1' color=gray>specifies how long-range electrostatic interactions. Simply cutting them off at a certain radius can introduce errors. <strong>PME</strong> (Particle Mesh Ewald) uses a combination of direct space calculations (for short distances) and reciprocal space calculations (using fast Fourier transforms) to accurately handle these interactions.</font></li><li><code>nonbondedCutoff=1.0*nanometer</code>: <font title='ChatGPT o1' color=gray>When using a cutoff-based approach (like nonbondedCutoff=1.0*nanometer), the simulation engine directly calculates the vdW (Lennard-Jones) interactions only between pairs of atoms that are within that 1 nm cutoff distance. If two atoms are farther apart than 1 nm, their vdW interactions are not explicitly computed.</font></li></ul></li><li><code>integrator</code>: This is the place to given the value of the Tm and time scale.</li></ul></li><li><code>simulation.minimizeEnergy()</code>: Start to minimize local Energy.</li><li>Setup report: set up the report to record the energy change and save the trajectory.</li><li>Simulate in the <mark>NVT equillibration</mark> and <mark>NPT production MD</mark> condition.<ul><li><code>1*bar</code>: bar is a standard measure of pressure, and 1 bar is approximately equal to atmospheric pressure at sea level.</li></ul></li></ol><div class="admonition note"><p class="admonition-title">What is NVT and NPT?</p><p><font title='ChatGPT o1' color=gray>In the NVT (constant Number of particles, Volume, and Temperature) ensemble, the system is thermally equilibrated at a fixed volume to achieve a stable temperature distribution. This step ensures that any initial structural distortions and non-equilibrium distributions of kinetic energy dissipate, providing a well-relaxed starting point. Following NVT equilibration, the system is often subjected to an NPT (constant Number of particles, Pressure, and Temperature) ensemble, where both temperature and pressure are maintained constant. This allows the simulation box volume to fluctuate to the pressure target, enabling the system’s density and structure to equilibrate under more experimentally relevant conditions. The transition from NVT to NPT thus facilitates a smooth pathway from initial equilibration to realistic production conditions, offering a balanced and physically representative environment for subsequent analyses of structural, thermodynamic, and dynamic properties.</font></p></div><table><thead><tr><th>Feature</th><th>NVT Equilibration</th><th>NPT Production MD</th></tr></thead><tbody><tr><td>Ensemble</td><td>Canonical (NVT)</td><td>Isothermal–Isobaric (NPT)</td></tr><tr><td>Variables Held Fixed</td><td>Number of particles (N), Volume (V), Temperature (T)</td><td>Number of particles (N), Pressure §, Temperature (T)</td></tr><tr><td>Volume Adjustment</td><td>Fixed volume</td><td>Volume fluctuates to maintain target pressure</td></tr><tr><td>Pressure Control</td><td>Not controlled, can fluctuate</td><td>Actively controlled via a barostat</td></tr><tr><td>Typical Use</td><td>Initial temperature equilibration after energy minimization</td><td>Production runs to simulate conditions resembling experimental environments</td></tr><tr><td>Realism</td><td>Less physically representative of ambient conditions (volume fixed)</td><td>More realistic: system adapts to pressure, resulting in stable density</td></tr><tr><td>Common Duration</td><td>Shorter (tens to hundreds of picoseconds)</td><td>Longer (nanoseconds to microseconds) for data collection</td></tr><tr><td>Outcome</td><td>Thermally equilibrated structure at given T</td><td>Equilibrium structure and dynamics at given P and T, suitable for analysis</td></tr></tbody></table><h2 id="Protein-Relaxation-Test">Protein Relaxation Test</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sys <span class="hljs-keyword">import</span> stdout<br><span class="hljs-keyword">from</span> openmm.app <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> openmm <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> openmm.unit <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># Input files</span><br>pdb_filename = <span class="hljs-string">&#x27;best_fix.pdb&#x27;</span>  <span class="hljs-comment"># Your starting protein structure (from cryo-EM)</span><br>forcefield_files = [<span class="hljs-string">&#x27;amber14-all.xml&#x27;</span>, <span class="hljs-string">&#x27;amber14/tip3pfb.xml&#x27;</span>]  <span class="hljs-comment"># Force fields</span><br>ionic_strength = <span class="hljs-number">0.15</span>*molar<br><br><span class="hljs-comment"># Load the PDB</span><br>pdb = PDBFile(pdb_filename)<br><br><span class="hljs-comment"># Create a forcefield object</span><br>forcefield = ForceField(*forcefield_files)<br><br><span class="hljs-comment"># Create a model of the system with solvent</span><br><span class="hljs-comment"># Add a water box around the protein (10 Å padding)</span><br>modeller = Modeller(pdb.topology, pdb.positions)<br><span class="hljs-comment">#modeller.addSolvent(forcefield, model=&#x27;tip3p&#x27;, boxSize=Vec3(10,10,20)*nanometer, ionicStrength=ionic_strength)</span><br>modeller.addSolvent(forcefield, padding=<span class="hljs-number">1.0</span>*nanometer, ionicStrength=ionic_strength)<br><br><span class="hljs-comment"># Create the system</span><br>system = forcefield.createSystem(<br>    modeller.topology,<br>    nonbondedMethod=PME,<br>    nonbondedCutoff=<span class="hljs-number">1.0</span>*nanometer,<br>    constraints=HBonds,<br>    hydrogenMass=<span class="hljs-number">4</span>*amu<br>)<br><br><span class="hljs-comment"># Add a thermostat and barostat for later (NPT)</span><br>temperature = <span class="hljs-number">300</span>*kelvin<br>pressure = <span class="hljs-number">1</span>*bar<br>friction = <span class="hljs-number">1</span>/picosecond<br>timestep = <span class="hljs-number">0.002</span>*picoseconds<br><br>system.addForce(MonteCarloBarostat(pressure, temperature))<br><br><span class="hljs-comment"># Create integrator (for equilibration and production)</span><br>integrator = LangevinIntegrator(temperature, friction, timestep)<br><br><span class="hljs-comment"># Create simulation object</span><br>platform = Platform.getPlatformByName(<span class="hljs-string">&#x27;CUDA&#x27;</span>)  <span class="hljs-comment"># or &#x27;CUDA&#x27;/&#x27;OpenCL&#x27; if available</span><br>simulation = Simulation(modeller.topology, system, integrator, platform)<br>simulation.context.setPositions(modeller.positions)<br><br><span class="hljs-comment"># Minimization</span><br>print(<span class="hljs-string">&quot;Minimizing...&quot;</span>)<br>simulation.minimizeEnergy(maxIterations=<span class="hljs-number">1000</span>)<br><br><span class="hljs-comment"># save the last structure from simulation</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_mini.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br>    <br><span class="hljs-comment"># NVT Equilibration: Remove barostat and fix volume for initial temp equilibration</span><br><span class="hljs-comment"># (Optional step: you can also start directly with NPT if you prefer)</span><br>forces = &#123; force.__class__.__name__: force <span class="hljs-keyword">for</span> force <span class="hljs-keyword">in</span> system.getForces() &#125;<br>system.removeForce(<span class="hljs-built_in">list</span>(forces.keys()).index(<span class="hljs-string">&#x27;MonteCarloBarostat&#x27;</span>))<br>simulation.context.reinitialize(preserveState=<span class="hljs-literal">True</span>)<br><br>simulation.context.setVelocitiesToTemperature(temperature)<br>print(<span class="hljs-string">&quot;Equilibrating under NVT conditions...&quot;</span>)<br>simulation.reporters.append(StateDataReporter(stdout, <span class="hljs-number">1000</span>, step=<span class="hljs-literal">True</span>, potentialEnergy=<span class="hljs-literal">True</span>, temperature=<span class="hljs-literal">True</span>))<br>simulation.reporters.append(DCDReporter(<span class="hljs-string">&#x27;relax.dcd&#x27;</span>, <span class="hljs-number">1000</span>))  <span class="hljs-comment"># Save a frame every 1000 steps</span><br><br>print(<span class="hljs-string">&#x27;start simulation&#x27;</span>)<br>simulation.step(<span class="hljs-number">50000</span>)  <span class="hljs-comment"># ~100 ps of NVT equilibration (adjust as needed)</span><br><br><span class="hljs-comment"># save the last structure from simulation</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_NVT.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br><br><span class="hljs-comment"># Re-introduce NPT conditions (barostat)</span><br>system.addForce(MonteCarloBarostat(pressure, temperature))<br>simulation.context.reinitialize(preserveState=<span class="hljs-literal">True</span>)<br><br>print(<span class="hljs-string">&quot;Equilibrating under NPT conditions...&quot;</span>)<br><span class="hljs-comment"># Remove old reporters and add a new one</span><br>simulation.step(<span class="hljs-number">50000</span>)  <span class="hljs-comment"># Another ~100 ps (adjust as needed)</span><br><br><span class="hljs-comment"># save the last structure from simulation</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_NPT.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br><br><span class="hljs-comment"># Now we have an equilibrated system at NPT.</span><br><span class="hljs-comment"># This is where you might start your production run.</span><br><br>production_steps = <span class="hljs-number">250000</span>  <span class="hljs-comment"># ~500 ps of production (adjust as needed)</span><br><span class="hljs-comment">#simulation.reporters.append(PDBReporter(&#x27;output_production.pdb&#x27;, 5000)) # Save frames every 10 ps</span><br>simulation.reporters.append(StateDataReporter(<span class="hljs-string">&#x27;production_log.csv&#x27;</span>, <span class="hljs-number">1000</span>, step=<span class="hljs-literal">True</span>, time=<span class="hljs-literal">True</span>, potentialEnergy=<span class="hljs-literal">True</span>,<br>                                               kineticEnergy=<span class="hljs-literal">True</span>, totalEnergy=<span class="hljs-literal">True</span>, temperature=<span class="hljs-literal">True</span>,<br>                                               volume=<span class="hljs-literal">True</span>, density=<span class="hljs-literal">True</span>))<br><br>print(<span class="hljs-string">&quot;Running Production MD...&quot;</span>)<br>simulation.step(production_steps)<br>print(<span class="hljs-string">&quot;Done!&quot;</span>)<br><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_final.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br></code></pre></td></tr></table></figure></div><p>In the final simulation, your protein may on the “edge” of the box. So, we need to adjust the relative position of the protein</p><table><thead><tr><th style="text-align:center">Before Recenter</th><th style="text-align:center">After Recenter</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/1obBq2B.png" alt=""></td><td style="text-align:center"><img src="https://imgur.com/kyKbB8I.png" alt=""></td></tr></tbody></table><h2 id="Recenter-the-Protein">Recenter the Protein</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mdtraj <span class="hljs-keyword">as</span> md<br><br><span class="hljs-comment"># Load the trajectory and topology</span><br>traj = md.load(<span class="hljs-string">&#x27;relax.dcd&#x27;</span>, top=<span class="hljs-string">&#x27;relax_test_mini.cif&#x27;</span>)<br>traj = traj.image_molecules()<br><br><span class="hljs-comment"># Re-center coordinates so the protein is centered in the box</span><br>centered_traj = traj.center_coordinates()<br><span class="hljs-comment"># Save the re-centered trajectory as a multi-model PDB (one MODEL per frame)</span><br><span class="hljs-comment">#centered_traj.save_pdb(&#x27;centered_system.pdb&#x27;)</span><br>centered_traj.save_dcd(<span class="hljs-string">&#x27;centered_system.dcd&#x27;</span>)<br></code></pre></td></tr></table></figure></div><h2 id="Trouble-Shot">Trouble Shot</h2><h3 id="No-template-found-for-residue-30730-HOH">No template found for residue 30730 (HOH)</h3><p>Citation: <a href=""></a><br>Error Code:</p><pre>ValueError: No template found for residue 30730 (HOH).  The set of atoms matches HOH, but the bonds are different.</pre><p>Bug reason: <a href="https://github.com/openmm/openmm/issues/3393">The PDB format doesn’t support models with more than 100,000 atoms.</a></p><p>How to solve: save the output as <code>cif</code> format by using <code>PDBxFile</code></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="DIFF"><figure class="iseeu highlight /diff"><table><tr><td class="code"><pre><code class="hljs diff"><span class="hljs-deletion">- with open(&#x27;best_fix_sld.pdb&#x27;, &#x27;w&#x27;) as f:</span><br><span class="hljs-addition">+ with open(&#x27;best_fix_sld.cif&#x27;, &#x27;w&#x27;) as f:</span><br><span class="hljs-deletion">-    app.PDBFile.writeFile(modeller.topology, modeller.positions, f)</span><br><span class="hljs-addition">+    app.PDBxFile.writeFile(modeller.topology, modeller.positions, f)</span><br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">OpenMM, Molecular Dynamic Simulation</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>HDF5 Data Format Introduction</title>
    <link href="https://karobben.github.io/2024/10/23/AI/hdf5/"/>
    <id>https://karobben.github.io/2024/10/23/AI/hdf5/</id>
    <published>2024-10-24T02:35:09.000Z</published>
    <updated>2024-10-30T17:49:23.621Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Structure-of-hdf5">Structure of hdf5</h2><p><mark>Key Features of HDF5</mark>:</p><ol><li>Hierarchical Structure: HDF5 files are organized like a file system, with “groups” that act like directories and “datasets” that act like files. This allows for complex, hierarchical data storage.</li><li>Efficient Storage: HDF5 is optimized for storing and retrieving large datasets. It uses compression techniques (like GZIP or SZIP) to reduce file size without losing data.</li><li>Cross-platform Compatibility: The format is portable across different platforms and operating systems, meaning that HDF5 files can be used on Windows, macOS, Linux, etc.</li><li>Self-describing Format: HDF5 files include metadata that describe the contents of the file. This makes it easy to understand the data structure without additional documentation.</li><li>Multidimensional Data: HDF5 supports storing complex, multidimensional data (such as arrays, tables, images, etc.).</li><li>Supports Many Data Types: It can store data in various types, such as integers, floats, strings, and more.</li></ol><pre>/root                (Group)    /experiment1     (Group)        /data        (Dataset)        /info        (Dataset)    /experiment2     (Group)        /data        (Dataset)        /info        (Dataset)</pre><p>Use Cases:</p><ul><li><strong>Scientific Data</strong>: For example, storing results from simulations, satellite data, or genome sequences.</li><li><strong>Machine Learning</strong>: Large training datasets can be stored in HDF5 format for efficient access during training.</li><li><strong>Image Storage</strong>: Storing large collections of images or medical imaging data (e.g., MRI scans).</li></ul><h2 id="Show-all-Names-of-Groups-and-Data">Show all Names of Groups and Data</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> h5py<br><br><span class="hljs-comment"># Open the file in read mode</span><br><span class="hljs-keyword">with</span> h5py.File(<span class="hljs-string">&#x27;file1.h5&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">print_hdf5_structure</span>(<span class="hljs-params">group, indent=<span class="hljs-number">0</span></span>):</span><br>        <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> group.keys():<br>            item = group[key]<br>            print(<span class="hljs-string">&quot;  &quot;</span> * indent + <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;key&#125;</span>: <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(item)&#125;</span>&quot;</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(item, h5py.Group):<br>                print_hdf5_structure(item, indent + <span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># Print structure from the root</span><br>    print_hdf5_structure(f)<br></code></pre></td></tr></table></figure></div><h2 id="How-to-Merge-Multiple-hdf5-Files">How to Merge Multiple hdf5 Files</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> h5py<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Function to recursively copy/merge the structure and data from source_group to target_group</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">copy_and_merge</span>(<span class="hljs-params">source_group, target_group</span>):</span><br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> source_group.keys():<br>        item = source_group[key]<br>        <span class="hljs-comment"># If the item is a group, we create the same group in the target and copy its contents</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(item, h5py.Group):<br>            <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> target_group:<br>                target_group.create_group(key)<br>            copy_and_merge(item, target_group[key])  <span class="hljs-comment"># Recursive call to merge the group&#x27;s contents</span><br>        <span class="hljs-comment"># If the item is a dataset, we merge it</span><br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(item, h5py.Dataset):<br>            <span class="hljs-comment"># If the dataset doesn&#x27;t exist in the target file, copy it</span><br>            <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> target_group:<br>                target_group.create_dataset(key, data=item[:])<br>            <span class="hljs-comment"># If the dataset exists, concatenate the data along the first axis</span><br>            <span class="hljs-keyword">else</span>:<br>                existing_data = target_group[key][:]<br>                new_data = item[:]<br>                <span class="hljs-comment"># Concatenate datasets along the first axis</span><br>                merged_data = np.concatenate((existing_data, new_data), axis=<span class="hljs-number">0</span>)<br>                <span class="hljs-comment"># Delete the old dataset and replace it with the merged one</span><br>                <span class="hljs-keyword">del</span> target_group[key]<br>                target_group.create_dataset(key, data=merged_data)<br><br><span class="hljs-comment"># Function to merge multiple HDF5 files and save the result to a new file</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge_multiple_hdf5</span>(<span class="hljs-params">files, output_file</span>):</span><br>    <span class="hljs-comment"># Create a new HDF5 file to store the merged result</span><br>    <span class="hljs-keyword">with</span> h5py.File(output_file, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> target_file:<br>        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:<br>            <span class="hljs-keyword">with</span> h5py.File(file, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> source_file:<br>                <span class="hljs-comment"># Merge the contents of each source file into the target file</span><br>                copy_and_merge(source_file, target_file)<br>        print(<span class="hljs-string">f&quot;All files have been merged into <span class="hljs-subst">&#123;output_file&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># List of HDF5 files to be merged</span><br>files_to_merge = [<span class="hljs-string">&#x27;file1.h5&#x27;</span>, <span class="hljs-string">&#x27;file2.h5&#x27;</span>, <span class="hljs-string">&#x27;file3.h5&#x27;</span>]  <span class="hljs-comment"># Add as many files as needed</span><br><span class="hljs-comment"># Specify the output file where the merged data will be saved</span><br>output_file = <span class="hljs-string">&#x27;merged_output.h5&#x27;</span><br><span class="hljs-comment"># Merge the files and save the result</span><br>merge_multiple_hdf5(files_to_merge, output_file)<br></code></pre></td></tr></table></figure></div><p>Explanation of the Code:</p><ol><li><strong><code>copy_and_merge</code> function</strong> remains the same, recursively merging groups and datasets from the source to the target.</li><li><strong><code>merge_multiple_hdf5</code> function</strong>:<ul><li>Accepts a list of HDF5 files (<code>files</code>) and an <code>output_file</code> name.</li><li>It creates a new HDF5 file (<code>output_file</code>) in <strong>write mode</strong> (<code>'w'</code>).</li><li>It loops through each file in the list, opens it in <strong>read mode</strong> (<code>'r'</code>), and calls the <code>copy_and_merge</code> function to copy the contents into the newly created file.</li><li>After all files are merged, it saves the result as <code>output_file</code>.</li></ul></li></ol><p>!!! note Key Points:<br>- Each dataset is merged by <strong>concatenating along the first axis</strong>. If you need to merge along a different axis or have more complex merging rules, we can adjust the code.<br>- Make sure the datasets you’re merging are compatible (same dimensionality along non-concatenated axes).</p><h2 id="Change-the-Group-Names">Change the Group Names</h2><p>To rename a group in an HDF5 file using <code>h5py</code>, you can’t directly change the group’s name. Instead, you can <strong>copy the group to a new group with the desired name</strong>, and then <strong>delete the original group</strong>.</p><p>Here’s how you can rename the group “4skj” to “4skj_10086”:</p><h2 id="Step-by-Step-Code">Step-by-Step Code:</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> h5py<br><span class="hljs-keyword">import</span> shutil<br><br><span class="hljs-comment"># Function to rename a group in an HDF5 file</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rename_group</span>(<span class="hljs-params">hdf5_file, old_group_name, new_group_name</span>):</span><br>    <span class="hljs-comment"># Open the file in read/write mode</span><br>    <span class="hljs-keyword">with</span> h5py.File(hdf5_file, <span class="hljs-string">&#x27;r+&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-comment"># Check if the group exists</span><br>        <span class="hljs-keyword">if</span> old_group_name <span class="hljs-keyword">in</span> f:<br>            <span class="hljs-comment"># Copy the old group to the new group</span><br>            f.copy(old_group_name, new_group_name)<br>            <span class="hljs-comment"># Delete the old group</span><br>            <span class="hljs-keyword">del</span> f[old_group_name]<br>            print(<span class="hljs-string">f&quot;Group &#x27;<span class="hljs-subst">&#123;old_group_name&#125;</span>&#x27; has been renamed to &#x27;<span class="hljs-subst">&#123;new_group_name&#125;</span>&#x27;&quot;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            print(<span class="hljs-string">f&quot;Group &#x27;<span class="hljs-subst">&#123;old_group_name&#125;</span>&#x27; does not exist in the file.&quot;</span>)<br><br><span class="hljs-comment"># Rename the group in the HDF5 file</span><br>hdf5_file = <span class="hljs-string">&#x27;file1.h5&#x27;</span>  <span class="hljs-comment"># Replace with your actual file path</span><br>old_group_name = <span class="hljs-string">&#x27;4skj&#x27;</span>  <span class="hljs-comment"># Original group name</span><br>new_group_name = <span class="hljs-string">&#x27;4skj_10086&#x27;</span>  <span class="hljs-comment"># New group name</span><br><br>rename_group(hdf5_file, old_group_name, new_group_name)<br></code></pre></td></tr></table></figure></div><ol><li><strong>Check if the group exists</strong>: The script checks if the group <code>&quot;4skj&quot;</code> exists in the HDF5 file.</li><li><strong>Copy the group</strong>: It uses the <code>f.copy()</code> function to copy the group and its contents to a new group with the desired name (<code>&quot;4skj_10086&quot;</code>).</li><li><strong>Delete the old group</strong>: After copying, the original group is deleted with <code>del f[old_group_name]</code>.</li><li><strong>Save changes</strong>: Since the file is opened in <code>'r+'</code> mode (read/write), all changes are saved automatically.</li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">HDF5 (Hierarchical Data Format version 5) is a file format designed for efficiently storing and organizing large, complex datasets. It uses a hierarchical structure of **groups** (like directories) and **datasets** (like files) to store data, supporting multidimensional arrays, metadata, and a wide variety of data types. Key advantages include **compression**, **cross-platform compatibility**, and the ability to handle large datasets that don’t fit in memory. It’s widely used in fields like scientific computing, machine learning, and bioinformatics due to its efficiency and flexibility.</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="Data Format" scheme="https://karobben.github.io/categories/Machine-Learning/Data-Format/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="AI" scheme="https://karobben.github.io/tags/AI/"/>
    
    <category term="Data" scheme="https://karobben.github.io/tags/Data/"/>
    
  </entry>
  
  <entry>
    <title>Render Your Protein in Blender with Molecular Nodes</title>
    <link href="https://karobben.github.io/2024/10/19/Bioinfor/blender-molecular-nodes/"/>
    <id>https://karobben.github.io/2024/10/19/Bioinfor/blender-molecular-nodes/</id>
    <published>2024-10-19T14:53:38.000Z</published>
    <updated>2024-11-02T22:56:43.703Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Who-to-Install-Molecular-Nodes-for-Blender">Who to Install Molecular Nodes for Blender</h2><p>First, you should download Blender yourself. Instead of the latest version, opt for a stable version because the newest release may have bugs or be incompatible with Molecular Nodes. I tried version 4.40, but when I changed the style of the molecule to Ribbon or another style, Blender crashed and closed itself. Then, I switched to version 4.2.2, and it worked fine.</p><p>As following the figure “Install the Extension”, you can find this plugin and install it. Once you down installation, you can find there is some thing new pops up like its show in figure “Update in Scene”. In this new module, you could download the pdb online or when you have pdb in the “Cache Downloads” directory, you could also load it with “Molecular Nodes”. When you load the molecular, it looks terrible. You need to follow the figure “Render By Cycles” and “Start Render” to get a normal view of molecular(“Atoms View”).</p><p><img src="https://imgur.com/uCbxiP9.png" alt="Install the Extension"></p><p><img src="https://imgur.com/9fEK7wf.png" alt="Update in Scene"></p><p><img src="https://imgur.com/ehHyKKP.png" alt="Render By Cycles"><br><img src="https://imgur.com/1DMegwb.png" alt="Start Render"><br><img src="https://imgur.com/LoFewhU.png" alt="Atoms View"></p><h2 id="Add-a-Pure-Perfect-Background">Add a Pure Perfect Background</h2><p>Source: <a href="https://www.youtube.com/watch?v=aegiN7XeLow">YouTube: EMPossible</a></p><p><img src="https://imgur.com/Wl0ea71.png" alt="Pure White Background"></p><p>How to set:</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/UndFMUw.png" alt="Set pure background"></td><td style="text-align:left">select <li>“Render” → “Film” → “Transparent”<li>“Render” → “Color Management” → “View Transform” → “Starndard”</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/jxy0bGJ.png" alt="Set Compositing"></td><td style="text-align:left">Set the Compositing. And that’s it. Go to rendering and it woud add an Perfectwhite at the background</td></tr></tbody></table><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># codes for set a pure white background</span><br><span class="hljs-comment"># Those codes only working on the first few steps.</span><br><span class="hljs-comment"># I didn&#x27;t figure how to use script to make the Geometry Nodes </span><br><span class="hljs-comment"># So, After you run those 3 commands, you still need to started from step 3 in the second pictures to manually finish the Geometry Nodes setting.</span><br>bpy.context.scene.render.engine = <span class="hljs-string">&#x27;CYCLES&#x27;</span><br>bpy.context.scene.cycles.device = <span class="hljs-string">&#x27;GPU&#x27;</span><br>bpy.context.space_data.shading.<span class="hljs-built_in">type</span> = <span class="hljs-string">&#x27;RENDERED&#x27;</span><br><br>bpy.context.<span class="hljs-built_in">object</span>.scale[<span class="hljs-number">0</span>] = <span class="hljs-number">3.5</span><br>bpy.context.<span class="hljs-built_in">object</span>.scale[<span class="hljs-number">1</span>] = <span class="hljs-number">3.5</span><br>bpy.context.<span class="hljs-built_in">object</span>.scale[<span class="hljs-number">2</span>] = <span class="hljs-number">3.5</span><br><br><br>bpy.context.scene.render.film_transparent = <span class="hljs-literal">True</span><br>bpy.context.scene.view_settings.view_transform = <span class="hljs-string">&#x27;Standard&#x27;</span><br>bpy.context.scene.use_nodes = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure></div><h2 id="Different-Colors-in-a-Surface">Different Colors in a Surface</h2><table><thead><tr><th style="text-align:left"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left"><img src="https://imgur.com/tTOeKDl.png" alt="Settings in Geomitry"></td><td style="text-align:left">The key idea for given different color is by rendern multiple layers of color on the surface. By reverse select residues , we could delete the colors from selected layer and expose the color from inner layer.</td></tr><tr><td style="text-align:left"><img src="https://imgur.com/MGA9Mqk.png" alt="Results"></td><td style="text-align:left">Final resutls show</td></tr></tbody></table><h2 id="Multiple-Style-in-One-Object">Multiple Style in One Object</h2><table><thead><tr><th style="text-align:left"><img src="https://imgur.com/TVEMkDe.png" alt="Blender Join Geometry"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left"><img src="https://imgur.com/AOUUkWK.png" alt=""></td><td style="text-align:left">Like the example in the picture, it rendered both surface model and the stick model in one object. This is achieved by <code>Join Geometry</code></td></tr></tbody></table><h2 id="Customize-the-Color-From-The-Surface">Customize the Color From The Surface</h2><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/QU1xa7K.png" alt=""></th><th style="text-align:center"><img src="https://imgur.com/7rdCxfl.png" alt=""></th></tr></thead><tbody></tbody></table><p>For Customizing the surface color, there are 2 ways to do it.</p><ol><li>using <code>pLDDT</code> nodes from <code>Color</code></li><li>using the <code>Color Attribute Map</code> nodes from <code>Color</code>.</li></ol><p>In both case, they are actually using the same set of value stored in <code>pdb</code> or <code>cif</code> file.<br>In the pdb format show below, the 11th column marked as white is the value for <code>pLDDT</code>. If you want to manage it with <code>Color Attribute Map</code>, the name of it is <code>b_factor</code></p><pre><font size=1>ATOM   4365  C   ASP C 150      17.854  27.766  83.090  1.00 <font color=white>99.42</font>      C    C  ATOM   4366  O   ASP C 150      17.369  28.239  82.038  1.00 <font color=white>95.32</font>      C    O  ATOM   4367  CB  ASP C 150      19.712  26.091  82.521  1.00 <font color=white>98.18</font>      C    C  ATOM   4368  CG  ASP C 150      20.447  24.817  82.987  1.00 <font color=white>96.59</font>      C    C  ATOM   4369  OD1 ASP C 150      20.121  24.255  84.056  1.00 <font color=white>96.78</font>      C    O  ATOM   4370  OD2 ASP C 150      21.402  24.406  82.277  1.00 <font color=white>96.06</font>      C    O1-ATOM   4371  OXT ASP C 150      18.041  28.393  84.184  1.00 <font color=white>95.18</font>      C    O1-</font></pre><h2 id="Watching-List">Watching List</h2><ul class="task-list"><li class="task-list-item"><input type="checkbox" id="cbx_0" disabled="true"><label for="cbx_0"> <a href="https://www.youtube.com/watch?v=sIblmWV0NuM">Select color pallet</a></label></li></ul><h2 id="Trouble-Shoot">Trouble Shoot</h2><h3 id="Dead-Black-in-Transparent">Dead Black in Transparent</h3><table><thead><tr><th>Dead Black</th><th>Change Setting</th><th>After Change</th></tr></thead><tbody><tr><td><img src="https://imgur.com/jCo5bO3.png" alt="Befor Change"></td><td><img src="https://imgur.com/IaT6UlB.png" alt="Change Setting"></td><td><img src="https://imgur.com/Zb3XWZz.png" alt="After Correction"></td></tr></tbody></table><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Render Your Protein in Blender with Molecular Nodes</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Protein Structure" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Protein-Structure/"/>
    
    
    <category term="3D" scheme="https://karobben.github.io/tags/3D/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Plot" scheme="https://karobben.github.io/tags/Plot/"/>
    
    <category term="Biochmistry" scheme="https://karobben.github.io/tags/Biochmistry/"/>
    
  </entry>
  
  <entry>
    <title>NCBI Data Submit with FTP/ASCP</title>
    <link href="https://karobben.github.io/2024/10/17/Bioinfor/ncbisubmit/"/>
    <id>https://karobben.github.io/2024/10/17/Bioinfor/ncbisubmit/</id>
    <published>2024-10-18T04:36:10.000Z</published>
    <updated>2024-10-19T21:44:30.634Z</updated>
    
    <content type="html"><![CDATA[<p>This post only talks about how to use the <code>ascp</code> to upload your sequencing data into NCBI.</p><h2 id="3-Different-Ways-of-Submit-Your-Data">3 Different Ways of Submit Your Data</h2><ul><li>Cloud from <strong>Amazon S3</strong> or <strong>Google Cloud</strong><br>If your data was stored in Amazon/Google Cloud at the beginning, you can easily and safely transfer them into NCBI. (I think so though I’ve never tried).</li><li><strong>FTP</strong> or <strong>ASCP</strong><br>I would recommend the second approach since our data was mostly stored in a Linux server. FTP and ASCP are very reliable. Especially for <code>ftp</code>, it is a very popular protocol. You could find a bunch of software like ‘<a href="https://filezilla-project.org/">FileZilla</a>’ to upload through ftp. The best feature of ‘FileZilla’ is it supports <strong>resume interrupted transfer</strong> and lists the fail-up loaded files so you can upload them again with one click. So, no matter how many and how big the files are, it can help you upload them safely. The main <strong>limitation</strong> for <strong>FileZilla</strong> is that it can’t used in command form and so, is not suitable for the server.</li><li><strong>Web Browser</strong><br>Unless your data are very small, you would never want to try uploading them online.</li></ul><h2 id="When-to-Upload-Your-Data">When to Upload Your Data</h2><p>You can upload your data whenever you want. It is better to upload your file before you start to fill the submission tables. In step 7, you could find the data/director you submitted and include them in the submission. But it seems like the NCBI would delete an inactivated file within 30 days. It is long enough to finish the submission.</p><p>I prefer to use <code>FileZilla</code> to upload my data. But since now, all of my data are on the server and I don’t want to download them again, I give the <code>ftp</code> and use <code>ascp</code>.</p><h2 id="How-to-use-the-ASCP">How to use the ASCP</h2><p><img src="https://imgur.com/Wik8zeG.png" alt=""></p><p>ASCP is very easy and works similarly to <code>scp</code>. In the Submission home page, select <mark>My submissions</mark> → <mark>Upload via Aspera command line or FTP</mark> → <mark>Aspera command line instructions</mark> to find the instructions. It would give you the download link and key for connecting to the NCBI server. After that, use it just like the <code>scp</code>.</p><p>Shortcomings or suggestions for <code>ascp</code></p><ol><li><strong>Keep everything in 1 director</strong>: You’d like to upload all files into one directory. Because during the submission step <strong>7 FILES</strong>, you could only select 1 directory.</li><li><strong>No sub-directories</strong>: You may want to upload the directory without subdirectories. Because in the submission portal, you can’t check files in subdirectories. So, it is hard to track back which filed upload failed.</li><li><strong>Keep ascp log</strong>: In the submission portal, you got only first 5 lines of uploaded files. So, remember to keep the <code>ascp</code> log to record the fail uploaded data.</li><li><mark><strong>upload the data one by one</strong></mark>: Strongly recommend to upload each <code>fq</code> or compressed file one by one with scripts. When you try to upload the entire directory, it may fail (I never get it done when upload a directory)</li><li>After you upload your data, you can’t see them until you go to step <strong>7 FILES</strong> in the submission portal.</li><li>You can’t check them immediately even from the submission portal. It takes time for them to show in the <strong>Step 7</strong></li></ol><p><mark>The good thing is it is easy to write a script to upload your data automatically.</mark></p><div class="admonition note"><p class="admonition-title">In the instructions, it suggest you to use those parameters: `-QT -l100m -k1`</p><ol><li><strong><code>-Q</code></strong>: This option disables the real-time display of progress and transfer statistics during the transfer. Normally, ASCP displays ongoing statistics, such as speed and percentage of completion, but using <code>-Q</code> will suppress this output.</li><li><strong><code>-T</code></strong>: This option disables encryption of the data stream during transfer. ASCP by default uses encryption for data security, but <code>-T</code> turns this off, which might improve transfer speed but at the cost of security.</li><li><strong><code>-l100m</code></strong>: This sets the transfer speed limit to <strong>100 megabits per second</strong>. You can adjust the value (e.g., <code>100m</code>) to control how fast the transfer is allowed to go, helping to prevent network congestion or manage bandwidth usage.</li><li><strong><code>-k1</code></strong>: This option controls file resume behavior. The value <code>1</code> means that if a transfer is interrupted, ASCP will resume from the point where it left off (resumable transfer). The other possible values for <code>-k</code> are:</li></ol><ul><li><code>0</code>: No resume. The transfer restarts from the beginning.</li><li><code>2</code>: Sparse resume. ASCP resumes only the missing parts of the file.</li></ul></div><h2 id="Personal-Experience">Personal Experience</h2><h3 id="Upload-your-data-into-a-specific-directory">Upload your data into a specific directory</h3><p>Though we gave the argument <code>-k1</code>, it could still fail. In the log, it says:</p><pre>Partial Completion: 19711732K bytes transferred in 3695 seconds(43691K bits/sec), in 6 files, 4 directories; 3 files failed.Session Stop  (Error: Disk write failed (server))</pre><p>After you went to the step 7, you could see:<br><img src="https://imgur.com/xxbYXVE.png" alt=""></p><p>Which means 2 directories are empty. In this case, you don’t need to worry too much. You can change the code a little bit and continue to upload could solve this problem.<br>For example, you uploaded a directory named <code>ALL_RNA</code> with code <code>ascp -i $key_file -QT -l100m -k1 -d ALL_RNA $AddressFromInstruction</code>, the data in the directory <code>ALL_RNA/SAMPLEX</code> was failed to upload, you can use the code <code>ascp -i $key_file -QT -l100m -k1 -d ALL_RNA/SAMPLEX $AddressFromInstruction/ALL_RNA</code> to continue upload the directory <code>SAMPLEX</code> into the <code>ALL_RNA</code> in NCBI server</p><pre> ascp -i $key_file -QT -l100m -k1 -d ALL_RNA $AddressFromInstructionascp -i $key_file -QT -l100m -k1 -d ALL_RNA<font color=red>/SAMPLEX</font> $AddressFromInstruction<font color=red>/ALL_RNA</font></pre><h3 id="Upload-your-date-in-the-script">Upload your date in the script</h3><p>When you have lots of data, one of the convenient ways I found is we could <code>ascp</code> each data independently. With a for loop, we could generate all codes into a script. When there are failed uploads, we just need to copy and paste the failed codes and run them again. Or we could also delete the code for the successfully uploaded one and run the entire script again.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> $(find YourDirectories -name <span class="hljs-string">&quot;.fastq.gz&quot;</span>); <span class="hljs-keyword">do</span><br>    <span class="hljs-built_in">echo</span> ascp -i <span class="hljs-variable">$key_file</span> -QT -l100m -k1 -d <span class="hljs-variable">$i</span> <span class="hljs-variable">$AddressFromInstruction</span><br><span class="hljs-keyword">done</span> &gt;&gt; ascp.sh<br><br>bash ascp.sh<br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">NCBI Data Submit</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Database" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Database/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Database" scheme="https://karobben.github.io/tags/Database/"/>
    
  </entry>
  
  <entry>
    <title>Protein Dock Overview</title>
    <link href="https://karobben.github.io/2024/10/15/AI/proteindock/"/>
    <id>https://karobben.github.io/2024/10/15/AI/proteindock/</id>
    <published>2024-10-15T20:51:32.000Z</published>
    <updated>2024-11-04T23:32:14.035Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Physical-Based-Docking">Physical Based Docking</h2><h3 id="1982-Dock-Kuntz-Irwin-D-et-al-Rigid-body-shape-based">1982: Dock; Kuntz, Irwin D., et al.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> (Rigid body-shape based)</h3><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/lsob6Ob.png" alt="Dock; Kuntz, Irwin D., et al. 1982"></th></tr></thead><tbody><tr><td style="text-align:center">© Kuntz, Irwin D., et al. 1982<sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1:1]</a></sup></td></tr></tbody></table><p>In this paper, Kuntz present a way of docking prediction by searching the steric overlap based on the knowing surface structure of 2 proteins. It originally developed by Irwin “Tack” Kuntz and colleagues at the University of California, San Francisco (<strong>UCSF</strong>), DOCK was initially used for small-molecule docking. However, it laid the foundation for the development of more advanced docking algorithms and software that could handle macromolecular docking.</p><p>In the first generation of the Dock, it focus on 2 rigid bodies. It treat 2 proteins as one object. The goal of this program is to <mark>fix the 6 degree of freedom (3 transitions and 3 orientations) that determine the best relative position</mark>. For achieving this goal, three rules are followed:</p><ol><li>No overlap between 2 proteins</li><li>all hydrogen are pared with N or O within 3.5 Å.</li><li>all ligand atoms within the receptor binding cite.</li></ol><p><strong>Dock families:</strong></p><ol><li>1994: Firstly extend the DOCK into DNA-protein Docking and by screening the Cambridge Crystallographic Database, they find that the protein CC-1065 has high score.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup><ul><li>1999: DREAM++<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>: It is a extent package for Dock. It use Dock to predict binding and evaluated the interaction and predicts the product, finally search to find the prohibits.</li></ul></li><li>2001: <strong>DOCK 4.0</strong><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>: It added incremental construction (to sample the internal degrees of freedom of the ligand) and random search. In the Dock4, the ligand is not rigid anymore. Ligands with rotatable-bonds generated multiple conformation by other model.</li><li>2006: <strong>DOCK 5.0</strong><sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>:<ul><li>anchoring: new scoring functions, sampling methods and analysis tools; energy minimizing was mentioned during the.</li><li>scoring: energy scoring function based on the AMBERL: only <strong>intermolecular</strong> van der Waals (VDW) and electrostatic components in the function.</li><li>main limitation: Ligands has lots of rotatable-bonds would cause lots of resource. During the test set, ligands with &gt; 7 rotatable bonds were removed.</li><li>Some test data correction: using “Compute” and “Biopolymer” from <strong>Sybyl</strong><sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> to calculate the Gasteiger–Hückel partial electrostatic charges and add hydrogen for residues.</li></ul></li><li>2009: <strong>DOCK 6</strong><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>: In this version, it extents it’s abilities in RNA-ligands. But the rotatable-bonds from the ligands are still limited into 7~13. With the increasing of the RNA, the accuracy are decreased.<ul><li>update scoring in <strong>solvation energy</strong>:<ul><li>Hawkins–Cramer–Truhlar (HCT) generalized Born with solvent-accessible surface area (GB/SA) solvation scoring with optional salt screening</li><li>Poisson–Boltzmann with solvent-accessible surface area (PB/SA) solvation scoring</li><li>AMBER molecular mechanics with GB/SA solvation scoring and optional receptor flexibility</li></ul></li><li>other scoring:<ul><li>VDW: grid-based form of the Lennard-Jones potential</li><li>electrostatic: Zap Tool Kit from OpenEye</li></ul></li></ul></li><li>2013: <strong>DOCK3.7</strong><sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>:</li></ol><table><thead><tr><th style="text-align:center">DOCK4</th><th style="text-align:center">DOCK5</th><th style="text-align:center">DOCK6</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/SJJfgKt.png" alt="DOCK4"></td><td style="text-align:center"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10822-006-9060-4/MediaObjects/10822_2006_9060_Fig1_HTML.gif" alt="DOCK5"></td><td style="text-align:center"><img src="https://rnajournal.cshlp.org/content/15/6/1219/F1.large.jpg" alt="DOCK6"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center">incremental: anchor-and-grow</td><td style="text-align:center">The number of<br>rotatable-bonds hashuge<br>effects on success rate</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">anchor-and-grow</p><p>The “anchor-and-grow” conformational search algorithm. The algorithm performs the following steps: (1) DOCK perceives the molecule’s rotatable bonds, which it uses to identify an anchor segment and overlapping rigid layer segments. (2) Rigid docking is used to generate multiple poses of the anchor within the receptor. (3) The first layer atoms are added to each anchor pose, and multiple conformations of the layer 1 atoms are generated. An energy score within the context of the receptor is computed for each conformation. (4) The partially grown conformations are ranked by their score and are spatially clustered. The least energetically favorable and spatially diverse conformations are discarded. (5) The next rigid layer is added to each remaining conformation, generating a new set of conformations. (6) Once all layers have been added, the set of completely grown conformations and orientations is returned</p></div><h4 id="Compare-to-Other-Related-Tools">Compare to Other Related Tools</h4><div class="c-article-table-container"><div class="c-article-table-border c-table-scroll-wrapper"><div class="c-table-scroll-wrapper__content c-table-scroll-wrapper__fade--transparent" data-component-scroll-wrapper=""><table class="data last-table"><thead class="c-article-table-head"><tr><th class="u-text-left "><p>Method</p></th><th class="u-text-left "><p>Ligand sampling method<sup>a</sup>                                          </p></th><th class="u-text-left "><p>Receptor sampling method<sup>a</sup>                                          </p></th><th class="u-text-left "><p>Scoring function<sup>b</sup>                                          </p></th><th class="u-text-left "><p>Solvation scoring<sup>c,d</sup>                                          </p></th></tr></thead><tbody><tr><td class="u-text-left "><p>DOCK 4/5 </p></td><td class="u-text-left "><p>IC</p></td><td class="u-text-left "><p>SE</p></td><td class="u-text-left "><p>MM</p></td><td class="u-text-left "><p>DDD, GB, PB</p></td></tr><tr><td class="u-text-left "><p>FlexX/FlexE </p></td><td class="u-text-left "><p>IC</p></td><td class="u-text-left "><p>SE</p></td><td class="u-text-left "><p>ED</p></td><td class="u-text-left "><p>NA</p></td></tr><tr><td class="u-text-left "><p>Glide</p></td><td class="u-text-left "><p>CE&nbsp;+&nbsp;MC</p></td><td class="u-text-left "><p>TS</p></td><td class="u-text-left "><p>MM&nbsp;+&nbsp;ED</p></td><td class="u-text-left "><p>DS</p></td></tr><tr><td class="u-text-left "><p>GOLD </p></td><td class="u-text-left "><p>GA</p></td><td class="u-text-left "><p>GA</p></td><td class="u-text-left "><p>MM&nbsp;+&nbsp;ED</p></td><td class="u-text-left "><p>NA</p></td></tr></tbody></table></div></div><div class="c-article-table-footer"><ol>                      <li>                                    <sup>a</sup>Sampling methods are defined as Genetic Algorithm (GA), Conformational Expansion (CE), Monte Carlo (MC), incremental construction (IC), merged target structure ensemble (SE), torsional search (TS)</li>                      <li>                                    <sup>b</sup>Scoring functions are defined as either empirically derived (ED) or based on molecule mechanics (MM)</li>                      <li>                                    <sup>c</sup>If the package does not accommodate this option, the symbol NA (Not Available) is used</li>                      <li>                                    <sup>d</sup>Additional accuracy can be added to the scoring function using implicit solvent models. The most commonly used options are distance dependent dielectric (DDD), a parameterized desolvation term (DS), generalized Born (GB) and linearized Poisson Boltzmann (PB)</li>                    </ol></div></div><hr><h3 id="2003-ZDock">2003: ZDock</h3><p>Version iteration:</p><ul><li>ZDOCK 2.3/2.3.2 Scoring Function: Chen R, Li L, Weng Z. (2003) ZDOCK<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup></li><li>ZDOCK 3.0/3.0.2 Scoring Function: Mintseris J, Pierce B, Wiehe K, Anderson R, Chen R, Weng Z. (2007)<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup></li><li>M-ZDOCK: Pierce B, Tong W, Weng Z. (2005) M-ZDOCK<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup></li><li>ZDOCK 3.0.2/2.3.2: Pierce BG, Hourai Y, Weng Z. (2011)<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup></li><li>Online Server: Pierce BG, Wiehe K, Hwang H, Kim BH, Vreven T, Weng Z. (2014) ZDOCK Server<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup></li></ul><h4 id="Abstract">Abstract</h4><p>ZDock was developed for ubbound docking. It is based on pairwise shape complementarity (Docking) with desolvation and electrostatics (Scoring). In there test, it shows high success rate in the <strong>antibody-antigen</strong> docking test case. It is especially helpful in <strong>“large concave binding pocket”</strong>.</p><p>Before the ZDock, there are:</p><ul><li><strong>FTDOck</strong>: gird-based shape complementarity (GSC) and electrostatic using a Fast Fourier Transform (FFT)</li><li><strong>DOT</strong>: FFT-based computes Poission-Bolzmann electrostatics.</li><li><strong>HEX</strong>: evaluates overlapping surface skins and electrostatic complementarity with Fourier coorelation.</li><li><strong>GRAMM</strong>: low-resolutoin docking with the similar scoring as FTDOck;</li><li><strong>PPD</strong>: matches critial poitns by using geometric hashing.</li><li><strong>GIGGER</strong>: maximal surface mapping and favorable amino acid contacts by bit-mapping.</li><li><strong>DARWIN</strong>: molecular mechanics energy defined according to CHARMM.</li></ul><p>For <strong>ZDock</strong>:</p><ul><li>Optimizes desolvation (<strong>GSC</strong>), <mark>key scoring function</mark>.<ul><li>GSC = grid points surrounding the receptor corresponding to ligand atoms - clash penalty</li></ul></li><li>*<em>FFT</em> for electrostatics</li><li>Novel pairwise shape complementarity function (<strong>PSC</strong>) by distance cut-off of receptor-ligand atom minus clash penalty.<ul><li>Favorable: Number of pair within cutoff</li><li>Penalty: The clash penalty for core-core, surface-core, and surface-surface (9<sup>9</sup>, 9<sup>3</sup>, 9)</li></ul></li><li><strong>DE</strong>: desolvation, estimated by atomic contact energy (<strong>ACE</strong>), which is a free energy change of breaking two protein atom-water contacts and forming a protein atom-protein atom contact and water-water contact. The sum of <strong>ACE</strong> is <strong>DE</strong></li></ul><p>Version for scoring functions:</p><ul><li><strong>ZDOCK1.3</strong><sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup>: GSC+DE+ELEC</li><li><strong>ZDOCK2.1</strong><sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup>: PSC</li><li><strong>ZDOCK2.2</strong><sup class="footnote-ref"><a href="#fn9" id="fnref9:1">[9:1]</a></sup>: PSC+DE</li><li><strong>ZDOCK2.3</strong><sup class="footnote-ref"><a href="#fn9" id="fnref9:2">[9:2]</a></sup>: PSC+DE+ELEC</li></ul><h3 id="2004-ClusPro">2004: ClusPro</h3><p><a href="https://academic.oup.com/nar/article/32/suppl_2/W96/1040440">ClusPro: a fully automated algorithm for protein–protein docking</a></p><h3 id="2010-Hex">2010: Hex</h3><p><a href="https://academic.oup.com/bioinformatics/article/26/19/2398/229220">Ultra-fast FFT protein docking on graphics processors</a></p><p><a href="https://hex.loria.fr/">Home page</a>, <a href="https://hex.loria.fr/manual800/hex_manual.html">Documentation</a></p><p>Hex is extremely fast but lack of accuracy. I tried to sampling over 100,1000 but results even close to native structure.<br>On the other hand, I didn’t find a way to mark the surface residues so we could focus on specific area. Although, GhatGPT said it could do constrained docking, but it seems we could only constrain the range angles of the receptor and the ligand.</p><table><thead><tr><th style="text-align:center"><img src="https://documentation.samson-connect.net/tutorials/hex/images/hex-results-animation.gif" alt="Hex Dock in SAMSON"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://documentation.samson-connect.net/tutorials/hex/protein-docking-with-hex/">© SAMSON</a></td></tr></tbody></table><h3 id="2014-rDock">2014: rDock</h3><p><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003571">rDock: a fast, versatile and open source program for docking ligands to proteins and nucleic acids</a></p><h3 id="2018-InterEvDock">2018: InterEvDock</h3><p><a href="https://link.springer.com/protocol/10.1007/978-1-4939-7759-8_28">Protein-Protein Docking Using Evolutionary Information</a></p><h2 id="Machine-Learning-Based-Docking">Machine Learning Based Docking</h2><h3 id="2021-DeepRank">2021: DeepRank</h3><table><thead><tr><th style="text-align:center">Model Grpah Abstract</th><th style="text-align:left">Model Name</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-021-27396-0/MediaObjects/41467_2021_27396_Fig1_HTML.png" alt="DeepRank"><br>© Chen, M., &amp; Zhou, X</td><td style="text-align:left">DeepRank</td></tr><tr><td style="text-align:center"><img src="https://raw.githubusercontent.com/DeepRank/Deeprank-GNN/master/deeprank_gnn.png" alt="DeepRank-GNN"><br>© Réau, M.</td><td style="text-align:left">DeepRank-GNN</td></tr><tr><td style="text-align:center"><img src="https://github.com/DeepRank/deeprank2/raw/main/deeprank2.png" alt="DeepRank2"><br>© Crocioni, G.</td><td style="text-align:left">Deeprank2</td></tr></tbody></table><p><strong>DeepRank</strong><sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup> is a <a href="https://github.com/DeepRank/deeprank">open source</a> framework designed to analyze 3D protein-protein interfaces by using deep learning to capture spatial and biochemical features. The paper presents DeepRank’s approach to transforming 3D structural data into 3D grids that a neural network can process. This setup allows DeepRank to identify interaction patterns, rank docking models, and predict binding affinities with high accuracy. It’s especially useful for discovering patterns in protein interfaces that might be overlooked with traditional scoring functions.</p><p>In this model, it turn the <strong>pdb into sql</strong> for efficient processing. The interfacing residues <strong>cut-off is 5.5 Å</strong>. When find all interfacing atoms, they would be mapped into **3D grid using a <strong>Gaussian mapping</strong>. The target value is very flexible, too. You can using any kind of values, iRMSD, FNAT, or DockQ score for instance, as the target values (Predicted value). The data was stored as <strong>hdf5</strong> format which keep the efficiency and small storage size.</p><p>DeepRank family:</p><ul><li><strong>DeepRank</strong><sup class="footnote-ref"><a href="#fn16" id="fnref16:1">[16:1]</a></sup>: 2021, Chen, M., et al.; It mapped the protein interfacing into a 3D grid and using CNN to train the regression model. It established the foundation of the architectural of DeepRank.<ul><li>In the DeepRank, it use information both from atom-level and residue-level. From the atom level, it calculates the atom density, charges, electrostatic energy, and VDW contacts. In residue-level, it included number of residue-residue contacts, buried surface area, and Position specific scoring matrix (PSSM)</li></ul></li><li><strong>DeepRank-GNN</strong><sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup>: 2023, Réau, M. et al.; from the same team replace the 3D grid based CNN into GNN which could avoid rotation challenge in 3D grid.<ul><li>The input information is very similar to the DeepRank. Instead of 3D grid, it relies on the adjacent matrix to build the network. In this time, the cut-off became 8.5 Å.</li><li>It has more rich features like Distance, residue half sphere exposure, Residue depth (from biopython, MSMS)</li></ul></li><li><strong>Deeprank_GNN_ESM</strong><sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup>: 2024, Xu, X., et al.; The <strong>PSSM</strong> calculating requires sequence alignment which consumes lots of time. For generate the graph efficiently, they replaced the <strong>PSSM</strong> with <strong>ESM</strong> embedding vectors.</li><li><strong>DeepRank2</strong><sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup>: 2024, Crocioni, G., et al.; In the DeepRank2., it supports both 3D grid and graph network as inputs. It also integrated the <a href="https://github.com/DeepRank/DeepRank-Mut">Deep-Mut</a> to do in silicon mutation screening.</li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p><a href="https://www.sciencedirect.com/science/article/pii/002228368290153X">Kuntz I D, Blaney J M, Oatley S J, et al. A geometric approach to macromolecule-ligand interactions[J]. Journal of molecular biology, 1982, 161(2): 269-288.</a> <a href="#fnref1" class="footnote-backref">↩︎</a> <a href="#fnref1:1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p><a href="https://link.springer.com/article/10.1007/BF00124018">Grootenhuis P D J, Roe D C, Kollman P A, et al. Finding potential DNA-binding compounds by using molecular shape[J]. Journal of Computer-Aided Molecular Design, 1994, 8: 731-750.</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p><a href="https://link.springer.com/article/10.1023/A:1008066310669">Makino S, Ewing T J A, Kuntz I D. DREAM++: flexible docking program for virtual combinatorial libraries[J]. Journal of computer-aided molecular design, 1999, 13: 513-532.</a> <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p><a href="https://link.springer.com/article/10.1023/a:1011115820450">Ewing T J A, Makino S, Skillman A G, et al. DOCK 4.0: search strategies for automated molecular docking of flexible molecule databases[J]. Journal of computer-aided molecular design, 2001, 15: 411-428.</a> <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p><a href="https://link.springer.com/article/10.1007/s10822-006-9060-4">Moustakas D T, Lang P T, Pegg S, et al. Development and validation of a modular, extensible docking program: DOCK 5[J]. Journal of computer-aided molecular design, 2006, 20: 601-619.</a> <a href="#fnref5" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p><a href="https://bcrf.biochem.wisc.edu/all-tutorials/tutorial-materials-guests/185-2/">S. Pérez, C. Meyer, A. Imberty. “Practical tools for accurate modeling of complex carbohydrates and their interactions with proteins” A. Pullman, J. Jortner, B. Pullman (Eds.), Modelling of Biomolecular Structures and Mechanisms, Kluwer Academic Publishers, Dordrecht (1996), pp. 425-454.</a> <a href="#fnref6" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p><a href="https://rnajournal.cshlp.org/content/15/6/1219.short">Lang P T, Brozell S R, Mukherjee S, et al. DOCK 6: Combining techniques to model RNA–small molecule complexes[J]. Rna, 2009, 15(6): 1219-1230.</a> <a href="#fnref7" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0075992">Coleman R G, Carchia M, Sterling T, et al. Ligand pose and orientational sampling in molecular docking[J]. PloS one, 2013, 8(10): e75992.</a> <a href="#fnref8" class="footnote-backref">↩︎</a></p></li><li id="fn9" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/12784371">Chen, R., Li, L., &amp; Weng, Z. (2003). ZDOCK: an initial‐stage protein‐docking algorithm. Proteins: Structure, Function, and Bioinformatics, 52(1), 80-87.</a> <a href="#fnref9" class="footnote-backref">↩︎</a> <a href="#fnref9:1" class="footnote-backref">↩︎</a> <a href="#fnref9:2" class="footnote-backref">↩︎</a></p></li><li id="fn10" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/17623839">Mintseris, J., Pierce, B., Wiehe, K., Anderson, R., Chen, R., &amp; Weng, Z. (2007). Integrating statistical pair potentials into protein complex prediction. Proteins: Structure, Function, and Bioinformatics, 69(3), 511-520.</a> <a href="#fnref10" class="footnote-backref">↩︎</a></p></li><li id="fn11" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/15613396">Pierce, B., Tong, W., &amp; Weng, Z. (2005). M-ZDOCK: a grid-based approach for C n symmetric multimer docking. Bioinformatics, 21(8), 1472-1478.</a> <a href="#fnref11" class="footnote-backref">↩︎</a></p></li><li id="fn12" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/21949741">Pierce, B. G., Hourai, Y., &amp; Weng, Z. (2011). Accelerating protein docking in ZDOCK using an advanced 3D convolution library. PloS one, 6(9), e24657.</a> <a href="#fnref12" class="footnote-backref">↩︎</a></p></li><li id="fn13" class="footnote-item"><p><a href="http://www.ncbi.nlm.nih.gov/pubmed/24532726">Pierce, B. G., Wiehe, K., Hwang, H., Kim, B. H., Vreven, T., &amp; Weng, Z. (2014). ZDOCK server: interactive docking prediction of protein–protein complexes and symmetric multimers. Bioinformatics, 30(12), 1771-1773.</a> <a href="#fnref13" class="footnote-backref">↩︎</a></p></li><li id="fn14" class="footnote-item"><p>Chen R, Weng Z. Docking unbound proteins using shape complementarity, desolvation, and electrostatics. Proteins 2002; 47: 281–294. <a href="#fnref14" class="footnote-backref">↩︎</a></p></li><li id="fn15" class="footnote-item"><p>Chen R, Weng Z. A novel shape complementarity scoring function for protein-protein docking. Proteins 2003; 51: 397–408. <a href="#fnref15" class="footnote-backref">↩︎</a></p></li><li id="fn16" class="footnote-item"><p><a href="https://www.nature.com/articles/s41467-021-27396-0">Renaud, N., Geng, C., Georgievska, S., Ambrosetti, F., Ridder, L., Marzella, D. F., … &amp; Xue, L. C. (2021). DeepRank: a deep learning framework for data mining 3D protein-protein interfaces. Nature communications, 12(1), 7068.</a> <a href="#fnref16" class="footnote-backref">↩︎</a> <a href="#fnref16:1" class="footnote-backref">↩︎</a></p></li><li id="fn17" class="footnote-item"><p><a href="https://academic.oup.com/bioinformatics/article/39/1/btac759/6845451">Réau, M., Renaud, N., Xue, L. C., &amp; Bonvin, A. M. (2023). DeepRank-GNN: a graph neural network framework to learn patterns in protein–protein interfaces. Bioinformatics, 39(1), btac759.</a> <a href="#fnref17" class="footnote-backref">↩︎</a></p></li><li id="fn18" class="footnote-item"><p>Xu, X., &amp; Bonvin, A. M. (2024). DeepRank-GNN-esm: a graph neural network for scoring protein–protein models using protein language model. Bioinformatics advances, 4(1), vbad191. <a href="#fnref18" class="footnote-backref">↩︎</a></p></li><li id="fn19" class="footnote-item"><p><a href="https://joss.theoj.org/papers/10.21105/joss.05983.pdf">Crocioni, G., Bodor, D. L., Baakman, C., Parizi, F. M., Rademaker, D. T., Ramakrishnan, G., … &amp; Xue, L. C. (2024). DeepRank2: Mining 3D Protein Structures with Geometric Deep Learning. Journal of Open Source Software, 9(94), 5983.</a> <a href="#fnref19" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">Protein Dock Tools and algorithm Overview</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Protein Structure" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Protein-Structure/"/>
    
    
    <category term="3D" scheme="https://karobben.github.io/tags/3D/"/>
    
    <category term="protein" scheme="https://karobben.github.io/tags/protein/"/>
    
    <category term="dock" scheme="https://karobben.github.io/tags/dock/"/>
    
  </entry>
  
  <entry>
    <title>Softmax</title>
    <link href="https://karobben.github.io/2024/10/09/AI/softmax/"/>
    <id>https://karobben.github.io/2024/10/09/AI/softmax/</id>
    <published>2024-10-09T22:46:15.000Z</published>
    <updated>2024-10-09T22:52:46.766Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Softmax">Softmax</h2><p>Key idea: $f_c(x) =$ posterior probability of cass $c$</p><ul><li>A perceptron has a one-hot output vector, in which $f_c(x) = 1$ if the<br>neural net thinks $c$ is the most likely value of $y$, and 0 otherwise</li><li>A softmax computes $f_c(x) \approx Pr(Y =c |x)$. The conditions for this to be true are:<ul><li>It needs to satisfy the axioms of probability:<br>$$ 0 \leq f_c(x) \leq 1, \quad \sum_{c=1}^{v} f_c(x) = 1$$</li><li>The weight matrix, $W$, is trained using a loss function that encourages $f(x)$ to approximate posterior probability of the labels on some training dataset:<br>$$f_c(x) \approx \Pr(Y = c|x)$$</li></ul></li></ul><h3 id="Softmax-satisfies-the-axioms-of-probability">Softmax satisfies the axioms of probability</h3><ul><li><p>Axiom #1, probabilities are non-negative $(f_k(x) \geq 0)$. There are many ways to do this, but one way that works is to choose:</p><p>$$<br>f_c(x) \propto \exp(w_c^T x + b_c)<br>$$</p></li><li><p>Axiom #2, probabilities should sum to one $(\sum_{k=1}^{v} f_k(x) = 1)$. This can be done by normalizing:</p></li></ul><p>$$<br>f(x) = [f_1(x), …, f_v(x)]^T<br>$$<br>$$<br>f_c(x) = \frac{\exp(w_c^T x + b_c)}{\sum_{k=0}^{v-1} \exp(w_k^T x + b_k)}<br>$$</p><p>where $w_k^T$ is the $k^{th}$ row of the matrix $W$.</p><h3 id="The-logistic-sigmoid-function">The logistic sigmoid function</h3><p>For a two-class classifier, we don’t really need the vector label. If we define $w = w_2 - w_1$ and $b = b_2 - b_1$, then the softmax simplifies to:</p><p>$$<br>f(Wx + b) =<br>\begin{bmatrix}<br>\text{Pr}(Y = 1|x) \\<br>\text{Pr}(Y = 2|x)<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\frac{1}{1+e^ {-(w^ Tx+b)}} \\<br>\frac{e^ {-(w^ Tx+b)}}{1+e^ {-(w^ Tx+b)}}<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\sigma(w^Tx + b) \\<br>1 - \sigma(w^Tx + b)<br>\end{bmatrix}<br>$$</p><p>… so instead of the softmax, we use a scalar function called the logistic sigmoid function:</p><p>$$<br>\sigma(z) = \frac{1}{1+e^{-z}}<br>$$</p><p>This function is called sigmoid because it is S-shaped.</p><p>For $z \to -\infty$, $\sigma(z) \to 0$</p><p>For $z \to +\infty$, $\sigma(z) \to 1$</p><h3 id="Gradient-descent">Gradient descent</h3><p>Suppose we have training tokens $(x_i, y_i)$, and we have some initial class vectors $w_1$ and $w_2$. We want to update them as</p><p>$$<br>w_1 \leftarrow w_1 - \eta \frac{\partial \mathcal{L}}{\partial w_1}<br>$$</p><p>$$<br>w_2 \leftarrow w_2 - \eta \frac{\partial \mathcal{L}}{\partial w_2}<br>$$</p><p>…where $\mathcal{L}$ is some loss function. What loss function makes sense?</p><h3 id="Zero-one-loss-function">Zero-one loss function</h3><p>The most obvious loss function for a classifier is its classification error rate,</p><p>$$<br>\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \ell(\hat{f}(x_i), y_i)<br>$$</p><p>Where $\ell(\hat{y}, y)$ is the zero-one loss function,</p><p>$$<br>\ell(f(x), y) =<br>\begin{cases}<br>0 &amp; \text{if } f(x) = y \\<br>1 &amp; \text{if } f(x) \neq y<br>\end{cases}<br>$$</p><p>The problem with zero-one loss is that it’s not differentiable.</p><h3 id="A-loss-function-that-learns-probabilities">A loss function that learns probabilities</h3><p>Suppose we have a softmax output, so we want $f_c(x) \approx \Pr(Y = c|x)$. We can train this by learning $W$ and $b$ to maximize the probability of the training corpus. If we assume all training tokens are independent, we get:</p><p>$$<br>W, b = \underset{W,b}{\text{argmax}} \prod_{i=1}^{n} \Pr(Y = y_i|x_i) = \underset{W,b}{\text{argmax}} \sum_{i=1}^{n} \ln \Pr(Y = y_i|x_i)<br>$$</p><p>But remember that $f_c(x) \approx \Pr(Y = c|x)$! Therefore, maximizing the log probability of training data is the same as minimizing the cross entropy between the neural net and the ground truth:</p><p>$$<br>W, b = \underset{W,b}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} \mathcal{L}_ i, \quad \mathcal{L}_ i = - \log f_ {y_ i}(x_ i)<br>$$</p><h3 id="Cross-entropy">Cross-entropy</h3><p>This loss function:</p><p>$$<br>\mathcal{L} = - \ln f_{y}(x)<br>$$</p><p>is called cross-entropy. It measures the difference in randomness between:</p><ul><li>Truth: $Y = y$ with probability 1.0, $\ln(1.0) = 0$, minus the</li><li>Neural net estimate: $Y = y$ with probability $f_{y}(x)$.</li></ul><p>Thus</p><p>$$<br>\mathcal{L} = 0 - \ln f_{y}(x)<br>$$</p><h3 id="Gradient-of-the-cross-entropy-of-the-softmax">Gradient of the cross-entropy of the softmax</h3><p>Since we have these definitions:</p><p>$$<br>\mathcal{L} = - \ln f_{y}(x), \quad f_{y}(x) = \frac{\exp(z_{y})}{\sum_{k=1}^{v} \exp(z_{k})}, \quad z_{c} = w_c^T x + b_c<br>$$</p><p>Then:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w_c} = \left( \frac{\partial \mathcal{L}}{\partial z_c} \right) \left( \frac{\partial z_c}{\partial w_c} \right) = \left( \frac{\partial \mathcal{L}}{\partial z_c} \right) x<br>$$</p><p>…where:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial z_c} =<br>\begin{cases}<br>f_{c}(x_i) - 1 &amp; c = y \\<br>f_{c}(x_i) &amp; c \neq y<br>\end{cases}<br>$$</p><h3 id="Similarity-to-linear-regression">Similarity to linear regression</h3><p>For linear regression, we had:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w} = \epsilon x, \quad \epsilon = f(x) - y<br>$$</p><p>For the softmax classifier with cross-entropy loss, we have</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w_c} = \epsilon_c x<br>$$</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y \text{ (output should be 1)} \\<br>f_c(x_i) &amp; \text{otherwise (output should be 0)}<br>\end{cases}<br>$$</p><h3 id="Similarity-to-perceptron">Similarity to perceptron</h3><p>Suppose we have a training token $(x, y)$, and we have some initial class vectors $w_c$. Using softmax and cross-entropy loss, we can update the weight vectors as</p><p>$$<br>w_c \leftarrow w_c - \eta \epsilon_c x<br>$$</p><p>…where</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y_i \\<br>f_c(x_i) &amp; \text{otherwise}<br>\end{cases}<br>$$</p><p>In other words, like a perceptron,</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>\epsilon_c &lt; 0 &amp; c = y_i \\<br>\epsilon_c &gt; 0 &amp; \text{otherwise}<br>\end{cases}<br>$$</p><h3 id="Outline">Outline</h3><ul><li><p>Softmax:<br>$$ f_c(x) = \frac{\exp(w_c^T x + b_c)}{\sum_{k=1}^{v} \exp(w_k^T x + b_k)} \approx \Pr(Y = c|x) $$</p></li><li><p>Cross-entropy:<br>$$ \mathcal{L} = - \ln f_{y}(x) $$</p></li><li><p>Derivative of the cross-entropy of a softmax:<br>$$ \frac{\partial \mathcal{L}}{\partial w_c} = \epsilon_c x, \quad \epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y \text{ (output should be 1)} \\<br>f_c(x_i) &amp; \text{otherwise (output should be 0)}<br>\end{cases} $$</p></li><li><p>Gradient descent:<br>$$ w_c \leftarrow w_c - \eta \epsilon_c x $$</p></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Softmax</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Support Vector Machine</title>
    <link href="https://karobben.github.io/2024/09/29/AI/supportvectormachine/"/>
    <id>https://karobben.github.io/2024/09/29/AI/supportvectormachine/</id>
    <published>2024-09-30T02:41:26.000Z</published>
    <updated>2024-10-09T22:55:34.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-Support-Vector-Machine">What is Support Vector Machine</h2><p>SVM was developed in the 1990s by Vladimir Vapnik and his colleagues. The development of SVM was rooted in statistical learning theory. It introduced the concept of finding the maximum margin hyperplane to separate classes effectively, with extensions to handle non-linear data through kernel functions. SVM gained popularity due to its ability to create powerful classifiers, especially in high-dimensional feature spaces.</p><h3 id="Compare-to-Random-Forest">Compare to Random Forest</h3><p>Random Forest is an ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting, handling both linear and non-linear data well. It’s good for large datasets and provides feature importance but is less interpretable.</p><p>SVM finds the optimal hyperplane to separate classes by maximizing the margin. It works well for smaller, high-dimensional datasets but is computationally expensive for large datasets and harder to interpret.</p><h3 id="Compare-to-Linear-Regression">Compare to Linear Regression</h3><p>The decision function of <strong>Support Vector Machine (SVM)</strong> looks very similar to a <strong>linear function</strong>—and indeed, it shares common elements with <strong>linear regression</strong>. However, the main differences lie in their objectives and the way they handle data:</p><h4 id="Similarities">Similarities</h4><ul><li><strong>Linear Function Form</strong>: Both SVM and Linear Regression use a linear function of the form:<br>$$<br>f(x) = w_1 x_1 + w_2 x_2 + \cdots + w_k x_k + b<br>$$<br>Where $ w_i $ are the weights, $ x_i $ are the features, and $ b $ is the bias term.</li><li><strong>Weight Optimization</strong>: Both models optimize the weights ($ w $) to achieve their goals.</li></ul><h4 id="Key-Differences">Key Differences</h4><ol><li><p><strong>Objective Function</strong>:</p><ul><li><strong>Linear Regression</strong>: The goal is to <strong>minimize the error</strong> (typically the mean squared error) between predicted and actual values. It aims to find the line (or hyperplane) that best fits the data points by minimizing the difference between predictions and true values.</li><li><strong>SVM</strong>: The goal is to <strong>maximize the margin</strong> between different classes. SVM seeks to find a hyperplane that not only separates the classes but does so with the largest possible gap between the nearest points of each class (called <strong>support vectors</strong>). This makes the decision boundary as robust as possible against errors or noise.</li></ul></li><li><p><strong>Loss Function</strong>:</p><ul><li><strong>Linear Regression</strong>: Uses <strong>squared loss</strong> to penalize errors, which means that even small deviations contribute to the overall loss.</li><li><strong>SVM</strong>: Uses a <strong>hinge loss function</strong> for classification, which penalizes misclassifications and ensures a margin of separation. The loss function focuses more on correctly classifying data points with maximum confidence.</li></ul></li><li><p><strong>Problem Type</strong>:</p><ul><li><strong>Linear Regression</strong>: Primarily used for <strong>regression</strong> problems, where the goal is to predict a continuous output.</li><li><strong>SVM</strong>: Primarily used for <strong>classification</strong> (though it can be adapted for regression as <strong>SVR</strong>), where the goal is to classify data points into different categories. In SVM, the function output is interpreted using a sign function, where:<br>$$<br>f(x) = w^T x + b \Rightarrow \text{classify as } \begin{cases}<br>+1, &amp; \text{if } f(x) &gt; 0 \\<br>-1, &amp; \text{if } f(x) &lt; 0<br>\end{cases}<br>$$</li></ul></li><li><p><strong>Margin and Support Vectors</strong>:</p><ul><li><strong>Linear Regression</strong>: There is no concept of a <strong>margin</strong> or <strong>support vectors</strong> in linear regression. It simply finds the line of best fit for all data points.</li><li><strong>SVM</strong>: Introduces the concept of <strong>margin</strong>, which is the distance between the hyperplane and the closest data points from each class. These closest points are called <strong>support vectors</strong>, and they are crucial to defining the decision boundary.</li></ul></li><li><p><strong>Use of Kernels (Non-linearity)</strong>:</p><ul><li><strong>Linear Regression</strong>: Strictly a linear model. To handle non-linearity, you would have to explicitly add polynomial features or transform the features.</li><li><strong>SVM</strong>: Supports <strong>kernel tricks</strong> (such as polynomial or radial basis function kernels) to project data into higher dimensions, allowing it to separate data that isn’t linearly separable in its original space. This feature makes SVM more powerful for complex, non-linear classification problems.</li></ul></li></ol><h4 id="Summary">Summary</h4><ul><li><strong>Linear Regression</strong>: Minimizes prediction error for a best-fit line, used for regression.</li><li><strong>SVM</strong>: Maximizes the margin to find an optimal separating hyperplane, used for classification.</li><li>While both use linear functions, SVM is fundamentally about <strong>classification and margin maximization</strong>, whereas linear regression focuses on <strong>minimizing the difference between predicted and actual continuous values</strong>. SVM also handles non-linearity more effectively through kernels, making it more versatile for complex datasets.</li></ul><h2 id="Overview-of-SVM">Overview of SVM</h2><ul><li>Decision Boundary: $w^T x + b$.</li><li>Classification: $f(x) = sign(w^T x + b)$</li><li>Cost function: Training error cost + $\lambda$ penalty</li></ul><table><thead><tr><th><strong>Number of Features</strong></th><th><strong>Decision Boundary Equation</strong></th><th><strong>Classification Equation</strong></th></tr></thead><tbody><tr><td>1 Feature</td><td>$ w_1 x_1 + b = 0 $</td><td>$ f(x) = \text{sign}(w_1 x_1 + b) $</td></tr><tr><td>2 Features</td><td>$ w_1 x_1 + w_2 x_2 + b = 0 $</td><td>$ f(x) = \text{sign}(w_1 x_1 + w_2 x_2 + b) $</td></tr><tr><td>$ k $ Features</td><td>$ w_1 x_1 + w_2 x_2 + \cdots + w_k x_k + b = 0 $</td><td>$ f(x) = \text{sign}(w_1 x_1 + w_2 x_2 + \cdots + w_k x_k + b) $</td></tr></tbody></table><div class="admonition question"><p class="admonition-title">What does $w^T$ mean</p><p><strong>Explanation</strong>:</p><ul><li>A vector $ w $ is typically represented as a column vector, meaning it has multiple rows and a single column.</li><li>$ w^T $ is the <strong>transpose</strong> of $ w $, which means converting a column vector into a row vector, or vice versa.</li></ul><p><strong>Mathematical Notation</strong>:</p><ul><li>If $ w $ is a column vector with elements:$$w = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}$$</li><li>Then the <strong>transpose</strong> $ w^T $ is w row vector:$$w^T = \begin{bmatrix} w_1 &amp; w_2 &amp; \cdots &amp; w_n \end{bmatrix}$$In <strong>SVM</strong> or <strong>machine learning</strong>, the transpose is often used to indicate a <strong>dot product</strong> operation when combined with another vector or matrix. For example, if you have: $w^T x $, it means you're calculating the <strong>dot product</strong> of vector $ w $ and vector $ x $, which is a scalar value used in calculating distances, projections, or in constructing decision boundaries in algorithms like SVM.</li></ul></div><h3 id="Features">Features</h3><p>$$<br>f(\mathbf{x}) = \text{sign}(\mathbf{w}^T \mathbf{x} + b)<br>$$</p><p>where</p><p>$$<br>\mathbf{x} = \begin{bmatrix} x_0 \ x_1 \end{bmatrix}, \quad \mathbf{w} = \begin{bmatrix} w_0 \ w_1 \end{bmatrix}, \quad \text{and} \quad b \text{ is a scalar.}<br>$$</p><p><strong>Boundary</strong></p><p>The boundary condition is given by:</p><p>$$<br>\begin{bmatrix} w_0 &amp; w_1 \end{bmatrix} \begin{bmatrix} x_0 \ x_1 \end{bmatrix} + b = 0<br>$$</p><p>Solving for $ x_1 $:</p><p>$$<br>w_0 x_0 + w_1 x_1 + b = 0<br>$$</p><p>$$<br>x_1 = -\frac{w_0}{w_1} x_0 - \frac{b}{w_1}<br>$$</p><p><strong>Classification</strong></p><p>The classification function is:</p><p>$$<br>y = \begin{cases}<br>1 &amp; \text{if } x_1 \geq -\frac{w_0}{w_1}x_0 - \frac{b}{w_1} \\<br>-1 &amp; \text{if } x_1 &lt; -\frac{w_0}{w_1}x_0 - \frac{b}{w_1}<br>\end{cases}<br>$$</p><h2 id="Training-Cost">Training Cost</h2><p>The training cost in SVM refers to the computational and resource-related costs involved in training the model, which is an important consideration when choosing an algorithm, especially for larger datasets. SVM’s training cost is influenced by its optimization problem, which involves finding the hyperplane that maximizes the margin while correctly classifying the training data (or with minimal misclassification for soft margins).</p><h3 id="Training-Cost-in-SVM">Training Cost in SVM</h3><ol><li><p><strong>Optimization Complexity</strong>:</p><ul><li>SVM training involves <strong>solving a quadratic optimization problem</strong> to find the best hyperplane.</li><li>This process is complex and takes more computation, especially with <strong>non-linear kernels</strong>.</li></ul></li><li><p><strong>Time Complexity</strong>:</p><ul><li><strong>Linear SVM</strong>: Training time is between $O(n * d)$ and $O(n^2 * d)$, where $ n $ is the number of data points and $ d $ is the number of features.</li><li><strong>Non-linear Kernel SVM</strong>: Training complexity is approximately $O(n^2)$ to $O(n^3)$, making it very expensive for large datasets.</li></ul></li><li><p><strong>Memory Usage</strong>:</p><ul><li>With kernels, SVM stores a <strong>kernel matrix</strong> of size $ n \times n $, which uses a lot of memory if $ n $ is large.</li></ul></li><li><p><strong>Support Vectors</strong>:</p><ul><li>More <strong>support vectors</strong> means more computation during both training and prediction. Complex datasets often need more support vectors.</li></ul></li></ol><h3 id="Why-Care-About-Training-Cost">Why Care About Training Cost?</h3><ul><li><strong>Scalability</strong>: SVM can become impractical for <strong>large datasets</strong> due to the high cost in terms of time and memory.</li><li><strong>Resources</strong>: It requires substantial <strong>CPU and memory</strong>, limiting its use on resource-constrained systems.</li><li><strong>Algorithm Selection</strong>: For small to medium datasets, SVM works well. For large datasets, other methods like <strong>Random Forest</strong> or <strong>SGD</strong> may be better.</li></ul><h3 id="Reducing-Training-Cost">Reducing Training Cost</h3><ol><li><strong>Linear SVM</strong>: Use for linearly separable data—it has lower complexity.</li><li><strong>Approximations</strong>: Use <strong>SGDClassifier</strong> or <strong>kernel approximations</strong> for faster training.</li><li><strong>Data Subset</strong>: Train on a <strong>smaller subset</strong> of data to speed up training.</li></ol><h3 id="Hinge-Loss">Hinge Loss</h3><table><thead><tr><th>Condition</th><th>Cost Function</th><th>Description</th></tr></thead><tbody><tr><td>$y_i \neq \text{sign}(\hat{y}_i)$</td><td>$C(y_ i, \hat{y}_ i) = |y_ i| + 1$</td><td>Large</td></tr><tr><td>$y_i = \text{sign}(\hat{y}_i)$ close</td><td>$C(y_ i, \hat{y}_ i) = |y_ i| + 1$</td><td>Medium</td></tr><tr><td>$y_i = \text{sign}(\hat{y}_i)$ away</td><td>$C(y_ i, \hat{y}_ i) = 0$</td><td>No cost</td></tr><tr><td>General Cost Function</td><td>$C(y_ i, \hat{y}_ i) = \max(0, 1 - y_ i \cdot \hat{y}_ i)$</td><td>-</td></tr></tbody></table><h2 id="Train-a-SVM">Train a SVM</h2><h3 id="Training-Error">Training Error</h3><ul><li>$ \frac{1}{N} \sum_{i=1}^N C(y_i, \hat{y}_ i)$<ul><li>$ = \frac{1}{N} \sum_{i=1}^N \max(0, 1 - y_ i \cdot \hat{y}_ i) $</li><li>$ = \frac{1}{N} \sum_{i=1}^N \max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b)) $</li></ul></li></ul><h3 id="Cost-Function">Cost Function</h3><p>$$ S(\mathbf{w}, b; \lambda) = \frac{1}{N} \sum_{i=1}^N [\max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b))] + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w} $$</p><h3 id="Stochastic-Gradient-Descent">Stochastic Gradient Descent</h3><p>In training a Support Vector Machine (SVM), the primary objective is to minimize the cost function. This cost function often includes terms that measure the classification error and possibly a regularization term. The minimization of the cost function aims to find the best hyperplane that separates the classes while also considering the margin maximization between different classes and controlling model complexity to prevent overfitting.</p><p>$$<br>\mathbf{u} = \begin{bmatrix} \mathbf{w} \ b \end{bmatrix}<br>$$</p><p><strong>Minimize cost function:</strong></p><p>$$<br>g(\mathbf{u}) = \left[ \frac{1}{N} \sum_{i=1}^N g_i(\mathbf{u}) \right] + g_0(\mathbf{u})<br>$$</p><p>where:</p><p>$$<br>g_i(\mathbf{u}) = \max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b))<br>$$</p><p>and:</p><p>$$<br>g_0(\mathbf{u}) = \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}<br>$$</p><p><strong>Iteratively, at step $(n)$:</strong></p><ul><li>Compute descent direction $p^{(n)}$ and step size $\eta$</li><li>Ensure that $g(u^{(n)} + \eta p^{(n)}) \leq g(u^{(n)})$</li><li>Update $u^{(n+1)} = u^{(n)} + \eta p^{(n)}$</li></ul><p><strong>Descent direction:</strong></p><p>$$<br>p^{(n)} = -\nabla g(\mathbf{u}^{(n)})<br>$$</p><p>$$<br>= -\left( \frac{1}{N} \sum_{i=1}^N \nabla g_i(\mathbf{u}) + \nabla g_0(\mathbf{u}) \right)<br>$$</p><p><strong>Estimation through mean of batch:</strong></p><p>$$<br>p^{(n)}_ {N_ b} = -\left( \frac{1}{N_b} \sum_ {j \in \text{batch}} \nabla g_ j(\mathbf{u}) + \nabla g_ 0(\mathbf{u}) \right)<br>$$</p><p><strong>Epoch</strong></p><ul><li>One pass on training set of size $N$</li><li>Each step sees a batch of $N_b$ items</li><li>The dataset is covered in $\frac{N}{N_b}$ steps</li><li>Step size in epoch $e$: $\eta^{(e)} = \frac{m}{e + l}$<ul><li>Constants $m$ and $l$: tune on small subsets</li></ul></li></ul><p><strong>Season</strong></p><ul><li>Constant number of iterations, much smaller than epochs</li><li>Each step sees a batch of $N_b$ items</li><li>Step size in season $s$: $\eta^{(s)} = \frac{m}{s + l}$<ul><li>Constants $m$ and $l$: tune on small subsets</li></ul></li></ul><h3 id="Full-SGD">Full SGD</h3><ul><li><p><strong>Vector u and its gradient:</strong><br>$$ \mathbf{u} = \begin{bmatrix} u_1 \\ \vdots \\ u_d \end{bmatrix}, \quad \nabla g = \begin{bmatrix} \frac{\partial g}{\partial u_1} \\ \vdots \\ \frac{\partial g}{\partial u_d} \end{bmatrix} $$</p></li><li><p><strong>Batches of 1 sample at each training step:</strong><br>$$ N_b = 1 $$</p></li><li><p><strong>Gradient of g(u):</strong><br>$$ \nabla g(\mathbf{u}) = \nabla \left( \max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b)) + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w} \right) $$</p></li><li><p><strong>Update rules for a and b:</strong><br>$$ \begin{bmatrix} \mathbf{w}^{(n+1)} \ b^{(n+1)} \end{bmatrix} = \begin{bmatrix} \mathbf{w}^{(n)} \ b^{(n)} \end{bmatrix} - \eta \begin{bmatrix} \nabla_{\mathbf{w}} \ \nabla_{b} \end{bmatrix} $$</p></li><li><p><strong>Condition for correct classification away from the boundary:</strong><br>$$ y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) \geq 1. \quad \text{Correct, away from boundary} $$<br>$$ \nabla_ {\mathbf{w}} (0 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = \lambda \mathbf{w}, \quad \nabla_ {b}(0 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = 0 $$</p></li><li><p><strong>Condition for classification close to the boundary or incorrect:</strong><br>$$ y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) &lt; 1. \quad \text{Correct, close to boundary, or incorrect} $$<br>$$ \nabla_ {\mathbf{w}} (1 - y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = -y_i \mathbf{x}_ i + \lambda \mathbf{w} $$<br>$$ \nabla_ {b} (1 - y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = -y_i $$</p></li></ul><h3 id="Stops">Stops</h3><p>Stop when</p><ul><li>predefined number of seasons or epochs</li><li>error on held-out data items is smaller than some threshold</li><li>other criteria</li></ul><p><strong>Regularization Constant $ \lambda $</strong></p><ul><li><p>Regularization constant $ \lambda $ in $ g(\mathbf{u}) = \frac{1}{2} \lambda \mathbf{w}^T \mathbf{w} $. Try at different scales (e.g., $ \lambda \in {10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1} $)</p></li><li><p><strong>Procedure for Cross-Validation:</strong></p><ul><li>Split dataset into Test Set and Train Set for cross-validation.</li><li>For each $ \lambda_i $ in set to try, iteratively:<ul><li>Generate a new Fold from Train Set with a Cross-Validation Train Set and Validation Set.</li><li>Using testing $ \lambda_i $, apply Stochastic Gradient Descent (SGD) on Cross-Validation Train Set to find $ \mathbf{w} $ and $ b $.</li><li>Evaluate $ \mathbf{w} $, $ b $, $ \lambda_i $ on Validation Set and record error for current Fold.</li><li>Cross-validation error for chosen $ \lambda_i $ is average error over all the Folds.</li></ul></li><li>Using $ \lambda $ with the lowest cross-validation error, apply SGD on whole training set to get final $ \mathbf{w} $ and $ b $.</li></ul></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Support vector machien is a very commonly used in machine learning</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Random Forest</title>
    <link href="https://karobben.github.io/2024/09/29/AI/randomforest/"/>
    <id>https://karobben.github.io/2024/09/29/AI/randomforest/</id>
    <published>2024-09-30T01:11:25.000Z</published>
    <updated>2024-10-09T22:59:13.565Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Basic-Architectures">Basic Architectures</h2><ol><li><strong>Bootstrap Sampling</strong>: Random subsets of the training data are created with replacement, known as bootstrap samples. Each subset is used to train an individual decision tree.</li><li><strong>Feature Randomness</strong>: For each tree, a random subset of features is considered at each split, reducing correlation among trees and improving generalization.</li><li><strong>Multiple Decision Trees</strong>: Multiple decision trees are grown independently using the bootstrap samples. Each tree makes a prediction for a given input.</li><li><strong>Ensemble Output</strong>: For classification, the output is typically based on majority voting across all trees, while for regression, the final output is an average of all tree predictions.</li></ol><h2 id="Limitations">Limitations</h2><ul><li>Many different trees can lead to similar classifications</li><li>The algorithm to build a decision tree grows each branch just deeply enough to perfectly classify the training examples<ul><li>potential overfit</li></ul></li><li>Randomness in identification of splits: features, thresholds<ul><li>better splits may have not been considered</li></ul></li><li>Addressed through Random Forests</li></ul><h2 id="Basic-of-Random-Forest">Basic of Random Forest</h2><h3 id="Tree-Expanding">Tree Expanding</h3><p>Random forest is build on number of decision trees. For avoiding the overfit, early stop is needed during the tree expanding.</p><ul><li>It stop when<ul><li>depth(branch_node) &gt;= max_depth (manual)</li><li>size(dataset) &lt;= min_leave_size (manual)</li><li>all elements in dataset in same class</li></ul></li></ul><h3 id="Decision-Making">Decision Making</h3><p>The best decision made could be evaluate by the “entropy”.</p><ol><li>2 classes:<br>Here are the formulas from the image converted into Markdown format:</li></ol><ul><li>$ \text{Entropy}(S) = -P(A) \log_2 P(A) - P(B) \log_2 P(B) $<ul><li>$ N = |A| + |B| $</li><li>$ P(A) = \frac{|A|}{N}, \quad P(B) = \frac{|B|}{N} $</li></ul></li></ul><ol start="2"><li>C classes:</li></ol><ul><li>$ \text{Entropy}(S) = - \sum_{i=1}^C P_i \log_2 P_i $<ul><li>Each class $ i $ with probability $ P_i $.</li></ul></li></ul><h3 id="Information-Gain">Information Gain</h3><p><strong>Goal</strong>: The goal is to maximize Information Gain at each split, which corresponds to choosing features that result in subsets with the least entropy, making the data more pure (less mixed) after the split. In Random Forest and decision tree learning, the feature with the highest Information Gain is selected for splitting at each node.</p><ul><li><strong>Definition</strong>: Information Gain measures the reduction in entropy (or uncertainty) after splitting a dataset. It helps to determine the best feature for splitting the data.</li><li><strong>Entropy Before Split</strong>: The initial dataset $ S $ has an entropy $ \text{Entropy}(S) $, which quantifies the impurity or randomness in the dataset.</li><li><strong>Entropy After Split</strong>: When the dataset is split into subsets $ S_l $ and $ S_r $, each subset has its own entropy: $ \text{Entropy}(S_l) $ and $ \text{Entropy}(S_r) $.</li><li><strong>Weighted Entropy</strong>: The weighted average of the entropy of the subsets after the split is given by:<br>$$<br>\text{Entropy}_{\text{after split}} = \frac{|S_l|}{|S|} \text{Entropy}(S_l) + \frac{|S_r|}{|S|} \text{Entropy}(S_r)<br>$$</li><li><strong>Information Gain Calculation</strong>: The Information Gain for the feature $ x^{(i)} $ is calculated as the difference between the entropy before and after the split:<br>$$<br>\text{Information Gain} = \text{Entropy}(S) - \text{Entropy}_{\text{after split}}<br>$$</li></ul><p>The symbols $ |S| $, $ |S_l| $, and $ |S_r| $ represent the <strong>cardinalities</strong> or the <strong>sizes</strong> of the respective sets, meaning they indicate the <strong>number of elements</strong> in each set.</p><h3 id="Missing-Values">Missing Values</h3><ul><li>In splits, if an item misses the feature value that decide where it goes<ul><li>Estimate it based on other examples: mode or mean</li></ul></li><li>Consider only the examples in the corresponding branch</li></ul><h2 id="Decision-Tree-in-Random-Forest">Decision Tree in Random Forest</h2><p>For get the best “Decision” in each branch, iterating through all possible splits at each node can be computationally expensive, especially for large datasets and numerous features. However, decision trees (and Random Forests) use several optimization techniques to find the best split efficiently, while managing computational cost:</p><h3 id="1-Feature-and-Threshold-Selection-Strategy">1. <strong>Feature and Threshold Selection Strategy</strong></h3><ul><li><strong>Greedy Algorithm</strong>: Decision tree algorithms commonly use a greedy approach to split at each node. They do not explore all possible trees but instead make the locally optimal choice (the split that maximizes Information Gain or minimizes entropy) at each step. While this doesn’t guarantee a globally optimal tree, it is computationally efficient.</li><li><strong>Threshold Optimization</strong>: Rather than testing every possible threshold for each feature, the algorithm often considers a subset of thresholds. If the feature is numeric, thresholds are typically evaluated at points between consecutive, sorted feature values.</li></ul><h3 id="2-Random-Forest-Feature-Subsampling">2. <strong>Random Forest Feature Subsampling</strong></h3><ul><li>In <strong>Random Forests</strong>, at each node, only a random subset of features is considered for splitting, rather than evaluating all features. This greatly reduces the number of calculations needed, enhances computational efficiency, and decorrelates the trees in the ensemble (increasing robustness).</li></ul><h3 id="3-Heuristics-to-Reduce-Computation">3. <strong>Heuristics to Reduce Computation</strong></h3><ul><li><strong>Best First Split</strong>: During the process, the split that gives the maximum Information Gain is stored, and the search for a better split continues until the end of the subset of considered thresholds. If no better split is found, the stored one is selected.</li><li><strong>Stopping Conditions</strong>: To further reduce resource usage, decision tree growth is often constrained by stopping criteria such as maximum depth, minimum number of samples per leaf, or if a split provides insufficient improvement.</li></ul><h3 id="4-Approximations-for-Efficiency">4. <strong>Approximations for Efficiency</strong></h3><ul><li><strong>Bin-based Thresholds</strong>: For numerical features, rather than considering every possible value as a split point, values can be grouped into bins. The potential split thresholds are then defined based on these bins.</li><li><strong>Pre-sorting Features</strong>: In some implementations, features are pre-sorted, so determining potential split points for numeric features can be faster.</li></ul><h3 id="5-Iterative-Splitting-and-Best-Split-Finding">5. <strong>Iterative Splitting and Best Split Finding</strong></h3><ul><li>For categorical features, the split can be done in subsets if there are many categories, or by considering binary splits. For numerical features, it evaluates splits between values.</li><li>Yes, it does involve iteration, but the optimizations listed above ensure that this iteration is performed in a manageable and efficient way without explicitly iterating through every possible split for all features.</li></ul><h3 id="Balancing-Efficiency-and-Quality-of-Decision-Trees"><strong>Balancing Efficiency and Quality of Decision Trees</strong></h3><p>The combination of the above techniques allows decision trees to strike a balance between:</p><ol><li><strong>Finding Good Splits</strong>: Even if the splits aren’t absolutely perfect, they are often good enough to form a strong decision tree.</li><li><strong>Limiting Resource Waste</strong>: Efficient search heuristics and optimizations are used to reduce the exhaustive computational cost.</li></ol><h3 id="In-Random-Forest">In Random Forest:</h3><ol><li><strong>Choose $ m = \sqrt{|x|} $ Features at Random</strong><ul><li>Instead of evaluating all features for a potential split, a random subset of features is selected to reduce computation. The number of features selected ($ m $) is proportional to the square root of the total number of features ($ |x| $).</li><li>This is a common technique in Random Forests to decorrelate individual decision trees and make the algorithm <mark>computationally efficient</mark>. It prevents overfitting by introducing randomness and limits the number of features under consideration at each node.</li></ul></li><li><strong>Identify Candidate Splits for the Selected Feature $ x^{(i)} $</strong><ul><li><strong>Feature Sorting</strong>:<ul><li>The feature values ($ x^{(i)} $) can be sorted to determine the best thresholds for splitting the dataset.</li></ul></li><li><strong>Class Boundaries as Thresholds</strong>:<ul><li>The sorted feature values are evaluated to find boundaries between different classes.</li></ul></li><li><strong>Sort Data Items According to Feature Value</strong>:<ul><li>All data points are sorted by their value for the feature $ x^{(i)} $. This allows easy identification of candidate split points.</li></ul></li><li><strong>Adjacent Pairs in Different Classes</strong>:<ul><li>The algorithm looks for adjacent pairs of data points where one belongs to a different class than the other. This suggests a potential decision boundary.</li><li>These pairs, $ (item_0, item_1) $, are identified since they may represent a significant change in class, making them good candidates for splitting.</li></ul></li><li><strong>Threshold Midway Between $ item_0 $ and $ item_1 $</strong>:<ul><li>The threshold for the split is placed midway between these adjacent items from different classes. This ensures that the split captures the difference between the classes as effectively as possible.</li></ul></li></ul></li><li><strong>Randomly Select $ k $ Thresholds</strong><ul><li>To further limit the number of potential splits to evaluate, the algorithm randomly selects $ k $ thresholds from the identified candidate thresholds. This further reduces computational cost while maintaining a good chance of finding an effective split.</li><li>This random sampling balances computational efficiency with the quality of the splits, ensuring that the decision tree doesn’t become too computationally expensive.</li></ul></li><li>Summary<br>The image explains a process that helps reduce the number of potential splits evaluated at each node:<ol><li><strong>Random Subset of Features</strong>: Only a random $ m $ features are considered.</li><li><strong>Identifying Thresholds</strong>: For each selected feature, potential split thresholds are identified by analyzing class boundaries.</li><li><strong>Random Selection of Split Points</strong>: A random subset of the identified thresholds is evaluated.<br>These steps are taken to avoid an exhaustive search, reduce computational resources, and prevent overfitting, particularly in Random Forests where multiple trees are built.</li></ol></li></ol><h3 id="Step-by-Step-Explanation-with-Equations-and-Code">Step-by-Step Explanation with Equations and Code</h3><h4 id="Step-1-Choosing-m-sqrt-x-Features-at-Random">Step 1: Choosing $ m = \sqrt{|x|} $ Features at Random</h4><ul><li>Suppose you have $ |x| $ features in your dataset.</li><li>To decide the split, randomly select $ m $ features to evaluate, where:<br>$$<br>m =  \sqrt{|x|}<br>$$</li></ul><h5 id="Python-Code-Representation">Python Code Representation:</h5><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># data from: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database?resource=download</span><br><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>data = pd.read_csv(<span class="hljs-string">&#x27;/home/yiran/Downloads/diabetes.csv&#x27;</span>)<br>features = d.columns[:-<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># Total number of features</span><br>num_features = <span class="hljs-built_in">len</span>(features)<br><br><span class="hljs-comment"># Choose m features at random</span><br>m = <span class="hljs-built_in">int</span>(np.sqrt(num_features))<br>selected_features = np.random.choice(features, m, replace=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure></div><h4 id="Step-2-Identify-Candidate-Splits-for-a-Feature-x-i">Step 2: Identify Candidate Splits for a Feature $ x^{(i)} $</h4><p>For each feature selected, you need to determine possible thresholds to split the data.</p><h5 id="Steps-in-Code">Steps in Code:</h5><ol><li><p><strong>Sort Feature Values</strong>:</p><ul><li>For the selected feature $ x^{(i)} $, sort the data points by their feature values.</li></ul></li><li><p><strong>Identify Boundaries Between Classes</strong>:</p><ul><li>Find the pairs of data points that belong to different classes.</li><li>Calculate candidate thresholds midway between these pairs.</li></ul></li></ol><h5 id="Equations-for-Finding-Thresholds">Equations for Finding Thresholds:</h5><ul><li><p>Let $ x^{(i)}_j $ represent the feature value for data point $ j $.</p></li><li><p>Sort all values of feature $ x^{(i)} $:<br>$$<br>x^{(i)}_1, x^{(i)}_2, \ldots, x^{(i)}_n \quad \text{where } x^{(i)}_1 &lt; x^{(i)}_2 &lt; \ldots &lt; x^{(i)}_n<br>$$</p></li><li><p>Identify the adjacent pairs that belong to different classes. For a pair of adjacent items $ (x^ {(i)}_ j, x^ {(i)}_ {j+1}) $ from different classes, the candidate threshold $ t_j $ is given by:<br>$$<br>t_ j = \frac{x^ {(i)}_ j + x^ {(i)}_ {j+1}}{2}<br>$$</p></li></ul><h5 id="Python-Code-Representation-v2">Python Code Representation:</h5><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Assume we have a data structure `data` which contains features and labels</span><br><span class="hljs-comment"># We are focusing on the selected feature xi</span><br><br>data = TB.T.to_dict()<br>data = [data[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> data]<br><br><span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> selected_features:<br>    <span class="hljs-comment"># Sort data based on the selected feature&#x27;s value</span><br>    sorted_data = <span class="hljs-built_in">sorted</span>(data, key=<span class="hljs-keyword">lambda</span> d: d[feature])<br>    <span class="hljs-comment"># Find candidate thresholds</span><br>    candidate_thresholds = []<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sorted_data) - <span class="hljs-number">1</span>):<br>        <span class="hljs-comment"># If the class label changes between adjacent items</span><br>        <span class="hljs-keyword">if</span> sorted_data[j][<span class="hljs-string">&#x27;Outcome&#x27;</span>] != sorted_data[j + <span class="hljs-number">1</span>][<span class="hljs-string">&#x27;Outcome&#x27;</span>]:<br>            <span class="hljs-comment"># Find the midpoint between two adjacent feature values</span><br>            threshold = (sorted_data[j][feature] + sorted_data[j + <span class="hljs-number">1</span>][feature]) / <span class="hljs-number">2</span><br>            candidate_thresholds.append(threshold)<br></code></pre></td></tr></table></figure></div><div class="admonition note"><p class="admonition-title">note</p><p>In the code <code>sorted_data = sorted(data, key=lambda d: d[feature])</code></p><pre>feature_value_pairs = [(d, d[feature]) for d in data]# Step 2: Sort based on the feature valuefeature_value_pairs_sorted = sorted(feature_value_pairs, key=lambda pair: pair[1])# Step 3: Extract the sorted data pointssorted_data = [pair[0] for pair in feature_value_pairs_sorted]</pre></div><h4 id="Step-3-Randomly-Select-k-Thresholds">Step 3: Randomly Select $ k $ Thresholds</h4><ul><li>Randomly pick $ k $ thresholds from the candidate thresholds identified in the previous step to reduce computation.</li></ul><h5 id="Equation">Equation:</h5><ul><li>Suppose $ T = { t_1, t_2, \ldots, t_p } $ is the set of candidate thresholds.</li><li>Select $ k $ thresholds randomly from $ T $:<br>$$<br>T_{\text{selected}} = { t_{i_1}, t_{i_2}, \ldots, t_{i_k} }, \quad \text{where } i_j \in {1, \ldots, p}<br>$$</li></ul><h5 id="Python-Code-Representation-v3">Python Code Representation:</h5><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><br>some_predefined_k = <span class="hljs-number">15</span><br><span class="hljs-comment"># Number of thresholds to randomly select</span><br>k = <span class="hljs-built_in">min</span>(<span class="hljs-built_in">len</span>(candidate_thresholds), some_predefined_k)<br><br><span class="hljs-comment"># Randomly select k thresholds from candidate_thresholds</span><br>selected_thresholds = np.random.choice(candidate_thresholds, k, replace=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure></div><h4 id="Step-4-Compute-Information-Gain-for-Each-Split">Step 4: Compute Information Gain for Each Split</h4><ul><li>Iterate through the selected thresholds to compute the Information Gain and select the best one.</li></ul><h5 id="Equation-for-Information-Gain">Equation for Information Gain:</h5><ul><li><p>For a given threshold $ t $, split the data into two subsets:<br>$$<br>S_l = { x \in S : x^{(i)} \le t }, \quad S_r = { x \in S : x^{(i)} &gt; t }<br>$$</p></li><li><p>Compute the weighted entropy after the split:<br>$$<br>\text{Entropy}_{\text{after}} = \frac{|S_l|}{|S|}\text{Entropy}(S_l) + \frac{|S_r|}{|S|}\text{Entropy}(S_r)<br>$$</p></li><li><p>Compute the Information Gain:<br>$$<br>\text{Information Gain} = \text{Entropy}(S) - \text{Entropy}_{\text{after}}<br>$$</p></li></ul><h5 id="Python-Code-Representation-v4">Python Code Representation:</h5><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">entropy</span>(<span class="hljs-params">data</span>):</span><br>    <span class="hljs-comment"># Assume `data` has a function to calculate entropy</span><br>    labels = [d[<span class="hljs-string">&#x27;Outcome&#x27;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> data]<br>    <span class="hljs-comment"># Count the occurrences of each label</span><br>    label_counts = Counter(labels)<br>    total_count = <span class="hljs-built_in">len</span>(data)<br>    <span class="hljs-comment"># Calculate the entropy</span><br>    ent = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> label_counts.values():<br>        <span class="hljs-comment"># Calculate the probability of each label</span><br>        p = count / total_count<br>        <span class="hljs-comment"># Add to entropy, using the formula -p * log2(p)</span><br>        <span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0</span>:<br>            ent -= p * np.log2(p)<br>    <span class="hljs-keyword">return</span> ent<br><br>best_gain = -np.inf<br>best_threshold = <span class="hljs-literal">None</span><br><br><span class="hljs-keyword">for</span> threshold <span class="hljs-keyword">in</span> selected_thresholds:<br>    <span class="hljs-comment"># Split the data into left and right based on the threshold</span><br>    left_split = [d <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> sorted_data <span class="hljs-keyword">if</span> d[feature] &lt;= threshold]<br>    right_split = [d <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> sorted_data <span class="hljs-keyword">if</span> d[feature] &gt; threshold]<br>    <br>    <span class="hljs-comment"># Calculate the weighted entropy of the two subsets</span><br>    p_left = <span class="hljs-built_in">len</span>(left_split) / <span class="hljs-built_in">len</span>(sorted_data)<br>    p_right = <span class="hljs-built_in">len</span>(right_split) / <span class="hljs-built_in">len</span>(sorted_data)<br>    <br>    entropy_after = p_left * entropy(left_split) + p_right * entropy(right_split)<br>    gain = entropy(sorted_data) - entropy_after<br>    <br>    <span class="hljs-comment"># Update the best gain and threshold</span><br>    <span class="hljs-keyword">if</span> gain &gt; best_gain:<br>        best_gain = gain<br>        best_threshold = threshold<br></code></pre></td></tr></table></figure></div><h4 id="Summary">Summary</h4><ol><li><strong>Select Features Randomly</strong>: $ m = \sqrt{|x|} $ features are selected randomly to evaluate.</li><li><strong>Determine Candidate Thresholds</strong>: For each feature, the data is sorted, and class boundaries are used to identify potential split points.</li><li><strong>Random Threshold Selection</strong>: From the candidate thresholds, a random subset is chosen to reduce computational cost.</li><li><strong>Calculate Information Gain</strong>: Evaluate the Information Gain for each threshold to find the best split.</li></ol><p>These steps ensure that the decision tree algorithm efficiently finds good splits without exhaustively considering all possible splits. The randomness helps reduce computational costs and enhances the model’s robustness, especially in Random Forests.</p><h2 id="Another-Example">Another Example</h2><ol><li><a href="https://github.com/HaoranTang/Applied-Machine-Learning/blob/main/ClassifyingImages.ipynb">Applied-Machine-Learning/ClassifyingImages.ipynb</a></li><li>Quick ChatGPT example:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Step 1: Import Libraries</span><br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><br><span class="hljs-comment"># Step 2: Load and Prepare Data</span><br><span class="hljs-comment"># We&#x27;ll use the Iris dataset as an example</span><br>iris = load_iris()<br>X, y = iris.data, iris.target<br><br><span class="hljs-comment"># Split the data into training and test sets (80% train, 20% test)</span><br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># Step 3: Train the Random Forest Classifier</span><br>clf = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, random_state=<span class="hljs-number">42</span>)  <span class="hljs-comment"># 100 trees in the forest</span><br>clf.fit(X_train, y_train)<br><br><span class="hljs-comment"># Step 4: Make Predictions and Evaluate</span><br>y_pred = clf.predict(X_test)<br><br><span class="hljs-comment"># Calculate the accuracy</span><br>accuracy = accuracy_score(y_test, y_pred)<br>print(<span class="hljs-string">f&#x27;Accuracy: <span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.2</span>f&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure></div><hr><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Random Forest</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>DLVO theory: Atom Interaction</title>
    <link href="https://karobben.github.io/2024/08/16/LearnNotes/DLVOtheory/"/>
    <id>https://karobben.github.io/2024/08/16/LearnNotes/DLVOtheory/</id>
    <published>2024-08-16T21:27:54.000Z</published>
    <updated>2024-08-16T22:53:35.344Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DLVO-Theory">DLVO Theory</h2><table><thead><tr><th style="text-align:center"><img src="https://ars.els-cdn.com/content/image/3-s2.0-B0080431526016223-gr3.gif" alt="DLVO theory"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.sciencedirect.com/science/article/abs/pii/B0080431526016223">© J.H. Adair; 2001</a></td></tr></tbody></table><p>DLVO theory is named after Derjaguin, Landau, Verwey, and Overbeek, who developed it in the 1940s. It describes the forces between charged surfaces interacting through a liquid medium. The theory combines two main types of forces:</p><ol><li><strong>Van der Waals forces:</strong> These are attractive forces that arise from induced electrical interactions between molecules or atoms.</li><li><strong>Electrostatic double-layer forces:</strong> These are repulsive forces that occur due to the overlap of electrical double layers surrounding charged particles.</li></ol><p>The balance between these forces determines whether particles will aggregate (if the attractive forces dominate) or remain stable in suspension (if the repulsive forces dominate). This theory is widely used in colloid chemistry, environmental science, and materials science to understand and predict the stability of colloidal dispersions.</p><p>The DLVO theory describes the interaction energy $ U_{total} $ between two colloidal particles as the sum of the Van der Waals attraction $ U_{VdW} $ and the electrostatic repulsion $ U_{elec} $. The general form of the DLVO potential is given by:</p><p>$$ U_{total}(h) = U_{VdW}(h) + U_{elec}(h) $$</p><p>where $ h $ is the distance between the surfaces of the particles.</p><h3 id="Van-der-Waals-Attraction-U-VdW">Van der Waals Attraction ($ U_{VdW} $)</h3><p>The Van der Waals attraction energy between two spherical particles of radius $ R $ at a separation distance $ h $ is given by:</p><p>$$ U_{VdW}(h) = - \frac{A}{6} \left( \frac{2R^2}{h(2R + h)} + \frac{2R^2}{(2R + h)^2} + \ln \left( \frac{h}{2R + h} \right) \right) $$</p><ul><li>$ A $: Hamaker constant (typically around $ 10^{-20} $ J for biological systems)</li><li>$ R $: Radius of the amino acid (approx. $ 0.5 $ nm or $ 0.5 \times 10^{-9} $ m)</li><li>$ h $: Separation distance between the particles</li></ul><div class="admonition note"><p class="admonition-title">What if the atoms are different?</p><p>$R^2= R_1 * R_2$$2R = R_1 + R_2$</p></div><h3 id="Electrostatic-Repulsion-U-elec">Electrostatic Repulsion ($ U_{elec} $)</h3><p>The electrostatic repulsion energy between two spherical particles with surface potential $ \psi_0 $ and radius $ R $ in a medium with Debye length $ \kappa^{-1} $ (which is related to the ionic strength of the medium) is given by:</p><p>$$ U_{elec}(h) = 2 \pi \epsilon R \psi_0^2 \ln \left( 1 + \exp(-\kappa h) \right) $$</p><ul><li>$ \epsilon $: Permittivity of the medium (water, typically $ 80 \times 8.854 \times 10^{-12} $ F/m)</li><li>$ \psi_0 $: Surface potential (approx. $ 25 $ mV or $ 25 \times 10^{-3} $ V)</li><li>$ \kappa $: Inverse Debye length (for a Debye length of $ 1 $ nm, $ \kappa \approx 10^9 $ m$^{-1}$)</li><li>$ h $: Separation distance between the particles</li></ul><h3 id="Total-Interaction-Energy">Total Interaction Energy</h3><p>Combining these two expressions, the total interaction energy is:</p><p>$$ U_{total}(h) = - \frac{A}{6} \left( \frac{2R^2}{h(2R + h)} + \frac{2R^2}{(2R + h)^2} + \ln \left( \frac{h}{2R + h} \right) \right) + 2 \pi \epsilon R \psi_0^2 \ln \left( 1 + \exp(-\kappa h) \right) $$</p><p>This equation allows us to predict whether the colloidal particles will repel each other and remain stable in suspension or attract each other and aggregate, depending on the balance of the attractive and repulsive forces.</p><h3 id="Separation-Distance-h">Separation Distance (h)</h3><ul><li>If the radii $ R_1 $ and $ R_2 $ of two spherical particles are known, and the center-to-center distance between them is $ D $, the separation distance $ h $ is calculated as:<br>$$<br>h = D - (R_1 + R_2)<br>$$</li><li>For identical particles with the same radius $ R $, it simplifies to:<br>$$<br>h = D - 2R<br>$$</li></ul><p>In the code snippet provided, the parameters can be categorized into <strong>constant parameters</strong> (those that remain the same across different residues) and <strong>variable parameters</strong> (those that may change depending on the specific residues or the system under consideration).</p><h2 id="Parameters">Parameters</h2><h3 id="Constant-Parameters">Constant Parameters:</h3><ol><li><p><strong>$ A $</strong> (Hamaker constant):</p><ul><li><strong>Value:</strong> $ 1 \times 10^{-20} $ J</li><li><strong>Description:</strong> This is a material-specific constant that depends on the nature of the interacting particles and the medium. For biological molecules in water, it’s often taken as a constant.</li></ul></li><li><p><strong>$ \epsilon $</strong> (Permittivity of the medium):</p><ul><li><strong>Value:</strong> $ 80 \times 8.854 \times 10^{-12} $ F/m (Permittivity of water)</li><li><strong>Description:</strong> The permittivity of the medium (usually water in biological contexts) is a constant based on the dielectric properties of the solvent.</li></ul></li><li><p><strong>$ \kappa $</strong> (Inverse Debye length):</p><ul><li><strong>Value:</strong> $ 1 \times 10^9 $ m$^{-1}$</li><li><strong>Description:</strong> The inverse Debye length is related to the ionic strength of the medium and is often considered constant under specific conditions, such as physiological ionic strength.</li></ul></li></ol><h3 id="Variable-Parameters">Variable Parameters:</h3><ol><li><p><strong>$ R $</strong> (Radius of the amino acid):</p><ul><li><strong>Value:</strong> $ 0.5 \times 10^{-9} $ m (0.5 nm)</li><li><strong>Description:</strong> The radius could vary slightly between different amino acids, especially when considering side chains. The value used here is an approximation and might need adjustment for specific residues.</li></ul></li><li><p><strong>$ \psi_0 $</strong> (Surface potential):</p><ul><li><strong>Value:</strong> $ 25 \times 10^{-3} $ V (25 mV)</li><li><strong>Description:</strong> The surface potential can vary depending on the charge state of the amino acid side chains. For example, charged residues like lysine or aspartic acid will have different surface potentials compared to neutral residues like alanine.</li></ul></li><li><p><strong>$ h $</strong> (Separation distance):</p><ul><li><strong>Value:</strong> Range from $ 0.1 \times 10^{-9} $ m to $ 10 \times 10^{-9} $ m</li><li><strong>Description:</strong> The separation distance between two residues or atoms is the primary variable in these calculations, often determined by the 3D structure of the protein or molecular complex being studied.</li></ul></li></ol><h2 id="Who-to-Know-the-R-and-h">Who to Know the R and h?</h2><p>To calculate the radius of amino acids such as valine (V) and phenylalanine (F), you’re generally referring to an approximation of the <strong>van der Waals (VDW) radius</strong> or the <strong>effective radius</strong> of the entire amino acid side chain. This radius can be used in models like DLVO theory to represent the size of the interacting particle.</p><h3 id="Methods-to-Determine-the-Radius">Methods to Determine the Radius:</h3><ol><li><p><strong>Van der Waals Radius of Atoms:</strong></p><ul><li>The van der Waals radius is an inherent property of atoms and can be summed up to approximate the radius of a molecule or side chain. For example, the VDW radius for carbon is about 1.7 Å, and for hydrogen, it’s about 1.2 Å.</li></ul></li><li><p><strong>Effective Radius from Crystal Structures:</strong></p><ul><li>If you have a crystal structure or molecular model, you can measure the effective radius of the side chain by considering the spatial extent of the side chain atoms. This is often done using software tools that can calculate the solvent-accessible surface area (SASA) or by directly measuring distances in a molecular viewer.</li></ul></li><li><p><strong>Using Approximate Values from Literature:</strong></p><ul><li>For many applications, approximate radii for amino acids are available in the literature based on their typical side chain sizes.</li></ul></li></ol><h3 id="Approximate-Radii-for-Valine-V-and-Phenylalanine-F">Approximate Radii for Valine (V) and Phenylalanine (F):</h3><ul><li><p><strong>Valine (V):</strong></p><ul><li>Valine has a branched, non-polar side chain. Its effective radius is often approximated as <strong>~3.0 Å (0.3 nm)</strong>.</li></ul></li><li><p><strong>Phenylalanine (F):</strong></p><ul><li>Phenylalanine has a larger, aromatic side chain. Its effective radius is typically around <strong>~3.5-4.0 Å (0.35-0.4 nm)</strong>.</li></ul></li></ul><p>These values are not exact but are generally used in theoretical calculations.</p><h3 id="Calculation-Example">Calculation Example:</h3><p>If you need to calculate the interaction between valine (V) and phenylalanine (F), you could use these approximate radii:</p><ul><li><strong>Valine (V):</strong> $ R_V \approx 0.3 $ nm</li><li><strong>Phenylalanine (F):</strong> $ R_F \approx 0.35 $ nm</li></ul><p>The separation distance $ h $ would then be calculated based on the center-to-center distance $ D $ between the residues:</p><p>$$<br>h = D - (R_V + R_F)<br>$$</p><p>If $ D $ is known (from a crystal structure or a model), this formula gives the separation distance $ h $ between the residues’ surfaces.</p><h3 id="Tools-for-More-Accurate-Measurements">Tools for More Accurate Measurements:</h3><ul><li><strong>Molecular Visualization Software (e.g., PyMOL, Chimera):</strong> You can load a protein structure and measure the distance between specific atoms or calculate the van der Waals surface.</li><li><strong>Computational Tools:</strong> Software packages like CHARMM, AMBER, or GROMACS can provide detailed calculations based on molecular dynamics or energy minimization, giving more precise values for radii in specific contexts.</li></ul><h2 id="Python-Script">Python Script</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">van_der_waals</span>(<span class="hljs-params">h, A, R</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Calculate Van der Waals attraction energy.&quot;&quot;&quot;</span><br>    term1 = (<span class="hljs-number">2</span> * R**<span class="hljs-number">2</span>) / (h * (<span class="hljs-number">2</span> * R + h))<br>    term2 = (<span class="hljs-number">2</span> * R**<span class="hljs-number">2</span>) / (<span class="hljs-number">2</span> * R + h)**<span class="hljs-number">2</span><br>    term3 = np.log(h / (<span class="hljs-number">2</span> * R + h))<br>    <span class="hljs-keyword">return</span> - (A / <span class="hljs-number">6</span>) * (term1 + term2 + term3)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">electrostatic_repulsion</span>(<span class="hljs-params">h, epsilon, R, psi_0, kappa</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Calculate electrostatic repulsion energy.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * np.pi * epsilon * R * psi_0**<span class="hljs-number">2</span> * np.log(<span class="hljs-number">1</span> + np.exp(-kappa * h))<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dlvo_total</span>(<span class="hljs-params">h, A, R, epsilon, psi_0, kappa</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Calculate total DLVO interaction energy.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> van_der_waals(h, A, R) + electrostatic_repulsion(h, epsilon, R, psi_0, kappa)<br><br><span class="hljs-comment"># Parameters</span><br>A = <span class="hljs-number">1e-20</span>  <span class="hljs-comment"># Hamaker constant in J</span><br>R = <span class="hljs-number">1e-7</span>   <span class="hljs-comment"># Radius of particles in m</span><br>epsilon = <span class="hljs-number">80</span> * <span class="hljs-number">8.854e-12</span>  <span class="hljs-comment"># Permittivity of water in F/m</span><br>psi_0 = <span class="hljs-number">25e-3</span>  <span class="hljs-comment"># Surface potential in V</span><br>kappa = <span class="hljs-number">1e8</span>    <span class="hljs-comment"># Inverse Debye length in 1/m</span><br>h = np.linspace(<span class="hljs-number">5e-10</span>, <span class="hljs-number">1e-7</span>, <span class="hljs-number">400</span>)  <span class="hljs-comment"># Separation distance in m</span><br><br><span class="hljs-comment"># Calculate DLVO potential</span><br>U_vdw = van_der_waals(h, A, R)<br>U_elec = electrostatic_repulsion(h, epsilon, R, psi_0, kappa)<br>U_total = dlvo_total(h, A, R, epsilon, psi_0, kappa)<br><br><span class="hljs-comment"># Plot results</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.plot(h * <span class="hljs-number">1e9</span>, U_vdw, label=<span class="hljs-string">&#x27;Van der Waals Attraction&#x27;</span>, linestyle=<span class="hljs-string">&#x27;-.&#x27;</span>)<br>plt.plot(h * <span class="hljs-number">1e9</span>, U_elec, label=<span class="hljs-string">&#x27;Electrostatic Repulsion&#x27;</span>, linestyle=<span class="hljs-string">&#x27;-.&#x27;</span>)<br>plt.plot(h * <span class="hljs-number">1e9</span>, U_total, label=<span class="hljs-string">&#x27;Total DLVO Potential&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Separation Distance (nm)&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Interaction Energy (J)&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;DLVO Theory Interaction Energy&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/GMxKF8T.png" alt=""></p><h2 id="Simplified-System">Simplified System</h2><h3 id="Empirical-Formula-in-the-Context-of-Your-Problem"><strong>Empirical Formula in the Context of Your Problem</strong></h3><p>A general empirical formula for the interaction energy $ \Delta G_{i,j} $ between two residues $ H_i $ and $ A_j $ might look like this:</p><p>$$<br>\Delta G_{i,j} = V_{\text{LJ}}(r_{ij}) + V_{\text{Coulomb}}(r_{ij})<br>$$</p><p>Where:</p><ul><li>$ r_{ij} $ is the distance between residue $ H_i $ and residue $ A_j $.</li><li>$ V_{\text{LJ}}(r_{ij}) $ represents the van der Waals interaction.</li><li>$ V_{\text{Coulomb}}(r_{ij}) $ represents the electrostatic interaction.</li></ul><h3 id="Lennard-Jones-Potential-van-der-Waals-Interactions"><strong>Lennard-Jones Potential (van der Waals Interactions)</strong></h3><p>The Lennard-Jones potential is a commonly used empirical formula to describe the van der Waals forces between two non-bonded atoms or molecules. It has the form:</p><p>$$<br>V_{\text{LJ}}( r ) = 4\epsilon \left[ \left(\frac{\sigma}{r}\right)^{12} - \left(\frac{\sigma}{r}\right)^{6} \right]<br>$$</p><ul><li>$ V_{\text{LJ}}( r ) $ is the potential energy as a function of distance $ r $ between two particles.</li><li>$ \epsilon $ is the depth of the potential well, representing the strength of the interaction.</li><li>$ \sigma $ is the distance at which the potential energy is zero (often related to the size of the atoms/molecules).</li><li>$ r $ is the distance between the two particles.</li></ul><p>The term $ \left(\frac{\sigma}{r}\right)^{12} $ represents the repulsive interaction at short distances (due to Pauli exclusion principle), and the term $ \left(\frac{\sigma}{r}\right)^{6} $ represents the attractive van der Waals forces at longer distances.</p><h3 id="Coulomb’s-Law-Electrostatic-Interactions"><strong>Coulomb’s Law (Electrostatic Interactions)</strong></h3><p>Coulomb’s law describes the electrostatic interaction between two charged particles:</p><p>$$<br>V_{\text{Coulomb}}( r ) = \frac{k_e \cdot q_1 \cdot q_2}{r}<br>$$</p><ul><li>$ V_{\text{Coulomb}}( r ) $ is the potential energy between two charges.</li><li>$ k_e $ is Coulomb’s constant ($ 8.9875 \times 10^9 , \text{N} \cdot \text{m}<sup>2/\text{C}</sup>2 $ in vacuum).</li><li>$ q_1 $ and $ q_2 $ are the charges of the two interacting particles.</li><li>$ r $ is the distance between the two charges.</li></ul><h3 id="Python-Script-v2">Python Script</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> prody <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Load the protein structure</span><br>structure = parsePDB(<span class="hljs-string">&#x27;your_structure.pdb&#x27;</span>)<br><br><span class="hljs-comment"># Select the side chains of the residues of interest</span><br>residue1_sidechain = structure.select(<span class="hljs-string">&#x27;resid 10 and sidechain&#x27;</span>)<br>residue2_sidechain = structure.select(<span class="hljs-string">&#x27;resid 20 and sidechain&#x27;</span>)<br><br><span class="hljs-comment"># Function to calculate interaction energy considering only side chains</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_interaction_energy</span>(<span class="hljs-params">residue1, residue2, cutoff=<span class="hljs-number">5.0</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Calculate the van der Waals and electrostatic interaction energy </span><br><span class="hljs-string">    between two residues&#x27; side chains using a simple empirical formula.</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    :param residue1: ProDy atom group for the first residue side chain</span><br><span class="hljs-string">    :param residue2: ProDy atom group for the second residue side chain</span><br><span class="hljs-string">    :param cutoff: Distance cutoff for interaction (in Å)</span><br><span class="hljs-string">    :return: Tuple of (vdW energy, electrostatic energy)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># van der Waals parameters (simplified example)</span><br>    epsilon = <span class="hljs-number">0.1</span>  <span class="hljs-comment"># Depth of the potential well (kcal/mol)</span><br>    sigma = <span class="hljs-number">3.5</span>  <span class="hljs-comment"># Distance at which the potential is zero (Å)</span><br>    <br>    <span class="hljs-comment"># Coulomb constant (for electrostatic energy calculation)</span><br>    k_e = <span class="hljs-number">8.9875517873681764e9</span>  <span class="hljs-comment"># N m² C⁻² (can be adjusted for unit compatibility)</span><br>    <br>    <span class="hljs-comment"># Simplified charges for electrostatic calculation</span><br>    charge1 = np.<span class="hljs-built_in">sum</span>([atom.getCharge() <span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> residue1])<br>    charge2 = np.<span class="hljs-built_in">sum</span>([atom.getCharge() <span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> residue2])<br>    <br>    vdW_energy = <span class="hljs-number">0.0</span><br>    electrostatic_energy = <span class="hljs-number">0.0</span><br><br>    <span class="hljs-comment"># Calculate pairwise interactions considering only side chains</span><br>    <span class="hljs-keyword">for</span> atom1 <span class="hljs-keyword">in</span> residue1:<br>        <span class="hljs-keyword">for</span> atom2 <span class="hljs-keyword">in</span> residue2:<br>            distance = np.linalg.norm(atom1.getCoords() - atom2.getCoords())<br>            <span class="hljs-keyword">if</span> distance &lt; cutoff:<br>                <span class="hljs-comment"># van der Waals energy (Lennard-Jones potential)</span><br>                vdW_energy += <span class="hljs-number">4</span> * epsilon * ((sigma / distance)**<span class="hljs-number">12</span> - (sigma / distance)**<span class="hljs-number">6</span>)<br>                <span class="hljs-comment"># Electrostatic energy (Coulomb&#x27;s law)</span><br>                electrostatic_energy += k_e * (charge1 * charge2) / distance<br>    <span class="hljs-keyword">return</span> vdW_energy, electrostatic_energy<br><br><span class="hljs-comment"># Calculate interaction energy</span><br>vdW_energy, electrostatic_energy = calculate_interaction_energy(residue1_sidechain, residue2_sidechain)<br>print(<span class="hljs-string">f&quot;van der Waals Energy: <span class="hljs-subst">&#123;vdW_energy:<span class="hljs-number">.2</span>f&#125;</span> kcal/mol&quot;</span>)<br>print(<span class="hljs-string">f&quot;Electrostatic Energy: <span class="hljs-subst">&#123;electrostatic_energy:<span class="hljs-number">.2</span>f&#125;</span> kcal/mol&quot;</span>)<br></code></pre></td></tr></table></figure></div><details><summary>Codes for the plot</summary><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">h = np.linspace(<span class="hljs-number">3.3</span>, <span class="hljs-number">10</span>, <span class="hljs-number">400</span>)  <span class="hljs-comment"># Separation distance in m</span><br><span class="hljs-comment"># Plot results</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.plot(h, [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> h], linestyle = <span class="hljs-string">&#x27;-.&#x27;</span>)<br>plt.plot(h, [calculate_interaction_energy(i)  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> h], color = <span class="hljs-string">&#x27;salmon&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Separation Distance (Åm)&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Interaction Energy&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Lennard-Jones Potential&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div></details><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/wmcPi2K.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">In this plot, it shows the change of the Lennard-Jones Potential with the change of the distance when the $\sigma = 3.5$</td></tr></tbody></table><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">DLVO theory describes the forces between charged surfaces interacting through a liquid medium. The theory combines two main types of forces</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="other" scheme="https://karobben.github.io/categories/Notes/other/"/>
    
    
    <category term="Physics" scheme="https://karobben.github.io/tags/Physics/"/>
    
  </entry>
  
  <entry>
    <title>Kernel Density Estimation (KDE)</title>
    <link href="https://karobben.github.io/2024/08/16/AI/KernelDensityEstimation/"/>
    <id>https://karobben.github.io/2024/08/16/AI/KernelDensityEstimation/</id>
    <published>2024-08-16T20:41:30.000Z</published>
    <updated>2024-10-09T23:01:25.526Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kernel-Density-Estimation-KDE">Kernel Density Estimation (KDE)</h2><p><strong>Kernel Density Estimation (KDE)</strong> is a non-parametric method to estimate the probability density function (PDF) of a random variable based on a finite set of data points. Unlike parametric methods, which assume that the underlying data follows a specific distribution (like normal, exponential, etc.), KDE makes no such assumptions and can model more complex data distributions.</p><h3 id="How-KDE-Works">How KDE Works:</h3><ol><li><p><strong>Kernel Function</strong>: The kernel function is a smooth, continuous, symmetric function that is centered on each data point. The most commonly used kernel is the Gaussian (normal) kernel, but other kernels like Epanechnikov, triangular, and uniform can also be used.</p></li><li><p><strong>Bandwidth (Smoothing Parameter)</strong>: The bandwidth is a crucial parameter that controls the smoothness of the KDE. It determines the width of the kernel functions. A smaller bandwidth leads to a more sensitive, less smooth estimate, while a larger bandwidth produces a smoother, less sensitive estimate.</p></li><li><p><strong>Summation of Kernels</strong>: KDE constructs the overall density estimate by summing the contributions of each kernel function across all data points. Each data point contributes a small “bump” to the estimate, and the sum of these bumps forms the estimated density function.</p></li></ol><h3 id="KDE-Formula">KDE Formula:</h3><p>Given a set of $ n $ data points $ x_1, x_2, \ldots, x_n $, the KDE at a point $ x $ is calculated as:</p><p>$$<br>\hat{f}(x) = \frac{1}{n \cdot h} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)<br>$$</p><p>Where:</p><ul><li>$ \hat{f}(x) $ is the estimated density at point $ x $.</li><li>$ n $ is the number of data points.</li><li>$ h $ is the bandwidth.</li><li>$ K $ is the kernel function.</li><li>$ x_i $ are the observed data points.</li></ul><h3 id="Example-of-KDE">Example of KDE:</h3><p>Imagine you have a dataset of people’s heights. Rather than assuming the heights follow a specific distribution (like normal), KDE allows you to estimate the distribution directly from the data, which may reveal subtle features like bimodal distributions (e.g., a mix of two distinct groups).</p><h3 id="Advantages-of-KDE">Advantages of KDE:</h3><ul><li><strong>Flexible</strong>: KDE doesn’t assume any specific form of the distribution, making it suitable for complex and unknown distributions.</li><li><strong>Smooth Estimation</strong>: It provides a smooth estimate of the density function, which can be more informative than histograms.</li></ul><h3 id="Disadvantages-of-KDE">Disadvantages of KDE:</h3><ul><li><strong>Choice of Bandwidth</strong>: The performance of KDE heavily depends on the choice of bandwidth. Too small a bandwidth can lead to overfitting, while too large a bandwidth can oversmooth important features.</li><li><strong>Computationally Intensive</strong>: KDE can be computationally intensive, especially for large datasets and high-dimensional data.</li></ul><h3 id="Applications-of-KDE">Applications of KDE:</h3><ul><li><strong>Data Visualization</strong>: KDE is often used to visualize the distribution of data, particularly in one-dimensional and two-dimensional cases.</li><li><strong>Anomaly Detection</strong>: KDE can be used to detect outliers by identifying areas of low probability density.</li><li><strong>Density-Based Clustering</strong>: In clustering methods like DBSCAN, KDE can help define regions of high density.</li></ul><h2 id="How-Do-It-in-Python">How Do It in Python</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KernelDensity<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br><span class="hljs-comment"># Step 2: Prepare Your Data</span><br><span class="hljs-comment"># Example list of values</span><br>data_list = [<span class="hljs-number">1.2</span>, <span class="hljs-number">2.3</span>, <span class="hljs-number">2.4</span>, <span class="hljs-number">2.5</span>, <span class="hljs-number">3.1</span>, <span class="hljs-number">3.6</span>, <span class="hljs-number">3.8</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">4.2</span>, <span class="hljs-number">5.0</span>] * <span class="hljs-number">30</span><br><span class="hljs-comment"># Convert the list to a NumPy array and reshape it for the model</span><br>data = np.array(data_list).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-comment"># Step 3: Fit the Kernel Density Estimation Model</span><br><span class="hljs-comment"># Fit the KDE model</span><br>kde = KernelDensity(kernel=<span class="hljs-string">&#x27;gaussian&#x27;</span>, bandwidth=<span class="hljs-number">0.2</span>).fit(data)<br><span class="hljs-comment"># Step 4: (Optional) Plot the Estimated Density</span><br><span class="hljs-comment"># Define a range of values</span><br>x_range = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">6</span>, <span class="hljs-number">1000</span>).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-comment"># Estimate density for the entire range</span><br>log_density = kde.score_samples(x_range)<br>density = np.exp(log_density)<br><span class="hljs-comment"># Plot the density</span><br>sns.kdeplot(data_list, bw_adjust=<span class="hljs-number">0.5</span>)<br>plt.plot(x_range, density)<br>plt.title(<span class="hljs-string">&quot;Kernel Density Estimation&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Value&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Density&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/Tr6nMEI.png" alt="KDE plot"></th></tr></thead><tbody><tr><td style="text-align:center"></td></tr></tbody></table><h2 id="Save-and-Load-the-Model">Save and Load the Model</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pickle<br><br><span class="hljs-comment"># Fit the KDE model (assuming you have already done this)</span><br><span class="hljs-comment"># kde = KernelDensity(kernel=&#x27;gaussian&#x27;, bandwidth=0.2).fit(data)</span><br><br><span class="hljs-comment"># Save the model to a file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;kde_model.pkl&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>    pickle.dump(kde, file)<br><br><span class="hljs-comment"># Load the model from the file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;kde_model.pkl&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>    loaded_kde = pickle.load(file)<br><br><span class="hljs-comment"># Value for which you want to estimate the density</span><br>value = <span class="hljs-number">3.5</span><br><br><span class="hljs-comment"># Estimate the density using the loaded model</span><br>log_density = loaded_kde.score_samples([[value]])<br>density = np.exp(log_density)<br>print(<span class="hljs-string">f&quot;Density of the value <span class="hljs-subst">&#123;value&#125;</span> using loaded model: <span class="hljs-subst">&#123;density[<span class="hljs-number">0</span>]:<span class="hljs-number">.6</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Kernel Density Estimation (KDE)</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="Regression" scheme="https://karobben.github.io/categories/Machine-Learning/Regression/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Regression" scheme="https://karobben.github.io/tags/Regression/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Taylor Series and Its Applications in Machine Learning</title>
    <link href="https://karobben.github.io/2024/08/09/AI/taylorseries/"/>
    <id>https://karobben.github.io/2024/08/09/AI/taylorseries/</id>
    <published>2024-08-09T22:40:44.000Z</published>
    <updated>2024-10-09T23:03:07.711Z</updated>
    
    <content type="html"><![CDATA[<p>The Taylor Series is a fundamental mathematical tool that finds applications across various domains, including machine learning. In this post, we’ll explore what the Taylor Series is, how it is used in machine learning, and the significant impact it can have on <strong>optimizing</strong> machine learning models. Here are some good videos to explain the basic of the Taylor Series: <a href="https://www.youtube.com/watch?v=3d6DsjIBzJ4">Taylor series | Chapter 11, Essence of calculus</a>, <a href="https://www.youtube.com/watch?v=LkLVMJQAj6A">Visualization of the Taylor Series</a>, <a href="https://www.youtube.com/watch?v=EYjBnnUJTP8">3 Applications of Taylor Series: Integrals, Limits, &amp; Series</a>, and <a href="https://www.youtube.com/watch?v=eX1hvWxmJVE">Dear Calculus 2 Students, This is why you’re learning Taylor Series</a></p><h2 id="What-is-the-Taylor-Series"><strong>What is the Taylor Series?</strong></h2><p>The Taylor Series is a mathematical concept that allows us to <strong>approximate complex functions</strong> using an infinite sum of terms, calculated from the derivatives of the function at a specific point. It essentially breaks down a function into a <strong>polynomial</strong> that closely approximates the function near a given point.</p><p>The general formula for the Taylor Series of a function $ f(x) $ around a point $ a $ is:</p><p>$$<br>f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!} (x - a)^n<br>$$</p><p>Where $ f^{(n)}(a) $ represents the $ n $-th derivative of $ f(x) $ at point $ a $, and $ n! $ is the factorial of $ n $.</p><p>This approximation is particularly useful when dealing with functions that are difficult to compute directly, as it allows us to work with simpler polynomial expressions instead.</p><h2 id="Taylor-Series-for-the-Cosine-Function"><strong>Taylor Series for the Cosine Function</strong></h2><p><img src="https://imgur.com/20cgEEk.png" alt="Taylor Series for the Cosine Function"></p><p>The cosine function, $ \cos(x) $, is a smooth and periodic function (<mark>line in black</mark>) that oscillates between -1 and 1. While it can be computed directly using trigonometric tables or built-in functions in programming languages, these computations can be resource-intensive, especially for small embedded systems or in scenarios requiring real-time processing.</p><p>The Taylor Series provides a way to approximate $ \cos(x) $ using a polynomial expansion around a specific point, typically $ x = 0 $ (the Maclaurin series, a special case of the Taylor Series). The Taylor Series for $ \cos(x) $ is given by:</p><p>$$<br>\cos(x) = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \dots<br>$$</p><p>This series can be written as:</p><p>$$<br>\cos(x) = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!} x^{2n}<br>$$</p><p>Where:</p><ul><li>$ (-1)^n $ alternates the sign of each term.</li><li>$ (2n)! $ is the factorial of $ 2n $, ensuring that the series converges.</li><li>$ x^{2n} $ means that only even powers of $ x $ are used, reflecting the symmetry of the cosine function.</li></ul><h3 id="How-the-Approximation-Works"><strong>How the Approximation Works</strong></h3><p>The beauty of the Taylor Series lies in its ability to approximate $ \cos(x) $ with just a few terms, depending on the desired accuracy.</p><ul><li><p><strong>First Few Terms</strong>: If you only take the first two terms (up to $ x^2 $), the approximation is:<br>$$<br>\cos(x) \approx 1 - \frac{x^2}{2}<br>$$<br>This provides a reasonable approximation for $ \cos(x) $ when $ x $ is close to 0, capturing the initial downward curve of the cosine function.</p></li><li><p><strong>Adding More Terms</strong>: As you include more terms (e.g., $ x^4 $, $ x^6 $), the approximation becomes increasingly accurate, even for values of $ x $ further from 0. Each additional term refines the curve, making the polynomial more closely match the actual cosine function.</p></li></ul><h3 id="Practical-Use-and-Computational-Benefits"><strong>Practical Use and Computational Benefits</strong></h3><p>In practical applications, such as in computer graphics, signal processing, or physics simulations, using the Taylor Series to approximate $ \cos(x) $ can significantly reduce computational cost. Instead of performing the full trigonometric calculation, which might involve iterative or complex operations, a system can compute a few polynomial terms, which are far less demanding.</p><ul><li><p><strong>Example</strong>: In embedded systems where processing power is limited, calculating $ \cos(x) $ using the Taylor Series with just a few terms can save time and energy, which is crucial in battery-powered devices.</p></li><li><p><strong>Trade-off</strong>: There is always a trade-off between the number of terms used and the accuracy of the approximation. For most practical purposes, using 4 to 6 terms provides a good balance between accuracy and computational efficiency.</p></li></ul><h3 id="Beyond-Cosine-General-Use-in-Trigonometric-Functions"><strong>Beyond Cosine: General Use in Trigonometric Functions</strong></h3><p>The approach used to approximate $ \cos(x) $ can also be applied to other trigonometric functions like $ \sin(x) $ and $ \tan(x) $. Each of these functions has its own Taylor Series expansion, enabling similar approximations and computational savings.</p><h2 id="Applications-of-Taylor-Series-in-Machine-Learning"><strong>Applications of Taylor Series in Machine Learning</strong></h2><p>While the Taylor Series is a powerful mathematical tool on its own, its applications in machine learning are particularly noteworthy, especially in the context of optimization algorithms and model behavior analysis.</p><h3 id="1-Gradient-Descent-and-Optimization"><strong>1. Gradient Descent and Optimization</strong></h3><p>In machine learning, gradient descent is a widely used optimization technique that minimizes a loss function by iteratively adjusting model parameters. The Taylor Series plays a crucial role in understanding and improving this process.</p><ul><li><p><strong>Basic Gradient Descent</strong>:</p><ul><li>Gradient descent uses the first-order Taylor approximation of the loss function to update parameters. However, the basic gradient descent approach can be slow and sensitive to the choice of the learning rate, often requiring careful tuning to avoid issues like overshooting or slow convergence.</li></ul></li><li><p><strong>Newton’s Method Using Taylor Series</strong>:</p><ul><li>By incorporating the second-order Taylor expansion of the loss function, Newton’s method leverages the Hessian matrix (a matrix of second derivatives) to make more informed updates. This results in faster and more stable convergence, especially near the optimum, although it comes at the cost of increased computational complexity.</li></ul></li></ul><p><strong>Before vs. After Applying the Taylor Series</strong>:</p><ul><li><strong>Before</strong>: Gradient descent can be slow and sensitive, requiring many iterations to reach a solution.</li><li><strong>After</strong>: Newton’s method, using the Taylor Series, accelerates convergence and provides more stability, particularly in challenging optimization landscapes.</li></ul><h3 id="2-Understanding-Model-Behavior"><strong>2. Understanding Model Behavior</strong></h3><p>The Taylor Series also helps in linearizing non-linear models, which is essential for understanding how small changes in input features affect the model’s output.</p><ul><li><strong>Linearization of Non-Linear Models</strong>: By approximating non-linear functions (like activation functions in neural networks) with a Taylor Series, we can analyze the local behavior of these functions. This is particularly useful for sensitivity analysis, where understanding the impact of small input perturbations is crucial for model robustness.</li></ul><h3 id="3-Regularization-and-Generalization"><strong>3. Regularization and Generalization</strong></h3><p>Regularization techniques, which are used to prevent overfitting in machine learning models, can also be viewed through the lens of the Taylor Series. By penalizing higher-order terms in the Taylor expansion, regularization methods like L2 regularization (Ridge) help in controlling model complexity and improving generalization.</p><h2 id="Real-World-Example-Logistic-Regression-and-Taylor-Series"><strong>Real-World Example: Logistic Regression and Taylor Series</strong></h2><p>To illustrate the practical application of the Taylor Series in machine learning, consider a logistic regression model used to classify emails as spam or not. The model uses a sigmoid function to predict probabilities, and the goal is to minimize the binary cross-entropy loss function.</p><ul><li><p><strong>Without Taylor Series</strong>: Using basic gradient descent, the model may take many iterations to converge, with convergence being highly dependent on the chosen learning rate.</p></li><li><p><strong>With Taylor Series (Newton’s Method)</strong>: By applying the Taylor Series, specifically the second-order approximation, the model can achieve faster and more stable convergence, even if each iteration is more computationally intensive.</p></li></ul><p>In this case, applying the Taylor Series through Newton’s method can drastically reduce the number of iterations required to reach an optimal solution, highlighting the power of this mathematical tool in machine learning optimization.</p><h2 id="Conclusion"><strong>Conclusion</strong></h2><p>The Taylor Series is more than just a mathematical concept; it’s a powerful tool that underpins several key techniques in machine learning. From optimizing models with gradient descent to understanding the behavior of complex functions, the Taylor Series enables us to make more accurate and efficient decisions in model training and evaluation. Whether you’re dealing with logistic regression or deep learning, understanding and applying the Taylor Series can significantly enhance your machine learning practice.</p><p>By incorporating second-order information through the Taylor Series, you can achieve faster convergence, better stability, and a deeper understanding of your models, ultimately leading to more robust and effective machine learning solutions.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">The Taylor Series is a mathematical tool that approximates complex functions with polynomials, playing a crucial role in machine learning optimization. It enhances gradient descent by incorporating second-order information, leading to faster and more stable convergence. Additionally, it aids in linearizing non-linear models and informs regularization techniques. This post explores the significance of the Taylor Series in improving model training efficiency and understanding model behavior. $$\cos(x) = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!} x^{2n}$$</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="Math" scheme="https://karobben.github.io/categories/Machine-Learning/Math/"/>
    
    
    <category term="Math" scheme="https://karobben.github.io/tags/Math/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>FoldX</title>
    <link href="https://karobben.github.io/2024/07/06/Bioinfor/foldx/"/>
    <id>https://karobben.github.io/2024/07/06/Bioinfor/foldx/</id>
    <published>2024-07-06T22:11:38.000Z</published>
    <updated>2024-07-23T02:49:21.088Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Run-the-FoldX">Run the FoldX</h2><p>In this example, I am using the <strong>7ekb</strong> as example</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># download the pdb</span><br>wget https://files.rcsb.org/view/7ekb.pdb<br><span class="hljs-comment"># Repair the PDB. After repaired it, you&#x27;ll get the 7ekb_Repair.pdb for the next step</span><br>FoldX --<span class="hljs-built_in">command</span>=RepairPDB --pdb=7ekb.pdb<br><span class="hljs-comment"># calculate the free energy of the PDB  </span><br>FoldX --<span class="hljs-built_in">command</span>=Stability  --pdb=7ekb_Repair.pdb<br></code></pre></td></tr></table></figure></div><h2 id="RepairPDB">RepairPDB</h2><h3 id="Why-Repair-PDB">Why Repair PDB?</h3><p>According to ChatGPT4o, <code>RepairPDB</code> command in FoldX is a crucial step to ensure the quality and integrity of your PDB file before performing stability calculations or other analyses. Also, you could found more information from <a href="https://foldxsuite.crg.eu/command/RepairPDB">document</a></p><ol><li><p><strong>Fix Structural Issues</strong>:</p><ul><li><strong>Correcting Errors</strong>: PDB files obtained from experiments like X-ray crystallography or cryo-EM often have missing atoms, residues, or other structural issues that can affect downstream analyses. <code>RepairPDB</code> fixes these issues to ensure a complete and accurate structure.</li><li><strong>Adding Missing Atoms</strong>: The command can add missing atoms, such as hydrogen atoms, which are essential for energy calculations.</li></ul></li><li><p><strong>Standardizing the Structure</strong>:</p><ul><li><strong>Normalization</strong>: <code>RepairPDB</code> standardizes the structure to ensure that all residues and atoms are in the correct format and positions. This includes correcting bond lengths and angles to standard values.</li><li><strong>Removing Non-standard Residues</strong>: It can remove or correct non-standard residues and ligands that might interfere with calculations.</li></ul></li><li><p><strong>Improving Energy Calculations</strong>:</p><ul><li><strong>Optimizing Geometry</strong>: The command optimizes the geometry of the protein, ensuring that the atomic positions are energetically favorable. This leads to more accurate stability and free energy calculations.</li><li><strong>Minimizing Steric Clashes</strong>: It identifies and resolves steric clashes (where atoms are too close to each other), which can distort energy calculations.</li></ul></li><li><p><strong>Ensuring Compatibility</strong>:</p><ul><li><strong>Consistency</strong>: Running <code>RepairPDB</code> ensures that your PDB file is compatible with FoldX’s algorithms, reducing the risk of errors during subsequent steps.</li></ul></li></ol><div class="admonition question"><p class="admonition-title">How does the Output looks like?</p></div><pre>Residue LYSH222 has high Energy, we mutate it to itselfRepair Residue ID= LYSH222BackHbond       =               -317.22SideHbond       =               -137.87Energy_VdW      =               -476.42Electro         =               -15.23Energy_SolvP    =               628.80Energy_SolvH    =               -624.90Energy_vdwclash =               15.60energy_torsion  =               9.33backbone_vdwclash=              143.43Entropy_sidec   =               245.04Entropy_mainc   =               632.72water bonds     =               0.00helix dipole    =               -0.35loop_entropy    =               0.00cis_bond        =               4.50disulfide       =               -13.95kn electrostatic=               -0.25partial covalent interactions = 0.00Energy_Ionisation =             1.07Entropy Complex =               0.00-----------------------------------------------------------Total          =   -49.10</pre><p>It took me <mark>2m 48s</mark>. It only work in single thread and cannot move on multiple threads. I guess because it works by following the order of the AA and the <code>Total</code> is depending on the previous values. So, it can’t work on multiple threads.</p><p>Here is the result of before and after repairing. The RMS=0.01 which means it almost the same. But the slightly different are mainly focus on the loos area. In the picture present below, the left panel with green structure is the raw pdb file from PDB database. The light blue structure on the right is the corrected by FoldX. Red structure is antigen. As I marked on the left panel, 2 beta-sheets and 1 alpha helix are deleted and become random loop. Those area from the antibody are very closing to the antigen. So, technically, random loop would make more sense to me.</p><p><img src="https://imgur.com/i8aPrTy.png" alt=""></p><h2 id="Stability-Calculations">Stability Calculations</h2><p>After repaired the PDB file, you can get the result immediately.</p><pre>   ********************************************   ***                                      ***   ***             FoldX 5.1 (c)            ***   ***                                      ***   ***     code by the FoldX Consortium     ***   ***                                      ***   ***     Jesper Borg, Frederic Rousseau   ***   ***    Joost Schymkowitz, Luis Serrano   ***   ***    Peter Vanhee, Erik Verschueren    ***   ***     Lies Baeten, Javier Delgado      ***   ***       and Francois Stricher          ***   *** and any other of the 9! permutations ***   ***   based on an original concept by    ***   ***   Raphael Guerois and Luis Serrano   ***   ********************************************Stability >>>1 models read: 7ekb_Repair.pdbBackHbond       =               -332.04SideHbond       =               -163.29Energy_VdW      =               -481.14Electro         =               -17.42Energy_SolvP    =               626.91Energy_SolvH    =               -633.28Energy_vdwclash =               13.20energy_torsion  =               9.65backbone_vdwclash=              144.57Entropy_sidec   =               259.27Entropy_mainc   =               634.24water bonds     =               0.00helix dipole    =               -0.40loop_entropy    =               0.00cis_bond        =               4.50disulfide       =               -13.95kn electrostatic=               -0.41partial covalent interactions = 0.00Energy_Ionisation =             1.14Entropy Complex =               0.00-----------------------------------------------------------Total          =   -93.01FINISHING STABILITY ANALYSIS OPTIONYour file run OKEnd time of FoldX: Sat Jul  6 17:23:18 2024Total time spend: 0.85 seconds.</pre><h2 id="Mutation-Energy-Change-Calculation">Mutation Energy Change Calculation</h2><p>With FoldX, you can predicted the mutations effects when you have the wild type structure. The command <code>BuildModel</code> could generate the new pdb structure with the ‘mutate_file’ you write. Here is an example of <code>mutate_file</code>:</p><pre>AA4P,FD4P;AA4F,QD4F;</pre><p>In this example, it would generate 2 new structures. For the first one, in chain A, the mutation is A4P, in chain D, the mutation is F4P. The “;” means the first mutate process is done. It would read the second line to create the another mutation file. You don’t need to calculate the mutation energy difference between before and after again. Because all they are saved in the file (*.fxout) as <code>tsv</code> format.</p><p>For running it, you just need to run like:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">FoldX --<span class="hljs-built_in">command</span>=BuildModel --pdb=protein_Repair.pdb --mutant-file=individual_list.txt<br></code></pre></td></tr></table></figure></div><p>An interesting thing is for the mutate-file, it has to start with “individual_list” or ‘mutate_file’ or you’ll get error. After the job is down, you’ll got 4 outputs: “Average_<em>.fxout&quot;, &quot;Dif_</em>.fxout”, “PdbList_<em>.fxout&quot;, and &quot;Raw_</em>.fxout”. Here is some details about those outputs:</p><ol><li><p><strong>Average_*.fxout</strong>:</p><ul><li>This file contains the average energy values from multiple runs of the BuildModel command. FoldX often performs multiple simulations to generate an average value to improve reliability and account for variability in the calculations.</li><li>It includes averaged energy terms like van der Waals interactions, electrostatics, solvation, and total energy for the mutated model.</li></ul></li><li><p><strong>Dif_*.fxout</strong>:</p><ul><li>This file contains the differences in energy values between the wild-type and mutated proteins.</li><li>It shows the ΔΔG (difference in free energy change) due to the introduced mutation(s), which helps in understanding the stability change caused by the mutation.</li></ul></li><li><p><strong>PdbList_*.fxout</strong>:</p><ul><li>This file lists the PDB files generated during the mutation process.</li><li>It includes the names of the mutated PDB files that FoldX generated, which you can further analyze or visualize using molecular visualization tools.</li></ul></li><li><p><strong>Raw_*.fxout</strong>:</p><ul><li>This file contains the raw energy values for each individual run of the BuildModel command.</li><li>It provides detailed energy components for each simulation, such as van der Waals interactions, electrostatics, solvation, and other energy terms for the mutated model.</li></ul></li></ol><div class="admonition note"><p class="admonition-title">Notice</p><ol><li>For the <code>mutate_file</code>, you can't add any extra expressions like space in it.</li><li>According to the <a href="https://pdb101.rcsb.org/learn/guide-to-understanding-pdb-data/protein-primary-sequences">PDB document</a>, the SEQRES and ATOM records may include only a portion of the molecule. So if your sequence was extracted from the PDB file, the numbering of it may incorrect. For achieve the correct position, you may like to extract extract the sequence from cif format.</li></ol></div><h3 id="BuildModel-in-Action">BuildModel in Action</h3><p>Here, the test data is from Qi Wen Teo<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> CR9114 (<strong>4FQI</strong>) as example. I randomly choose 4FQI as the standard. In the paper, they mutated the resi through 93 to 102 (kabat numbering) which is 97 to 110. So, we could do it with a <code>mutate_file</code>. For FoldX, it only recognize the digital numbering. But in antibody (show below) sometimes was numbered by kabat numbering or something similar methods. So it may contain numbering like 100A, 100B, etc. They can’t recognized by FoldX and we need to renumbering them. Pymol is very complicated in this kind of task. But Biopython could handle it very well. You could using the script from <a href="https://github.com/Karobben/Bio_tools">Karobben/Bio_tools</a> with code: <code>python PDBreNumbering.py -i 4FQI_Repair.pdb -o renumbered.pdb</code></p><pre>ATOM   4942  CD1 TYR H 100     -17.095  54.149 -23.690  1.00 17.83      C    C  ATOM   4943  CD2 TYR H 100     -15.503  54.016 -21.927  1.00 17.51      C    C  ATOM   4944  CE1 TYR H 100     -16.055  54.246 -24.606  1.00 18.76      C    C  ATOM   4945  CE2 TYR H 100     -14.431  54.115 -22.848  1.00 20.56      C    C  ATOM   4946  CZ  TYR H 100     -14.740  54.248 -24.173  1.00 21.30      C    C  ATOM   4947  OH  TYR H 100     -13.735  54.378 -25.146  1.00 22.06      C    O  ATOM   4948  N   TYR H 100A    -19.396  56.114 -18.971  1.00 19.55      C    N  ATOM   4949  CA  TYR H 100A    -20.277  56.072 -17.797  1.00 21.40      C    C  ATOM   4950  C   TYR H 100A    -21.609  56.741 -18.067  1.00 25.46      C    C  ATOM   4951  O   TYR H 100A    -22.655  56.288 -17.527  1.00 25.98      C    O  ATOM   4952  CB  TYR H 100A    -19.611  56.821 -16.587  1.00 18.28      C    C  ATOM   4953  CG  TYR H 100A    -18.192  56.412 -16.276  1.00 19.12      C    C  ATOM   4954  CD1 TYR H 100A    -17.753  55.092 -16.396  1.00 21.46      C    C  </pre><p>Script to create the <code>mutate_file</code>. In this script, the target region is from number 97-110 and the sequence is “ARHGNYYYYSGMDV”.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">WT = <span class="hljs-built_in">list</span>(<span class="hljs-string">&quot;ARHGNYYYYSGMDV&quot;</span>)<br>All20 = <span class="hljs-built_in">list</span>(<span class="hljs-string">&quot;ARNDCEQGHILKMFPSTWYV&quot;</span>)<br>Num = <span class="hljs-number">96</span><br>sublist = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> WT:<br>    Num += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> ii <span class="hljs-keyword">in</span> All20:<br>        <span class="hljs-keyword">if</span> i != ii:<br>             sublist += [<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;i&#125;</span>H<span class="hljs-subst">&#123;Num&#125;</span><span class="hljs-subst">&#123;ii&#125;</span>;&quot;</span>]<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;individual_list.txt&quot;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> F:<br>    F.write(<span class="hljs-string">&quot;\n&quot;</span>.join(sublist))<br></code></pre></td></tr></table></figure></div><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># repair the PDB</span><br>FoldX --<span class="hljs-built_in">command</span>=RepairPDB --pdb=4FQI.pdb<br><span class="hljs-comment"># renumbering the resi for FoldX</span><br>python PDBreNumbering.py -i 4FQI_Repair.pdb -o renumbered.pdb<br><span class="hljs-comment"># calculate the results </span><br>FoldX --<span class="hljs-built_in">command</span>=BuildModel --pdb=renumbered.pdb --mutant-file=individual_list.txt<br></code></pre></td></tr></table></figure></div><p>After that, the result is saved in the file <code>Raw_renumbered.fxout</code>. The table was started at line 9. We could use R to sorting and compare the experiment result. For the experiment result, you can download from <a href="https://github.com/nicwulab/CR9114_LC_CDRH3_screen/blob/main/result/CDRH3_KD_table_summary.csv">nicwulab/CR9114_LC_CDRH3_screen</a></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(ggplot2)<br>library(reshape2)<br>library(stringr)<br><br>TB &lt;- read.csv(<span class="hljs-string">&#x27;Raw_renumbered.fxout&#x27;</span>, skip = <span class="hljs-number">8</span>, sep = <span class="hljs-string">&#x27;\t&#x27;</span>)<br>TB$Type &lt;- <span class="hljs-string">&quot;Mute&quot;</span><br>TB$Type[grep(<span class="hljs-string">&quot;WT_&quot;</span>, TB$Pdb)] &lt;- <span class="hljs-string">&quot;WT&quot;</span><br>TB &lt;- TB[<span class="hljs-built_in">c</span>(<span class="hljs-string">&#x27;Pdb&#x27;</span>, <span class="hljs-string">&#x27;total.energy&#x27;</span>, <span class="hljs-string">&#x27;Type&#x27;</span>)]<br>TB$Pdb &lt;- str_remove(TB$Pdb, <span class="hljs-string">&quot;WT_&quot;</span>)<br>TBM &lt;- reshape(TB, idvar = <span class="hljs-string">&#x27;Pdb&#x27;</span>, timevar = <span class="hljs-string">&#x27;Type&#x27;</span>, direction = <span class="hljs-string">&#x27;wide&#x27;</span>)<br>colnames(TBM) &lt;- str_remove(colnames(TBM), <span class="hljs-string">&#x27;total.energy.&#x27;</span>)<br><br>Anno &lt;- read.csv(<span class="hljs-string">&#x27;individual_list.txt&#x27;</span>, header = <span class="hljs-built_in">F</span>)<br>TBM$Anno &lt;- str_remove(Anno$V1, <span class="hljs-string">&quot;;&quot;</span>)<br>TBM$Diff = TBM$Mute - TBM$WT<br><br>library(scales)<br>library(readr)<br>library(tidyr)<br>library(dplyr)<br>library(gridExtra)<br><br>aa_level &lt;- rev(<span class="hljs-built_in">c</span>(<span class="hljs-string">&#x27;E&#x27;</span>,<span class="hljs-string">&#x27;D&#x27;</span>,<span class="hljs-string">&#x27;R&#x27;</span>,<span class="hljs-string">&#x27;K&#x27;</span>,<span class="hljs-string">&#x27;H&#x27;</span>,<span class="hljs-string">&#x27;Q&#x27;</span>,<span class="hljs-string">&#x27;N&#x27;</span>,<span class="hljs-string">&#x27;S&#x27;</span>,<span class="hljs-string">&#x27;T&#x27;</span>,<span class="hljs-string">&#x27;P&#x27;</span>,<span class="hljs-string">&#x27;G&#x27;</span>,<span class="hljs-string">&#x27;C&#x27;</span>,<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-string">&#x27;V&#x27;</span>,<span class="hljs-string">&#x27;I&#x27;</span>,<span class="hljs-string">&#x27;L&#x27;</span>,<span class="hljs-string">&#x27;M&#x27;</span>,<span class="hljs-string">&#x27;F&#x27;</span>,<span class="hljs-string">&#x27;Y&#x27;</span>,<span class="hljs-string">&#x27;W&#x27;</span>,<span class="hljs-string">&#x27;_&#x27;</span>))<br><br>df &lt;- read_csv(<span class="hljs-string">&#x27;CDRH3_KD_table_summary.csv&#x27;</span>) %&gt;%<br>  filter(grepl(<span class="hljs-string">&#x27;CR9114&#x27;</span>,ID)) %&gt;%<br>  mutate(log10_Kd=log10(Kd)) %&gt;%<br>  filter((log10_Kd &lt; -<span class="hljs-number">8</span> &amp; p.value &lt; <span class="hljs-number">0.2</span>) | (log10_Kd &gt;= -<span class="hljs-number">8</span>)) %&gt;%<br>  mutate(Mutation=gsub(<span class="hljs-string">&#x27;CR9114_&#x27;</span>,<span class="hljs-string">&quot;&quot;</span>,ID)) %&gt;%<br>  filter(Mutation != <span class="hljs-string">&#x27;WT&#x27;</span>) %&gt;%<br>  mutate(resi=str_sub(Mutation,<span class="hljs-number">1</span>,-<span class="hljs-number">2</span>)) %&gt;%<br>  mutate(aa=str_sub(Mutation,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>)) %&gt;%<br>  filter(aa %in% aa_level) %&gt;%<br>  mutate(aa=factor(aa,levels=aa_level)) %&gt;%<br>  complete(resi, aa) %&gt;%<br>  mutate(Pos=str_sub(resi,<span class="hljs-number">2</span>,-<span class="hljs-number">1</span>)) %&gt;%<br>  mutate(Pos=<span class="hljs-built_in">as.numeric</span>(<span class="hljs-built_in">as.character</span>(Pos))) %&gt;%<br>  arrange(Pos) %&gt;%<br>  mutate(resi=factor(resi,levels=unique(resi))) %&gt;%<br>  mutate(log10_Kd=case_when(str_sub(resi,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)==aa ~ log10(<span class="hljs-number">5.19e-10</span>), <span class="hljs-literal">TRUE</span> ~ log10_Kd)) %&gt;%<br>  mutate(Mutation=paste(resi,aa,sep=<span class="hljs-string">&#x27;&#x27;</span>)) %&gt;%<br>  select(Mutation, resi, Pos, aa, log10_Kd)<br><br>df$Pos = df$Pos + <span class="hljs-number">96</span><br>df$Anno &lt;- paste(gsub(<span class="hljs-string">&quot;[0-9]&quot;</span>, <span class="hljs-string">&quot;&quot;</span>, df$resi), df$Pos, df$aa, sep = <span class="hljs-string">&#x27;&#x27;</span>)<br><br>remove_second_letter &lt;- <span class="hljs-keyword">function</span>(x) &#123;<br>  paste0(substr(x, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), substr(x, <span class="hljs-number">3</span>, nchar(x)))<br>&#125;<br><br>TBM$Anno &lt;- sapply( TBM$Anno, remove_second_letter)<br>TBM$log10_K &lt;- df$log10_Kd[match(TBM$Anno, df$Anno)]<br>TBMF &lt;- TBM[!<span class="hljs-built_in">is.na</span>(TBM$log10_K),]<br><br>ggplot(TBMF, aes(Diff, log10_K )) + geom_point() + <br>  geom_smooth(method = <span class="hljs-string">&#x27;lm&#x27;</span>) + <br>  theme_bw()<br><br>ggplot(TBMF[TBMF$Diff &lt;= <span class="hljs-number">2</span>,], aes(Diff, log10_K )) + geom_point() + <br>  geom_smooth(method = <span class="hljs-string">&#x27;lm&#x27;</span>) +<br>  geom_vline( xintercept = <span class="hljs-number">0</span>, linetype = <span class="hljs-number">4</span>) + <br>  geom_hline( yintercept = -<span class="hljs-number">9.28</span>, linetype = <span class="hljs-number">4</span>) + <br>  theme_bw()<br></code></pre></td></tr></table></figure></div><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/zstiOTG.png" alt=""></td><td style="text-align:center"><img src="https://imgur.com/Ceu7diL.png" alt=""></td></tr></tbody></table><p>According to this plot, the correlation between experiments and the prediction is terrible. I think the main reason is because all those positions are located on CDRH3 region which not only they are random loop, but also the key site to determine the binding affinity of the antibody. So, the prediction result would be extrimly hard. But I think the result is not totally useless. At least when the Δ G of the complex predicted became more stable ($\Delta_ {mutate} - \Delta_ {wt} &lt; 0$), most of experiments results are very closing to the wild type.</p><table><thead><tr><th>Mute</th><th>WT</th><th>Anno</th><th>Diff</th><th>log10_K</th></tr></thead><tbody><tr><td>-111.068</td><td>-110.525</td><td>D109E</td><td>-0.543</td><td>-9.444906</td></tr><tr><td>-110.702</td><td>-110.512</td><td>D109Q</td><td>-0.190</td><td>-9.422508</td></tr><tr><td>-110.532</td><td>-110.525</td><td>D109C</td><td>-0.007</td><td>-9.343902</td></tr><tr><td>-111.447</td><td>-110.503</td><td>D109M</td><td>-0.944</td><td>-9.296702</td></tr><tr><td>-112.243</td><td>-110.512</td><td>G100M</td><td>-1.731</td><td>-9.222573</td></tr><tr><td>-111.185</td><td>-110.503</td><td>D109T</td><td>-0.682</td><td>-9.180450</td></tr><tr><td>-110.902</td><td>-110.857</td><td>S106M</td><td>-0.045</td><td>-9.170053</td></tr><tr><td>-111.330</td><td>-110.525</td><td>D109R</td><td>-0.805</td><td>-9.156767</td></tr><tr><td>-112.243</td><td>-110.504</td><td>D109Y</td><td>-1.739</td><td>-9.136677</td></tr><tr><td>-111.697</td><td>-110.893</td><td>G100I</td><td>0.804</td><td>-9.114074</td></tr><tr><td>-112.226</td><td>-110.512</td><td>G100K</td><td>-1.714</td><td>-9.100179</td></tr><tr><td>-111.195</td><td>-110.525</td><td>G100C</td><td>-0.670</td><td>-9.075721</td></tr><tr><td>-113.282</td><td>-110.512</td><td>G100R</td><td>-2.770</td><td>-9.057495</td></tr><tr><td>-111.521</td><td>-111.362</td><td>Y104F</td><td>-0.159</td><td>-9.040850</td></tr><tr><td>-110.971</td><td>-110.611</td><td>G107F</td><td>-0.360</td><td>-9.038579</td></tr><tr><td>-111.158</td><td>-110.503</td><td>D109F</td><td>-0.655</td><td>-9.028260</td></tr><tr><td>-110.824</td><td>-110.503</td><td>D109L</td><td>-0.321</td><td>-9.026410</td></tr><tr><td>-112.106</td><td>-110.820</td><td>G100N</td><td>-1.286</td><td>-8.995671</td></tr><tr><td>-111.164</td><td>-110.470</td><td>V110L</td><td>-0.694</td><td>-8.978111</td></tr><tr><td>-111.718</td><td>-110.818</td><td>G100H</td><td>-0.900</td><td>-8.869666</td></tr><tr><td>-111.044</td><td>-110.873</td><td>S106N</td><td>-0.171</td><td>-8.545155</td></tr><tr><td>-110.944</td><td>-110.837</td><td>N101H</td><td>-0.107</td><td>-8.423659</td></tr></tbody></table><h3 id="How-it-work">How it work?</h3><p>Here is the corrected version of your text:</p><p>According to the documentation: This is the workhorse of the FoldX mutation engine. This command ensures that whenever you are mutating a protein, you always move the same neighbors in the WT and in the mutant, producing for each mutant PDB a corresponding PDB for its WT. Each mutation will move different neighbors, and therefore you need different WT references.</p><p>From the experience above, I think it works under the assumption that the structure of the protein won’t change due to the point mutation. As shown below, even though the amino acid changed from <strong>Y</strong> to <strong>R</strong>, the position remains unchanged, and the RMSD is 0. So, I think it would be more reliable when this amino acid is in the alpha helix or beta sheet. When a point mutation happens in these regions, the rough structure remains relatively the same. However, when the mutation occurs in the loop region, the result would be less reliable. According to Yuan M, et al.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>, when the mutation only happens in the V gene of the antibody, the correct ratio was about 70%. But according to the test above, the result is inconsistent and lacks convinciveness.</p><p><img src="https://imgur.com/JpXnrlY.png" alt=""></p><h2 id="Interface-Analysis">Interface Analysis</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># after repair the PDB</span><br>FoldX --<span class="hljs-built_in">command</span>=AnalyseComplex --pdb=4FQI_Repair.pdb --analyseComplexChains=A,B,H,L<br><span class="hljs-comment"># output: Indiv_energies_4FQI_Repair_AC.fxout  Interaction_4FQI_Repair_AC.fxout  Interface_Residues_4FQI_Repair_AC.fxout  Summary_4FQI_Repair_AC.fxout</span><br></code></pre></td></tr></table></figure></div><ul><li><strong>Indiv_energies_4FQI_Repair_AC</strong>:<br>This file contains individual energy contributions of residues to the overall stability of the protein complex. To be notice, the total energy of the complex is not equals to the sum of individual total energy.</li><li><strong>Interaction_4FQI_Repair_AC</strong>:<br>It records detailed interaction between each chain pairs. I think <strong>Interaction Energy</strong> is one of most important results from it.</li><li><strong>Summary_4FQI_Repair_AC.fxout</strong>:<br>It contains only a few important columns from the <em>Interaction_4FQI_Repair_AC.fxout</em>.</li><li><strong>Interface_Residues_4FQI_Repair_AC.fxout</strong>:<br>It records all residues in the interface in a list.</li></ul><div class="admonition note"><p class="admonition-title">Notice</p><p>Before running <code>AnalyseComplex</code>, you should renumber the residues as well. Residue numbers like 100A and 100B in an antibody can both be shown as 100, leading to multiple entries like YH100 in the list. After renumbering the whole PDB with the script above, the results would become &quot;YH103, YH104, YH105&quot;.Additionally, the <code>Interface_residues</code> results may contain some unusual entries such as <strong>oA11</strong>. Technically, this means that in chain A, position 11, there is a non-typical residue, such as a Zn ion.</p></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p><a href="https://www.sciencedirect.com/science/article/pii/S2211124723014225">Teo Q W, Wang Y, Lv H, et al. Stringent and complex sequence constraints of an IGHV1-69 broadly neutralizing antibody to influenza HA stem[J]. Cell reports, 2023, 42(11).</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Yuan M, Feng Z, Lv H, et al. Widespread impact of immunoglobulin V-gene allelic polymorphisms on antibody reactivity[J]. Cell Reports, 2023, 42(10). <a href="#fnref2" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">FoldX</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/"/>
    
    
    <category term="Protein Structure" scheme="https://karobben.github.io/tags/Protein-Structure/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
  </entry>
  
  <entry>
    <title>Nanopre and PacBio based Genome Assembly</title>
    <link href="https://karobben.github.io/2024/06/30/Bioinfor/LongReads/"/>
    <id>https://karobben.github.io/2024/06/30/Bioinfor/LongReads/</id>
    <published>2024-06-30T20:50:52.000Z</published>
    <updated>2024-07-05T16:33:10.980Z</updated>
    
    <content type="html"><![CDATA[<p>Related Papers:</p><ul><li>Rayamajhi N, Cheng C H C, Catchen J M. Evaluating Illumina-, Nanopore-, and PacBio-based genome assembly strategies with the bald notothen, Trematomus borchgrevinki[J]. G3, 2022, 12(11): jkac192. <a href="https://academic.oup.com/g3journal/article/12/11/jkac192/6651842">Paper</a></li><li>van Rengs W M J, Schmidt M H W, Effgen S, et al. A chromosome scale tomato genome built from complementary PacBio and Nanopore sequences alone reveals extensive linkage drag during breeding[J]. The Plant Journal, 2022, 110(2): 572-588. <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/tpj.15690">Paper</a></li><li>Murigneux V, Rai S K, Furtado A, et al. Comparison of long-read methods for sequencing and assembly of a plant genome[J]. GigaScience, 2020, 9(12): giaa146. <a href="https://academic.oup.com/gigascience/article/9/12/giaa146/6042729">Paper</a></li></ul><h2 id="Evaluating-Illumina-Nanopore-and-PacBio-based-genome-assembly-strategies-with-the-bald-notothen-Trematomus-borchgrevinki">Evaluating Illumina-, Nanopore-, and PacBio-based genome assembly strategies with the bald notothen, Trematomus borchgrevinki</h2><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left">In this paper, they did long reads assembly and Long-reads, short-reads hybrid assembly comparing. The experiment organism is “Trematomus borchgrevinki” (<strong>fish</strong>), a cold specialized Antarctic notothenioid fish with an estimated genome size of 1.28 Gb</td><td style="text-align:center"><img src="https://www.researchgate.net/profile/Gregory-Sloop/publication/364608485/figure/fig6/AS:11431281091299426@1666380222030/Trematomus-bernacchii-Courtesy-of-Zureks-and-Wikimedia-Commons.png" alt=""><br><a href="https://www.researchgate.net/publication/364608485_The_Cardiovascular_System_of_Antarctic_Icefish_Appears_to_Have_Been_Designed_to_Utilize_Hemoglobinless_Blood?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Gregory Sloop</a></td></tr></tbody></table><p>Sequencing Size:</p><ul><li>Nanopore: 24.29 Gb</li><li>PacBio: 118.42 Gb</li></ul><p><strong>Hybrid</strong> assemblies can generate <mark>higher contiguity</mark> they tend to suffer from lower quality. <strong>long-read-only assemblies</strong> can be optimized for <mark>contiguity</mark> by subsampling length-restricted raw reads. Long-read contig assembly is the current <strong>best choice</strong> and that assemblies from phase I and phase II were of lower quality.</p><p>Strategies:</p><ul><li>Long-reads and short-reads hybrid: quickmerge<ol><li>Long-reads assembly independently: <code>Canu</code> and <code>WTDBG2</code> assembly, assessed with <code>QUAST</code></li><li>2 rounds of polishing with <code>Pilon</code>. (First round: SNPs adn indels, Second round: local reassembly)</li><li>Gap filling with <code>PBJELLY</code></li></ol></li><li>Long-reads only was assembly by variaties of tools. The yacrd (Marijon et al. 2020) it the tool to identify potential <strong>chimeric reads</strong><ol><li><code>WTDBG2</code> was used to do the assembly</li></ol></li></ul><p>For long-reads, comparing to short-reads assembled genome, it has high continuity but also more number of duplicated BUSCO genes. Chimeric reads are exist. In this paper, they also applied the subsampling to deleted chimeric reads. By cooperate with the limiting reads lengths, the PacBio reads assembly results could be improved. The number of contigs dropped from 10,848 to 4,409 with only 70 Gb of data (generated by sampling minimum and maximum read lengths of 10 and 40 kb)</p><p>In this paper, the data shows that the assembly results from ONT reads are not as good as those from PacBio reads. However, because they used very different methods for pre-processing reads and assembly, the results are somewhat incomparable. Therefore, we can only conclude that the PacBio pipeline is more advanced.</p><h2 id="Comparison-of-long-read-methods-for-sequencing-and-assembly-of-a-plant-genome">Comparison of long-read methods for sequencing and assembly of a plant genome</h2><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left">This paper targets <em><strong>Macadamia jansenii</strong></em>, a type of tree. The PacBio data surprised others because it has higher coverage and longer reads than the typical ONT data. Therefore, <strong>cross-comparison is meaningless</strong>. However, they assembled the genome using <strong>multiple tools</strong>, so the <strong>internal data-type comparison</strong> is still valuable.</td><td style="text-align:center"><img src="https://www.researchgate.net/profile/Ian-Cock/publication/264458556/figure/fig1/AS:392523655729155@1470596340564/Macadamia-integriflora-leaves-and-flowers-photographed-accessed-from-Wikipedia-Commons.png" alt="Macadamia integriflora"><a href="https://www.researchgate.net/publication/264458556_Evaluation_of_the_potential_of_Macadamia_integriflora_extracts_as_antibacterial_food_agents?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Ian Edwin Cock</a></td></tr></tbody></table><table><thead><tr><th>Category</th><th>ONT</th><th>PacBio</th><th>BGI</th></tr></thead><tbody><tr><td><strong>Mean Reads Length</strong></td><td>7,962</td><td>20,575</td><td>2 × 100</td></tr><tr><td><strong>Assembly</strong></td><td>Redbean v2.5, Flye v2.5, and Canu</td><td>Redbean v2.5, Flye v2.5, and Canu</td><td>SuperPlus v1.0, Supernova v2.1.1, TGS-GapCloser</td></tr><tr><td><strong>Data Size</strong></td><td>24.9 Gb</td><td>65.2 Gb</td><td>74.5 Gb</td></tr><tr><td><strong>Largest Contigs</strong></td><td>9,683,794</td><td>23,824,472</td><td>517,998</td></tr><tr><td><strong>Scaffold</strong></td><td>5,332</td><td>5,446</td><td>5,065</td></tr><tr><td><strong>Scaffold N50</strong></td><td>3.52</td><td>3.50</td><td>3.54</td></tr><tr><td><strong>Contigs</strong></td><td>6,022</td><td>5,717</td><td>19,954</td></tr><tr><td><strong>Contigs N50(M)</strong></td><td>1.04</td><td>1.60</td><td>0.036</td></tr><tr><td><strong>BUSCO</strong></td><td>1,963 (92.5)</td><td>1,983 (93.5)</td><td>1,873 (88.3)</td></tr></tbody></table><p>Hybrid assembly:</p><ol><li>MaSuRCA v3.3.3: Illumina + ONT/PacBio</li><li>Flye v2.5 to perform the final assembly</li></ol><p>Diploid de novo genome assembly: PacBio reads was performed with FALCON v1.3.0</p><p>Assembly Evaluation: QUAST v5.0.2;  publicly available reference genome of M. integrifolia v2 (Genbank accession: GCA_900631585.1); subjected to BUSCO v3.0.2 with the eudicotyledons_odb10 database (2,121 genes).</p><p>In this result, the PacBio sequences dominate everything. This is because ultra-long ONT reads were not used here. Consequently, not only the length of the reads but also the accuracy and coverage of the ONT reads are lower than those of the PacBio. This comparison is extremely uneven. By <strong>comparing different long-reads assembly tools</strong> (Redbean, Flye, Falcon, Canu, Raven), the <mark>Rave</mark> is the best for both PacBio and ONT data. An interesting thing is, according to the paper, Rave supports the GPU-accelerate. But in this research, they only given 12 threads for Rave though, technically, we could give more than 1,000 of threads if we have a professional GPU.</p><h2 id="A-chromosome-scale-tomato-genome-built-from-complementary-PacBio-and-Nanopore-sequences-alone-reveals-extensive-linkage-drag-during-breeding">A chromosome scale tomato genome built from complementary PacBio and Nanopore sequences alone reveals extensive linkage drag during breeding</h2><pre class="mermaid">graph LR;　　Nanopore-->|NECAT|Assembly1;　　PacBio-->|Hifiasm|Assembly2;    Assembly1-->|quickmerge| Sinlge;    Assembly2-->|quickmerge| Sinlge</pre><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left">In this research, they target into the cultivated <strong>tomato</strong> (Solanum lycopersicum). They applied PacBio HiFi and ONT Nanopore sequencing to develop <strong>independent</strong>. After then, they <strong>merged the HiFi and ONT assemblies</strong> to generate a long-read-only assembly where all 12 chromosomes were represented as 12 contiguous sequences (N50 = 68.5 Mbp).</td><td style="text-align:center"><img src="https://www.researchgate.net/profile/Gbenga-Orunmolase/publication/371350058/figure/fig1/AS:11431281165942236@1686127227760/Tomato-Solanum-lycopersicum.jpg" alt=""><br><a href="https://www.researchgate.net/publication/371350058_MICROORGANISMS_ASSOCIATED_WITH_SOFT_ROT_OF_TOMATOES?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Gbenga Emmanuel Orunmolase</a></td></tr></tbody></table><table><thead><tr><th></th><th>ONT</th><th>PacBio HiFi</th></tr></thead><tbody><tr><td><strong>Data Size</strong></td><td>100~ Gb (x2)</td><td>20~ Gb (x2)</td></tr><tr><td><strong>Assembled Contigs</strong></td><td>100~300</td><td>700~2000</td></tr><tr><td><strong>Computational Costs</strong></td><td>3 days with 256 threads (NECAT)</td><td>2 hours with 128 threads (Hifiasm)</td></tr></tbody></table><p>Although the <strong>ONT has fewer contigs</strong>, it has a lower BUSCO (complete) percentage due to uncorrectable base errors. For the <mark>saturating test</mark>, they found that for PacBio HiFi reads, <strong>20 Gb</strong> would be enough to finish a good assembly with <strong>Hifiasm</strong>. For longer ONT reads, <strong>50 Gb</strong> could do a similar job with <strong>NECAT</strong>. After that, they conducted the <mark>merge test</mark> because they found partial complementarity of the assemblies as the breakpoints were different. After merging the two results, they obtained 12 super contigs, which correspond to the 12 chromosomes. Along with these 12 super contigs, they also obtained 54 contigs that could not be assembled into the 12 chromosomes; these could be chloroplast, mitochondrial, rDNA, and satellite repeat-derived sequences.</p><h3 id="MbTMV-assembly-pipeline-Merge-ONT-and-PacBio-results">MbTMV assembly pipeline (Merge ONT and PacBio results)</h3><ol><li>Assembly result polishing</li><li>nucmer (part of mummer v.4.0.0rc1) with the -l parameter to prevent invalid contig links</li><li>quickmerge<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> was used to merge 2 assemblies with the parameter -c 7.0.</li></ol><p>A very interesting thing is they use a customized script to convert the Salsa2 output to Hi-C file and plot the contact plot with jucibox</p><hr><p>A recent comparison pointed out that PacBio HiFi reads tend to lead to better assembly of the barley (Hordeum vulgare) genome than ONT (Mascher et al., 2021)</p><style>pre {//  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Chakraborty M, Baldwin-Brown J G, Long A D, et al. Contiguous and accurate de novo assembly of metazoan genomes with modest long read coverage[J]. Nucleic acids research, 2016, 44(19): e147-e147. <a href="https://github.com/mahulchak/quickmerge">GitHub</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">Nanopre and PacBio based Genome Assembly</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/WGS/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="Genome" scheme="https://karobben.github.io/tags/Genome/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/tags/WGS/"/>
    
  </entry>
  
  <entry>
    <title>Juicer: a One-Click System for Analyzing Loop-Resolution Hi-C Experiments</title>
    <link href="https://karobben.github.io/2024/06/27/Bioinfor/juicer/"/>
    <id>https://karobben.github.io/2024/06/27/Bioinfor/juicer/</id>
    <published>2024-06-27T20:55:58.000Z</published>
    <updated>2024-07-05T19:41:51.202Z</updated>
    
    <content type="html"><![CDATA[<p>Prerequisite :</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">conda install bwa               <span class="hljs-comment"># for short reads alignment</span><br>conda install samtools          <span class="hljs-comment"># for reading the align results </span><br></code></pre></td></tr></table></figure></div><p>Resources:</p><ul><li>Paper: Durand NC, Shamim MS, Machol I, Rao SSP, Huntley MH, Lander ES, et al. Juicer Provides a One-Click System for Analyzing Loop-Resolution Hi-C Experiments. Cell Syst. 2016;3:95–8.</li><li>GitHub Source code: <a href="https://github.com/aidenlab/juicer">aidenlab/juicer</a></li><li>Example Pipeline: <a href="https://github.com/ENCODE-DCC/hic-pipeline">ENCODE-DCC/hic-pipeline</a></li><li>Forums: <a href="https://groups.google.com/g/3d-genomics">3d-genomics; google</a>. They suggest to talk and ask on the google group rather than the github issue because you could got faster responds there.</li></ul><p>For working on <strong>hic-pipeline</strong>, if you want to run it in local machine, make sure that <code>docker</code> is installed. I don’t have docker installed, so, I’ll giving this try up.</p><p>When you got the mistake and want run again, make sure remove those directories first.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">rm -rf /home/wenkanl2/Tomas/20240614_DuckGenome/myJuicerDir/aligned<br></code></pre></td></tr></table></figure></div><h2 id="Juicer-in-Action">Juicer in Action</h2><p>According to the <a href="https://github.com/aidenlab/juicer/wiki/Usage">documentation</a>, there are 5 steps for running this juicer:</p><ol><li>Download genome fasta file, put in references folder</li><li>Run <code>bwa index</code> on the fasta file</li><li>At the same time, run <a href="https://github.com/theaidenlab/juicer/blob/master/misc/generate_site_positions.py">generate_site_positions.py</a> on the fasta file + your restriction enzyme (see <a href="https://github.com/theaidenlab/juicer/wiki/Pre#restriction-site-file-format">this site about the restriction site file format</a>)</li><li>Once generate_site_positions is done, run <code>awk 'BEGIN&#123;OFS="\t"&#125;&#123;print $1, $NF&#125;' mygenome_myenzyme.txt &gt; mygenome.chrom.sizes</code> (where mygenome is your genome, like hg19, and myenzyme is your enzyme, like MboI)</li><li>Run juicer.sh with the flags <code>-z &lt;path to genome fasta file&gt;</code>, <code>-p &lt;path to mygenome.chrom.sizes&gt;</code>, and  <code>-y &lt;path to  mygenome_myenzyme.txt&gt;</code></li></ol><h3 id="1-Prepare-Your-Data">1. Prepare Your Data</h3><p>The data download from NCBI is not applicable for this pipeline. We need to adapt the name of each reads. According to the error codes <test>(-: Aligning files matching <em>/myJuicerDir/fastq/</em>_R*.fastq*</test>, we could know that the name of the reads should be <code>*_R*.fastq*</code>. Specified, according to the test data, the name of the paired ends reads should be: <code>*_R1*.fastq*</code> and <code>*_R2*.fastq</code>. So, make sure you have the correct name for each of reads.</p><h3 id="3-Generate-Restriction-Site">3. Generate Restriction Site</h3><p>It seams like you’d like to naming your ref genome first. For example, it automatically supplies the <strong>hg19</strong> and <strong>hg38</strong>. If you list the <code>restriction_site</code> directory, it has <code>hg19_MboI.txt</code></p><p>Before running the pipeline, we need to ready the <code>restriction_site</code> file, too. Here is a script from juicer to help us to generate it: <code>misc/generate_site_positions.py</code>. It works as below. To be notice, the helpers said that <test>Usage: ./generate_site_positions.py <restriction enzyme> <genome> [location]</test>. But the genome here means the name of the genome. In the example, I give it <em><strong>ZJU1.0</strong></em>. The third parameter <code>[location]</code> is the location of the genome fasta file. With the code below, they would output the file <code>ZJU1.0_HindIII.txt</code></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">python generate_site_positions.py HindIII ZJU1.0 GCF_015476345.1_ZJU1.0_genomic.fna<br>mv ZJU1.0_HindIII.txt ../restriction_sites/<br></code></pre></td></tr></table></figure></div><p>Here is the few supported name of restriction enzymes:</p><pre>    'HindIII'     : 'AAGCTT',    'DpnII'       : 'GATC',    'MboI'        : 'GATC',    'Sau3AI'      : 'GATC',    'Arima'       : [ 'GATC', 'GANTC' ],</pre><h3 id="4-Create-Chromosome-Size-File">4. Create Chromosome Size File</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;BEGIN&#123;OFS=&quot;\t&quot;&#125;&#123;print $1, $NF&#125;&#x27;</span> restriction_sites/ZJU1.0_HindIII.txt &gt; ZJU1.0.chrom.sizes<br></code></pre></td></tr></table></figure></div><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># HiCCUPS</span><br>scripts/common/juicer_tools hiccups --ignore_sparsity aligned/inter_30.hic aligned/inter_30_loops<br><span class="hljs-comment"># APA: </span><br>scripts/common/juicer_tools apa aligned/inter_30.hic aligned/inter_30_loops apa_results<br></code></pre></td></tr></table></figure></div><p>In the test data, it generally takes 90GB RAM and 7 GB of GPU RAM</p><h3 id="5-Run-with-the-New-Parameters">5. Run with the New Parameters</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">&lt;myJuicerDir&gt;/scripts/juicer.sh -D &lt;myJuicerDir&gt; <br></code></pre></td></tr></table></figure></div><h3 id="Result">Result</h3><p>I didn’t processing the Juicer successfully yet. It was always exit at post data processing. I get the aligned result successfully. But it seems like failed to find the loop and get the <code>apa_resutls</code>.</p><p>So, according to the ChatGPT4o, we expected to get the results below after juicer:</p><ol><li>Contact Maps: These are heatmap-like visualizations showing the frequency of interactions between different regions of the genome.</li><li>.hic Files: The primary output format of Juicer, .hic files contain the processed Hi-C data, which can be visualized using tools like Juicebox.</li><li>Statistics and Quality Metrics; Genome-Wide Interaction Profiles; Contact Frequency Plots; and Visualizations in Juicebox</li></ol><p>In the <code>aligned</code> directory, we got 2 <code>.hic</code> file, one is <code>inter_30.hic</code>, another is <code>inter.hic</code>. According to ChatGPT4o, <code>inter.hic</code> typically contains the raw or minimally processed interaction data. <code>inter_30.hic</code> contains interaction data that has been <strong>normalized</strong> and possibly <strong>filtered</strong> to remove noise and low-quality interactions. The “30” in the name usually refers to a specific bin size (e.g., 30 kb). And typically, the <code>inter_30.hic</code> file or another similarly named file with a specific bin size (e.g., <code>inter_5.hic</code>, <code>inter_10.hic</code>) is considered the final, high-quality result suitable for detailed analysis.</p><h2 id="Troubleshooting">Troubleshooting</h2><pre>HiCCUPS:GPUs are not installed so HiCCUPs cannot be run(-: Postprocessing successfully completed, maps too sparse to annotate or GPUs unavailable (-:***! Error! Either inter.hic or inter_30.hic were not createdEither inter.hic or inter_30.hic were not created.  Check  for results</pre><p>Check if cuda is installed appropriate. If so, Check if it is in your working environment.</p><p>How to add it in your working environment: (edit your <code>~/.bashrc</code> or <code>~/.zshrc</code> file)</p><pre>export PATH="/usr/local/cuda-8.0/bin:$PATH"export LD_LIBRARY_PATH="/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH"</pre><p>Or you could add the <code>--cpu</code> flag on file <code>scripts/common/juicer_postprocessing.sh</code></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="DIFF"><figure class="iseeu highlight /diff"><table><tr><td class="code"><pre><code class="hljs diff"><span class="hljs-deletion">- if hash nvcc 2&gt;/dev/null</span><br><span class="hljs-deletion">- then</span><br><span class="hljs-deletion">-    $&#123;juicer_tools_path&#125; hiccups $&#123;hic_file_path&#125; $&#123;hic_file_path%.*&#125;&quot;_loops&quot;</span><br><span class="hljs-addition">+    $&#123;juicer_tools_path&#125; hiccups --cpu $&#123;hic_file_path&#125; $&#123;hic_file_path%.*&#125;&quot;_loops&quot;</span><br><span class="hljs-deletion">-    if [ $? -ne 0 ]; then</span><br><span class="hljs-deletion">-echo &quot;***! Problem while running HiCCUPS&quot;;</span><br><span class="hljs-deletion">-exit 1</span><br><span class="hljs-deletion">-    fi</span><br><span class="hljs-deletion">-else</span><br><span class="hljs-deletion">-    echo &quot;GPUs are not installed so HiCCUPs cannot be run&quot;;</span><br><span class="hljs-deletion">-fi</span><br></code></pre></td></tr></table></figure></div><p>When the <code>if hash nvcc 2&gt;/dev/null</code> detected that the <code>nvcc</code> doesn’t in the environment, it would exit. So, you may like to delete the entail if statement.</p><pre>HiCCUPS:Picked up _JAVA_OPTIONS: -Xmx150000m -Xms150000mReading file: /home/wenkanl2/Tomas/20240614_DuckGenome/myJuicerDir/aligned/inter_30.hicNo valid configurations specified, using default settingsWarning Hi-C map is too sparse to find many loops via HiCCUPS.Exiting. To disable sparsity check, use the --ignore_sparsity flag.</pre><p>As it suggests, you need to add the <code>--ignore_sparsity</code> flag. But, again, you can only make this change by alter <code>scripts/common/juicer_postprocessing.sh</code></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="DIFF"><figure class="iseeu highlight /diff"><table><tr><td class="code"><pre><code class="hljs diff">if hash nvcc 2&gt;/dev/null<br>then<br><span class="hljs-deletion">-    $&#123;juicer_tools_path&#125; hiccups $&#123;hic_file_path&#125; $&#123;hic_file_path%.*&#125;&quot;_loops&quot;</span><br><span class="hljs-addition">+    $&#123;juicer_tools_path&#125; hiccups  --ignore_sparsity $&#123;hic_file_path&#125; $&#123;hic_file_path%.*&#125;&quot;_loops&quot;</span><br>    if [ $? -ne 0 ]; then<br>echo &quot;***! Problem while running HiCCUPS&quot;;<br>exit 1<br>    fi<br>else<br>    echo &quot;GPUs are not installed so HiCCUPs cannot be run&quot;;<br>fi<br></code></pre></td></tr></table></figure></div><p>0 loops</p><pre>100% 0 loops written to file: ...HiCCUPS complete</pre><p>According to this <a href="https://groups.google.com/g/3d-genomics/c/9f5UUhuS8O4/m/RTE1YVTKAgAJ">post answer by Neva Durand</a>, it could be the data is too sparse.</p><blockquote><p>Yes, it’s an order of magnitude too few reads to find loops. You need to do deeper sequencing  / more replicates and then combine them. You need at least 1 billion reads. Otherwise your experiments simply don’t have the depth to determine loops (with any algorithm).</p></blockquote><pre>Not including fragment mapError while reading graphs file: java.io.FileNotFoundException: /home/wenkanl2/Tomas/20240614_DuckGenome/myJuicerDir/aligned/inter_30_hists.m (No such file or directory)Start preprocessWriting headerWriting bodyjava.lang.RuntimeException: No reads in Hi-C contact matrices. This could be because the MAPQ filter is set too high (-q) or because all reads map to the same fragment.at juicebox.tools.utils.original.Preprocessor$MatrixZoomDataPP.mergeAndWriteBlocks(Preprocessor.java:1650)at juicebox.tools.utils.original.Preprocessor$MatrixZoomDataPP.access$000(Preprocessor.java:1419)at juicebox.tools.utils.original.Preprocessor.writeMatrix(Preprocessor.java:832)at juicebox.tools.utils.original.Preprocessor.writeBody(Preprocessor.java:582)at juicebox.tools.utils.original.Preprocessor.preprocess(Preprocessor.java:346)at juicebox.tools.clt.old.PreProcessing.run(PreProcessing.java:116)at juicebox.tools.HiCTools.main(HiCTools.java:96)real2m7.365suser0m33.647ssys0m49.450sPicked up _JAVA_OPTIONS: -Xmx150000m -Xms150000mError reading datasetnulljava.io.EOFExceptionat htsjdk.tribble.util.LittleEndianInputStream.readFully(LittleEndianInputStream.java:138)at htsjdk.tribble.util.LittleEndianInputStream.readLong(LittleEndianInputStream.java:80)at htsjdk.tribble.util.LittleEndianInputStream.readDouble(LittleEndianInputStream.java:100)at juicebox.data.DatasetReaderV2.readFooter(DatasetReaderV2.java:470)at juicebox.data.DatasetReaderV2.read(DatasetReaderV2.java:235)at juicebox.tools.utils.original.NormalizationVectorUpdater.updateHicFile(NormalizationVectorUpdater.java:78)at juicebox.tools.clt.old.AddNorm.run(AddNorm.java:84)at juicebox.tools.HiCTools.main(HiCTools.java:96)real0m0.706suser0m1.229ssys0m0.399s/home/wenkanl2/Tomas/20240614_DuckGenome/myJuicerDir/scripts/common/juicer_postprocessing.sh: option requires an argument -- gUsage: /home/wenkanl2/Tomas/20240614_DuckGenome/myJuicerDir/scripts/common/juicer_postprocessing.sh [-h] -j <juicer_tools_file_path> -i <hic_file_path> -m <bed_file_dir> -g <genome ID>***! Error! Either inter.hic or inter_30.hic were not createdEither inter.hic or inter_30.hic were not created.  Check  for results</pre><h2 id="Other-Pipelines-for-Hi-C-Data">Other Pipelines for Hi-C Data</h2><p><img src="https://s3.amazonaws.com/4dn-dcic-public/static-pages/hicpipeline.png" alt="© 4dn"></p><ul><li><a href="https://data.4dnucleome.org/resources/data-analysis/hi_c-processing-pipeline">Hi-C Processing Pipeline</a></li><li><a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0831-x">HiC-Pro: an optimized and flexible pipeline for Hi-C data processing</a></li><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4706059/">HiCUP: pipeline for mapping and processing Hi-C data</a></li><li><a href="https://www.bioinformatics.babraham.ac.uk/projects/hicup/">Babraham Bioinformatics: HiCUP (Hi-C User Pipeline)</a></li><li><a href="https://www.encodeproject.org/hic/">HiC Data Standards and Processing Pipeline</a></li><li><a href="https://nf-co.re/hic/2.1.0">nf-core/hic</a></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}test {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Juicer: a One-Click System for Analyzing Loop-Resolution Hi-C Experiments</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/WGS/"/>
    
    
    <category term="Software" scheme="https://karobben.github.io/tags/Software/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="Genome" scheme="https://karobben.github.io/tags/Genome/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/tags/WGS/"/>
    
  </entry>
  
  <entry>
    <title>NextDenovo: an efficient error correction and accurate assembly tool for noisy long reads</title>
    <link href="https://karobben.github.io/2024/06/26/Bioinfor/NextDenovo/"/>
    <id>https://karobben.github.io/2024/06/26/Bioinfor/NextDenovo/</id>
    <published>2024-06-26T16:11:48.000Z</published>
    <updated>2024-06-30T21:16:55.880Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="https://nextdenovo.readthedocs.io/en/latest/QSTART.html#quick-start">Quick Start</a></li><li><a href="https://nextdenovo.readthedocs.io/en/latest/TEST1.html">Tutorial</a></li><li>Other related Long reads Assembly Tools, check the end of the post, for example: Falcon (Chin et al. 2016), Canu (Koren et al. 2017), WTDBG2 (Ruan and Li 2020), or Flye (Kolmogorov et al. 2019)</li></ul><h2 id="NextDenovo">NextDenovo</h2><p>Paper: <a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-024-03252-4">Hu J, Wang Z, Sun Z, et al. NextDenovo: an efficient error correction and accurate assembly tool for noisy long reads[J]. Genome Biology, 2024, 25(1): 107.</a></p><h3 id="Background">Background</h3><ul><li>Third-generation long-read:<br>PacBio has high-fidelity (HiFi) reads but they are relatively short (~ 15 kb). So, it is unable to span long tandem or highly homologous multi-copy repeats like centromeres. ONT sequencing can generate &gt; 100-kb “ultra-long” reads.</li><li>CTA and ATC:<br>“correction then assembly” (CTA, an assembler first corrects errors in the reads and then uses the corrected reads for assembly) and “assembly then correction” (ATC, an assembler uses error-prone reads to assemble the genome and then corrects errors in the assembled genome) are commonly used in assembly. CTA is much slower. But in terms of the assembly of segmental duplications/repeats, and especially for large plant genome assemblies, the CTA-based strategy usually has an enhanced ability to distinguish different gene copies and produce more accurate and continuous assemblies. <mark>NextDenovo is the tool of CTA-based assembly tool</mark></li></ul><h3 id="Steps">Steps</h3><ol><li><p>Detecting Overlapping Reads</p><ul><li><strong>Initial Detection</strong>: Detects overlapping reads (Fig. 1A).</li><li><strong>Filtering</strong>: Filters out alignments caused by repeats.</li><li><strong>Splitting</strong>: Splits chimeric seeds based on overlapping depth (Fig. 1B).</li></ul></li><li><p>Rough Correction with KSC Algorithm</p><ul><li><strong>Algorithm Used</strong>: Kmer score chain (KSC) algorithm, used in NextPolish [19], for initial rough correction (Fig. 1C).</li></ul></li><li><p>Handling Repeated Regions</p><ul><li><strong>Detection of Low-Score Regions (LSRs)</strong>: Uses a heuristic algorithm during traceback within the KSC algorithm.</li><li><strong>Accurate Correction</strong>:<ul><li>Combines partial order alignment (POA) [20] and KSC.</li><li>Collects subsections spanning LSRs and generates kmer sets at flanking sequences.</li><li>Filters subsections with lower kmer scores.</li><li>Creates pseudo-LSR seeds from top-ranked subsections using a greedy POA consensus algorithm.</li><li>Maps and corrects pseudo-LSR seeds multiple times for accuracy.</li><li>Integrates corrected LSRs back into the primary corrected seed (Fig. 1D).</li></ul></li></ul></li><li><p>Pairwise Overlapping and Dovetail Alignments</p><ul><li><strong>Two Rounds of Overlapping</strong>:<ul><li><strong>First Round</strong>: Uses rapid detection parameters.</li><li><strong>Second Round</strong>: Applies rigorous parameters for accurate alignments.</li></ul></li><li><strong>Graph Construction</strong>:<ul><li>Constructs a directed string graph.</li><li>Removes transitive edges using the “best overlap graph” (BOG) algorithm.</li><li>For repeat nodes, edges are only removed if below specific thresholds to maintain connectivity.</li><li>Removes tips and resolves bubbles.</li></ul></li></ul></li><li><p>Progressive Graph Cleaning</p><ul><li><strong>Simplifying Subgraphs</strong>:<ul><li>Uses a progressive cleaning strategy with increasingly stringent thresholds.</li><li>Breaks paths at nodes with multiple connections.</li><li>Outputs contigs from broken linear paths.</li></ul></li><li><strong>Reducing Misassemblies</strong>:<ul><li>Maps all seeds to contigs.</li><li>Breaks contigs at lower mapping depth regions (LDRs) (Fig. 1E).</li></ul></li></ul></li></ol><h3 id="Key-Algorithms-and-Techniques">Key Algorithms and Techniques</h3><ul><li><strong>KSC Algorithm</strong>: Used for initial rough correction and handling LSRs.</li><li><strong>Heuristic and Accurate Algorithms</strong>: For detecting and correcting LSRs.</li><li><strong>BOG Algorithm</strong>: For removing transitive edges in the graph.</li></ul><h2 id="Error-Correction">Error Correction</h2><p>NextDenovo is 1.63 times faster on real data compared to Consent, Canu, and Necat. As the read length increases, the time required for correction also increases. However, NextDenovo and Necat demonstrated only slight increases, while Canu exhibited a significant increase in processing time</p><h2 id="Installation">Installation</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Prerequirement</span><br>pip install paralleltask<br><br><span class="hljs-comment"># Install from github </span><br>git <span class="hljs-built_in">clone</span> git@github.com:Nextomics/NextDenovo.git<br><span class="hljs-built_in">cd</span> NextDenovo &amp;&amp; make<br><br><span class="hljs-comment"># Test</span><br>nextDenovo test_data/run.cfg<br></code></pre></td></tr></table></figure></div><h2 id="Run">Run</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">nextDenovo run.cfg<br></code></pre></td></tr></table></figure></div><p>Example of <code>run.cfg</code></p><pre>[General]job_type = local job_prefix = nextDenovotask = allrewrite = yesdeltmp = yesparallel_jobs = 22input_type = rawread_type = ont # clr, ont, hifiinput_fofn = input.fofninput_fofn2 = input2.fofnworkdir = HG002_NA24385_son_assemble[correct_option]read_cutoff = 1kgenome_size = 3g # estimated genome sizesort_options = -m 50g -t 30minimap2_options_raw = -t 8pa_correction = 5correction_options = -p 30[assemble_option]minimap2_options_cns = -t 8nextgraph_options = -a 1</pre><h2 id="Result">Result</h2><ul><li>Sequence: <code>01_rundir/03.ctg_graph/nd.asm.fasta</code></li><li>Statistics: <code>01_rundir/03.ctg_graph/nd.asm.fasta.stat</code></li></ul><p>Assembly data: 109G+98G<br>RAM utility: about 400GB. (You can also make it run with 64 RAM but it would takes much loger time to finish)<br>Time: about 2 days.</p><h2 id="After-Assembly">After Assembly</h2><h3 id="Compare-the-result-from-the-SKLA1-0-by-MUMmer">Compare the result from the SKLA1.0 by MUMmer</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">~/software/mummer-4.0.0rc1/mummer  -l 200 -threads 30 -qthreads 30  -mum -b -c data/NextDenovo_result.fa  data/SKLA1.0.chrall.fa &gt; result/NextDenovo_SKLA1.0.mums<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/nsV3ADI.png" alt="mummer"></p><p>According to this result, the first three chromosomes from bottom to top are chr1, chr2, and chr3. The x-axis, from left to right, is sorted by the length of the contigs. As we can see, the first contig represents the full length of chr3. Contigs 2, 3, and 7 represent chr1, while contigs 6, 8, and 9 are three pieces of chr2. Another very interesting result is that, except for chr2, both chr1 and chr3 are complemented and reversed.</p><h2 id="NextPolish">NextPolish</h2><p>NextPolish was also recomand. You can download and install by following the instruction form <a href="https://github.com/Nextomics/NextPolish">github</a>. But I am not that luck to install it in my Ubuntu server. It come with the error:</p><pre>gcc -g -Wall -Wno-unused-function -O2 -DHAVE_PTHREAD -DUSE_MALLOC_WRAPPERS bwashm.o bwase.o bwaseqio.o bwtgap.o bwtaln.o bamlite.o bwape.o kopen.o pemerge.o maxk.o bwtsw2_core.o bwtsw2_main.o bwtsw2_aux.o bwt_lite.o bwtsw2_chain.o fastmap.o bwtsw2_pair.o main.o -o bwa -L. -lbwa -lm -lz -lpthread -lrt/usr/bin/ld: ./libbwa.a(rope.o):/raid/home/wenkanl2/BioTools/NextPolish/util/bwa/rle.h:33: multiple definition of `rle_auxtab'; ./libbwa.a(bwtindex.o):/raid/home/wenkanl2/BioTools/NextPolish/util/bwa/rle.h:33: first defined here/usr/bin/ld: ./libbwa.a(rle.o):/raid/home/wenkanl2/BioTools/NextPolish/util/bwa/rle.h:33: multiple definition of `rle_auxtab'; ./libbwa.a(bwtindex.o):/raid/home/wenkanl2/BioTools/NextPolish/util/bwa/rle.h:33: first defined herecollect2: error: ld returned 1 exit statusmake[2]: *** [Makefile:30: bwa] Error 1make[2]: Leaving directory '/raid/home/wenkanl2/BioTools/NextPolish/util/bwa'make[1]: *** [Makefile:19: bwa_] Error 2make[1]: Leaving directory '/raid/home/wenkanl2/BioTools/NextPolish/util'make: *** [Makefile:18: all] Error 2</pre><p>So, I tied install it with bioconda:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">conda install NextPolish<br></code></pre></td></tr></table></figure></div><pre>nextpolish-1.4.1           |  py311h99925d8_3         1.7 MB  bioconda</pre><p>And them, it installed the version 1.4.1. Next, I tried the test: <code>nextPolish test_data/run.cfg</code> and it finished the test correctly:</p><pre>Type           Length (bp)            Count (#)N10                60501                   1N20                60501                   1N30                60501                   1N40                60501                   1N50                60501                   1N60                51048                   2N70                51048                   2N80                51048                   2N90                51048                   2Min.               51048                   -Max.               60501                   -Ave.               55774                   -Total             111549                   2</pre><h3 id="Options-for-NextPolish">Options for NextPolish</h3><p>[sgs_option]: Polishing using short reads only<br>[lgs_option]: Polishing using long reads only</p><h3 id="In-Action">In Action:</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># prepare the short reads information</span><br>ls data/WGS/*.fastq &gt; sgs.fofn<br><span class="hljs-comment"># Start running</span><br>nohup nextPolish Polish.cfg &gt; Polish.log &amp;<br></code></pre></td></tr></table></figure></div><p>cfg file for this experiment:</p><pre>[General]job_type = localjob_prefix = nextPolishtask = defaultrewrite = yesrerun = 3parallel_jobs = 20multithread_jobs = 20genome = result/NextDenovo_result.fagenome_size = autoworkdir = ./01_rundirpolish_options = -p {multithread_jobs}[sgs_option]sgs_fofn = ./sgs.fofnsgs_options = -max_depth 100</pre><p>In this config file, it given the number of parallel jobs as 20 and multithread jobs as 20, which means the max threads allocated would be 20*20 = 400. So,be sure about that you have that much of threads for calculation or it would make the whole processes slower than normal.</p><h2 id="Other-Long-Reads-Assembly-Tools">Other Long-Reads Assembly Tools</h2><h3 id="LongStitch-Enhancing-Genome-Assembly-with-Long-Reads">LongStitch: Enhancing Genome Assembly with Long Reads</h3><p>LongStitch<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> is a powerful computational tool designed to improve the quality of genome assemblies by utilizing long-read sequencing data. It addresses common issues in genome assembly, such as errors and gaps, that are often introduced during the assembly of short-read sequences.</p><p>It was published at 2021. Until the July 2024, it got 34 citations. It is an open source software and deposit in <a href="https://github.com/bcgsc/longstitch">GitHub</a> with 42 starts.</p><p>Key features of LongStitch include:</p><ul><li><strong>Error Correction</strong>: By aligning long reads to the existing genome assembly, LongStitch identifies and corrects misassemblies, leading to a more accurate genomic representation.</li><li><strong>Scaffolding</strong>: LongStitch leverages long reads to link contigs into scaffolds, significantly enhancing the continuity and completeness of the genome assembly.</li><li><strong>High-Quality Output</strong>: The resulting assemblies are more comprehensive and accurate, making them invaluable for further genomic analysis and research.</li></ul><p>LongStitch’s ability to handle repetitive regions and complex genomic structures makes it an essential tool for researchers aiming to achieve high-quality genome assemblies.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Coombe L, Li J X, Lo T, et al. LongStitch: high-quality genome assembly correction and scaffolding using long reads[J]. BMC bioinformatics, 2021, 22: 1-13. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">NextDenovo is a string graph-based de novo assembler for long reads (CLR, HiFi and ONT)</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="Genome" scheme="https://karobben.github.io/tags/Genome/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/tags/WGS/"/>
    
  </entry>
  
  <entry>
    <title>IgCaller</title>
    <link href="https://karobben.github.io/2024/06/24/Bioinfor/IgCaller/"/>
    <id>https://karobben.github.io/2024/06/24/Bioinfor/IgCaller/</id>
    <published>2024-06-24T21:45:27.000Z</published>
    <updated>2024-06-25T17:34:50.131Z</updated>
    
    <content type="html"><![CDATA[<p>IgCaller is used extensively in immunology research to study B-cell receptor diversity and antibody generation mechanisms. Clinically, it helps identify clonal B-cell expansions, monitor minimal residual disease in leukemias and lymphomas, and analyze antibody responses to vaccines. Additionally, it supports therapeutic antibody development by identifying candidate antibodies from strong immune responses.</p><p>It is an <a href="https://github.com/ferrannadeu/IgCaller">open-source</a> tool designed to study human B cell Ig gene rearrangements. According to the documentation, it only supports the human hg19 or hg38 genome as the input reference, so the application of this tool is limited to humans. It requires selecting specific areas of the genome.</p><p>I am currently working on nonhuman Ig. I may update with more details later when I work with human Ig.</p><p>The basic use only requires the short reads aligned BAM file:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">IgCaller -I /path/to/IgCaller/IgCaller_reference_files/ -V hg19 -C ensembl -T /path/to/bams/tumor.bam -N /path/to/bams/normal.bam -R /path/to/reference/genome_hg19.fa -o /path/to/IgCaller/outputs/<br></code></pre></td></tr></table></figure></div><ul><li>Output: IgCaller returns a set of tab-separated files:<ul><li>tumor_sample_output_filtered.tsv: High confidence rearrangements passing the defined filters.</li><li>tumor_sample_output_IGH.tsv: File containing all IGH rearrangements.</li><li>tumor_sample_output_IGK.tsv: File containing all IGK rearrangements.</li><li>tumor_sample_output_IGL.tsv: File containing all IGL rearrangements.</li><li>tumor_sample_output_class_switch.tsv: File containing all CSR rearrangements.</li><li>tumor_sample_output_oncogenic_IG_rearrangements.tsv: File containing all oncogenic IG rearrangements (translocations, deletions, inversions, and gains) identified genome-wide.</li></ul></li></ul><p>More details:<br>Nadeu, F., Mas-de-les-Valls, R., Navarro, A. et al. IgCaller for reconstructing immunoglobulin gene rearrangements and oncogenic translocations from whole-genome sequencing in lymphoid neoplasms. Nature Communications 11, 3390 (2020). <a href="https://doi.org/10.1038/s41467-020-17095-7">https://doi.org/10.1038/s41467-020-17095-7</a>.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Reconstructing immunoglobulin gene rearrangements and oncogenic translocations from WGS, WES, and capture NGS data</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="Genome" scheme="https://karobben.github.io/tags/Genome/"/>
    
    <category term="Antibody" scheme="https://karobben.github.io/tags/Antibody/"/>
    
    <category term="Immunology" scheme="https://karobben.github.io/tags/Immunology/"/>
    
  </entry>
  
  <entry>
    <title>MUMmer: Rapidly Genomes Alignment</title>
    <link href="https://karobben.github.io/2024/06/21/Bioinfor/MUMmer/"/>
    <id>https://karobben.github.io/2024/06/21/Bioinfor/MUMmer/</id>
    <published>2024-06-21T23:29:31.000Z</published>
    <updated>2024-06-24T21:55:49.415Z</updated>
    
    <content type="html"><![CDATA[<p>Installation:</p><p>In linux, you could simply install it by apt install mummer. The version of the mummer is around 3.1. The version from Bioconda is little bit new than the apt source but still kind of old. If you install teh MUMmer from those 2 source, you’ll meet error when the reads is too long.</p><h2 id="NUCmer">NUCmer:</h2><p>Test target: Chicken Genome (GRCg6a) and Duck Genome (SKLA1.0)<br>It could be RAM monster when you compare 2 Genome directly. Especially when you want to use the multiple threads, the RAM would be occupied very quick and the program would be killed.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">nucmer -maxmatch -c 100 -p output_prefix reference.fasta query.fasta<br>show-coords -rcl output_prefix.delta &gt; output_prefix.coords<br>cat output_prefix.coords<br>grep -Ev <span class="hljs-string">&quot;^$|=|\[&quot;</span> result/test.coords| grep <span class="hljs-string">&quot;^ &quot;</span>| awk <span class="hljs-string">&#x27;BEGIN &#123; OFS=&quot;\t&quot;; print &quot;Ref_Start&quot;, &quot;Ref_End&quot;, &quot;Query_Start&quot;, &quot;Query_End&quot;, &quot;Length1&quot;, &quot;Length2&quot;, &quot;Identity&quot;, &quot;Ref&quot;, &quot;Query&quot; &#125; &#123;OFS=&quot;\t&quot;;  print $1, $2, $4, $5, $7, $8, $10, $12, $13&#125;&#x27;</span> &gt; test.coords<br><br></code></pre></td></tr></table></figure></div><p>But the problem is there are no such parameter for NUCmer to limited the use of RAM. The only way to solving this problem is split the genome into single sequences and processing one by one.</p><pre>[1]    70706 killed     nucmer --maxmatch -c 100 -p result/Chicken-SKLA1.0</pre><p><img src="https://imgur.com/iXM6Kr2.png" alt="btop"></p><p>I was tried to extract the first chromosome (196,202,544 bp, 192M) from the Chicken align against the Duck genome (1.2 Gb) and it takes 41 GB RAM if you given only 1 thread. If you run it without given threads, it would run with 3 threads and the RAM would increased into 70 GB. So, it seams it is save to run with about 7 threads with this size of data. But after 1h 12min, it was killed because of the increased demand of RAM. If you use single thread, the RAM would increased in to 77 after about 1h and 12min. Finally, it takes 13h 3m.</p><h3 id="MUMmer">MUMmer</h3><p>MUMmer is focusing on the difference between the reference and the Subject. It output is very sample. It only contains the start, end, and the length of the reference. Based on this, we could know that they are forward or reverse-complemented. It doesn’t has the responded position for Subject chromosome. So, because of the low dimension information, it requires very low RAM and works very efficient. It could finishing calculation in a very short time.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">mummer -mum -b -c ref.fa sub.fa  &gt; ref.mums<br>mummerplot --postscript --prefix=ref_qry ref.mums<br>gnuplot ref_qry.gp<br></code></pre></td></tr></table></figure></div><p>For visualization, you could use the package provide tools. You could also convert it into tables and visualize it with your favorite tools. So, after convert the mummer result into <code>tsv</code> by a python script, we could visualize the result with ggplot.</p><p><img src="https://imgur.com/KcnMXqg.png" alt="mummer plots"></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">MUMmer is a system for rapidly aligning entire genomes</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="Genome" scheme="https://karobben.github.io/tags/Genome/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/tags/WGS/"/>
    
  </entry>
  
  <entry>
    <title>Whole Genome Sequencing (WGS)</title>
    <link href="https://karobben.github.io/2024/06/17/Bioinfor/wholegenomesequencing/"/>
    <id>https://karobben.github.io/2024/06/17/Bioinfor/wholegenomesequencing/</id>
    <published>2024-06-17T20:34:03.000Z</published>
    <updated>2024-07-17T15:02:46.483Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Whole-Genome-Sequencing-WGS">Whole Genome Sequencing (WGS)</h2><blockquote><p>Whole-genome sequencing (WGS) is a comprehensive method for analyzing entire genomes. Genomic information has been instrumental in identifying inherited disorders, characterizing the mutations that drive cancer progression, and tracking disease outbreaks. Rapidly dropping sequencing costs and the ability to produce large volumes of data with today’s sequencers make whole-genome sequencing a powerful tool for genomics research. (<a href="https://www.illumina.com/techniques/sequencing/dna-sequencing/whole-genome-sequencing.html">Illumina</a>)</p></blockquote><h2 id="Illumina-WGS">Illumina WGS</h2><p>KEY WHOLE-GENOME SEQUENCING METHODS</p><ul><li>Large whole-genome sequencing</li><li>Small whole-genome sequencing</li><li>De novo sequencing<ul><li>Targeting to species without reference genome</li></ul></li><li>Phased sequencing</li><li>Human whole-genome sequencing<ul><li>optimized for human</li></ul></li><li>Long-reads sequencing</li></ul><h2 id="Examples-from-Publications">Examples from Publications</h2><h3 id="SKLA1-0-Duck">SKLA1.0 (Duck)</h3><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left">A chromosome-scale Beijing duck assembly (SKLA1.0<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>) by integrating Nanopore, Bionano, and Hi-C data covers 40 chromosomes, improves the contig N50 of the previous duck assembly with highest contiguity (ZJU1.0) of more than a 5.79-fold and contains a complete genomic map of the MHC.</td><td style="text-align:center"><img src="https://www.researchgate.net/profile/Inma-Aznar/publication/51580858/figure/fig19/AS:669632922923023@1536664338755/Mallard-Anas-platyrhynchos-Photo-John-Carey.png" alt="Duck"><a href="https://www.researchgate.net/publication/51580858_A_review_of_Ireland's_waterbirds_with_emphasis_on_wintering_migrants_and_reference_to_H5N1_avian_influenza?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Olivia Crowe</a></td></tr></tbody></table><p>Species: Anas platyrhynchos (a native breed in China, using a hierarchical and hybrid approach)</p><p>Reads types: Nanopore, Bionano, and Hi-C data.</p><ul><li>71-fold normal and 24-fold ultra-long Nanopore reads</li><li>117-fold 150bp paired-end Illumina reads for polishing</li><li>216-fold optical map reads and 234-fold PE150 Hi-C</li></ul><p>Result:</p><ul><li>40 chromosomes, improves the contig N50 of the previous duck assembly with highest contiguity (ZJU1.0) of more than a 5.79-fold</li><li>a complete genomic map of the MHC</li></ul><p>Solved challenges:</p><ul><li>traditional assembly tools have not enabled proper genomic draft of highly repetitive and GC-rich sequences, such as the MHC</li></ul><p>Something I don’t understand:</p><ul><li>C18 Duck?</li><li>heterozygosity estimation: why they do it? How could it help on the genome assembly?</li><li>What is BUSCO score?</li></ul><h4 id="Steps-for-Genome-assembly">Steps for Genome assembly:</h4><ol><li><strong>Estimate Genome Heterozygosity</strong><ul><li>Before starting the assembly, the genome heterozygosity of the C18 duck was estimated. The heterozygosity was found to be as low as 0.58% (Additional file 1: Table S1 and Additional file 2: Fig. S1-S3).</li></ul></li><li><strong>Assemble Genome with <mark>Nanopore Reads</mark></strong><ul><li>Using 71-fold normal and 24-fold ultra-long Nanopore reads, the duck genome was assembled into 151 contigs covering a total length of 1.22 Gb with a contig N50 of 32.81 Mb (Additional file 1: Table S2-S3).</li><li><a href="https://github.com/Nextomics/NextDenovo"><strong>NextDenovo</strong></a>: Clean and assembly</li></ul></li><li><strong>Polish Contigs with <mark>Illumina Reads</mark></strong><ul><li>The 151 contigs were then polished with 912 million 150-bp Illumina pair-end reads, corrected, and integrated with high-quality optical maps (Additional file 1: Table S4-S5). This effort generated 69 scaffolds with a scaffold N50 of 72.53 Mb (Additional file 1: Table S6).</li><li><strong>Nextpolish-1.2.3</strong><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>: polished three rounds</li></ul></li><li><strong>Use <mark>Hi-C Data</mark> for Scaffold Ordering</strong><ul><li>A total of 274 Gb PE150 Hi-C data was used to order and orient the duck scaffolds, correct mis-joined sections, and merge overlaps, resulting in 40 super-scaffolds (Additional file 1: Table S7).</li><li><strong>Trimmomatic-0.36</strong><sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>; <strong>Juicer software-1.5</strong><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>; <strong>3d-DNA package-180922</strong><sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>; <strong>Juicebox-1.13.01</strong><sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup></li></ul></li><li><strong>Perform Gap Filling</strong><ul><li>Gap filling was performed using 95-fold corrected Nanopore reads to remove gaps, generating the final duck assembly (SKLA1.0), representing 1.16 Gb of the genomic sequence, approximately 99.11% of the estimated genome size (Table 1).</li><li><strong>Gapcloser-0.56</strong><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup></li></ul></li><li><strong>Chromosome Coverage and Comparison</strong><ul><li>Since the duck contains 80 chromosomes (diploid, 2n=80), it was inferred that this duck assembly had covered all chromosomes except W (Additional file 1: Table S8). The SKLA1.0 assembly was compared with the previous duck BGI_duck_1.0 assembly, the ZJU1.0 assembly, and two high-quality avian reference genomes (chicken GRCg6a and zebra finch bTaeGut1.4.pri). These analyses indicated that the SKLA1.0 assembly represents a major improvement over the previous assemblies in terms of contiguity, completeness, and chromosome size. The contiguity and completeness of SKLA1.0 is also higher than that of the zebra finch bTaeGut1.4.pri and the chicken GRCg6a (Fig. 1a–d and Table 1).</li></ul></li></ol><h4 id="After-Assembly">After Assembly</h4><ul><li>Funannotate pipeline and the <strong>GETA pipeline</strong> together with a manual curation of key gene families: 17,896 duck coding genes. Quality was validated by number of coding genes, # of transcripts, # of gaps, and <strong>BUSCO</strong> score.</li><li>Visualization: Bionano map-<a href="https://bionanogenomics.com/support/software-downloads">SOLVE</a></li></ul><h3 id="ASM2904224v1-Greater-scaup-Aythya-marila">ASM2904224v1: Greater scaup (Aythya marila)</h3><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left">First high-quality chromosome-level genome assembly of A. marila<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>, with a final genome size of 1.14 Gb, scaffold N50 of 85.44 Mb, and contig N50 of 32.46 Mb. A total of 154.94 Mb of repetitive sequences were identified. 15,953 protein-coding genes were predicted in the genome, and 98.96% of genes were functionally annotated.</td><td style="text-align:center"><img src="https://www.researchgate.net/profile/Inma-Aznar/publication/51580858/figure/fig19/AS:669632922923023@1536664338755/Mallard-Anas-platyrhynchos-Photo-John-Carey.png" alt="Duck"><a href="https://www.researchgate.net/publication/51580858_A_review_of_Ireland's_waterbirds_with_emphasis_on_wintering_migrants_and_reference_to_H5N1_avian_influenza?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Olivia Crowe</a></td></tr></tbody></table><ol><li><strong>Nanopore long reads</strong>, errors corrected using <strong>Illumina short reads</strong></li><li>Quality: Final size: 1.14 Gb, scaffold N50 of 85.44 Mb, and contig N50 of 32.46 Mb.</li><li>106 contigs were clustered and ordered onto 35 chromosomes based on <strong>Hi-C data</strong>, covering approximately 98.28% of the genome</li><li>BUSCO assessment showed that 97.0% of the highly conserved genes</li></ol><p>Source: muscle tissue of wild male.</p><ul><li>60.77 GB for Illumina HiSeq 4000: Illumina® TruSeq® Nano DNA Library Prep kits to generate sequencing libraries of genomic DNA</li><li>122.55 GB from PromethION platform (91.36 fold of the greater scaup’s genome)</li><li>63.43 Gb for Hi-C data</li></ul><h4 id="Genome-Assembly">Genome Assembly</h4><ol><li><strong>Quality Control</strong>:<ul><li>K = 17, the estimated genome size was 1,341.4 Mb, the heterozygosity was 0.47%, and the proportion of repetitive sequences was 42.28%</li><li>jellyfish (v2.2.7)</li></ul></li><li><strong>Assembly</strong>:<ul><li>assemble the genome with Oxford nanopore technologies (ONT) long reads</li><li>NextDenovo (v2.4.0)</li></ul></li><li><strong>Polish</strong>:<ul><li>increase the precision of single base with Illumina short reads</li><li>NextPolish12 (v1.3.1)<sup class="footnote-ref"><a href="#fn2" id="fnref2:1">[2:1]</a></sup></li></ul></li><li><strong>Scaffold Ordering</strong>:<ul><li>mount the contigs in preliminarily assembly onto chromosomes based on the signal strength after Hi-C data</li><li>ALLHiC (v0.9.8)<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> and Juicebox (v1.11.08)</li></ul></li></ol><table><thead><tr><th style="text-align:center">HiC Results for the global heat map of all the chromosomes.</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41597-023-02142-x/MediaObjects/41597_2023_2142_Fig1_HTML.png" alt=""></td></tr><tr><td style="text-align:center"><a href="https://www.nature.com/articles/s41597-023-02142-x">© Shengyang Zhou</a></td></tr></tbody></table><h4 id="After-Assembly-v2">After Assembly</h4><ol><li>Assessment<ul><li>Burrows-Wheeler aligner14 (BWA) (v0.7.8) to map Illumina reads to the genome with matching rate was approximately 98.80%.</li><li>Merqury15 (v1.3) was ran to evaluate assembly quality value (QV), and a high QV (42.14)</li><li>Benchmarking Universal Single-Copy Orthologs16 (BUSCO) (v5.4.4) (use option “–augustus”) and Core Eukaryotic Genes Mapping Approach17 (CEGMA) (v2.5) were also used to assess the integrity</li><li>238 of 248 core eukaryotic genes were detected using CEGMA</li></ul></li><li>Comparison<ul><li>Mummer18 (v4.0.0) was used to identify the synteny between A. marila and tufted duck19 (Aythya fuligula) genomes to determine orthologous chromosome pairs, and we used TBtools20 (v1.112) to draw the synteny between their chromosomes.</li></ul></li><li>Annotation of repetitive sequences<ul><li>de novo prediction: Tandem Repeats Finder21 (TRF) (v4.09) to detect tandem repeat sequences</li><li>RepeatModeler (v2.0.3), RepeatScout22 (v1.0.6) and RECON (v1.08) were used to build a database of transposable element (TE)</li><li>RepeatProteinMask and RepeatMasker (v4.1.2-p1) were used for homology prediction with Repbase database23 and Dfam database24, the species parameter used was chicken.</li></ul></li><li>Gene structure prediction<ul><li><strong>Prediction Methods</strong>:<ul><li><strong>Ab Initio Prediction</strong>: Used software Augustus (v3.3.2), GlimmerHMM (v3.0.4), and Geneid (v1.4.5).</li><li><strong>Homology-Based Prediction</strong>: Utilized genomes and annotation files from six related species (Anser cygnoides, Anas platyrhynchos, Aythya fuligula, Cygnus olor, Cygnus atratus, Gallus gallus) downloaded from NCBI.</li><li><strong>RNA-Seq Prediction</strong>: Processed raw data from six transcriptomic samples using fastp (v0.23.1), assembled paired-end reads with SPAdes (v3.15.3), identified candidate coding regions using TransDecoder (v5.5.0), and clustered sequences using CD-hit (v4.8.1).</li></ul></li><li><strong>Integration</strong>:<ul><li><strong>Matching and Splicing</strong>: Protein sequences from related species were matched to the A. marila genome using Spaln (v2.4.6) and accurately spliced with GeneWise (v2.4.1).</li><li><strong>Gene Set Generation</strong>: Combined homology-based, RNA-Seq, and ab initio predictions using EvidenceModeler (EVM) (v1.1.1) and incorporated masked repeats.</li></ul></li></ul></li><li><strong>Databases and Tools</strong>:<ul><li><strong>DIAMOND</strong>: Used for sequence alignment against SwissProt, TrEMBL, NR (Non-Redundant Protein Sequence Database), Gene Ontology (GO), and Kyoto Encyclopedia of Genes and Genomes Orthology (KO) databases, with an e-value cutoff of 1e-5.</li><li><strong>InterPro</strong>: Utilized for classifying proteins into families and predicting domains and important sites using InterProScan (v5.53).</li></ul></li><li>Filtering and Verification of Gene Set for A. marila<ol><li><strong>Ortholog Identification</strong>:<ul><li><strong>OrthoFinder</strong>: Used to identify orthologs among A. marila and six related species.</li><li>Resulted in 4,086 unassigned genes, of which 3,421 were not annotated in any database.</li></ul></li><li><strong>Filtering Process</strong>:<ul><li>Most unannotated genes (3,417/3,421) were predicted by at least one de novo prediction software, with only four supported by other evidence.</li><li>Removal of these unassigned genes did not affect the BUSCO test results, indicating they may not represent real genes.</li></ul></li><li><strong>Final Gene Set</strong>:<ul><li>After filtering out unassigned genes without annotations and 159 prematurely terminated genes, 15,953 genes remained, including 182 partial genes.</li><li>98.96% of the final gene set was annotated.</li></ul></li></ol></li></ol><h3 id="Pig-Sscrofa11-1">Pig Sscrofa11.1</h3><p>Warr A, Affara N, Aken B, et al. An improved pig reference genome sequence to enable pig genetics and genomics research[J]. Gigascience, 2020, 9(6): giaa051.</p><p>TJ Tabasco<br>corrected and assembled using Falcon (v.0.4.0)<br>65-fold coverage (176 Gb) of the genome<br>3,206 contigs with a contig N50 of 14.5 Mb.</p><p>Compare<br>contigs were mapped to the previous draft assembly (Sscrofa10.2) using Nucmer<br>gap closure using PBJelly</p><table><thead><tr><th>Statistic</th><th>Sscrofa10.2</th><th>Sscrofa11</th><th>Sscrofa11.1</th><th>USMARCv1.0</th><th>GRCh38.p13</th></tr></thead><tbody><tr><td>Total sequence length</td><td>2,808,525,991</td><td>2,456,768,445</td><td>2,501,912,388</td><td>2,755,438,182</td><td>3,099,706,404</td></tr><tr><td>Total ungapped length</td><td>2,519,152,092</td><td>2,454,899,091</td><td>2,472,047,747</td><td>2,623,130,238</td><td>2,948,583,725</td></tr><tr><td>No. of scaffolds</td><td>9,906</td><td>626</td><td>706</td><td>14,157</td><td>472</td></tr><tr><td>Gaps between scaffolds</td><td>5,323</td><td>24</td><td>93</td><td>0</td><td>349</td></tr><tr><td>No. of unplaced scaffolds</td><td>4,562</td><td>583</td><td>583</td><td>14,136</td><td>126</td></tr><tr><td>Scaffold N50</td><td>576,008</td><td>88,231,837</td><td>88,231,837</td><td>131,458,098</td><td>67,794,873</td></tr><tr><td>Scaffold L50</td><td>1,303</td><td>9</td><td>9</td><td>9</td><td>16</td></tr><tr><td>No. of unspanned gaps</td><td>5,323</td><td>24</td><td>93</td><td>0</td><td>349</td></tr><tr><td>No. of spanned gaps</td><td>233,116</td><td>79</td><td>413</td><td>661</td><td>526</td></tr><tr><td>No. of contigs</td><td>243,021</td><td>705</td><td>1,118</td><td>14,818</td><td>998</td></tr><tr><td>Contig N50</td><td>69,503</td><td>48,231,277</td><td>48,231,277</td><td>6,372,407</td><td>57,879,411</td></tr><tr><td>Contig L50</td><td>8,632</td><td>15</td><td>15</td><td>104</td><td>18</td></tr><tr><td>No. of chromosomes*</td><td>*21</td><td>19</td><td>*21</td><td>*21</td><td>24</td></tr></tbody></table><p>pig (Sscrofa10.2, Sscrofa11.1, USMARCv1.0), human (GRCh38.p13)</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Hu J, Song L, Ning M, Niu X, Han M, Gao C, Feng X, Cai H, Li T, Li F, Li H, Gong D, Song W, Liu L, Pu J, Liu J, Smith J, Sun H, Huang Y. A new chromosome-scale duck genome shows a major histocompatibility complex with several expanded multigene families. BMC Biol. 2024 Feb 5;22(1):31. doi: 10.1186/s12915-024-01817-0. PMID: 38317190; PMCID: PMC10845735. <a href="https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-024-01817-0#Sec26">Paper</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Hu J, Fan JP, Sun ZY, Liu SL. NextPolish: a fast and efficient genome polishing tool for long-read assembly. Bioinformatics. 2020;36:2253–5. <a href="#fnref2" class="footnote-backref">↩︎</a> <a href="#fnref2:1" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Bolger AM, Lohse M, Usadel B. Trimmomatic: a flexible trimmer for Illumina sequence data. Bioinformatics. 2014;30:2114–20. <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>Durand NC, Shamim MS, Machol I, Rao SSP, Huntley MH, Lander ES, et al. Juicer Provides a One-Click System for Analyzing Loop-Resolution Hi-C Experiments. Cell Syst. 2016;3:95–8. <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>Dudchenko O, Batra SS, Omer AD, Nyquist SK, Hoeger M, Durand NC, et al. De novo assembly of the Aedes aegypti genome using Hi-C yields chromosome-length scaffolds. Science. 2017;356:92–5. <a href="#fnref5" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p>Durand NC, Robinson JT, Shamim MS, Machol I, Mesirov JP, Lander ES, et al. Juicebox Provides a Visualization System for Hi-C Contact Maps with Unlimited Zoom. Cell Syst. 2016;3:99–101. <a href="#fnref6" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p>Xu MY, Guo LD, Gu SQ, Wang O, Zhang R, Peters BA, et al. TGS-GapCloser: A fast and accurate gap closer for large genomes with low coverage of error-prone long reads. Gigascience. 2020;9:giaa094–104. <a href="#fnref7" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p>Zhou S, Xia T, Gao X, et al. A high-quality chromosomal-level genome assembly of Greater Scaup (Aythya marila)[J]. Scientific Data, 2023, 10(1): 254. <a href="#fnref8" class="footnote-backref">↩︎</a></p></li><li id="fn9" class="footnote-item"><p>Zhang, X., Zhang, S., Zhao, Q., Ming, R. &amp; Tang, H. Assembly of allele-aware, chromosomal-scale autopolyploid genomes based on Hi-C data. Nature Plants 5, 833–845 (2019). <a href="#fnref9" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">Whole Genome Sequencing (WGS)</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/WGS/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="Genome" scheme="https://karobben.github.io/tags/Genome/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/tags/WGS/"/>
    
  </entry>
  
  <entry>
    <title>Simulated Annealing (SA)</title>
    <link href="https://karobben.github.io/2024/05/30/AI/SAnnealing/"/>
    <id>https://karobben.github.io/2024/05/30/AI/SAnnealing/</id>
    <published>2024-05-30T21:34:39.000Z</published>
    <updated>2024-08-16T21:01:46.659Z</updated>
    
    <content type="html"><![CDATA[<p>Video tutorial: <a href="https://www.youtube.com/watch?v=FyyVbuLZav8">Challenging Luck, 2021</a></p><p>Practicing Python code: <a href="https://github.com/challengingLuck/youtube/blob/master/sudoku/sudoku.py">challengingLuck: Using Annealing Algorithm to Solve the Sudo Challenge</a></p><blockquote><p>Simulated Annealing (SA) is a probabilistic technique used for finding an approximate solution to an optimization problem. It is particularly useful for problems where the search space is large and complex, and other methods might get stuck in local optima. Here’s a structured way to start learning about the Simulated Annealing algorithm:</p></blockquote><p>$$ P(E, E’, T) = \begin{cases}<br>1 &amp; \text{if } E’ &lt; E \\<br>\exp\left(\frac{-(E’ - E)}{T}\right) &amp; \text{if } E’ \ge E<br>\end{cases}<br>$$</p><p>This idea is similar to DNA annealing during PCR. During the temperature drop after the high-temperature mutation, the DNA gradually returns to the double strand with its reverse complement strand or the primer due to the decrease in entropy. Unlike DNA annealing, Simulated Annealing (SA) introduces random values to replace the original ones. After that, the “Energy” is calculated, and only when the new score is lower than the previous one is the new value accepted, continuing the iteration until the lowest value is found. It works like this:</p><p>Calculate the initial <code>E</code> → randomly mutate the value and calculate the new <code>E'</code> → if <code>E' ≤ E</code>, then accept the mutated element; otherwise, try another value → if <code>E'</code> meets the lowest <code>E</code>, stop; otherwise, continue until no smaller <code>E'</code> can be found.</p><p>As a result, you could expect that it would <mark>waste a lot of resources on exploration</mark> and easily <mark>fall into local optima</mark>.</p><p>An example of the SA apply in sudo challenge</p><p><img src="https://imgur.com/k4jbsQK.gif" alt="Simulated annealing in Sudo"><br><img src="https://imgur.com/uWGjeSm.png" alt="Simulated annealing in Sudo"><br>In this example, it actually <mark>failed</mark> to get the result because it <strong>fall into local optimal</strong>. It is a very good example to show the capability and limitations of the SA.</p><h2 id="SA-and-Stochastic-gradient-descent">SA and Stochastic gradient descent</h2><p>$$<br>w_{t+1} = w_t - \eta \nabla f(w_t; x_i)<br>$$</p><ul><li>$w$ is the parameter, weight matrix, for example. While $w_t$ is the old one and the $w_{t-1}$ is updated parameter.</li><li>$\eta$ is the learning rate</li><li>$x$ is the input</li></ul><blockquote><p><strong>Stochastic Gradient Descent (SGD)</strong> is an optimization algorithm used primarily for training machine learning models. It iteratively updates the model parameters by computing the gradient of the loss function using a randomly selected subset (mini-batch) of the training data, rather than the entire dataset. This randomness introduces variability in the updates, which helps escape local optima and speeds up convergence. The learning rate controls the step size of each update, determining how far the parameters move in the direction of the negative gradient. By continuously adjusting the parameters, SGD aims to minimize the loss function and improve the model’s performance.</p></blockquote><p>So, it is very similar to Stochastic gradient descent (SGD). But for SGD, there are a learning process from the data. SGD is primally based on the exploitation. But in SA, exploration has more contribution compared with SGD because it hugely relies on random generation first and evaluating later.</p><h2 id="SA-and-Genetic-Algorithm-GA">SA and Genetic Algorithm (GA)</h2><blockquote><p>What is GA?<br>Genetic Algorithms (GA) are evolutionary algorithms inspired by the principles of natural selection and genetics. They work by evolving a population of candidate solutions through successive generations. Each generation undergoes selection, where the fittest individuals are chosen based on their performance. These selected individuals then undergo crossover (recombination) to produce offspring that combine their parents’ characteristics. Additionally, mutation introduces random changes to some individuals to maintain genetic diversity within the population. This process of selection, crossover, and mutation allows GAs to explore a wide search space and balance between exploring new solutions and exploiting the best solutions found so far. This diversity helps GAs avoid getting trapped in local optima, making them effective for solving complex optimization problems, including those that are non-differentiable and non-convex.</p></blockquote><p>From a personal point of view, GA is like an upgraded version of SA. SA works at the level of a single individual, while GA operates at the population level. Similar to SA, GA evaluates and selects the “fitness scores” of each individual. The next generation introduces many random mutations, just like SA. However, unlike SA, GA also includes “crossover” steps, which can help enrich the “better phenotypes”.</p><table><thead><tr><th>Feature</th><th>Simulated Annealing (SA)</th><th>Stochastic Gradient Descent (SGD)</th><th>Genetic Algorithm (GA)</th></tr></thead><tbody><tr><td><strong>Approach</strong></td><td>Probabilistic, accepts worse solutions occasionally</td><td>Deterministic, updates in the direction of the gradient</td><td>Evolutionary, uses selection, crossover, and mutation</td></tr><tr><td><strong>Objective Function</strong></td><td>Non-differentiable and non-convex functions</td><td>Differentiable functions</td><td>Non-differentiable and non-convex functions</td></tr><tr><td><strong>Exploration vs. Exploitation</strong></td><td>Balances both, reduces acceptance of worse solutions over time</td><td>Primarily exploitation with some exploration via mini-batches</td><td>Balances both, uses population diversity to explore the search space</td></tr><tr><td><strong>Cooling Schedule / Learning Rate</strong></td><td>Uses a cooling schedule to reduce probability of accepting worse solutions</td><td>Uses a learning rate to control step size of updates</td><td>Uses selection pressure to favor better solutions and mutation rate to introduce diversity</td></tr><tr><td><strong>Population-Based</strong></td><td>No</td><td>No</td><td>Yes, operates on a population of solutions</td></tr><tr><td><strong>Escape Local Optima</strong></td><td>Yes, by accepting worse solutions with a probability</td><td>Limited, may get stuck in local optima</td><td>Yes, by maintaining a diverse population</td></tr><tr><td><strong>Gradient Requirement</strong></td><td>No</td><td>Yes</td><td>No</td></tr><tr><td><strong>Applications</strong></td><td>Combinatorial and continuous optimization without gradients</td><td>Training machine learning models, especially in deep learning</td><td>Optimization problems, including scheduling, design, and artificial intelligence</td></tr><tr><td><strong>Natural Inspiration</strong></td><td>Annealing in metallurgy</td><td>Gradient descent in calculus</td><td>Natural selection and genetics</td></tr><tr><td><strong>Operators</strong></td><td>Acceptance probability based on temperature</td><td>Gradient-based updates</td><td>Selection, crossover (recombination), and mutation</td></tr></tbody></table><blockquote><p>table from: ChatGPT4o</p></blockquote><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Simulated Annealing (SA) is a probabilistic technique used for finding an approximate solution to an optimization problem. It is particularly useful for problems where the search space is large and complex, and other methods might get stuck in local optima.</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="Math" scheme="https://karobben.github.io/categories/Machine-Learning/Math/"/>
    
    
    <category term="Math" scheme="https://karobben.github.io/tags/Math/"/>
    
    <category term="Algorithm" scheme="https://karobben.github.io/tags/Algorithm/"/>
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>AI Tools for Protein Structures</title>
    <link href="https://karobben.github.io/2024/05/29/AI/protein3dml/"/>
    <id>https://karobben.github.io/2024/05/29/AI/protein3dml/</id>
    <published>2024-05-29T19:28:45.000Z</published>
    <updated>2024-10-11T22:11:38.468Z</updated>
    
    <content type="html"><![CDATA[<h2 id="trRosetta">trRosetta</h2><p>They inverted this network to generate new protein sequences from scratch, aiming to design proteins with structures and functions not found in <a href="http://nature.By">nature.By</a> conducting <strong>Monte Carlo sampling</strong> in sequence space and optimizing the predicted structural features, they managed to produce a variety of new protein sequences.</p><h2 id="RFdiffusion">RFdiffusion</h2><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/iCXildL.png" alt="RFdiffsion"></td><td style="text-align:left">Watson, Joseph L., et al<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> published the RFdiffusion at <a href="https://github.com/RosettaCommons/RFdiffusion">github</a> in 2023. It fine-tune the <strong>RoseTTAFold</strong><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> and designed for tasks like: protein <strong>monomer</strong> design, protein <strong>binder</strong> design, <strong>symmetric oligomer</strong> design, <strong>enzyme active site</strong> scaffolding and symmetric <strong>motif scaffolding</strong> for therapeutic and <strong>metal-binding</strong> protein design. It is a very powerful tool according to the paper. It is based on the Denoising diffusion probabilistic models (<strong>DDPMs</strong>) which is a powerful class of machine learning models demonstrated to generate new photorealistic images in response to text prompts<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>.</td></tr></tbody></table><p>They use the ProteinMPNN<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> network to subsequently design sequences encoding theses structure. The diffusion model is based on the <strong>DDPMs</strong>. It can not only design a protein from generation, but also able to predict multiple types of interactions as is shown of the left. It was based on the RoseTTAFold.</p><p><strong>Compared with AF2</strong></p><ul><li>AlphaFold2 is like a very smart detective that can figure out the 3D shape of a protein just by looking at its amino acid sequence. On the other hand, RFdiffusion is more like an architect that designs entirely new proteins with specific properties. Instead of just figuring out shapes, it creates new proteins that can do things like bind to specific molecules or perform certain reactions. This makes it incredibly useful for designing new therapies and industrial enzymes.</li></ul><h2 id="ImmuneBuilder">ImmuneBuilder</h2><p><a href="https://www.nature.com/articles/s42003-023-04927-7">ImmuneBuilder: Deep-Learning models for predicting the structures of immune proteins</a></p><h3 id="Method-of-ABodyBuilder2">Method of ABodyBuilder2</h3><blockquote><ul><li>First, the heavy and light chain sequences are fed into four separate deep-learning models to predict an ensemble of structures. The closest structure to the average is then selected and refined using OpenMM<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup> to remove clashes and other stereo-chemical errors. The same pipeline is used for NanoBodyBuilder2 and TCRBuilder2.</li><li><strong>Training data set</strong>: 7084 structures from SAbDab. Filtering: No missing residues and resolution ≤ 3.5 Å</li><li><strong>Architect</strong>: The architecture of the deep learning model behind ABodyBuilder2 is an antibody-specific version of the structure module in <strong>AlphaFold-Multimer</strong> with several modifications</li><li><strong>Frame Aligned Point Error (FAPE) loss</strong> (like AFM)</li></ul></blockquote><p>A set of deep learning models trained to accurately predict the structure of antibodies (ABodyBuilder2), nanobodies (NanoBodyBuilder2) and T-Cell receptors (TCRBuilder2). ImmuneBuilder generates structures with state of the art accuracy while being much faster than AlphaFold2.</p><p>Experience it online: <a href="https://colab.research.google.com/github/brennanaba/ImmuneBuilder/blob/main/notebook/ImmuneBuilder.ipynb">Google Colab</a><br>GitHub: <a href="https://github.com/oxpig/ImmuneBuilder">oxpig/ImmuneBuilder</a></p><p>They have built three models</p><ul><li><strong>ABodyBuilder2</strong>, an antibody-specific model</li><li><strong>NanoBodyBuilder2</strong>, a nanobody-specific model</li><li><strong>TCRBuilder2</strong>, a TCR-specific model.</li></ul><p>It compared the performance with other similar tools:</p><ul><li>homology modelling method; <strong>ABodyBuilder</strong><sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup></li><li>general protein structure prediction method: <strong>AlphaFold-Multimer</strong><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup></li><li>antibody-specific methods: ABlooper<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> (ABL), IgFold<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> (IgF) and EquiFold<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup> (EqF)</li></ul><p>How: compare 34 antibody structures recently added</p><table class="data last-table"><thead class="c-article-table-head"><tr><th class="u-text-left "><p>Method</p></th><th class="u-text-left "><p>CDR-H1</p></th><th class="u-text-left "><p>CDR-H2</p></th><th class="u-text-left "><p>CDR-H3</p></th><th class="u-text-left "><p>Fw-H</p></th><th class="u-text-left "><p>CDR-L1</p></th><th class="u-text-left "><p>CDR-L2</p></th><th class="u-text-left "><p>CDR-L3</p></th><th class="u-text-left "><p>Fw-L</p></th></tr></thead><tbody><tr><td class="u-text-left "><p>ABodyBuilder (ABB)</p></td><td class="u-text-left "><p>1.53</p></td><td class="u-text-left "><p>1.09</p></td><td class="u-text-left "><p>3.46</p></td><td class="u-text-left "><p>0.65</p></td><td class="u-text-left "><p>0.71</p></td><td class="u-text-left "><p>0.55</p></td><td class="u-text-left "><p>1.18</p></td><td class="u-text-left "><p>0.59</p></td></tr><tr><td class="u-text-left "><p>ABlooper (ABL)</p></td><td class="u-text-left "><p>1.18</p></td><td class="u-text-left "><p>0.96</p></td><td class="u-text-left "><p>3.34</p></td><td class="u-text-left "><p>0.63</p></td><td class="u-text-left "><p>0.78</p></td><td class="u-text-left "><p>0.63</p></td><td class="u-text-left "><p>1.08</p></td><td class="u-text-left "><p>0.61</p></td></tr><tr><td class="u-text-left "><p>IgFold (IgF)</p></td><td class="u-text-left "><p>0.86</p></td><td class="u-text-left "><p>0.77</p></td><td class="u-text-left "><p>3.28</p></td><td class="u-text-left "><p>0.58</p></td><td class="u-text-left "><p>0.55</p></td><td class="u-text-left "><p>0.43</p></td><td class="u-text-left "><p>1.12</p></td><td class="u-text-left "><p>0.60</p></td></tr><tr><td class="u-text-left "><p>EquiFold (EqF)</p></td><td class="u-text-left "><p>0.86</p></td><td class="u-text-left "><p>0.80</p></td><td class="u-text-left "><p>3.29</p></td><td class="u-text-left "><p>0.56</p></td><td class="u-text-left "><p>0.47</p></td><td class="u-text-left "><p>0.41</p></td><td class="u-text-left "><p>0.93</p></td><td class="u-text-left "><p><b>0.54</b></p></td></tr><tr><td class="u-text-left "><p>AlphaFold-M (AFM)</p></td><td class="u-text-left "><p>0.86</p></td><td class="u-text-left "><p><b>0.68</b></p></td><td class="u-text-left "><p>2.90</p></td><td class="u-text-left "><p>0.55</p></td><td class="u-text-left "><p>0.47</p></td><td class="u-text-left "><p><b>0.40</b></p></td><td class="u-text-left "><p><b>0.83</b></p></td><td class="u-text-left "><p><b>0.54</b></p></td></tr><tr><td class="u-text-left "><p>ABodyBuilder2 (AB2)</p></td><td class="u-text-left "><p><b>0.85</b></p></td><td class="u-text-left "><p>0.78</p></td><td class="u-text-left "><p><b>2.81</b></p></td><td class="u-text-left "><p><b>0.54</b></p></td><td class="u-text-left "><p><b>0.46</b></p></td><td class="u-text-left "><p>0.44</p></td><td class="u-text-left "><p>0.87</p></td><td class="u-text-left "><p>0.57</p></td></tr></tbody></table><ul><li>What is an acceptable RMSD<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup>?</li></ul><div class="admonition note"><p class="admonition-title">What is an acceptable RMSD?</p><p>The experimental error in protein structures generated via X-ray crystallography has been estimated to be around <strong>0.6Å</strong> for regions with organised secondary structures (such as the antibody frameworks) and around <strong>1Å</strong> for protein loops.</p></div><h3 id="Side-Chain-Prediction">Side Chain Prediction</h3><p>ABlooper and IgFold only predict the position of backbones, leaving the side chain to OpenMM<sup class="footnote-ref"><a href="#fn5" id="fnref5:1">[5:1]</a></sup> and Rosetta<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup>, while EquiFold, AlphaFold-Multimer and ABodyBuilder2, all of which output all-atom structures.</p><h2 id="equifold">equifold</h2><p>Designing proteins to achieve specific functions often requires in silico modeling of their properties at high throughput scale and can significantly benefit from fast and accurate protein structure prediction. We introduce EquiFold, a new end-to-end differentiable, SE(3)-equivariant, all-atom protein structure prediction model. EquiFold uses a novel coarse-grained representation of protein structures that does not require multiple sequence alignments or protein language model embeddings, inputs that are commonly used in other state-of-the-art structure prediction models. Our method relies on geometrical structure representation and is substantially smaller than prior state-of-the-art models. In preliminary studies, EquiFold achieved comparable accuracy to AlphaFold but was orders of magnitude faster. The combination of high speed and accuracy make EquiFold suitable for a number of downstream tasks, including protein property prediction and design.</p><p><a href="https://github.com/Genentech/equifold">https://github.com/Genentech/equifold</a></p><h2 id="IgFold">IgFold</h2><p>Official repository for IgFold: Fast, accurate antibody structure prediction from deep learning on massive set of natural antibodies.</p><p>The code and pre-trained models from this work are made available for non-commercial use (including at commercial entities) under the terms of the JHU Academic Software License Agreement. For commercial inquiries, please contact Johns Hopkins Tech Ventures at <a href="mailto:awichma2@jhu.edu">awichma2@jhu.edu</a>.</p><p>Try antibody structure prediction in Google Colab.</p><p><a href="https://github.com/Graylab/IgFold">https://github.com/Graylab/IgFold</a></p><p>!!! Personal experience<br>I feel that the IgFold is kind of horrible in CDRH3 regions. It predicted CDRH3 loop in an erect conformation incorrectly. It is worse than ABodyBuilder2. It is even slower than ABodyBuilder2, too. It only takes the perfect Fab sequences. Any longer seqeunces would end up as a mass.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Watson J L, Juergens D, Bennett N R, et al. De novo design of protein structure and function with RFdiffusion[J]. Nature, 2023, 620(7976): 1089-1100. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Baek M, et al. Accurate prediction of protein structures and interactions using a 3-track network. Science. July 2021. <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Ramesh, A. et al. Zero-shot text-to-image generation. in Proc. 38th International Conference on Machine Learning Vol. 139 (eds Meila, M. &amp; Zhang, T.) 8821–8831 (PMLR, 2021). <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>Dauparas J, Anishchenko I, Bennett N, et al. Robust deep learning–based protein sequence design using ProteinMPNN[J]. Science, 2022, 378(6615): 49-56. <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>Eastman, P. et al. OpenMM 7: rapid development of high-performance algorithms for molecular dynamics. PLoS Comput. Biol. 13, e1005659 (2017). <a href="#fnref5" class="footnote-backref">↩︎</a> <a href="#fnref5:1" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p>Leem, J., Dunbar, J., Georges, G., Shi, J. &amp; Deane, C. M. ABodyBuilder: automated antibody structure prediction with data-driven accuracy estimation. MAbs 8, 1259–1268 (2016). <a href="#fnref6" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p>Evans, R. et al. Protein complex prediction with AlphaFold-Multimer. bioRxiv (2021). <a href="#fnref7" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p>Abanades, B., Georges, G., Bujotzek, A. &amp; Deane, C. M. ABlooper: fast accurate antibody CDR loop structure prediction with accuracy estimation. Bioinformatics 38, 1877–1880 (2022). <a href="#fnref8" class="footnote-backref">↩︎</a></p></li><li id="fn9" class="footnote-item"><p>Ruffolo, J. A., Chu, L.-S., Mahajan, S. P. &amp; Gray, J. J. Fast, accurate antibody structure prediction from deep learning on massive set of natural antibodies. Nat. Commun. 14, 2389 (2023). <a href="#fnref9" class="footnote-backref">↩︎</a></p></li><li id="fn10" class="footnote-item"><p>Lee, J. H. et al. Equifold: Protein structure prediction with a novel coarse-grained structure representation. bioRxiv (2022). <a href="#fnref10" class="footnote-backref">↩︎</a></p></li><li id="fn11" class="footnote-item"><p>Eyal, E., Gerzon, S., Potapov, V., Edelman, M. &amp; Sobolev, V. The limit of accuracy of protein modeling: influence of crystal packing on protein structure. J. Mol. Biol. 351, 431–442 (2005).Return to ref 35 in article <a href="#fnref11" class="footnote-backref">↩︎</a></p></li><li id="fn12" class="footnote-item"><p>Alford, R. F. et al. The Rosetta all-atom energy function for macromolecular modeling and design. J. Chem. Theory Comput. 13, 3031–3048 (2017). <a href="#fnref12" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">AI Tools for Protein Structures</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="LM" scheme="https://karobben.github.io/categories/Machine-Learning/LM/"/>
    
    <category term="Protein" scheme="https://karobben.github.io/categories/Machine-Learning/LM/Protein/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="AI" scheme="https://karobben.github.io/tags/AI/"/>
    
    <category term="3D" scheme="https://karobben.github.io/tags/3D/"/>
    
    <category term="Protein Structure" scheme="https://karobben.github.io/tags/Protein-Structure/"/>
    
  </entry>
  
</feed>
