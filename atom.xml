<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Karobben</title>
  
  <subtitle>Engjoy~</subtitle>
  <link href="https://karobben.github.io/atom.xml" rel="self"/>
  
  <link href="https://karobben.github.io/"/>
  <updated>2025-07-17T17:57:31.867Z</updated>
  <id>https://karobben.github.io/</id>
  
  <author>
    <name>Karobben</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Cell Ranger for scRNA-Seq Data Analysis</title>
    <link href="https://karobben.github.io/2025/07/09/Bioinfor/cellranger-scRNA/"/>
    <id>https://karobben.github.io/2025/07/09/Bioinfor/cellranger-scRNA/</id>
    <published>2025-07-09T14:56:04.000Z</published>
    <updated>2025-07-17T17:57:31.867Z</updated>
    
    <content type="html"><![CDATA[<h2 id="References">References</h2><p>Cell Ranger is a software suite for analyzing single-cell RNA sequencing (scRNA-Seq) data, developed by 10x Genomics. It provides tools for processing raw sequencing data, performing quality control, and generating gene expression matrices.</p><p>It has detailed documentation available on the <a href="https://www.10xgenomics.com/support/software/cell-ranger/latest/tutorials/cr-tutorial-mr">10x Genomics website</a>.</p><p>Basically, we only need to download the fasta and gtf files for the reference genome, and then run the <code>cellranger mkgtf</code> command with the appropriate parameters.</p><p>A very sample tutorial from 10x Genomics after you download the reference genome:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Reference filtering: keep protein coding genes only</span><br>cellranger mkgtf \<br>    Danio_rerio.GRCz11.105.gtf \<br>    Danio_rerio.GRCz11.105.filtered.gtf \<br>    --attribute=gene_biotype:protein_coding<br><br><span class="hljs-comment"># Start to build the reference genome</span><br>cellranger mkref \<br>    --genome=Drerio_genome \<br>    --fasta=Danio_rerio.GRCz11.dna.primary_assembly.fa \<br>    --genes=Danio_rerio.GRCz11.105.filtered.gtf<br></code></pre></td></tr></table></figure></div><div class="admonition note"><p class="admonition-title">Why Filter???</p><p>Filtering the GTF file to include only protein-coding genes is a common practice in single-cell RNA-seq analysis. This is because the focus is typically on the expressed genes that are relevant for downstream analyses, such as clustering and differential expression. Non-coding genes, pseudogenes, and other non-protein-coding elements may not provide useful information for these analyses and can add noise to the data.</p><p>10x Genomics provides a <a href="https://www.10xgenomics.com/support/software/cell-ranger/downloads/cr-ref-build-steps">detailed guide</a> on the filter log of reference they provide. For human GRCh38, they using the following command:<br />&quot;(protein_coding|protein_coding_LoF|lncRNA|<br />IG_C_gene|IG_D_gene|IG_J_gene|IG_LV_gene|IG_V_gene|<br />IG_V_pseudogene|IG_J_pseudogene|IG_C_pseudogene|<br />TR_C_gene|TR_D_gene|TR_J_gene|TR_V_gene|<br />TR_V_pseudogene|TR_J_pseudogene)&quot;</p><p>So, double check your goal and then filter the GTF file accordingly.</p></div><h2 id="On-Chip-Multiplexing-OCM">On Chip Multiplexing (OCM)</h2><p>On Chip Multiplexing (OCM) is a feature in Cell Ranger that allows for the analysis of multiple samples in a single sequencing run. This is particularly useful for reducing costs and increasing throughput in single-cell RNA-seq experiments. More detailed pipeline can be found in the <a href="https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/running-pipelines/cr-3p-multi">10x Genomics documentation</a>.</p><p>Example of data structure for OCM:</p><pre>Project_Wu_VDJ├── Sample_Rat_VDJ_S2_L004_I1_001.fastq.gz├── Sample_Rat_VDJ_S2_L004_I2_001.fastq.gz├── Sample_Rat_VDJ_S2_L004_R1_001.fastq.gz└── Sample_Rat_VDJ_S2_L004_R2_001.fastq.gzProject_Wu_GEX├── Sample_Rat_GEX_S1_L004_I1_001.fastq.gz├── Sample_Rat_GEX_S1_L004_I2_001.fastq.gz├── Sample_Rat_GEX_S1_L004_R1_001.fastq.gz└── Sample_Rat_GEX_S1_L004_R2_001.fastq.gz</pre><p>In this case, the Sample names are <code>Sample_Rat_VDJ</code> and <code>Sample_Rat_GEX</code>. This is the <code>fastq_id</code> we need to mention in the <code>library</code> section.</p><div class="admonition note"><p class="admonition-title">Error for sample_id</p><p>Technically, CellRanger looks at FASTQ files with this pattern: <code>Sample_&lt;sample_id&gt;_Si</code> and you just need to put the <code>&lt;sample_id&gt;</code> in the <code>fastq_id</code> field of the <code>library</code> section in the <code>sample_sheet.csv</code> file. But you can get error in some cases and you have to use <code>Sample_&lt;sample_id&gt;</code> format. This part waste me lots of time.</p></div><p>In this case, I have both VDJ and GEX data. By using <code>multi</code> would be very easy and convient to run the Cell Ranger pipeline.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="YAML"><figure class="iseeu highlight /yaml"><table><tr><td class="code"><pre><code class="hljs yaml">[<span class="hljs-string">gene-expression</span>]<br><span class="hljs-string">reference,refdata-gex-mm10-2020-A/</span><br><span class="hljs-string">create-bam,false</span><br><br>[<span class="hljs-string">vdj</span>]<br><span class="hljs-string">reference,refdata-cellranger-vdj-GRCh38-alts-ensembl-7.1.0/</span><br><br>[<span class="hljs-string">libraries</span>]<br><span class="hljs-string">fastq_id,fastqs,feature_types,</span><br><span class="hljs-string">Sample_Rat_GEX,Project_Wu_GEX,Gene</span> <span class="hljs-string">Expression</span><br><span class="hljs-string">Sample_Rat_VDJ,Project_Wu_VDJ,VDJ-B</span><br><br>[<span class="hljs-string">samples</span>]<br><span class="hljs-string">sample_id,ocm_barcode_ids</span><br><span class="hljs-string">sample1,OB1</span><br><span class="hljs-string">sample2,OB2</span><br><span class="hljs-string">sample3,OB3</span><br><span class="hljs-string">sample4,OB4</span><br></code></pre></td></tr></table></figure></div><p>In the <code>[libraries]</code> section, the <code>VDJ-B</code> is the feature type for VDJ data which means V(D)J library from B cells (heavy/light chains, BCR).</p><p>After preparing the <code>sample_sheet.csv</code> file, you can run the Cell Ranger pipeline with the following command:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">cellranger multi \<br>    --id=Project_Wu \<br>    --csv=sample_sheet.csv \<br>    --localcores=8 \<br>    --localmem=64<br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Cell Ranger for scRNA-Seq Data Analysis</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Single Cell" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Single-Cell/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="RNA-Seq" scheme="https://karobben.github.io/tags/RNA-Seq/"/>
    
    <category term="scRNA-Seq" scheme="https://karobben.github.io/tags/scRNA-Seq/"/>
    
  </entry>
  
  <entry>
    <title>Reading Public scRNA-seq Data into Seurat</title>
    <link href="https://karobben.github.io/2025/07/06/Bioinfor/scRNA-read/"/>
    <id>https://karobben.github.io/2025/07/06/Bioinfor/scRNA-read/</id>
    <published>2025-07-06T22:37:43.000Z</published>
    <updated>2025-07-17T18:02:44.420Z</updated>
    
    <content type="html"><![CDATA[<h2 id="10x-Genomics-style">10x Genomics-style</h2><p>Exp: GSE163558</p><pre>GSE163558├── GSM5004188_Li1_barcodes.tsv.gz├── GSM5004188_Li1_features.tsv.gz├── GSM5004188_Li1_matrix.mtx.gz...├── GSM5004189_Li2_barcodes.tsv.gz├── GSM5004189_Li2_features.tsv.gz└── GSM5004189_Li2_matrix.mtx.gz</pre><p>Based on this patter, we need to separate the files by sample into separated directory first.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> $(ls | awk -F<span class="hljs-string">&quot;_&quot;</span> <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>| uniq);<span class="hljs-keyword">do</span> <br>    mkdir <span class="hljs-variable">$i</span>;<br><span class="hljs-keyword">done</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> `ls *gz`;<span class="hljs-keyword">do</span><br>    NAME=$(<span class="hljs-built_in">echo</span> <span class="hljs-variable">$i</span>| awk -F<span class="hljs-string">&quot;_&quot;</span> <span class="hljs-string">&#x27;&#123;print $2&quot;/&quot;$3&#125;&#x27;</span>); <br>    mv <span class="hljs-variable">$i</span> <span class="hljs-variable">$NAME</span>;<br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure></div><p>After separating, the structure will look like this:</p><pre>GSE163558├── Li1│   ├── barcodes.tsv.gz│   ├── features.tsv.gz│   └── matrix.mtx.gz...└── PT3    ├── barcodes.tsv.gz    ├── features.tsv.gz    └── matrix.mtx.gz</pre><p>Now, we can read and convert each sample into Seurat object, and merge them together.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(Seurat)<br>library(stringr)<br><br>setwd(<span class="hljs-string">&quot;../GSE163558&quot;</span>)<br>files &lt;- list.files()<br><br>seurat_list &lt;- <span class="hljs-built_in">list</span>()<br><span class="hljs-keyword">for</span> (f <span class="hljs-keyword">in</span> files) &#123;<br>  <span class="hljs-comment"># Read each file</span><br>  df_temp &lt;- Read10X(data.dir = f)<br>  <span class="hljs-comment"># Use GSM ID or tag from filename</span><br>  tag &lt;- f<br>  <span class="hljs-comment"># Create Seurat object and store</span><br>  seurat_list[[tag]] &lt;- CreateSeuratObject(counts = df_temp, project = f)<br>&#125;<br><br><span class="hljs-comment"># Merge all Seurat objects with cell prefix (to avoid name clashes)</span><br>seurat_obj &lt;- merge(<br>  seurat_list[[<span class="hljs-number">1</span>]],<br>  y = seurat_list[-<span class="hljs-number">1</span>],<br>  add.cell.ids = <span class="hljs-built_in">names</span>(seurat_list)<br>)<br></code></pre></td></tr></table></figure></div><h2 id="Separated-DataFrame">Separated DataFrame</h2><p>Exp: GSE134520</p><p>In this dataset, we have separated expression matrix. What we need to do is to read each file, convert it into Seurat object, and merge.</p><pre>GSE134520├── GSM3954946_processed_NAG1.txt├── GSM3954947_processed_NAG2.txt├── GSM3954948_processed_NAG3.txt...├── GSM3954956_processed_IMS3.txt├── GSM3954957_processed_IMS4.txt└── GSM3954958_processed_EGC.txt</pre><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(Seurat)<br>library(stringr)<br><br>setwd(<span class="hljs-string">&quot;GSE134520&quot;</span>)<br>files &lt;- list.files(pattern = <span class="hljs-string">&quot;GSM.*\\.txt$&quot;</span>)<br><br>seurat_list &lt;- <span class="hljs-built_in">list</span>()<br><br><span class="hljs-keyword">for</span> (f <span class="hljs-keyword">in</span> files) &#123;<br>  <span class="hljs-comment"># Read each file</span><br>  mat &lt;- read.table(f, header = <span class="hljs-literal">TRUE</span>, row.names = <span class="hljs-number">1</span>, sep = <span class="hljs-string">&quot;\t&quot;</span>, check.names = <span class="hljs-literal">FALSE</span>)<br>  <span class="hljs-comment"># Use GSM ID or tag from filename</span><br>  tag &lt;- gsub(<span class="hljs-string">&quot;.txt&quot;</span>, <span class="hljs-string">&#x27;&#x27;</span>,  str_split(f, <span class="hljs-string">&quot;_&quot;</span>)[[<span class="hljs-number">1</span>]][<span class="hljs-number">3</span>]) <span class="hljs-comment"># Or str_split(f, &quot;_&quot;)[[1]][1]</span><br>  <span class="hljs-comment"># Create Seurat object and store</span><br>  seurat_list[[tag]] &lt;- CreateSeuratObject(counts = mat, project = tag)<br>&#125;<br><br><span class="hljs-comment"># Merge all Seurat objects with cell prefix (to avoid name clashes)</span><br>seurat_obj &lt;- merge(<br>  seurat_list[[<span class="hljs-number">1</span>]],<br>  y = seurat_list[-<span class="hljs-number">1</span>],<br>  add.cell.ids = <span class="hljs-built_in">names</span>(seurat_list)<br>)<br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">This page provides a guide on how to read public scRNA-seq datasets into Seurat objects, including examples for specific datasets.</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Single Cell" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Single-Cell/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="RNA-Seq" scheme="https://karobben.github.io/tags/RNA-Seq/"/>
    
    <category term="scRNA-Seq" scheme="https://karobben.github.io/tags/scRNA-Seq/"/>
    
  </entry>
  
  <entry>
    <title>Backpropagation Hand by Hand</title>
    <link href="https://karobben.github.io/2025/04/28/AI/backpropagation/"/>
    <id>https://karobben.github.io/2025/04/28/AI/backpropagation/</id>
    <published>2025-04-28T17:53:29.000Z</published>
    <updated>2025-04-28T22:00:01.614Z</updated>
    
    <content type="html"><![CDATA[<p>$$<br>W^\ell!\leftarrow W^\ell - \eta, (a^ {\ell-1}) ^\top \delta^\ell,\quad<br>b^\ell!\leftarrow b^\ell - \eta,\sum\delta^\ell.<br>$$</p><h2 id="A-Very-Sample-NN-Example">A Very Sample NN Example</h2><p>Codes below is a simple example of a neural network with one hidden layer, using the sigmoid activation function. The network is trained to learn the XOR function, which is a classic problem in neural networks. The <strong>Input</strong> consists of two features. The <strong>Output</strong> is a single value (either 0 or 1). The network has two layers: an <strong>input layer</strong> with 2 neurons and a <strong>hidden layer</strong> also with 2 neurons. The <strong>output layer</strong> has 1 neuron. The <strong>weights</strong> and <strong>biases</strong> are <em>initialized randomly</em>. The network is trained using the <strong>backpropagation</strong> algorithm, which adjusts the weights and biases based on the error between the predicted output and the actual output.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid_derivative</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> - x)<br><br><span class="hljs-comment"># Example input and output</span><br><span class="hljs-comment"># There are 4 samples, and each sample has 2 features (e.g., like a point in 2D space).</span><br>x = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])<br><span class="hljs-comment"># Each sample has a single output (either 0 or 1).</span><br>y = np.array([[<span class="hljs-number">0</span>], [<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>], [<span class="hljs-number">0</span>]])<br><br><span class="hljs-comment">#########################################</span><br><span class="hljs-comment"># Step 1. Initialize weights and biases</span><br><span class="hljs-comment">#########################################</span><br>W1 = np.random.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># (input_dim=2 → hidden_dim=2)</span><br>b1 = np.random.rand(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># bias for hidden layer (1 row, 2 neurons)</span><br>W2 = np.random.rand(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># (hidden_dim=2 → output_dim=1)</span><br>b2 = np.random.rand(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># bias for output layer (1 row, 1 neuron)</span><br><br><span class="hljs-comment"># Learning rate</span><br>eta = <span class="hljs-number">0.1</span><br><br><span class="hljs-comment"># Training loop</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>):<br>    <span class="hljs-comment">############################################</span><br>    <span class="hljs-comment"># Step 2. Forward pass</span><br>    <span class="hljs-comment">############################################</span><br>    z1 = np.dot(x, W1) + b1<br>    a1 = sigmoid(z1)<br>    z2 = np.dot(a1, W2) + b2<br>    a2 = sigmoid(z2)<br>    <span class="hljs-comment">############################################</span><br>    <span class="hljs-comment"># Step 3. Loss calculation</span><br>    <span class="hljs-comment">############################################</span><br>    loss = <span class="hljs-number">0.5</span> * (y - a2)**<span class="hljs-number">2</span><br>    <span class="hljs-comment">############################################</span><br>    <span class="hljs-comment"># Step 4. Backpropagation</span><br>    <span class="hljs-comment">############################################</span><br>    delta2 = (a2 - y) * sigmoid_derivative(a2)<br>    delta1 = np.dot(delta2, W2.T) * sigmoid_derivative(a1)<br>    <span class="hljs-comment"># Update weights and biases</span><br>    W2 -= eta * np.dot(a1.T, delta2)<br>    b2 -= eta * np.<span class="hljs-built_in">sum</span>(delta2, axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-literal">True</span>)<br>    W1 -= eta * np.dot(x.T, delta1)<br>    b1 -= eta * np.<span class="hljs-built_in">sum</span>(delta1, axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-literal">True</span>)<br><br>print(<span class="hljs-string">&quot;Final output after training:&quot;</span>)<br>print(a2)<br></code></pre></td></tr></table></figure></div><h2 id="Steps-of-Backpropagation">Steps of Backpropagation</h2><ol><li><p><strong>Initialization:</strong> (Step 1)</p><ul><li>Initialize the weights and biases of the network with small random values.</li></ul></li><li><p><strong>Forward Pass:</strong> (See Step 2 in the for loop)</p><ul><li>For each layer $ l $, compute the input $ z^l $ and output $ a^l $:<ul><li>$z^l = W^l a^{l-1} + b^l$</li><li>$a^l = \sigma(z^l)$</li></ul></li><li>Here, $ W^l $ are the weights (<code>W1</code> and <code>W2</code>), $ b^l $ are the biases (<code>b1</code> and <code>b2</code>), $ \sigma $ is the activation function (<code>sigmoid</code>), and $ a^{l-1} $ is the output from the previous layer (the first $a$ is $x$ which is the input).</li></ul></li><li><p><strong>Compute Loss:</strong> (See codes in Step 3)</p><ul><li>Compute the loss $ L $ using a suitable loss function.</li></ul></li><li><p><strong>Backward Pass:</strong> (See codes in Step 4)</p><ul><li>Calculate the gradient of the loss with respect to the output of the last layer $ \delta^L $:<ul><li>$\delta^L = \nabla_a L \cdot \sigma’(z^L)$</li></ul></li><li>For each layer $ l $ from $ L-1 $ to 1, compute:<br>-$\delta^l = (\delta^{l+1} \cdot W^{l+1}) \cdot \sigma’(z^l)$</li><li>Update the weights and biases:<ul><li>$W^l = W^l - \eta \cdot \delta^l \cdot (a^{l-1}) ^T$</li><li>$b^l = b^l - \eta \cdot \delta^l$</li></ul></li><li>Here, $ \eta $ is the learning rate, and $ \sigma’ $ is the derivative of the activation function.</li></ul></li></ol><p>$$<br>\frac{d(\text{Loss})}{dW_2} = \frac{d(\text{Loss})}{da_2} \times \frac{da_2}{dz_2} \times \frac{dz_2}{dW_2}<br>$$</p><ul><li>$ \frac{d(\text{Loss})}{da_2} $: how much the loss changes if $ a_2 $ changes</li><li>$ \frac{da_2}{dz_2} $: how much $ a_2 $ changes if $ z_2 $ changes</li><li>$ \frac{dz_2}{dW_2} $: how much $ z_2 $ changes if $ W_2 $ changes</li></ul><details><summary>Click to go through this step by step very carefully</summary><p>The delta calculation in the code:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">delta2 = (a2 - y) * sigmoid_derivative(a2)<br></code></pre></td></tr></table></figure></div><p>and how we get it based on chain rule:<br>$$<br>\frac{d(\text{Loss})}{dW_2} = \frac{d(\text{Loss})}{da_2} \times \frac{da_2}{dz_2} \times \frac{dz_2}{dW_2}<br>$$</p><p><strong>Step 1: Loss function</strong></p><p>Suppose the loss is Mean Squared Error (MSE):</p><p>$$<br>\text{Loss} = \frac{1}{2} (a_2 - y)^2<br>$$</p><p><strong>Step 2: Derivatives</strong></p><table><thead><tr><th>Main</th><th>Derive from</th><th>Result</th></tr></thead><tbody><tr><td>$ \frac{d(\text{Loss})}{da_2} $</td><td>$Loss = 0.5 * (y - a_2)^2 $</td><td>$(a_2 - y)$</td></tr><tr><td>$ \frac{da_2}{dz_2} $</td><td>$a_2 = \sigma(z_2)$</td><td>$\sigma’(z_2)$</td></tr><tr><td>$ \frac{dz_2}{dW_2} $</td><td>$ z_2 = a_1 W_2 + b_2 $</td><td>$\frac{dz_2}{dW_2} = a_1$</td></tr></tbody></table><p>$\sigma’(z_2) = a_2 (1 - a_2)$</p><hr><p><strong>Step 3: Chain them together</strong></p><p>Thus:</p><p>$$<br>\text{delta2} = \frac{d(\text{Loss})}{da_2} \times \frac{da_2}{dz_2}<br>$$<br>which is exactly:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">delta2 = (a2 - y) * sigmoid_derivative(a2)<br></code></pre></td></tr></table></figure></div><p>Later, the update for $ W_2 $ uses $ delta2 $ multiplied by $ a_1 $ (previous activations).</p><hr><p><strong>Summary:</strong></p><ul><li><code>(a2 - y)</code> is $\frac{d(\text{Loss})}{da_2}$</li><li><code>sigmoid_derivative(a2)</code> is $\frac{da_2}{dz_2}$</li><li>We haven’t yet multiplied by $a_1$ — that happens during weight update!</li></ul></details><div class="admonition question"><p class="admonition-title">Why we made delta2 vector without multiplying by $a_1$?</p><p>Except we will use it to update $W_2$ later by multiplication with $a_1$, we also need to update <strong>$b_2$</strong> which doesn't require multiplication $a_1$. So, we don't need to multiply $a_1$ here.</p></div><h2 id="Expansions">Expansions</h2><ol><li><p><strong>Derive δ¹ Explicitly</strong><br>Work through the chain rule for the hidden layer:<br>$$<br>\delta^1 = \bigl(\delta^2 W^{2\top}\bigr)\odot \sigma’(z^1).<br>$$<br>Mapping each term to code deepens your grasp of how errors “flow back” through layers  (<a href="https://www.3blue1brown.com/lessons/backpropagation-calculus?utm_source=chatgpt.com">Backpropagation calculus - 3Blue1Brown</a>).</p></li><li><p><strong>Bias Gradients</strong><br>Note that<br>$\partial L/\partial b^\ell = \sum_i \delta^\ell_i$,<br>which the code implements via <code>np.sum(deltaℓ, axis=0, keepdims=True)</code>  (<a href="https://en.wikipedia.org/wiki/Backpropagation?utm_source=chatgpt.com">Backpropagation - Wikipedia</a>).</p></li><li><p><strong>Alternative Losses</strong><br>Show how the backprop equations simplify when using binary cross‐entropy:<br>$$<br>L = -y\log(a^2) - (1-y)\log(1-a^2)<br>\quad\Longrightarrow\quad<br>\delta^2 = a^2 - y,<br>$$<br>eliminating the explicit $\sigma’(z^2)$ term  (<a href="https://en.wikipedia.org/wiki/Backpropagation?utm_source=chatgpt.com">Backpropagation - Wikipedia</a>).</p></li><li><p><strong>Regularization in Loss</strong><br>Introduce weight decay by adding $\tfrac{\lambda}{2}|W|^2$ to $L$.  Its gradient $\lambda W$ simply augments $\partial L/\partial W$ before the update  (<a href="https://en.wikipedia.org/wiki/History_of_artificial_neural_networks?utm_source=chatgpt.com">History of artificial neural networks</a>).</p></li><li><p><strong>Vectorizing Over Batches</strong><br>Generalize from a single example to a batch by keeping matrices <code>X</code> and <code>Y</code> shaped <code>(batch_size, features)</code>, ensuring the update formulas remain the same  (<a href="https://cs231n.github.io/optimization-2/?utm_source=chatgpt.com">Backpropagation - CS231n Deep Learning for Computer Vision</a>).</p></li><li><p><strong>Modern Optimizers</strong><br>Briefly demonstrate how to plug in Adam: maintain per-parameter running averages <code>m</code> and <code>v</code> and adjust <code>W</code> with<br>$\displaystyle m\leftarrow\beta_1 m + (1-\beta_1)\nabla W,\quad<br>v\leftarrow\beta_2 v + (1-\beta_2)(\nabla W)^2$,<br>then<br>$$<br>W\leftarrow W - \eta,\frac{m/(1-\beta_1^ t)}{\sqrt{v/(1-\beta_2^ t)}+\epsilon}.<br>$$<br>This typically outperforms vanilla SGD on noisy or sparse data  (<a href="https://en.wikipedia.org/wiki/History_of_artificial_neural_networks?utm_source=chatgpt.com">History of artificial neural networks</a>).</p></li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Backpropagation Hand by Hand</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>GNN: Graph Neural Networks</title>
    <link href="https://karobben.github.io/2025/04/23/AI/gnn/"/>
    <id>https://karobben.github.io/2025/04/23/AI/gnn/</id>
    <published>2025-04-23T21:56:36.000Z</published>
    <updated>2025-07-17T18:04:30.544Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Graph-Neural-Networks-GNNs">Graph Neural Networks (GNNs)</h2><p>Strong suggest to read: <a href="https://distill.pub/2021/gnn-intro/">A Gentle Introduction to Graph Neural Networks; Adam Pearce, et al.; distill; 2021</a></p><p>A Graph Neural Network (GNN) is a type of deep learning model designed to operate directly on graph-structured data by iteratively “passing messages” along edges: each node maintains a feature vector, exchanges information with its neighbors through learnable aggregation functions, and updates its embedding to capture both local connectivity and node attributes; by stacking multiple such layers, a GNN can learn representations for individual nodes (e.g., for node classification), entire graphs (e.g., for graph classification), or edges (e.g., for link prediction), making it highly effective for any task where relationships between entities matter.</p><p>!!! Pure Personal Understanding for Quick Understanding the GNN<br>For CNN, after multiple rounds of convolution by using different kernel like Alex net, the 2D image is encoded into a single vector. So as the GNN. For graphic data, we sort the layers of each nodes based on the graphic sctrcutre. Then we convlution the nodes by using the same kernel which included the nodes from the next layer. The nodes would updated as the sum of node<em>weight</em>(nodes adjacent). If there are attention, the attention are further included. Finally, then entire graphic data would be encoded into a single vector as input according to the features. The most straight forward way is take the sum/mean value of all encoded nodes as the final graph representation. During the training, the weight used in the nodes update are update. The attention are also a learnable parameter. Because the vector then input into the neural network, the next steps work exactly the same as CNN.</p><h2 id="Basic-for-GNN">Basic for GNN</h2><h3 id="Adjacency">Adjacency</h3><p>The adjacency matrix (A) is a square matrix used to represent a finite graph. The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph. For an undirected graph, the adjacency matrix is symmetric.</p><table><thead><tr><th style="text-align:center"><img src="https://media.geeksforgeeks.org/wp-content/uploads/20230727130331/Undirected_to_Adjacency_matrix.png" alt="adjacent matrix"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.geeksforgeeks.org/adjacency-matrix-meaning-and-definition-in-dsa/">© geeksforgeeks; for more, click here</a></td></tr></tbody></table><h3 id="Simple-Information-Passing">Simple Information Passing</h3><p>$$<br>\mathbf{x}^\prime_i =\ \bigoplus_{j \in \mathcal{N}(i)} e_{ji} \cdot \mathbf{x}_j<br>$$</p><p><code>SimpleConv</code> is a <strong>non-trainable</strong> message-passing operator in PyTorch Geometric that simply propagates (and optionally weights) neighbor features:</p><ul><li>$\bigoplus$ is the aggregation operator you choose (<code>sum</code>, <code>mean</code>, <code>max</code>, etc.).</li><li>$e_{ji}$ is an <strong>edge weight</strong> (if provided) or 1 by default.</li><li><strong>It does not learn any linear transforms</strong>—it just spreads information across the graph ([PyTorch Geometric][1]).</li></ul><h4 id="Key-Parameters">Key Parameters</h4><ul><li><p><strong><code>aggr</code></strong> (<code>str</code> or <code>Aggregation</code>, default=<code>'sum'</code>):<br>How to combine messages from neighbors. Options include <code>&quot;sum&quot;</code>, <code>&quot;mean&quot;</code>, <code>&quot;max&quot;</code>, <code>&quot;min&quot;</code>, <code>&quot;mul&quot;</code>, or a custom <code>Aggregation</code> module ([PyTorch Geometric][1]).</p></li><li><p><strong><code>combine_root</code></strong> (<code>str</code>, optional):<br>If not <code>None</code>, specifies how to combine the central node’s own feature with the aggregated neighbor message. Choices:</p><ul><li><code>&quot;sum&quot;</code>: add $\mathbf{x}_i$ to the aggregated message</li><li><code>&quot;cat&quot;</code>: concatenate $\mathbf{x}_i$ with the aggregated message</li><li><code>&quot;self_loop&quot;</code>: treat self-loops just like any other edge</li><li><code>None</code> (default): ignore the root feature ([<a href="http://aidoczh.com">aidoczh.com</a>][2]).</li></ul></li></ul><h4 id="2-Basic-Usage-Example">2. Basic Usage Example</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> torch_geometric.data <span class="hljs-keyword">import</span> Data<br><span class="hljs-keyword">from</span> torch_geometric.nn <span class="hljs-keyword">import</span> SimpleConv<br><br><span class="hljs-comment"># 1) Define a tiny graph with 3 nodes and 3 directed edges:</span><br><span class="hljs-comment">#    0 → 1, 1 → 2, 2 → 0</span><br>edge_index = torch.tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>],<br>                           [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>]], dtype=torch.long)<br><br><span class="hljs-comment"># 2) Node features: a single scalar per node</span><br>x = torch.tensor([[<span class="hljs-number">1.0</span>],   <span class="hljs-comment"># node 0</span><br>                  [<span class="hljs-number">2.0</span>],   <span class="hljs-comment"># node 1</span><br>                  [<span class="hljs-number">3.0</span>]],  <span class="hljs-comment"># node 2</span><br>                 dtype=torch.<span class="hljs-built_in">float</span>)<br><br><span class="hljs-comment"># 3) Instantiate SimpleConv:</span><br>conv = SimpleConv(aggr=<span class="hljs-string">&#x27;sum&#x27;</span>,       <span class="hljs-comment"># sum up neighbor messages</span><br>                  combine_root=<span class="hljs-string">&#x27;sum&#x27;</span>)  <span class="hljs-comment"># add the node’s own feature back in</span><br><br><span class="hljs-comment"># 4) Forward pass (no edge weights supplied → all weights = 1):</span><br>out = conv(x, edge_index)<br>print(out)<br></code></pre></td></tr></table></figure></div><pre># tensor([[4.],#         [4.],#         [4.]], grad_fn=<AddBackward0>)# Explanation: each out[i] = sum of neighbors + x[i]:#   out[0] = x[2] + x[0] = 3 + 1 = 4#   out[1] = x[0] + x[1] = 1 + 2 = 3 (oh—and since combine_root='sum', you get +2 again → 4)#   out[2] = x[1] + x[2] = 2 + 3 = 5 (plus x[2]=3 → 8) </pre><h4 id="3-Using-Edge-Weights">3. Using Edge Weights</h4><p>If your graph has edge weights, pass them in as a third argument:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">edge_weight = torch.tensor([<span class="hljs-number">0.5</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>], dtype=torch.<span class="hljs-built_in">float</span>)<br><span class="hljs-comment"># same graph structure, but each message is scaled by edge_weight</span><br><br>conv = SimpleConv(aggr=<span class="hljs-string">&#x27;sum&#x27;</span>, combine_root=<span class="hljs-literal">None</span>)  <span class="hljs-comment"># no root combine</span><br>out = conv(x, edge_index, edge_weight)<br>print(out)<br></code></pre></td></tr></table></figure></div><pre>tensor([[1.5 ],  # 0’s only neighbor is 2, so 2*0.5 = 1.5        [0.5 ],  # neighbor 0 with weight 0.5 = 0.5        [2.0 ]]) # neighbor 1 with weight 1.0 = 2.0</pre><h4 id="4-Integrating-into-an-nn-Module">4. Integrating into an <code>nn.Module</code></h4><p>Typically you’ll wrap it in a small GNN:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> torch_geometric.nn <span class="hljs-keyword">import</span> SimpleConv<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SimpleGNN</span>(<span class="hljs-params">torch.nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_channels, hidden_channels, out_channels</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># one SimpleConv layer</span><br>        self.conv = SimpleConv(aggr=<span class="hljs-string">&#x27;mean&#x27;</span>, combine_root=<span class="hljs-string">&#x27;sum&#x27;</span>)<br>        <span class="hljs-comment"># follow up with a trainable linear layer</span><br>        self.lin  = torch.nn.Linear(in_channels, out_channels)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x, edge_index, edge_weight=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-comment"># propagate and combine</span><br>        x = self.conv(x, edge_index, edge_weight)<br>        x = F.relu(x)<br>        <span class="hljs-comment"># then map to final outputs</span><br>        <span class="hljs-keyword">return</span> self.lin(x)<br><br><span class="hljs-comment"># Usage:</span><br>model = SimpleGNN(in_channels=<span class="hljs-number">1</span>, hidden_channels=<span class="hljs-literal">None</span>, out_channels=<span class="hljs-number">2</span>)<br>data = Data(x=x, edge_index=edge_index, edge_weight=edge_weight)<br>logits = model(data.x, data.edge_index, data.edge_weight)<br>print(logits)<br></code></pre></td></tr></table></figure></div><div class="admonition note"><p class="admonition-title">Why work it in this way?</p><p>Applying <code>F.relu</code> and then a trainable <code>Linear</code> layer lets the model learn useful, non‐linear combinations of the propagated features:</p><ol><li><strong><code>SimpleConv</code> is purely linear</strong>: it just averages or sums neighbor features (possibly weighted), but has no trainable parameters (beyond edge weights if you supply them).</li><li><strong><code>F.relu</code> adds non-linearity</strong>: without it, stacking another linear operation would collapse into a single linear transformation. The ReLU ensures the network can learn more complex functions of the aggregated features.</li><li><strong><code>Linear</code> learns the final mapping</strong>: you typically want to transform your hidden node features into whatever target space you need (e.g. class logits, embedding space, regression value). The <code>Linear</code> layer is where the model’s weights get updated to solve your specific task.</li></ol><p>Putting it together:</p><p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1) conv aggregates neighbor info (no params)</span><br>x = self.conv(x, edge_index, edge_weight)<br><br><span class="hljs-comment"># 2) ReLU injects non-linearity</span><br>x = F.relu(x)<br><br><span class="hljs-comment"># 3) Linear layer learns to map to desired outputs</span><br>out = self.lin(x)<br></code></pre></td></tr></table></figure></div></p><p>This pattern—(propagate) → (non‐linearity) → (trainable transform)—is the bread-and-butter of deep GNNs.</p></div><h4 id="When-to-Use-SimpleConv">When to Use <code>SimpleConv</code></h4><ul><li><strong>As a quick propagation step</strong> (e.g., label propagation) without extra parameters.</li><li><strong>As a baseline</strong> to test how much plain feature diffusion alone helps your task.</li><li><strong>In combination</strong> with trainable layers (like <code>Linear</code>) to control which parts of the network actually learn.</li></ul><h3 id="Laplacian">Laplacian</h3><ul><li><a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/laplacian/v/laplacian-intuition">Quick Explain of Laplacian by Khan Academy</a></li></ul><h3 id="spectral-insights">spectral insights</h3><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Graph Neural Networks (GNNs) are a class of neural networks designed to work directly with graph-structured data. They have gained significant attention in recent years due to their ability to model complex relationships and interactions in various domains, including social networks, molecular biology, and recommendation systems.</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Influenza</title>
    <link href="https://karobben.github.io/2025/04/11/LearnNotes/influenza/"/>
    <id>https://karobben.github.io/2025/04/11/LearnNotes/influenza/</id>
    <published>2025-04-11T18:12:39.000Z</published>
    <updated>2025-04-22T17:07:47.856Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Resources">Resources</h2><ul><li>YouTube:<ul><li><a href="https://www.youtube.com/watch?v=PmcUJgYpikE&amp;t">Michael Vahey: Phynotypic Variability and Plasticity in Influenza A Virus; 2019; UC Berkeley</a></li><li><a href="https://www.youtube.com/watch?v=_KSQk2Hsjmc">Nicholas Wu: Influenza Evolution: A Major Challenge in Vaccine Development; 2021; UIUC</a></li><li><a href="https://www.youtube.com/watch?v=p6oEk2LX9rs">Paul Genecin: The Exam Room: Influenza &amp; the Vaccine Evaluation; 2015; Yale School Of Medicine</a></li><li><a href="https://www.youtube.com/watch?v=yjARth3Tz2A&amp;t=234s">Paul Van Buynder: Current controversies with influenza vaccines: hope for the future?; 2016; Simon Fraser University</a></li><li><a href="https://www.youtube.com/watch?v=7Omi0IPkNpY">3D animation for influenza life circle; 2014; Nucleus Medical Media</a></li><li><a href="https://www.youtube.com/watch?v=oXzwtGFyBik">3D animation for influenza life circle; 2020; XVIVO Scientific Animation</a></li><li><a href="https://www.youtube.com/watch?v=GyNyFqWtFyk">Influenza Virus Microbiology Animation (more detailed on mechanism); 2020</a></li><li><a href="https://www.youtube.com/watch?v=gAmfI5T7XOM">James McSharry: Influenza Viruses by James McSharry, PhD; 2017</a></li></ul></li><li>Paper:<ul><li><a href="https://journals.asm.org/doi/10.1128/mbio.00175-24">Li Y, Huo S, Yin Z, Tian Z, Huang F, Liu P, Liu Y, Yu F.2024.Retracted and republished from: “The current state of research on influenza antiviral drug development: drugs in clinical trial and licensed drugs”. mBio15:e00175-24.https://doi.org/10.1128/mbio.00175-24</a></li></ul></li></ul><h2 id="Basic-Information">Basic Information</h2><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/yKYU0jB.png" alt="Life Cycle of the Influenza"></th></tr></thead><tbody><tr><td style="text-align:center">Figure 9 Flint et al. 2004 ASM Press</td></tr></tbody></table><p>Influenza A gets the most number of variance. They are named based on the <strong>subtype</strong> of the surface protein HA and NA. For example, H1N1, H3N2, H5N1, etc. For HA, there are 18 subtypes (H1-H18), For NA, there are 11 subtypes (N1-N11)</p><p>They are transmitted by <strong>respiratory droplets</strong>. The virus can survive on surfaces for 24-48 hours. The virus can be transmitted before the symptoms show up.</p><h2 id="Morphalogy-of-the-Influenza-Virus">Morphalogy of the Influenza Virus</h2><table><thead><tr><th style="text-align:left"><img src="https://imgur.com/rLaV5k8.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:left">Unlike the Zika virus, it has a uniform on size and shape. The surface protein alos aligned with recognizable patterns. In the resconstructred influenza, it has heterogeneous progeny, the surface protein HA (green) and NA (yellow) distributed by random.</td></tr></tbody></table><p>What’s the advantages of the heterogeneous progeny? Influenza’s enter and release is based on the 2 main surface protein, HA an NA. By changing the morphalogy of the virus, it could regulate the ratio and distribution of the HA and NA to fit the environment.</p><h2 id="Segmented-Genome">Segmented Genome</h2><table><thead><tr><th style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/0/00/Viruses-12-00504-g001.webp" alt=""><b>© wikipidia</th></tr></thead><tbody><tr><td style="text-align:center">Unlike the Zika virus, the influenza virus has a <strong>segmented genome</strong>. The segmented genome allows for <strong>reassortment</strong> of genes between <strong>different strains</strong> of the virus, which can lead to the emergence of new strains with different properties. This is one of the reasons why influenza viruses are able to evolve rapidly and evade the immune system.</td></tr></tbody></table><p><img src="https://imgur.com/WabPEs9.png" alt=""></p><p>Influenza causing pandemic almost every 2 years.</p><p><img src="https://imgur.com/TjGzWun.png" alt=""><br>Increased the glycosylation of the HA protein. This glycosylation change help the virus to evade the immune system.</p><h2 id="Treatment">Treatment</h2><table><thead><tr><th style="text-align:center"><img src="https://journals.asm.org/cms/10.1128/mbio.00175-24/asset/db62eb36-3bcd-4f96-9430-420957f1fd56/assets/images/large/mbio.00175-24.f002.jpg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://journals.asm.org/doi/10.1128/mbio.00175-2x4table">© Yanbai Li, et al; 2024</a></td></tr></tbody></table><h3 id="Ion-Channel-Blockers"><strong>Ion Channel Blockers</strong></h3><p>These drugs target the <strong>M2 ion channel protein</strong> in influenza <strong>Type A</strong> viruses. By blocking this channel, they <strong>prevent acidification inside the viral particle</strong>, which is necessary for uncoating and releasing the viral RNA.</p><ul><li><strong>Amantadine</strong> – Effective only against <strong>influenza A</strong> viruses.</li><li><strong>Rimantadine</strong> – Also active against <strong>influenza A only</strong>, with a slightly better side effect profile than amantadine.</li></ul><blockquote><p>⚠️ Note: Many influenza A strains have developed resistance to these drugs, limiting their current use.</p></blockquote><h3 id="Neuraminidase-Inhibitors"><strong>Neuraminidase Inhibitors</strong></h3><p>These medications block the <strong>neuraminidase enzyme</strong>, which is required for the virus to <strong>release new viral particles</strong> from infected cells. They work against <strong>both influenza A and B</strong>.</p><ul><li><strong>Oseltamivir carboxylate (Tamiflu)</strong> – An oral drug that inhibits neuraminidase in both influenza A and B.</li><li><strong>Zanamivir (Relenza)</strong> – An inhaled version that also targets both influenza A and B.</li></ul><blockquote><p>These drugs are most effective when taken <strong>within 48 hours</strong> of symptom onset.</p></blockquote><h3 id="PA-Inhibitors"><strong>PA Inhibitors</strong></h3><ul><li><strong>Baloxavir marboxil (Xofluza)</strong> – A <strong>newer antiviral</strong> that inhibits the <strong>PA protein</strong> of the influenza virus, preventing viral replication. It is effective against both influenza A and B viruses.</li></ul><h3 id="HA-Stem-Inhibitors"><strong>HA Stem Inhibitors</strong></h3><ul><li><strong>Arbidol</strong> – A broad-spectrum antiviral that targets the hemagglutinin (HA) protein of the virus. It is thought to inhibit the fusion of the viral envelope with the host cell membrane, preventing viral entry.</li></ul><blockquote><p>⚠️ Note: Arvudol is <strong>not FDA-approved</strong> for influenza but is used in some countries like Russia and China.</p></blockquote><h3 id="Monoclonal-Antibodies">Monoclonal Antibodies</h3><p>All the monoclonal antibodies are <strong>not FDA-approved</strong> for influenza. They are still on clinical trials.</p><table data-shading="custom"><thead><tr data-xml-align="left" data-xml-valign="top"><th>Name</th><th>Binding to</th><th>IC<sub>50</sub><br>(μg/mL)</th><th>100% protection<br><i>in vivo</i></th><th>Adverse<br>event</th><th>Usage</th><th>Clinical studies</th><th>Reference</th></tr></thead><tbody><tr><td>CR6261</td><td>HA-stalk<br>(H1, H2, H5, H6, H9, H13, and H16)</td><td>0.12–14.87</td><td>Pre: 5 mg/kg<br>Post: 15 mg/kg</td><td>−<a href="#T4_FN1" role="doc-noteref"><sup><i>a</i></sup></a></td><td>Intravenous</td><td>I NCT01406418,<br>February 2013 to November 2013;<br>II NCT02371668,<br>February 2015 to November 2018</td><td>(<a href="#core-B149" role="doc-biblioref" data-xml-rid="B149" id="body-ref-B149-3" href-manipulated="true" aria-label="Reference 149">149</a><a href="#core-B150" role="doc-biblioref" data-xml-rid="B150 B151 B152" id="body-ref-B152-2" href-manipulated="true">–</a><a href="#core-B153" role="doc-biblioref" data-xml-rid="B153" id="body-ref-B153-2" href-manipulated="true" aria-label="Reference 153">153</a>)</td></tr><tr><td>CR8020</td><td>HA-stalk (H3, H4, H7, H10, H14, and H15)</td><td>1.1–13.1</td><td>Pre: 3 mg/kg<br>Post: 15 mg/kg</td><td>−</td><td>Intravenous</td><td>I NCT01756950,<br>January 2013 to November 2013;<br>II NCT01938352,<br>October 2013 to January 2014</td><td>(<a href="#core-B150" role="doc-biblioref" data-xml-rid="B150" id="body-ref-B150-3" href-manipulated="true" aria-label="Reference 150">150</a>)</td></tr><tr><td>VIS410</td><td>HA-stalk (H1, H2, H3, H5, H6, H7, and H9)</td><td>0.3–11</td><td>Post: 2.5 mg/kg</td><td>Mild diarrhea</td><td>Intravenous</td><td>I NCT02045472,<br>September 2014 to May 2015;<br>II NCT02989194,<br>January 2017 to October 2017</td><td>(<a href="#core-B154" role="doc-biblioref" data-xml-rid="B154" id="body-ref-B154-1" href-manipulated="true" aria-label="Reference 154">154</a><a href="#core-B155" role="doc-biblioref" data-xml-rid="B155 B156" id="body-ref-B156-1" href-manipulated="true">–</a><a href="#core-B157" role="doc-biblioref" data-xml-rid="B157" id="body-ref-B157-1" href-manipulated="true" aria-label="Reference 157">157</a>)</td></tr><tr><td>MHAA4549A</td><td>HA-stalk<br>(H1, H2, H3, H5, and H7)</td><td>1.3–45.1</td><td>Post: 100 and 900 µg</td><td>Headache</td><td>Intravenous</td><td>I NCT01877785,<br>July 2013 to November 2013;<br>I NCT02284607,<br>November 2014 to March 2015;<br>IIa NCT01980966,<br>November 2013 to June 2014;<br>IIb NCT02293863,<br>January 2015 to May 2017</td><td>(<a href="#core-B158" role="doc-biblioref" data-xml-rid="B158" id="body-ref-B158-1" href-manipulated="true" aria-label="Reference 158">158</a><a href="#core-B159" role="doc-biblioref" data-xml-rid="B159 B160" id="body-ref-B160-1" href-manipulated="true">–</a><a href="#core-B161" role="doc-biblioref" data-xml-rid="B161" id="body-ref-B161-1" href-manipulated="true" aria-label="Reference 161">161</a>)</td></tr><tr><td>MEDI8852</td><td>All HA-stalk</td><td>0.064</td><td>Pre: 1 mg/kg<br>Post: 10 mg/kg</td><td>Headache, hypoglycemia, and bronchitis</td><td>Monotherapy</td><td>I II NCT02350751,<br>December 2015 to December 2016</td><td>(<a href="#core-B162" role="doc-biblioref" data-xml-rid="B162" id="body-ref-B162-1" href-manipulated="true" aria-label="Reference 162">162</a>, <a href="#core-B163" role="doc-biblioref" data-xml-rid="B163" id="body-ref-B163-1" href-manipulated="true" aria-label="Reference 163">163</a>)</td></tr><tr><td>TCN-032</td><td>M2e (H1, H2, H3, H5, H7, andH9)</td><td>−</td><td>−</td><td>−</td><td>Monotherapy</td><td>I NCT01390025,<br>September 2011 to March 2012;<br>II NCT01719874,<br>August 2012 to March 2013</td><td>(<a href="#core-B145" role="doc-biblioref" data-xml-rid="B145" id="body-ref-B145-2" href-manipulated="true" aria-label="Reference 145">145</a>, <a href="#core-B164" role="doc-biblioref" data-xml-rid="B164" id="body-ref-B164-1" href-manipulated="true" aria-label="Reference 164">164</a>)</td></tr><tr><td>1G01</td><td>N1-N9 NA<br>and IBV NA</td><td>0.01–2</td><td>Pre: 0.3 mg/kg<br>Post: 5 mg/kg</td><td>−</td><td>−</td><td>−</td><td>(<a href="#core-B129" role="doc-biblioref" data-xml-rid="B129" id="body-ref-B129-2" href-manipulated="true" aria-label="Reference 129">129</a>)</td></tr></tbody></table><p><a href="https://journals.asm.org/doi/10.1128/mbio.00175-2x4table">© Yanbai Li, et al; 2024</a></p><h2 id="Vaccine">Vaccine</h2><p>The circulating of the influenza: H3N2 (60%), H1N1 (15%), B (25%). (Paul Van Buynder; 2016)</p><p>For influenza A, technically, the <strong>antigenic drift</strong> make leads to epidemic, but the <strong>antigenic shift</strong> is the one that leads to pandemic.<br>Influenza B only causes epidemic, not pandemic.</p><p>For influenza vaccine, there are:</p><ul><li>SD-IIV: Standard-dose, for most people (most common)</li><li>HD-IIV: High-dose, for elderly (more antigens)</li><li>LAIV: Live attenuated, nasal spray</li><li>aIIV: Adjuvanted, for stronger immune response</li><li>ccIIV: Cell culture-based</li><li>RIV: Recombinant Influenza Vaccine.</li></ul><p>IIV = Inactivated Influenza Vaccine (the virus is killed, not live)</p><h3 id="Limitation-of-the-Vaccine">Limitation of the Vaccine</h3><ol><li><strong>Predicted sub-strains</strong><br>Unlike other vaccine, because the flu happens almost every year, getting the vaccine is suggested before each flue season.<br>The sub-strain of the virus are <strong>predicted</strong> based on the previous year. So, the vaccine only getting around 60% of protections.</li><li><strong>Egg-based vaccine</strong><br>95% of the vaccine are made from eggs. Birds’ sialic acid connection is slightly different from human’s. When the influenza virus infects the eggs, they would mutates to adapt to the eggs. This could lead to a <strong>mismatch</strong> between the vaccine and the circulating virus.</li></ol><p><img src="https://imgur.com/1veUScC.png" alt="sialic acid difference between human and avian"></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Influenza</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Notes/Biology/"/>
    
    <category term="Immunity" scheme="https://karobben.github.io/categories/Notes/Biology/Immunity/"/>
    
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Antibody" scheme="https://karobben.github.io/tags/Antibody/"/>
    
    <category term="Immunology" scheme="https://karobben.github.io/tags/Immunology/"/>
    
    <category term="Influenza" scheme="https://karobben.github.io/tags/Influenza/"/>
    
  </entry>
  
  <entry>
    <title>Rosetta, the Pioneer of Protein Structure Prediction</title>
    <link href="https://karobben.github.io/2025/04/10/AI/rosetta/"/>
    <id>https://karobben.github.io/2025/04/10/AI/rosetta/</id>
    <published>2025-04-10T18:25:47.000Z</published>
    <updated>2025-04-10T18:43:07.210Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Rosetta">Rosetta</h2><p>The Rosetta suite has significantly transformed the landscape of computational structural biology by providing an integrated platform for protein structure prediction, design, and engineering. Initially developed for de novo protein folding simulations, Rosetta has evolved into a diverse ecosystem of tools that address a broad spectrum of biological questions, from understanding protein dynamics and stability to enabling the design of novel therapeutic molecules. Its success lies in the combination of physics-based energy functions, extensive conformational sampling, and algorithmic innovations that together replicate the complex interplay of forces governing protein structure. In this paper, I will introduce the myriad of tools derived from the Rosetta framework, exploring how each extension contributes uniquely to our capacity to model, predict, and even design biomolecules with high precision. This exploration not only underscores Rosetta’s pivotal role in advancing molecular biology research but also highlights its versatility in adapting to the ever-evolving challenges in the field.</p><h2 id="References">References</h2><ul><li><a href="https://www.youtube.com/watch?v=Eez9LsOcaI0">David Baker: Protein Design Using Deep Learning</a></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Rosetta, the Pioneer of Protein Structure Prediction</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="LM" scheme="https://karobben.github.io/categories/Machine-Learning/LM/"/>
    
    <category term="Protein" scheme="https://karobben.github.io/categories/Machine-Learning/LM/Protein/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="AI" scheme="https://karobben.github.io/tags/AI/"/>
    
    <category term="3D" scheme="https://karobben.github.io/tags/3D/"/>
    
    <category term="Protein Structure" scheme="https://karobben.github.io/tags/Protein-Structure/"/>
    
  </entry>
  
  <entry>
    <title>AlphaFold</title>
    <link href="https://karobben.github.io/2025/04/05/AI/alphafold/"/>
    <id>https://karobben.github.io/2025/04/05/AI/alphafold/</id>
    <published>2025-04-05T23:33:11.000Z</published>
    <updated>2025-04-10T04:51:41.888Z</updated>
    
    <content type="html"><![CDATA[<h2 id="AlphaFold2">AlphaFold2</h2><p>Main source from:</p><ul><li>YouTube:<ul><li><a href="https://www.youtube.com/watch?v=jTO6odQNp90">Kendrew Lecture 2021 pt2 - John Jumper</a>: first author of AlphaFold2</li><li><a href="https://www.youtube.com/watch?v=qjFgthkKxcA">Review and discussion of AlphaFold3; 2024</a>: Sergey Ovchinnikov, MIT</li><li><a href="https://www.youtube.com/watch?v=yJKfn6rvHmg">MRC Laboratory of Molecular Biology; 2024</a></li><li><a href="https://www.youtube.com/watch?v=7q8Uw3rmXyE">What Is AlphaFold? | NEJM</a></li><li><a href="https://www.youtube.com/watch?v=3gSy_yN9YBo">how AlphaFold <em>actually</em> works</a></li></ul></li><li>Papers:<ul><li>Jumper J, Evans R, Pritzel A, et al. Highly accurate protein structure prediction with AlphaFold[J]. nature, 2021, 596(7873): 583-589.</li><li>Abramson J, Adler J, Dunger J, et al. Accurate structure prediction of biomolecular interactions with AlphaFold 3[J]. Nature, 2024, 630(8016): 493-500.</li></ul></li></ul><p><img src="https://imgur.com/a0ULWSB.png" alt=""></p><p>Main architecture of the AlphaFold2:</p><ul><li>Input<ul><li><strong>MSA</strong>: Multiple Sequence Alignment, to align the sequences to gain the evolutionary information. The idea is: When residues pair-mutated, they are highly likely to be contact with each other. So, the model could learn the contact map from the MSA.</li><li><strong>Pairing</strong>: It might the <strong>most important</strong> part for the model. Captures pairwise relationships between residues.<ul><li>At the beginning, there are almost no information.</li><li><strong>triangle inequality</strong>: ij+kj&gt;=ik (i, j, k are the residues)</li></ul></li><li><strong>Structure Database</strong>: The model will use the structure database to get the information of the protein structure.</li><li><mark>MSA results</mark> would be used to generate the <strong>MSA representation</strong>. <mark>Pairing</mark> and <mark>Structure Database</mark> would be used to generate the <strong>Pair representation</strong>.</li></ul></li><li>Evoformer<ul><li>The Evoformer is a transformer model which is used to learn the MSA representation and Pair representation. The Evoformer will use the MSA representation and Pair representation to refine the <strong>MSA representation</strong> and <strong>Pair representation</strong></li></ul></li><li>Structure<ul><li>The structure module would use the <strong>representations</strong> to do rotation and translation to generate the final structure.</li></ul></li><li>Recycle:<ul><li>The final structure would be collected and used to refine the <strong>refined representations</strong> in the <strong>Evoformer</strong> module and <strong>Structure</strong> module multiple times.</li></ul></li></ul><h2 id="Multiple-Sequence-Alignment-MSA">Multiple Sequence Alignment (MSA)</h2><p>Multiple sequences are critical for the prediction of protein structures. The more sequences you have, the better the prediction. Even taking away the sequences in PDB databse, AF2 cans till give a very good prediction. “The sequence is very very clear for the structure” (John)</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:left">MSA is important</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/yaOCXym.png" alt=""></td><td style="text-align:left">How does the Multiple Sequence Alignment (MSA) help: the sequences alignment could help to encode the residues contact map. When a residues mutated, the contact residues are likely to be mutated, too. This kind of features could be captured by models.</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/agW7Dyw.png" alt=""></td><td style="text-align:left">Without the MSA, the predicted result is terrible.</td></tr></tbody></table><table><thead><tr><th style="text-align:center">MAS</th><th style="text-align:center">pLDDT</th><th style="text-align:center">PAE</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/537sn8D.png" alt=""></td><td style="text-align:center"><img src="https://imgur.com/HzSz3t5.png" alt=""></td><td style="text-align:center"><img src="https://imgur.com/BtkO7wh.png" alt=""></td></tr></tbody></table><p>As you can see from this graphic, the depth of the MSA results are difference. The depth would effect the final quality of the model. And the depth of the MSA is also highly positive associated with the pLDDT score. The more sequences you have, the better the prediction. Except the depth, the pLDDT score is also highly related to the second structures. Random loop usually get a low pLDDT.</p><p><strong>PAE</strong>’s concept is pretty similar to pLDDT. pLDDT is predicts the single residue confidence, while PAE is predicts the pairwise residue confidence. The higher the PAE score, the more confident the model is about the distance between two residues.</p><h2 id="Results">Results</h2><ul><li><strong>pLDDT</strong>: a per-atom confidence estimate on a 0-100 scale where a higher value indicates higher confidence.</li><li><strong>PAE</strong> (predicted aligned error): estimate of the error in the relative position and orientation between two tokens in the predicted structure.</li><li><strong>pTM</strong> and <strong>ipTM</strong> scores: the <mark>predicted template modeling</mark> (pTM) score and the interface predicted template modeling (ipTM) score are both derived from a measure called the template modeling ™ score. This measures the accuracy of the <s>entire structure</s></li></ul><p>In alphafold2, you’ll get 5 results for each seed because there are 5 different models which trained slightly different. The model would rank the results by pLDDT, pTM, and, ipTM.</p><p>You may tell the false positive based on the PEA score</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/xXFSAfk.png" alt="False Negative Prediction"></td><td style="text-align:left"><strong>False Negative</strong>: The results from the top is predicted incorrectly. The PAE score between chain with different color is very high. The results from the bottom is predicted correctly. The PAE score between chain with different color is very low which means the result my reliable.</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/QeZG4is.png" alt="False Positive"></td><td style="text-align:left"><strong>False Positive</strong>: Though the PAE are very good, but they are not supposed to be interact to each other. It could be caused by some protein from other families may interacted in this way. So, the model learned this possibilities.</td></tr></tbody></table><table><thead><tr><th style="text-align:center"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41586-021-03819-2/MediaObjects/41586_2021_3819_Fig4_HTML.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">Ablation results on two target sets: the CASP14 set of domains (n = 87 protein domains) and the PDB test set of chains with template coverage of ≤30% at 30% identity (n = 2,261 protein chains). Domains are scored with GDT and chains are scored with lDDT-Cα. The ablations are reported as a difference compared with the average of the three baseline seeds. Means (points) and 95% bootstrap percentile intervals (error bars) are computed using bootstrap estimates of 10,000 samples. b, Domain GDT trajectory over 4 recycling iterations and 48 Evoformer blocks on CASP14 targets LmrP (T1024) and Orf8 (T1064) where D1 and D2 refer to the individual domains as defined by the CASP assessment. Both T1024 domains obtain the correct structure early in the network, whereas the structure of T1064 changes multiple times and requires nearly the full depth of the network to reach the final structure. Note, 48 Evoformer blocks comprise one recycling iteration.</td></tr></tbody></table><h2 id="Limitations">Limitations</h2><p>As you can expect, MSA brings huge convenience to the model. But it also brings the major limitation: it not sensitive to the mutations. In some cases, a point mutation would make the inner structure or interaction between proteins fail. But the MSA would not be able to capture this kind of information and database lacking such structures. So, the model would have high false positive rate. This features makes antibody-antigen interaction prediction very hard. (It was said that RoseTTAFold can do much better on handling mutations.)</p><p>On the other hand, as long as AF relies on the MSA, it is not good at fast mutated proteins interactions. For example, the virus protein and antibodies. Virus mutation every years and antibodies mutated every few days<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>. MSA would more reliable on the co-evolution over thousands or millions of years. But for the fast mutated proteins, the MSA would not be able to capture the co-evolution information. So, the model would not be able to learn the structure from scratch.</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/lLYMCB4.png" alt=""></td><td style="text-align:left">It is also not hard to imagine that the model would not be able to predict the structure of the protein which has no homologous sequences. The model is not able to learn the structure from scratch.</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/BCTQZeZ.png" alt=""></td><td style="text-align:left">It seems like AF2 can get a much better DockQ score then the other models. But the this kind of compairation is very tricky</td></tr></tbody></table><p>John Jumper claimed that the AF2-multimer “performs poorly on antibody interactions”, could be caused by “still miss quite a few interactions”</p><h2 id="AlphaFold3">AlphaFold3</h2><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/fQM7ut8.png" alt=""></th></tr></thead><tbody></tbody></table><p>In AF3, John Jumper became the first corresponding author with Demis. The first author is Josh Abramson.</p><p>The main difference between AF2 and AF3 is that AF3 moved the structure model out of the cycling and only put the Pairformer in the cycling. The structure model is only used once. And it is replaced by the Diffusion model. Meanwhile, the contribution of MSA was changed. In Af3, there is an independent MSA module and only 4 blocks. If Af2, MSA would used for 48 blocks. This change makes the training much faster. For 5000 tokens, AF2 would takes about 2 to 3 days. But now, it only takes about 20 minutes.</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/2TEVTBI.png" alt="AF3 vs Af2"></td><td style="text-align:left">AF3 made some improvements on monomers and prtotein-protein interaction prediction caompare to AF2. It seems like the AF3 made huge improvement on the protein-antibody prediction. But according to Sergey, this comparision is very tricky. They didn’t take the results by single try. They actually test different seeds independently to get over 1000 results. And then, they took the best result. So, the results are not very reliable. This hind may tell use that diffusion model may not solve the sampling problem well</td></tr></tbody></table><ul><li>reproducibility: For AF model, if you change the random seeds, the result would be totally different. So, when we try to use it to predict the protein-antibody, we would got all different results and it is hard to tell which one is the true positive.</li></ul><div class="admonition note"><p class="admonition-title">Why Diffusion model doesn't work well in protein-antibody prediction?</p><p>Personally, I think the main limitation still bring from the MSA. The homology modeling method would be ok for protein-protein prediction because this type of interaction could be described and protein family-family interaction. But for antibodies, the interaction is very specific and restricted to the unique loop region which mostly not exist in the any of database. Meanwhile, the contribution of the residues are extremely unbalanced. Some residues credit as motif plays the most important role in the recognition. Rest of residues are maybe play the role in supporting the structure, complementary to the surface to present the motif in a correct position, etc. So, antibodies are more sensitive to the surface structure. But the diffusion model would fail in the initial interface prediction. And during the de-noise process, this error would be amplified. Meanwhile, the model doesn't know if the antibody can dock to the target or not. During the diffusion steps, it only tries is best to dock on it during the de-noise steps and make false positive result.</p></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p><a href="https://www.youtube.com/watch?v=oSTHQQYoGQs">Jeffrey Gray: Artificial Intelligence Tools for Antibody Engineering and Protein Docking; 2024; YouTube</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">AlphaFold2</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="LM" scheme="https://karobben.github.io/categories/Machine-Learning/LM/"/>
    
    <category term="Protein" scheme="https://karobben.github.io/categories/Machine-Learning/LM/Protein/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="AI" scheme="https://karobben.github.io/tags/AI/"/>
    
    <category term="3D" scheme="https://karobben.github.io/tags/3D/"/>
    
    <category term="Protein Structure" scheme="https://karobben.github.io/tags/Protein-Structure/"/>
    
  </entry>
  
  <entry>
    <title>Hemagglutinin, the Influenza Virus Protein</title>
    <link href="https://karobben.github.io/2025/03/13/LearnNotes/hemagglutinin/"/>
    <id>https://karobben.github.io/2025/03/13/LearnNotes/hemagglutinin/</id>
    <published>2025-03-13T17:45:19.000Z</published>
    <updated>2025-04-13T04:14:55.833Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://cdn.rcsb.org/pdb101/motm/302/H5_1JSN_1RVT.png" alt=""><br><a href="https://pdb101.rcsb.org/motm/302">© PDB</a></td><td style="text-align:left">Influenza is able to enter and infect cells through the action of hemagglutinin, which recognizes and attaches to specific molecules on the cell surface. Most hemagglutinins have been found to target sialic acids, a family of nine-carbon sugar molecules that are commonly found at the tips of glycans, or sugar chains, that are attached to proteins and lipids on the cell surface. Sialic acid can be linked to glycans in different ways.</td></tr></tbody></table><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://www.pnas.org/cms/10.1073/pnas.1810927115/asset/7e80d138-dd37-4523-b210-b015a43262b9/assets/graphic/pnas.1810927115fig04.jpeg" alt=""><br><a href="https://www.pnas.org/doi/10.1073/pnas.1810927115">© Donald J. Benton, 2018</a></td><td style="text-align:left">(A) Cryotomogram section showing cross-section of a liposome with examples of HAs tilted with respect to the lipid bilayer (white boxes). (Scale bar: 20 nm.)<br>(B) Gallery of subtomograms of tilted HA in liposomes. Images in the second row are identical to those above but indicate HAs (blue) and liposome bilayer (red lines). (Scale bar: 10 nm.)</td></tr></tbody></table><h2 id="Sialic-Acid">Sialic Acid</h2><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/b0FwqbS.png" alt=""></th><th style="text-align:center"><img src="https://imgur.com/LGLAWv0.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">Birds sialic acid (Neu5Ac) with α-2,3 bound whith also happen in <strong>human deep lung</strong></td><td style="text-align:center">α-2,6 sialic acid (Neu5Ac) bound with also happen in human <strong>upper respiratory tract</strong>.</td></tr></tbody></table><p><strong>© Dr.G Bhanu</strong><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p><h2 id="Hemagglutinin-Cleavage">Hemagglutinin Cleavage</h2><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/JHTlQyR.png" alt=""><br>© Dr.G Bhanu<sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1:1]</a></sup></td><td style="text-align:left">Hemagglutinin (HA) has lots of conformations. The HA0 is the original form of the protein on the virus surface. It then cleaved into HA1 and HA2 by host proteases in the endosomes. Proteases only cleave inner cell HA0<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>. After the cleavage, the fusion peptid is exposed which is in the end of the HA2</td></tr></tbody></table><h2 id="Conformation-Change-of-Hemagglutinin">Conformation Change of Hemagglutinin</h2><table><thead><tr><th style="text-align:center"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41586-020-2333-6/MediaObjects/41586_2020_2333_Fig1_HTML.png?as=webp" alt=""></th><th style="text-align:left"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41586-020-2333-6/MediaObjects/41586_2020_2333_Fig3_HTML.png?as=webp" alt=""></th></tr></thead><tbody></tbody></table><p><a href="https://www.nature.com/articles/s41586-020-2333-6?fromPaywallRec=false">© Donald J. Benton, 2020</a></p><p>At low pH in endosomes, about pH 5.5, depending on the virus strain, fusion of virus and endosomal membranes is activated in a process that involves extensive changes in HA conformation<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></p><h2 id="Anchor-of-the-HA-Protein">Anchor of the HA Protein</h2><p>Donald J. Benton, et al., observe the structure of the region that anchors HA in the virus membrane as <strong>a bundle of three α-helices</strong> that are joined to the ectodomain by flexible linkages. And the <strong>fab</strong>, HA−FISW84, binds to near there (<a href="https://www.rcsb.org/structure/6HJQ">6HJQ</a>) my affects the <strong>flexibility</strong> of the linker and prevents the conformational changes that are necessary for membrane <strong>fusion</strong><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>.</p><table><thead><tr><th style="text-align:center"><img src="https://www.pnas.org/cms/10.1073/pnas.1810927115/asset/c53d13e7-4b4c-4de9-8bb5-57449231b859/assets/graphic/pnas.1810927115fig02.jpeg" alt=""></th><th style="text-align:left">The structure of the membrane-associated region. Detailed views of (A) tilted and (B) straight micelles, as shown in Fig. 1 B and C respectively.</th></tr></thead><tbody></tbody></table><blockquote><p><a href="https://www.pnas.org/doi/10.1073/pnas.1810927115">© Donald J. Benton, 2018</a>: The flexible linker region (purple) runs between Gly-175, at the C terminus of the 160 helix, and Gly-182 and extends to the N termini of the α-helices of a trimeric α-helical bundle, residues 186 to 203 (cyan). (C) The amino acid sequence of the transmembrane domain. The sequence shown begins at the 160 helix to the C terminus of HA2. Color-coded block diagrams indicate the positions of these structural elements in the sequence.</p></blockquote><h2 id="Fusion-Model">Fusion Model</h2><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://journals.asm.org/cms/10.1128/jvi.00240-16/asset/65d32174-509e-4279-8b22-72a0325989fa/assets/graphic/zjv9991818150002.jpeg" alt=""></td><td style="text-align:left">A 3.2-nm-thick computational slice through a reconstructed cryo-electron tomogram shows several examples of HA bridging (blue arrowheads) between virus particles and 80% DOPC:20% Chol liposomes after 30 s of incubation at pH 5.5<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup></td></tr></tbody></table><table><thead><tr><th style="text-align:center"><img src="https://journals.asm.org/cms/10.1128/jvi.00240-16/asset/e96f8f67-a919-4f4b-9c14-047f5ccec992/assets/graphic/zjv9991818150003.jpeg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://journals.asm.org/cms/10.1128/jvi.00240-16/asset/2cbd36cf-fc05-48a9-a525-a9e2ff260d61/assets/graphic/zjv9991818150010.jpeg" alt=""></td></tr><tr><td style="text-align:center"><a href="https://journals.asm.org/doi/10.1128/jvi.00240-16">© Lee lab</a></td></tr></tbody></table><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p><a href="https://www.youtube.com/watch?v=GyNyFqWtFyk">Influenza Virus Microbiology Animation; YouTuBe; 2020</a> <a href="#fnref1" class="footnote-backref">↩︎</a> <a href="#fnref1:1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Zhirnov O P, Ikizler M R, Wright P F. Cleavage of influenza a virus hemagglutinin in human respiratory epithelium is cell associated and sensitive to exogenous antiproteases[J]. Journal of virology, 2002, 76(17): 8682-8689. <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>PA Bullough, FM Hughson, JJ Skehel, DC Wiley, Structure of influenza haemagglutinin at the pH of membrane fusion. Nature 371, 37–43 (1994). <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p><a href="https://www.pnas.org/doi/10.1073/pnas.1810927115">Benton D J, Nans A, Calder L J, et al. Influenza hemagglutinin membrane anchor[J]. Proceedings of the National Academy of Sciences, 2018, 115(40): 10112-10117.</a> <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p><a href="https://journals.asm.org/doi/10.1128/jvi.00240-16">Gui L, Ebner J L, Mileant A, et al. Visualization and sequencing of membrane remodeling leading to influenza virus fusion[J]. Journal of virology, 2016, 90(15): 6948-6962.</a> <a href="#fnref5" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">Hemagglutinin is a protein found on the surface of the influenza virus.</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Notes/Biology/"/>
    
    <category term="Immunity" scheme="https://karobben.github.io/categories/Notes/Biology/Immunity/"/>
    
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Antibody" scheme="https://karobben.github.io/tags/Antibody/"/>
    
    <category term="Immunology" scheme="https://karobben.github.io/tags/Immunology/"/>
    
    <category term="Influenza" scheme="https://karobben.github.io/tags/Influenza/"/>
    
  </entry>
  
  <entry>
    <title>Protein Loop Refinement</title>
    <link href="https://karobben.github.io/2025/03/03/Bioinfor/protein-loop/"/>
    <id>https://karobben.github.io/2025/03/03/Bioinfor/protein-loop/</id>
    <published>2025-03-03T15:19:36.000Z</published>
    <updated>2025-03-04T02:26:40.556Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DaReUS-Loop-2018-Web-Server-Only">DaReUS-Loop 2018 (Web Server Only)</h2><table><thead><tr><th style="text-align:center"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-018-32079-w/MediaObjects/41598_2018_32079_Fig1_HTML.png" alt=""></th></tr></thead><tbody></tbody></table><p>DaReUS-Loop (Data-based approach using Remote or Unrelated Structures for Loop modeling) (<a href="https://mobyle.rpbs.univ-paris-diderot.fr/cgi-bin/portal.py#forms::DaReUS-Loop">Web Server</a>) is a data-based approach that identifies loop candidates mining the complete set of experimental structures available in the Protein Data Bank. Candidate filtering relies on local conformation profile-profile comparison, together with physico-chemical scoring<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>. This work is the extension of using Binet-Cauchy kernel to mine large collections of structures efficiently<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>.</p><p>The database they used: CASP11, CASP12 and HOMSTRAD.</p><p>In this paper, They classified protein loop-modeling tools are generally into three main categories:</p><ol><li><strong>Knowledge-based (Template-based) Methods</strong>:<ul><li>These approaches utilize structural repositories to extract observed loop conformations that match specific sequences and geometric constraints. By mining databases of known protein structures, they identify loop candidates that fit the target region, offering computational efficiency. However, their effectiveness is limited by the availability of suitable loop conformations in existing structural data.</li><li>Exp: SuperLooper2<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> mines the Loop In Protein (LIP) database;<ul><li>MoMA-LoopSampler<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>: a knowledge-based method that uses a database of loop fragments to model missing segments in proteins. It has <a href="https://moma.laas.fr/results/">web server</a> but queue is too long to wait.</li></ul></li></ul></li></ol><ol start="2"><li><strong>Ab Initio (De Novo) Methods</strong>:<ul><li>These techniques involve sampling a wide range of possible loop conformations <strong>without relying on existing structural templates</strong>. They <strong>dependent on energy optimization techniques and are consequently highly time consuming</strong>. They often employ <strong>exhaustive searches</strong> of the loop’s torsional angles to explore conformational space. While capable of modeling loops without suitable templates, ab initio methods are computationally intensive and typically more successful with <strong>shorter loops</strong> due to the vast number of possible conformations.</li><li>Exp:Rosetta Next-Generation KIC (NGK)<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>, GalaxyLoop-PS2<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup></li></ul></li></ol><ol start="3"><li><strong>Hybrid Methods</strong>:<ul><li>Combining elements of both knowledge-based and ab initio approaches, hybrid methods use small structural fragments from databases within an ab initio sampling framework. This integration aims to balance computational efficiency with modeling accuracy, leveraging known structural motifs to guide the exploration of conformational space.</li><li>Exp: Sphinx<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup></li></ul></li></ol><h3 id="How-to-Use">How to Use</h3><p>In the <a href="https://mobyle.rpbs.univ-paris-diderot.fr/cgi-bin/portal.py#forms::DaReUS-Loop">Web Server</a>, you can upload your protein structure file and the sequences.<br>For the sequences, you need to <mark>mask your loop with gap (‘-’)</mark>.</p><p>In my case, I masked a 14 amino acid loop in the sequence and this is the log:</p><pre>[00:00:00] Please bookmark this page to have access to your results![00:00:00] 1/14: Verifying input files[00:00:00] Warning! Only the first chain from the input PDB is considered![00:00:11] 2/14: BCLoopSearch[00:01:42] 3/14: clustering[00:01:59] 4/14: Conformational profiles[00:04:16] Found 353 hits for Loop1_AMTMVVASFFQYYA[00:04:16] 5/14: Measuring Jensen Shannon distances10%..20%..30%..40%..50%..60%..70%..80%..90%..100%[00:08:06] 6/14: selecting top candidates per loop[00:08:21] 7/14: preparing top candidates for minimization[00:08:43] 8/14: positioning linker side chains[00:09:36] 9/14: minimization[00:11:01] 10/14: Scoring the candidates[00:11:01] ... 3 jobs already in the queue, please wait ...[00:23:16] 11/14: Measuring energy values (KORP potential)[00:23:45] 12/14: Preparing the final energy report[00:23:55] 13/14: Generating combinatorial models[00:24:04] 14/14: detecting possible clashes between candidates[00:24:34] ... 3 jobs already in the queue, please wait ...[00:26:46] ... 2 jobs already in the queue, please wait ...[00:27:52] Finished</pre><p><img src="https://imgur.com/vzrRJXM.png" alt=""></p><p>In the end, you can download the top  10 models.<br>I compared it with the original structure and the loop is refined and find <strong>none of them are making sense</strong>. Another biggest problem is that they <strong>only</strong> took the chain with masked loops and <strong>disgard the rest of other chains</strong>. In this case, lots of results would have a serious <strong>atom crush</strong> problem. So, it takes about half an hour to get the result and the result is not good even for a very small protein.</p><p>By checking the performance from the paper, it also doesn’t show a dramatic improvements.</p><p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-018-32079-w/MediaObjects/41598_2018_32079_Fig5_HTML.png" alt=""></p><h2 id="Foldseek-2024">Foldseek 2024</h2><p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41587-023-01773-0/MediaObjects/41587_2023_1773_Fig1_HTML.png" alt=""></p><p>FOldSeek<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> also has a <a href="https://search.foldseek.com">web server</a>. It also deposited there codes on Github at <a href="https://github.com/steineggerlab/foldseek">steineggerlab/foldseek</a>. After that, the also development foldseek-multimer<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> for protein complex alignment.</p><details><summary>Abstract</summary>As structure prediction methods are generating millions of publicly available protein structures, searching these databases is becoming a bottleneck. Foldseek aligns the structure of a query protein against a database by describing tertiary amino acid interactions within proteins as sequences over a structural alphabet. Foldseek decreases computation times by four to five orders of magnitude with 86%, 88% and 133% of the sensitivities of Dali, TM-align and CE, respectively.</details><p>It is a fast loop searching tool which is very powerful if you want to use it to search the loop in a large database. It is a very good tool if you want to development a new knowledge-based loop refinement tool. <mark>You can’t use it to refine the loop directly.</mark></p><h2 id="KarmaLoop-2024">KarmaLoop 2024</h2><table><thead><tr><th style="text-align:center"><img src="https://spj.science.org/cms/10.34133/research.0408/asset/ccac7451-395e-4551-b33e-ed4cf0bd212d/assets/graphic/research.0408.fig.001.jpg" alt=""></th></tr></thead><tbody></tbody></table><p>KarmaLoop<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup> is an <a href="https://github.com/karma211225/KarmaLoop">open source</a> deep learning-based framework designed for rapid and precise full-atom protein loop modeling. It addresses the challenges of predicting loop conformations, which are critical for accurate protein structure determination.</p><p><strong>Key Features of KarmaLoop</strong>:</p><ul><li><p><strong>Full-Atom Modeling</strong>: Unlike many methods that focus primarily on backbone atoms, KarmaLoop predicts the positions of both backbone and side-chain heavy atoms, providing comprehensive loop conformations.</p></li><li><p><strong>Deep Learning Architecture</strong>: The framework employs advanced neural network architectures, including Graph Transformers and Geometric Vector Perceptrons, to effectively capture intricate structural patterns within proteins.</p></li><li><p><strong>High Accuracy and Efficiency</strong>: KarmaLoop has demonstrated superior performance in both accuracy and computational speed compared to traditional and other deep learning-based methods. For instance, it achieved average root-mean-square deviations (RMSDs) of 1.77 Å and 1.95 Å on the CASP13+14 and CASP15 benchmark datasets, respectively, with a significant speed advantage over other methods. citeturn0search0</p></li><li><p><strong>Versatility</strong>: The tool has shown effectiveness in modeling general protein loops as well as specific regions like antibody complementarity-determining region (CDR) H3 loops, which are known for their structural complexity.</p></li></ul><h3 id="How-to-Use-v2">How to Use</h3><p>It is very easy to installing.<br>Before you refine you loop, you need to truncate you structure with the script they provide and convert it into graphic. Then, you can use their pre-trained model to refine your structure.</p><p><mark>The problem</mark> is the script to make the graphic has different features on the model the give you. So, you can use their model, at all. It seems they updated the codes for model alone, but forget to update the codes for graphic making. Someone asked this question on the github but didn’t get any response.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style><h2 id="PLM-GAN">PLM-GAN</h2><p>PLM-GAN looks like a good model. But they deposited their trained model on github as a large file and we can’t download it any more. So, we can’t test the performace of it.</p><p>It applies Generative Adversarial Networks (GANs) to model loop structures, leveraging the generator-discriminator framework to produce realistic loop conformations. It utilizes the pix2pix GAN model to generate and inpaint protein distance matrices, facilitating the folding of protein structures</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Karami Y, Guyon F, De Vries S, et al. DaReUS-Loop: accurate loop modeling using fragments from remote or unrelated proteins[J]. Scientific reports, 2018, 8(1): 13673. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Guyon F, Tufféry P. Fast protein fragment similarity scoring using a binet–cauchy kernel[J]. Bioinformatics, 2014, 30(6): 784-791. <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Ismer J, Rose A S, Tiemann J K S, et al. SL2: an interactive webtool for modeling of missing segments in proteins[J]. Nucleic acids research, 2016, 44(W1): W390-W394. <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>MoMA-LoopSampler: a web server to exhaustively sample protein loop conformations <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>Stein A, Kortemme T. Improvements to robotics-inspired conformational sampling in rosetta[J]. PloS one, 2013, 8(5): e63090. <a href="#fnref5" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p>Park H, Lee G R, Heo L, et al. Protein loop modeling using a new hybrid energy function and its application to modeling in inaccurate structural environments[J]. PloS one, 2014, 9(11): e113811. <a href="#fnref6" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p>Marks C, Nowak J, Klostermann S, et al. Sphinx: merging knowledge-based and ab initio approaches to improve protein loop prediction[J]. Bioinformatics, 2017, 33(9): 1346-1353. <a href="#fnref7" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p>Van Kempen M, Kim S S, Tumescheit C, et al. Fast and accurate protein structure search with Foldseek[J]. Nature biotechnology, 2024, 42(2): 243-246. <a href="#fnref8" class="footnote-backref">↩︎</a></p></li><li id="fn9" class="footnote-item"><p>Kim W, Mirdita M, Levy Karin E, et al. Rapid and sensitive protein complex alignment with foldseek-multimer[J]. Nature Methods, 2025: 1-4. <a href="#fnref9" class="footnote-backref">↩︎</a></p></li><li id="fn10" class="footnote-item"><p>Wang T, Zhang X, Zhang O, et al. Highly accurate and efficient deep learning paradigm for full-atom protein loop modeling with KarmaLoop[J]. Research, 2024, 7: 0408. <a href="#fnref10" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">Protein Loop refinement, Paper Read</summary>
    
    
    
    <category term="Paper" scheme="https://karobben.github.io/categories/Paper/"/>
    
    
    <category term="AI" scheme="https://karobben.github.io/tags/AI/"/>
    
    <category term="Protein" scheme="https://karobben.github.io/tags/Protein/"/>
    
    <category term="Structure" scheme="https://karobben.github.io/tags/Structure/"/>
    
  </entry>
  
  <entry>
    <title>AlexNet</title>
    <link href="https://karobben.github.io/2025/02/06/AI/alexnet/"/>
    <id>https://karobben.github.io/2025/02/06/AI/alexnet/</id>
    <published>2025-02-06T21:39:05.000Z</published>
    <updated>2025-02-06T23:18:49.849Z</updated>
    
    <content type="html"><![CDATA[<h2 id="AlexNet">AlexNet</h2><p>AlexNet is a convolutional neural network that won the ImageNet Large Scale Visual Recognition Challenge in 2012. It was designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. The network has eight layers, five of which are convolutional layers and three are fully connected layers. It uses ReLU activation functions, dropout for regularization, and data augmentation techniques to improve performance. AlexNet significantly advanced the field of deep learning and computer vision.</p><p>In the structure illustration, it was usually split into 2 parts. It is because backing that time, GPU has limited memory. So, they split each layer into 2 parts, and each part is trained on a different GPU.</p><p><img src="https://upload.wikimedia.org/wikipedia/commons/c/cc/Comparison_image_neural_networks.svg" alt="AlexNet, wikipedia"></p><p>As you can see from this illustration, the input imge is 227x227x3, which is then passed through five convolutional layers, followed by three fully connected layers. The output is a 1000-dimensional vector representing the probabilities of the image belonging to each of the 1000 classes in the ImageNet dataset.</p><p>Let’s break down the architecture of AlexNet:</p><ol><li><strong>Input Layer</strong>: The input layer takes an image of size 227x227x3 (height, width, channels or RGB values).</li><li><strong>Convolutional Layer 1</strong>: The first convolutional layer applies 96 filters of size 11x11 with a stride of 4, resulting in an output of size 55x55x96. This layer uses ReLU activation and is followed by a max pooling layer with a 3x3 window and a stride of 2.<ul><li>What does ReLU do? It introduces non-linearity into the model by replacing all negative pixel values in the feature map with zero. By doing this, the model can learm more complex patterns in the data. So, it basically makes the negative values as a closed signal.</li></ul></li><li><strong>Convolutional Layer 2</strong>: The second convolutional layer applies 256 filters of size 5x5 with a stride of 1, resulting in an output of size 27x27x256. This layer also uses ReLU activation and is followed by a max pooling layer with a 3x3 window and a stride of<br>…</li><li>Finally, we got a 5x5x256 feature map. This is then flattened into a 1D vector of size 6400. It then passes through three fully connected layers with 4096, 4096, and 1000 neurons, respectively. The final layer uses a softmax activation function to output the probabilities of the image belonging to each of the 1000 classes in the ImageNet dataset.</li></ol><p>So, as you can see, the original image contains 227x227x3 = 154,587. In daily life, the picture we using are 1920*1080, which could have 6,220,800 input. With this convolutional techniques, we can reduce the size of the image while still keeping the important information.</p><div class="admonition note"><p class="admonition-title">**Terminology** Explained</p><ul><li><strong>filters</strong>: The filters are also known as kernels, and they are used to detect features in the input image. In this case, the first convolutional layer uses 96 filters of size 11x11.</li><li><strong>stride</strong>: The stride is the number of pixels by which the filter is moved across the image. You can also think of it as the step size. In this case, the stride is 4, which means that the filter is moved 4 pixels at a time. <strong>55</strong> = (227 - 11) / 4 + 1</li><li><strong>5.6 times of decreasing</strong>:  Now, instsad of 227x227x3, we have 55x55x96. You can image that the image is now much smaller, but it still contains a lot of information. Though, the &quot;pixels&quot; are less than the original image, each information from the single pixel is now represented by 96 values rather than 3 values. By multiplying the number of pixels with the number of filters, we can see that the amount of information has increased significantly. (227x227x3 = 154,587, 55x55x9 = 27225)</li><li><strong>ReLU activation</strong>: The ReLU activation function is applied to introduce non-linearity into the model. It replaces all negative pixel values in the feature map with zero.</li><li><strong>Max Pooling</strong>: The max pooling layer reduces the spatial dimensions of the feature map by taking the <strong>maximum value in each 3x3 window</strong> with a stride of 2. This helps to reduce the computational complexity and makes the model more robust to variations in the input image. The difference between the convolutional layer and the max pooling layer is that the convolutional layer applies a filter to the input image, while the max pooling layer reduces the spatial dimensions of the feature map by taking the maximum value in each window.</li></ul></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">AlexNet is a convolutional neural network that won the ImageNet Large Scale Visual Recognition Challenge in 2012.</summary>
    
    
    
    <category term="AI" scheme="https://karobben.github.io/categories/AI/"/>
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/AI/Machine-Learning/"/>
    
    
    <category term="neuralnetworks" scheme="https://karobben.github.io/tags/neuralnetworks/"/>
    
    <category term="deeplearning" scheme="https://karobben.github.io/tags/deeplearning/"/>
    
    <category term="computer-vision" scheme="https://karobben.github.io/tags/computer-vision/"/>
    
  </entry>
  
  <entry>
    <title>esm, Evolutionary Scale Modeling</title>
    <link href="https://karobben.github.io/2025/01/22/Bioinfor/esm/"/>
    <id>https://karobben.github.io/2025/01/22/Bioinfor/esm/</id>
    <published>2025-01-22T16:06:18.000Z</published>
    <updated>2025-01-22T16:21:58.294Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Basic-Use">Basic Use</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> esm<br><br><span class="hljs-comment"># Load ESM-2 model</span><br>model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()<br>batch_converter = alphabet.get_batch_converter()<br>model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># disables dropout for deterministic results</span><br><br><span class="hljs-comment"># Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)</span><br>data = [<br>    (<span class="hljs-string">&quot;protein1&quot;</span>, <span class="hljs-string">&quot;MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG&quot;</span>),<br>    (<span class="hljs-string">&quot;protein2&quot;</span>, <span class="hljs-string">&quot;KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE&quot;</span>),<br>    (<span class="hljs-string">&quot;protein2 with mask&quot;</span>,<span class="hljs-string">&quot;KALTARQQEVFDLIRD&lt;mask&gt;ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE&quot;</span>),<br>    (<span class="hljs-string">&quot;protein3&quot;</span>,  <span class="hljs-string">&quot;K A &lt;mask&gt; I S Q&quot;</span>),<br>]<br>batch_labels, batch_strs, batch_tokens = batch_converter(data)<br>batch_lens = (batch_tokens != alphabet.padding_idx).<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># Extract per-residue representations (on CPU)</span><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    results = model(batch_tokens, repr_layers=[<span class="hljs-number">33</span>], return_contacts=<span class="hljs-literal">True</span>)<br>token_representations = results[<span class="hljs-string">&quot;representations&quot;</span>][<span class="hljs-number">33</span>]<br><br><span class="hljs-comment"># Generate per-sequence representations via averaging</span><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> token 0 is always a beginning-of-sequence token, so the first residue is token 1.</span><br>sequence_representations = []<br><span class="hljs-keyword">for</span> i, tokens_len <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(batch_lens):<br>    sequence_representations.append(token_representations[i, <span class="hljs-number">1</span> : tokens_len - <span class="hljs-number">1</span>].mean(<span class="hljs-number">0</span>))<br><br><span class="hljs-comment"># Look at the unsupervised self-attention map contact predictions</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">for</span> (_, seq), tokens_len, attention_contacts <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(data, batch_lens, results[<span class="hljs-string">&quot;contacts&quot;</span>]):<br>    plt.matshow(attention_contacts[: tokens_len, : tokens_len])<br>    plt.title(seq)<br>    plt.show()<br></code></pre></td></tr></table></figure></div><h2 id="Get-the-Embedding-for-Each-Residues">Get the Embedding for Each Residues</h2><p><strong>ESM</strong> (like many transformer-based models) uses “special tokens” plus padding so that all sequences in a batch have the same length. Specifically:</p><ol><li><p><strong>Start and End Tokens</strong>: For any single sequence of length ( n ), the ESM model prepends a start token and appends an end token. That gives you ( n + 2 ) positions for a single sequence.</p></li><li><p><strong>Batch Processing Requires Padding</strong>: When you process multiple sequences in a single batch, they all get padded (on the right) to match the length of the <em>longest</em> sequence in the batch. So if the longest sequence has ( n ) residues, <em>all</em> sequences become length ( n + 2 ) (including the special tokens), and shorter sequences get padding tokens to fill in the gap.</p></li></ol><p>Hence, whether a sequence originally has ( k ) residues or ( m ) residues, in a batch whose <em>longest</em> sequence is ( n ) residues, everyone ends up with a vector length of ( n + 2 ). This ensures the entire input tensor in the batch has a uniform shape.</p><p>Here is an example of extract the embedding by following codes above:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">Seq_Embeding = &#123;i[<span class="hljs-number">0</span>]:token_representations[<span class="hljs-number">0</span>][:<span class="hljs-built_in">len</span>(i[<span class="hljs-number">1</span>])+<span class="hljs-number">2</span>] <span class="hljs-keyword">for</span> i,ii <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(data,token_representations) &#125;<br><span class="hljs-comment"># also, this is for remove the start and end</span><br>Seq_Embeding = &#123;i[<span class="hljs-number">0</span>]:token_representations[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>:<span class="hljs-built_in">len</span>(i[<span class="hljs-number">1</span>])+<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> i,ii <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(data,token_representations) &#125;<br></code></pre></td></tr></table></figure></div><p>The embedding results form batch and single chain are general the same but slightly different. If you embedding them one by one and calculate the difference, you’ll find there are slight different. According to the ChatGPT, it could be caused by:</p><ol><li><p><strong>Position Embeddings</strong></p><ul><li>ESM (like most Transformer models) uses positional embeddings. If the model sees a “longer” padded batch, the position indices for each token can differ from the single-sequence scenario, so the sequence’s tokens may be mapped to slightly different (learned) position embeddings.</li></ul></li><li><p><strong>Attention Masking and Context</strong></p><ul><li>In a batched setting, the model creates a larger attention mask (covering all tokens up to the longest sequence in the batch). Although it’s not supposed to mix information across sequences, the internal computations (e.g., how attention is batched or chunked) can differ from the single-sequence forward pass, leading to small numeric discrepancies.</li></ul></li><li><p><strong>Dropout or Other Stochastic Layers</strong></p><ul><li>If your model isn’t in <code>eval()</code> mode (or if dropout is enabled for any reason), you’ll get random differences each pass. Always ensure <code>model.eval()</code> and (ideally) a fixed random seed for more reproducible outputs.</li></ul></li><li><p><strong>Floating-Point Rounding</strong></p><ul><li>GPU parallelization can cause minor floating-point differences, especially between batched and single-inference calls. These are typically very small numerical deviations.</li></ul></li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">esm, Evolutionary Scale Modeling</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Protein Structure" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Protein-Structure/"/>
    
    
    <category term="AI" scheme="https://karobben.github.io/tags/AI/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Biochmistry" scheme="https://karobben.github.io/tags/Biochmistry/"/>
    
  </entry>
  
  <entry>
    <title>PCA</title>
    <link href="https://karobben.github.io/2025/01/06/AI/ai-pca/"/>
    <id>https://karobben.github.io/2025/01/06/AI/ai-pca/</id>
    <published>2025-01-07T05:00:57.000Z</published>
    <updated>2025-01-08T18:06:58.301Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Example Dataset</strong><br>Suppose we have the following dataset with 3 data points and 2 features ($x_1$, $x_2$):</p><p>$$<br>X =<br>\begin{bmatrix}<br>2.5 &amp; 2.4 \\<br>0.5 &amp; 0.7 \\<br>2.2 &amp; 2.9<br>\end{bmatrix}<br>$$</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">X = np.array([[<span class="hljs-number">2.5</span>, <span class="hljs-number">2.4</span>], [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.7</span>], [<span class="hljs-number">2.2</span>, <span class="hljs-number">2.9</span>]])<br></code></pre></td></tr></table></figure></div><h2 id="Step-1-Center-the-Data">Step 1: Center the Data</h2><p>First, subtract the mean of each feature from the dataset to center it:</p><ol><li><p>Compute the means:</p><ul><li>Mean of $x_1$: $\text{mean}(x_1) = \frac{2.5 + 0.5 + 2.2}{3} = 1.73$</li><li>Mean of $x_2$: $\text{mean}(x_2) = \frac{2.4 + 0.7 + 2.9}{3} = 2.0$</li></ul></li><li><p>Subtract the means:<br>$$<br>X_{ \text{centered} } =<br>\begin{bmatrix}<br>2.5 - 1.73 &amp; 2.4 - 2.0 \\<br>0.5 - 1.73 &amp; 0.7 - 2.0 \\<br>2.2 - 1.73 &amp; 2.9 - 2.0<br>\end{bmatrix} =<br>\begin{bmatrix}<br>0.77 &amp; 0.4 \\<br>-1.23 &amp; -1.3 \\<br>0.47 &amp; 0.9<br>\end{bmatrix}<br>$$</p></li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">X_centered = X - X.mean(axis = <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure></div><h3 id="Step-2-Compute-the-Covariance-Matrix">Step 2: Compute the Covariance Matrix</h3><p>The covariance matrix shows how the features are related. Calculate it as:<br>$$<br>\text{Cov}(X) = \frac{1}{n-1} X_{\text{centered}}^\top X_{\text{centered}}<br>$$</p><ol><li><p>Compute $X_{\text{centered}}^\top X_{\text{centered}}$:<br>$$<br>X_{\text{centered}}^\top X_{\text{centered}} =<br>\begin{bmatrix}<br>0.77 &amp; -1.23 &amp; 0.47 \\<br>0.4 &amp; -1.3 &amp; 0.9<br>\end{bmatrix}<br>\begin{bmatrix}<br>0.77 &amp; 0.4 \\<br>-1.23 &amp; -1.3 \\<br>0.47 &amp; 0.9<br>\end{bmatrix} =<br>\begin{bmatrix}<br>2.32 &amp; 2.33 \\<br>2.33 &amp; 2.66<br>\end{bmatrix}<br>$$</p></li><li><p>Divide by $n-1 = 2$ (since $n=3$):<br>$$<br>\text{Cov}(X) =<br>\begin{bmatrix}<br>1.163 &amp; 1.165 \\<br>1.165 &amp; 1.33<br>\end{bmatrix}<br>$$</p></li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># based on the equation</span><br>n = <span class="hljs-number">3</span><br>CovX = X_centered.T@X_centered /(n-<span class="hljs-number">1</span>)<br><span class="hljs-comment"># using function from numpy</span><br>np.cov(X.T)<br></code></pre></td></tr></table></figure></div><pre>array([[1.16333333, 1.165     ],       [1.165     , 1.33      ]])</pre><h3 id="Step-3-Eigenvalues-and-Eigenvectors">Step 3: Eigenvalues and Eigenvectors</h3><p>Find the eigenvalues ($\lambda$) and eigenvectors ($u$) of the covariance matrix.</p><ol><li>Solve $\text{det}(\text{Cov}(X) - \lambda I) = 0$ for $\lambda$:<br>$$<br>\text{det}<br>\begin{bmatrix}<br>1.163 - \lambda &amp; 1.165 \\<br>1.165 &amp; 1.33 - \lambda<br>\end{bmatrix}<br>= 0<br>$$<br>This results in eigenvalues:<br>$$<br>\lambda_1 = 2.41, \quad \lambda_2 = 0.08<br>$$</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Lambda</span>(<span class="hljs-params">a,b,c</span>):</span><br>    lambda1 = (-b+np.sqrt(b**<span class="hljs-number">2</span> - <span class="hljs-number">4</span>*a*c))/<span class="hljs-number">2</span>/a<br>    lambda2 = (-b-np.sqrt(b**<span class="hljs-number">2</span> - <span class="hljs-number">4</span>*a*c))/<span class="hljs-number">2</span>/a<br>    <span class="hljs-keyword">return</span> lambda1, lambda2<br><br>a = <span class="hljs-number">1</span><br>b = -(CovX[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>] + CovX[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]) <br>c = CovX[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]*CovX[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>] - CovX[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]*CovX[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br><br>lm = Lambda(a,b,c)<br></code></pre></td></tr></table></figure></div><pre>(2.414643312171381, 0.07869002116195278)</pre><ol start="2"><li>Find eigenvectors ($u$):<br>Solve $(\text{Cov}(X) - \lambda I)u = 0$ for each $\lambda$. The eigenvectors are:<br>$$<br>u_1 = \begin{bmatrix} 8.48  \\ -9.11 \end{bmatrix}, \quad<br>$$</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">tmp1 = CovX - lm[<span class="hljs-number">0</span>]* np.eye(CovX.shape[<span class="hljs-number">0</span>])<br>xx = tmp.<span class="hljs-built_in">sum</span>()<br>x2 = xx[<span class="hljs-number">0</span>]/xx[<span class="hljs-number">1</span>]<br>Length = np.sqrt(xx[<span class="hljs-number">0</span>]**<span class="hljs-number">2</span>+xx[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span>)<br>x1 = <span class="hljs-number">1</span>/Length<br>x2 /=Length * -<span class="hljs-number">1</span><br><br>u = np.array([x1, x2])<br>print(u)<br></code></pre></td></tr></table></figure></div><pre>array([ 8.47987336, 9.10811172])</pre><ol start="3"><li>Variance of this component</li></ol><p>$$<br>Variance = \frac{max(\lambda _1, \lambda _2)}{\lambda _1+\lambda _2}<br>$$</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">max</span>(lm)/<span class="hljs-built_in">sum</span>(lm)<br></code></pre></td></tr></table></figure></div><pre>0.9684</pre><h3 id="Step-4-Project-Data-onto-Principal-Components">Step 4: Project Data onto Principal Components</h3><p>Use the top eigenvector ($u_1$) to project the data into 1D (reduce dimensionality):</p><p>$$<br>X_{\text{projected}} = X_{\text{centered}} \cdot u_1<br>$$</p><ol><li>Compute the projection:<br>$$<br>X_{\text{projected}} =<br>\begin{bmatrix}<br>0.77 &amp; 0.4 \\<br>-1.23 &amp; -1.3 \\<br>0.47 &amp; 0.9<br>\end{bmatrix}<br>\begin{bmatrix}<br>8.48 \\<br>-9.11<br>\end{bmatrix} =<br>\begin{bmatrix}<br>10.14 \\<br>-22.30 \\<br>12.15<br>\end{bmatrix}<br>$$</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">X_centered*u<br></code></pre></td></tr></table></figure></div><pre>array([ 10.14448093, -22.29905572,  12.15457479])</pre><h3 id="Final-Results">Final Results:</h3><ol><li><p><strong>Principal Components</strong>:</p><ul><li>The first principal component explains most of the variance ($97%$).</li></ul></li><li><p><strong>Transformed Data</strong>:</p><ul><li>The dataset in 1D space:<br>$$<br>X_{\text{projected}} = \begin{bmatrix}    10.14 \\ -22.30 \\ 12.15 \end{bmatrix}<br>$$</li></ul></li></ol><hr><h2 id="How-to-calculate-Eigenvalues">How to calculate Eigenvalues</h2><h3 id="Step-3-1-Find-Eigenvalues">Step 3.1: Find Eigenvalues</h3><p>Eigenvalues are the roots of the <strong>characteristic equation</strong>:<br>$$<br>\text{det}(\text{Cov}(X) - \lambda I) = 0<br>$$</p><p>$$<br>\text{Cov}(X) =<br>\begin{bmatrix}<br>2.01   &amp; 1.91 \\<br>1.91 &amp; 2.03<br>\end{bmatrix}<br>$$</p><ol><li><p>Subtract $\lambda I$ from $\text{Cov}(X)$:<br>$$<br>\text{Cov}(X) - \lambda I =<br>\begin{bmatrix}<br>2.01 - \lambda &amp; 1.91 \\<br>1.91 &amp; 2.03 - \lambda<br>\end{bmatrix}<br>$$</p></li><li><p>Compute the determinant of this matrix:<br>$$<br>\text{det}(\text{Cov}(X) - \lambda I) = (2.01 - \lambda)(2.03 - \lambda) - (1.91)^2<br>$$</p></li><li><p>Expand the determinant:<br>$$<br>\text{det}(\text{Cov}(X) - \lambda I) = (2.01)(2.03) - (2.01)\lambda - (2.03)\lambda + \lambda^2 - 1.91^2<br>$$<br>$$<br>= \lambda^2 - (2.01 + 2.03)\lambda + (2.01 \cdot 2.03 - 1.91^2)<br>$$</p></li><li><p>Simplify:<br>$$<br>\lambda^2 - 4.04\lambda + (4.0803 - 3.6481) = 0<br>$$<br>$$<br>\lambda^2 - 4.04\lambda + 0.4322 = 0<br>$$</p></li><li><p>Solve this quadratic equation using the quadratic formula:<br>$$<br>\lambda = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}<br>$$<br>Here:</p></li></ol><ul><li>$a = 1$, $b = -4.04$, $c = 0.4322$</li></ul><p>$$<br>\lambda = \frac{-(-4.04) \pm \sqrt{(-4.04)^2 - 4(1)(0.4322)}}{2(1)}<br>$$<br>$$<br>\lambda = \frac{4.04 \pm \sqrt{16.3216 - 1.7288}}{2}<br>$$<br>$$<br>\lambda = \frac{4.04 \pm \sqrt{14.5928}}{2}<br>$$<br>$$<br>\lambda = \frac{4.04 \pm 3.82}{2}<br>$$</p><ol start="6"><li>Compute the two eigenvalues:<br>$$<br>\lambda_1 = \frac{4.04 + 3.82}{2} = 3.96, \quad \lambda_2 = \frac{4.04 - 3.82}{2} = 0.08<br>$$</li></ol><h3 id="Step-3-2-Find-Eigenvectors">Step 3.2: Find Eigenvectors</h3><p>For each eigenvalue $\lambda$, solve $(\text{Cov}(X) - \lambda I)u = 0$.</p><h4 id="For-lambda-1-3-96">For $\lambda_1 = 3.96$:</h4><ol><li><p>Substitute $\lambda_1$ into $\text{Cov}(X) - \lambda I$:<br>$$<br>\text{Cov}(X) - 3.96 I =<br>\begin{bmatrix}<br>2.01 - 3.96 &amp; 1.91 \\<br>1.91 &amp; 2.03 - 3.96<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>-1.95 &amp; 1.91 \\<br>1.91 &amp; -1.93<br>\end{bmatrix}<br>$$</p></li><li><p>Solve the equation:<br>$$<br>\begin{bmatrix}<br>-1.95 &amp; 1.91 \\<br>1.91 &amp; -1.93<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_1 \\<br>x_2<br>\end{bmatrix}<br>= 0<br>$$</p></li></ol><p>This expands to two equations:<br>$$<br>-1.95x_1 + 1.91x_2 = 0<br>$$<br>$$<br>1.91x_1 - 1.93x_2 = 0<br>$$</p><ol start="3"><li><p>Simplify:<br>$$<br>x_2 = \frac{1.95}{1.91}x_1 \quad \text{(from the first equation)}<br>$$</p></li><li><p>Normalize the vector (scale so that the length is 1):<br>$$<br>u_1 = \begin{bmatrix}<br>0.71 \\<br>0.71<br>\end{bmatrix}<br>$$</p></li></ol><h4 id="For-lambda-2-0-08">For $\lambda_2 = 0.08$:</h4><ol><li><p>Substitute $\lambda_2$ into $\text{Cov}(X) - \lambda I$:<br>$$<br>\text{Cov}(X) - 0.08 I =<br>\begin{bmatrix}<br>2.01 - 0.08 &amp; 1.91 \\<br>1.91 &amp; 2.03 - 0.08<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>1.93 &amp; 1.91 \\<br>1.91 &amp; 1.95<br>\end{bmatrix}<br>$$</p></li><li><p>Solve the equation:<br>$$<br>1.93x_1 + 1.91x_2 = 0<br>$$<br>$$<br>1.91x_1 + 1.95x_2 = 0<br>$$</p></li><li><p>Simplify:<br>$$<br>x_2 = -\frac{1.93}{1.91}x_1<br>$$</p></li><li><p>Normalize the vector:<br>$$<br>u_2 = \begin{bmatrix}<br>-0.71 \\<br>0.71<br>\end{bmatrix}<br>$$</p></li></ol><h3 id="Final-Result">Final Result:</h3><ul><li><strong>Eigenvalues</strong>:<br>$$<br>\lambda_1 = 3.96, \quad \lambda_2 = 0.08<br>$$</li><li><strong>Eigenvectors</strong>:<br>$$<br>u_1 = \begin{bmatrix} 0.71 \\ 0.71 \end{bmatrix}, \quad<br>u_2 = \begin{bmatrix} -0.71 \\ 0.71 \end{bmatrix}<br>$$</li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">PCA</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>High Dimension Data</title>
    <link href="https://karobben.github.io/2025/01/01/AI/highdimension/"/>
    <id>https://karobben.github.io/2025/01/01/AI/highdimension/</id>
    <published>2025-01-01T21:23:33.000Z</published>
    <updated>2025-01-07T05:00:08.685Z</updated>
    
    <content type="html"><![CDATA[<h2 id="High-Dimensional-Data">High Dimensional Data</h2><p>$$<br>\begin{bmatrix}<br>\mathbf{x}_ 1 \\<br>\mathbf{x}_ 2 \\<br>\vdots \\<br>\mathbf{x}_ N<br>\end{bmatrix} =<br>\begin{bmatrix}<br>x_1^{(1)} &amp; x_1^{(2)} &amp; \cdots &amp; x_1^{(d)} \\<br>x_2^{(1)} &amp; x_2^{(2)} &amp; \cdots &amp; x_2^{(d)} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>x_N^{(1)} &amp; x_N^{(2)} &amp; \cdots &amp; x_N^{(d)}<br>\end{bmatrix}<br>$$</p><h3 id="Mean-and-Covariance-of-High-Dimensional-Data">Mean and Covariance of High-Dimensional Data</h3><p>When working with high-dimensional data, it is important to understand the <strong>mean</strong> and <strong>covariance matrix</strong>, which are essential statistical measures that summarize the data’s location and spread.</p><hr><h3 id="1-Mean-Vector">1. <strong>Mean Vector</strong></h3><p>For a dataset with $ n $ samples and $ d $ features (dimensions):</p><ul><li>Let $ \mathbf{X} \in \mathbb{R}^{n \times d} $ be the dataset, where each row $ \mathbf{x}_i \in \mathbb{R}^d $ represents a data point and each column corresponds to a feature.</li></ul><h4 id="Definition">Definition:</h4><p>The mean vector $ \mathbf{\mu} \in \mathbb{R}^d $ is defined as:<br>$$<br>\mathbf{\mu} = \frac{1}{n} \sum_{i=1}^n \mathbf{x}_i<br>$$</p><h4 id="Element-Wise">Element-Wise:</h4><p>The $ j $-th element of the mean vector is:<br>$$<br>\mu_j = \frac{1}{n} \sum_{i=1}^n x_{ij}, \quad j = 1, 2, \dots, d<br>$$<br>Where $ x_{ij} $ is the $ j $-th feature of the $ i $-th sample.</p><h3 id="2-Covariance-Matrix">2. <strong>Covariance Matrix</strong></h3><p>The covariance matrix $ \mathbf{\Sigma} \in \mathbb{R}^{d \times d} $ captures the pairwise relationships between features.</p><h4 id="Definition-v2">Definition:</h4><p>$$<br>\mathbf{\Sigma} = \frac{1}{n-1} \sum_{i=1}^n (\mathbf{x}_i - \mathbf{\mu})(\mathbf{x}_i - \mathbf{\mu})^T<br>$$</p><h4 id="Element-Wise-v2">Element-Wise:</h4><p>The $ (j, k) $-th entry of the covariance matrix is:<br>$$<br>\Sigma_{jk} = \frac{1}{n-1} \sum_{i=1}^n (x_{ij} - \mu_j)(x_{ik} - \mu_k)<br>$$<br>Where:</p><ul><li>$ \Sigma_{jk} $: Covariance between feature $ j $ and feature $ k $.</li><li>$ x_{ij} $: Value of the $ j $-th feature for the $ i $-th sample.</li></ul><h4 id="Properties">Properties:</h4><ul><li>$ \Sigma_{jj} $: Variance of feature $ j $.</li><li>$ \Sigma_{jk} $: Correlation between features $ j $ and $ k $ if scaled by their standard deviations.</li><li>The matrix $ \mathbf{\Sigma} $ is symmetric: $ \Sigma_{jk} = \Sigma_{kj} $.</li></ul><h3 id="3-Matrix-Representation">3. <strong>Matrix Representation</strong></h3><p>Using matrix notation, the mean vector $ \mathbf{\mu} $ and covariance matrix $ \mathbf{\Sigma} $ can be computed efficiently:</p><h4 id="Mean-Vector">Mean Vector:</h4><p>$$<br>\mathbf{\mu} = \frac{1}{n} \mathbf{X}^T \mathbf{1}<br>$$<br>Where:</p><ul><li>$ \mathbf{X}^T $: Transpose of the data matrix.</li><li>$ \mathbf{1} $: A column vector of ones with size $ n $.</li></ul><h4 id="Covariance-Matrix">Covariance Matrix:</h4><p>$$<br>\mathbf{\Sigma} = \frac{1}{n-1} (\mathbf{X} - \mathbf{1} \mathbf{\mu}^T )^T (\mathbf{X} - \mathbf{1} \mathbf{\mu}^T)<br>$$</p><h3 id="Summary-of-Notation">Summary of Notation</h3><table><thead><tr><th><strong>Symbol</strong></th><th><strong>Meaning</strong></th></tr></thead><tbody><tr><td>$ \mathbf{\mu} $</td><td>Mean vector of the dataset ($ d $-dimensional).</td></tr><tr><td>$ \mathbf{\Sigma} $</td><td>Covariance matrix ($ d \times d $).</td></tr><tr><td>$ \mu_j $</td><td>Mean of the $ j $-th feature.</td></tr><tr><td>$ \Sigma_{jk} $</td><td>Covariance between feature $ j $ and $ k $.</td></tr></tbody></table><h3 id="Example-of-the-Covariance">Example of the Covariance</h3><p>The <strong>Iris dataset</strong> has been successfully loaded. Here’s a brief look at the dataset:</p><table><thead><tr><th><strong>Sepal Length (cm)</strong></th><th><strong>Sepal Width (cm)</strong></th><th><strong>Petal Length (cm)</strong></th><th><strong>Petal Width (cm)</strong></th></tr></thead><tbody><tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td></tr><tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td></tr><tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td></tr><tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td></tr><tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td></tr></tbody></table><p>Let’s compute the <strong>mean</strong> and <strong>covariance matrix</strong> for this dataset and visualize their insights.</p><h3 id="Results-from-the-Iris-Dataset">Results from the Iris Dataset:</h3><h4 id="Mean-Vector-v2"><strong>Mean Vector</strong>:</h4><p>The mean of each feature (measured in centimeters) is:</p><ul><li><strong>Sepal Length</strong>: $ 5.843 $</li><li><strong>Sepal Width</strong>: $ 3.057 $</li><li><strong>Petal Length</strong>: $ 3.758 $</li><li><strong>Petal Width</strong>: $ 1.199 $</li></ul><h4 id="Covariance-Matrix-v2"><strong>Covariance Matrix</strong>:</h4><p>The covariance matrix is:<br>$$<br>\mathbf{\Sigma} =<br>\begin{bmatrix}<br>0.6857 &amp; -0.0424 &amp; 1.2743 &amp; 0.5163 \\<br>-0.0424 &amp; 0.1900 &amp; -0.3297 &amp; -0.1216 \\<br>1.2743 &amp; -0.3297 &amp; 3.1163 &amp; 1.2956 \\<br>0.5163 &amp; -0.1216 &amp; 1.2956 &amp; 0.5810<br>\end{bmatrix}<br>$$</p><h4 id="Interpretation">Interpretation:</h4><ol><li><p><strong>Diagonal Entries</strong>:</p><ul><li>These are the variances of the features:<ul><li>Variance of <strong>Sepal Length</strong>: $ 0.6857 $</li><li>Variance of <strong>Sepal Width</strong>: $ 0.1900 $</li><li>Variance of <strong>Petal Length</strong>: $ 3.1163 $</li><li>Variance of <strong>Petal Width</strong>: $ 0.5810 $</li></ul></li></ul></li><li><p><strong>Off-Diagonal Entries</strong>:</p><ul><li>These represent covariances between pairs of features:<ul><li><strong>Positive covariance</strong> (e.g., $ 1.2743 $ between Sepal Length and Petal Length) suggests a positive relationship.</li><li><strong>Negative covariance</strong> (e.g., $ -0.3297 $ between Sepal Width and Petal Length) suggests a negative relationship.</li></ul></li></ul></li></ol><p>The <strong>Covariance Matrix with Species Encoding</strong> is as follows:</p><table><thead><tr><th>Feature</th><th>Sepal Length (cm)</th><th>Sepal Width (cm)</th><th>Petal Length (cm)</th><th>Petal Width (cm)</th><th>Species Encoded</th></tr></thead><tbody><tr><td><strong>Sepal Length (cm)</strong></td><td>0.6857</td><td>-0.0424</td><td>1.2743</td><td>0.5163</td><td>0.5309</td></tr><tr><td><strong>Sepal Width (cm)</strong></td><td>-0.0424</td><td>0.1900</td><td>-0.3297</td><td>-0.1216</td><td>-0.1523</td></tr><tr><td><strong>Petal Length (cm)</strong></td><td>1.2743</td><td>-0.3297</td><td>3.1163</td><td>1.2956</td><td>1.3725</td></tr><tr><td><strong>Petal Width (cm)</strong></td><td>0.5163</td><td>-0.1216</td><td>1.2956</td><td>0.5810</td><td>0.5973</td></tr><tr><td><strong>Species Encoded</strong></td><td>0.5309</td><td>-0.1523</td><td>1.3725</td><td>0.5973</td><td>0.6711</td></tr></tbody></table><h3 id="Key-Observations">Key Observations:</h3><ol><li><p><strong>Species Encoded Relationships</strong>:</p><ul><li>Positive covariance with features like petal length ($1.3725$) and petal width ($0.5973$).</li><li>Indicates that these features strongly vary with the species.</li></ul></li><li><p><strong>Feature Variability</strong>:</p><ul><li>Variance (diagonal values) is high for petal length ($3.1163$), meaning it varies most across the dataset.</li><li>Sepal width ($0.1900$) has the least variance.</li></ul></li></ol><p><img src="https://imgur.com/gTRpILR.png" alt=""></p><div id="chart_bar" style="width: 100%; height: 400px;"></div><div class="admonition note"><p class="admonition-title">What can we get from this results?</p><p>As you can see, <strong>Petal Length</strong> has the largest variance (var = 3.1163). Meanwhile, it also has the highest covariance with species (1.37). This indicates that much of its variance is explained by the species, making Petal Length a potentially good feature for species classification.</p></div><h2 id="Transformations">Transformations</h2><p>High-dimensional data transformation refers to the process of modifying or converting data that exists in a high-dimensional space (i.e., data with a large number of features or variables) into a more manageable or meaningful representation. This transformation can involve reducing dimensions, re-organizing data, or mapping it to a different space while preserving important information or relationships.</p><ol><li><p><strong>Source Dataset (${x}$)</strong> and <strong>Target Dataset (${m}$)</strong>:</p><ul><li>The target dataset ${m_i}$ is generated by applying a rotation and translation to the source dataset:<br>$$<br>m_i = A x_i + b<br>$$</li><li>Here, $A$ is the rotation matrix, and $b$ is the translation vector.</li></ul></li><li><p><strong>Mean Transformation</strong>:</p><ul><li>The mean of the transformed dataset (${m}$) can be expressed as:<br>$$<br>\text{mean}({m}) = A \cdot \text{mean}({x}) + b<br>$$</li></ul></li><li><p><strong>Covariance Transformation</strong>:</p><ul><li>The covariance matrix of the transformed dataset (${m}$) is derived as:<br>$$<br>\text{Covmat}({m}) = A \cdot \text{Covmat}({x}) \cdot A^\top<br>$$</li><li>This shows how the covariance matrix of the source dataset transforms under a linear transformation.</li><li>The covariance matrix of ${x}$ is defined as:<br>$$<br>\text{Covmat}({x}) = \frac{1}{N} \sum_i (x_i - \text{mean}({x}))(x_i - \text{mean}({x}))^\top<br>$$</li></ul></li></ol><h2 id="Eigenvector-and-Eigenvalue">Eigenvector and Eigenvalue</h2><p>Imagine you’re analyzing data (like in machine learning or physics). Eigenvalues and eigenvectors can:</p><ul><li>Find patterns: In large data (like PCA), eigenvectors show the “directions” of most variation, and eigenvalues tell how important each direction is.</li><li>Simplify problems: Diagonalization makes hard matrix computations easier.</li></ul><h3 id="1-Eigenvector-u-and-Eigenvalue-lambda"><strong>1. Eigenvector ($u$) and Eigenvalue ($\lambda$)</strong></h3><ul><li>An <strong>eigenvector</strong> $u$ of a matrix $S$ is a vector that does not change direction when $S$ is applied to it. Instead, it is scaled by a factor $\lambda$, the <strong>eigenvalue</strong>:<br>$$<br>S u = \lambda u<br>$$<ul><li>$S$: A square matrix.</li><li>$u$: An eigenvector (non-zero vector).</li><li>$\lambda$: The corresponding eigenvalue.</li></ul></li></ul><h3 id="2-Symmetric-Matrices-S"><strong>2. Symmetric Matrices ($S$)</strong></h3><ul><li>If $S$ is symmetric ($S = S^\top$), it has special properties:<ul><li>All eigenvalues are <strong>real</strong>.</li><li>Eigenvectors corresponding to distinct eigenvalues are <strong>orthogonal</strong>:<br>$$<br>u_i \perp u_j \quad \text{if} \quad i \neq j<br>$$</li><li>Eigenvectors can also be <strong>normalized</strong> to form an orthonormal set ($|u| = 1$).</li></ul></li></ul><h3 id="3-Orthonormal-Matrix-U"><strong>3. Orthonormal Matrix ($U$)</strong></h3><ul><li>By stacking all the eigenvectors of $S$ as columns into a matrix $U$:<br>$$<br>U = [u_1, u_2, \dots, u_d]<br>$$<ul><li>$U$ is an <strong>orthonormal matrix</strong>, meaning:<br>$$<br>U^\top U = I \quad \text{(identity matrix)}<br>$$</li></ul></li></ul><h3 id="4-Eigenvalues-as-a-Diagonal-Matrix-Lambda"><strong>4. Eigenvalues as a Diagonal Matrix ($\Lambda$)</strong></h3><ul><li>Arrange eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_d$ into a diagonal matrix:<br>$$<br>\Lambda =<br>\begin{bmatrix}<br>\lambda_1 &amp; 0 &amp; \dots &amp; 0 \\<br>0 &amp; \lambda_2 &amp; \dots &amp; 0 \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>0 &amp; 0 &amp; \dots &amp; \lambda_d<br>\end{bmatrix}<br>$$</li></ul><h3 id="5-Diagonalization"><strong>5. Diagonalization</strong></h3><ul><li>If $S$ is symmetric, it can be <strong>diagonalized</strong> using its eigenvectors and eigenvalues:<br>$$<br>S = U \Lambda U^\top<br>$$<ul><li>$U$: Matrix of eigenvectors.</li><li>$\Lambda$: Diagonal matrix of eigenvalues.</li></ul></li></ul><h3 id="6-Key-Properties-of-Diagonalization"><strong>6. Key Properties of Diagonalization</strong></h3><ul><li>Simplifies computations, e.g., powers of $S$:<br>$$<br>S^k = U \Lambda^k U^\top<br>$$<ul><li>$\Lambda^k$ is simply the diagonal matrix with each eigenvalue raised to the power $k$.</li></ul></li><li>Used in many fields such as:<ul><li>Principal Component Analysis (PCA).</li><li>Solving differential equations.</li><li>Modal analysis in engineering.</li></ul></li></ul><h2 id="Principal-Component-Analysis-PCA">Principal Component Analysis (PCA)</h2><p>Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of data while retaining as much variance as possible. It identifies new axes, called principal components, which are uncorrelated and align with the directions of maximum variance. PCA transforms the data to these new axes, ranks the components by their variance (eigenvalues), and allows dimensionality reduction by selecting the top components.</p><h3 id="PCA-in-3-Steps-More-Accurate-Breakdown">PCA in 3 Steps (More Accurate Breakdown):</h3><ol><li><p><strong>Transformation (Centering the Data)</strong>:</p><ul><li>Before applying PCA, you need to <strong>center</strong> the data by subtracting the mean of each feature. This step ensures that the principal components (axes of maximum variance) pass through the origin.</li><li>Mathematically:<br>$$<br>x_{\text{centered}} = x - \text{mean}(x)<br>$$</li></ul></li><li><p><strong>Rotation (Find Eigenvalues and Eigenvectors)</strong>:</p><ul><li>The goal of PCA is to find the directions (principal components) where the data has the most variance.</li><li>This involves computing the <strong>eigenvectors</strong> and <strong>eigenvalues</strong> of the covariance matrix:<br>$$<br>S = \frac{1}{n} X^\top X<br>$$<ul><li>The eigenvectors represent the new axes (principal components).</li><li>The eigenvalues indicate how much variance is captured by each axis.</li></ul></li><li><strong>Rotation</strong> refers to aligning the data along the directions of these principal components.</li></ul></li><li><p><strong>Dimensional Reduction (Keep Principal Components)</strong>:</p><ul><li>After identifying the principal components, you can choose the top $k$ components with the highest eigenvalues (the directions of the most variance) and ignore the rest.</li><li>This step reduces the dimensionality while retaining as much information as possible.</li></ul></li></ol><h3 id="1-Original-Dataset">1. <strong>Original Dataset</strong>:</h3><ul><li>The dataset ${x}$:<ul><li>It has $d$ features (dimensions).</li><li>Each data point is a vector in a $d$-dimensional space.</li></ul></li></ul><h3 id="2-Step-1-Covariance-Matrix">2. <strong>Step 1: Covariance Matrix</strong>:</h3><ul><li>The covariance matrix captures how features are correlated. It is computed as:<br>$$<br>\text{Covmat}({x}) = \frac{1}{N} \sum_{i=1}^N \left( x_i - \text{mean}({x}) \right) \left( x_i - \text{mean}({x}) \right)^\top<br>$$</li><li>PCA works by <strong>diagonalizing</strong> this covariance matrix.</li></ul><h3 id="3-Step-2-Eigen-Decomposition">3. <strong>Step 2: Eigen Decomposition</strong>:</h3><ul><li>Decompose the covariance matrix into <strong>eigenvalues ($\lambda$)</strong> and <strong>eigenvectors ($u$)</strong>:<br>$$<br>U^\top \text{Covmat}({x}) U = \Lambda<br>$$<ul><li>$U$: Matrix of eigenvectors (principal components).</li><li>$\Lambda$: Diagonal matrix of eigenvalues (variance explained by each principal component).</li></ul></li></ul><h3 id="4-Step-3-Choose-s-Principal-Components">4. <strong>Step 3: Choose $s$ Principal Components</strong>:</h3><ul><li>Eigenvalues represent the <strong>variance</strong> explained by each principal component. They are sorted in descending order.</li><li>To reduce dimensions:<ul><li>Choose the top $s$ eigenvalues that explain the most variance.</li><li>Often, the ratio is calculated:<br>$$<br>\frac{\sum_{j=s+1}^d \lambda_j}{\sum_{j=1}^d \lambda_j}<br>$$<ul><li>This ratio helps decide $s$ by ensuring the <strong>remaining variance (error)</strong> is small.</li></ul></li><li>Plotting $\lambda_i$ vs. $i$ (as shown in the slide) can help visualize where most variance is captured (the “elbow” point).</li></ul></li></ul><h3 id="5-Step-4-Project-Data-to-Lower-Dimensions">5. <strong>Step 4: Project Data to Lower Dimensions</strong>:</h3><ul><li>Once you have selected $s$ principal components, project the original data onto this lower-dimensional space:<br>$$<br>\hat{x}_ i = \sum_ {j=1}^s \left[ u_ j^\top (x_ i - \text{mean}({x})) \right] u_j + \text{mean}({x})<br>$$<ul><li>Here:<ul><li>$u_j$: The eigenvectors corresponding to the top $s$ eigenvalues.</li><li>$\hat{x}_i$: The low-dimensional representation of $x_i$.</li></ul></li></ul></li></ul><h3 id="6-Visualization-from-the-Slide">6. <strong>Visualization from the Slide</strong>:</h3><ul><li>The graph shows the eigenvalues ($\lambda_i$) vs. their indices ($i$):<ul><li>The blue curve represents the sorted eigenvalues.</li><li>The orange circle highlights the “elbow” point, which suggests the optimal number of principal components to retain.</li></ul></li></ul><h3 id="Summary-of-PCA-Calculation">Summary of PCA Calculation:</h3><ol><li>Center the data (subtract the mean).</li><li>Compute the covariance matrix.</li><li>Find eigenvalues and eigenvectors of the covariance matrix.</li><li>Choose the top $s$ eigenvalues to decide the number of principal components.</li><li>Project the data onto the top $s$ eigenvectors to get a reduced representation.</li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style><script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script><script type="text/javascript">  google.charts.load('current', { packages: ['corechart'] });  google.charts.setOnLoadCallback(drawChart);  function drawChart() {    // Data for the bar chart with error bars    var data = google.visualization.arrayToDataTable([      ['Feature', 'Setosa Mean', { role: 'interval' }, { role: 'interval' },                 'Versicolor Mean', { role: 'interval' }, { role: 'interval' },                 'Virginica Mean', { role: 'interval' }, { role: 'interval' }],      ['Sepal Length',       5.006, 4.8, 5.2,  // Setosa       5.936, 5.7, 6.2,  // Versicolor       6.588, 6.4, 6.8], // Virginica      ['Sepal Width',       3.428, 3.2, 3.6,  // Setosa       2.770, 2.6, 2.9,  // Versicolor       2.974, 2.8, 3.1], // Virginica      ['Petal Length',       1.462, 1.3, 1.6,  // Setosa       4.260, 4.0, 4.5,  // Versicolor       5.552, 5.3, 5.8], // Virginica      ['Petal Width',       0.246, 0.2, 0.3,  // Setosa       1.326, 1.2, 1.5,  // Versicolor       2.026, 1.9, 2.2]  // Virginica    ]);    // Chart options    var options = {      title: 'Mean Values with Error Bars by Species',      hAxis: { title: 'Feature' },      vAxis: { title: 'Mean Value', minValue: 0 },      legend: { position: 'top' },      bar: { groupWidth: '75%' },    };    // Draw the chart    var chart = new google.visualization.ColumnChart(document.getElementById('chart_bar'));    chart.draw(data, options);  }</script>]]></content>
    
    
    <summary type="html">High Dimension Data</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>AI: Logistic Regression</title>
    <link href="https://karobben.github.io/2024/12/30/AI/ai-logistic-reg/"/>
    <id>https://karobben.github.io/2024/12/30/AI/ai-logistic-reg/</id>
    <published>2024-12-31T02:03:29.000Z</published>
    <updated>2024-12-31T04:01:44.345Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Logistic-Regression">Logistic Regression</h2><p>Logistic regression is a <strong>supervised machine learning algorithm</strong> used for <strong>binary classification</strong> tasks. Unlike linear regression, which predicts continuous values, logistic regression predicts the <strong>probability</strong> that a given input belongs to a certain class.</p><h3 id="Key-Concepts-in-Logistic-Regression">Key Concepts in Logistic Regression</h3><ol><li><p><strong>Logistic Function (Sigmoid Function)</strong>:</p><ul><li>Logistic regression uses the <strong>sigmoid function</strong> to map predicted values to probabilities:<br>$$<br>\sigma(z) = \frac{1}{1 + e^{-z}}<br>$$<ul><li>$ z = X \beta $: Linear combination of features.</li><li>The output of $ \sigma(z) $ is always between 0 and 1, representing the probability.</li></ul></li></ul></li><li><p><strong>Logit Link Function</strong>:</p><ul><li>The logit function is the natural logarithm of the odds (log-odds) of the binary outcome:<br>$$<br>g(\theta) = \log\left(\frac{P(y=1|X)}{P(y=0|X)}\right)<br>$$</li><li>It transforms probabilities into log-odds:<br>$$<br>g(\theta) = X^T\beta<br>$$</li></ul></li><li><p><strong>Inverse Link Function</strong>:</p><ul><li>To map the log-odds ($ X^T\beta $) back to probabilities, we use the <strong>inverse of the logit function</strong>:<br>$$<br>P(y=1|X, \beta) = \frac{e<sup>{X</sup>T\beta}}{1 + e<sup>{X</sup>T\beta}}<br>$$<ul><li>This is the <strong>sigmoid function</strong>, which outputs probabilities between 0 and 1.</li></ul></li></ul></li><li><p><strong>Decision Boundary</strong>:</p><ul><li>For binary classification:<ul><li>If $ \sigma(z) \geq 0.5 $, classify the input as Class 1.</li><li>If $ \sigma(z) &lt; 0.5 $, classify the input as Class 0.</li></ul></li></ul></li><li><p><strong>Log-Likelihood</strong>:</p><ul><li>Logistic regression optimizes the <strong>log-likelihood</strong> instead of minimizing residuals (like in linear regression):<br>$$<br>\ell(\beta) = \sum_{i=1}^n \left[ y_i \ln(\hat{y}_i) + (1 - y_i) \ln(1 - \hat{y}_i) \right]<br>$$<br>Where:<ul><li>$ \hat{y}_i = \sigma(z_i) $: Predicted probability.</li><li>$ y_i $: Actual class (0 or 1).</li></ul></li></ul></li><li><p><strong>Negative Log-Likelihood</strong>:</p><ul><li>The optimization process in machine learning (and statistics) often involves <strong>minimizing</strong> a cost function. Since the log-likelihood is a measure of fit (higher is better), we take its <strong>negative</strong> to convert the maximization problem into a <strong>minimization problem</strong>:</li><li>$$ -\ln L(\beta) = -\sum_{i=1}^n \left[ y_i X_i^T \beta - \ln(1 + e^ {X_i^ T \beta}) \right] $$</li></ul></li><li><p><strong>Optimization</strong>:</p><ul><li>The goal is to find the coefficients $ \beta $ that maximize the log-likelihood using algorithms like <strong>Gradient Descent</strong> or <strong>Newton’s Method</strong>.</li></ul></li></ol><div class="admonition note"><p class="admonition-title">What is a Link Function?</p><p>A <strong>link function</strong> connects the <strong>linear predictor</strong> ($ X\beta $) to the <strong>mean of the response variable</strong> in a generalized linear model (GLM). It provides a transformation that ensures the predicted values from the model stay within the valid range for the response variable.</p></div><div class="admonition note"><p class="admonition-title">Why negative Log-likelihood function?</p><ul><li>The negative log-likelihood is used to simplify optimization by turning a maximization problem into a minimization one.</li><li>The formula on the slide and in my explanation are equivalent, just written in slightly different forms.</li></ul></div><h3 id="Applications-of-Logistic-Regression">Applications of Logistic Regression</h3><ul><li>Binary classification problems such as:<ul><li>Email spam detection (Spam/Not Spam).</li><li>Disease diagnosis (Positive/Negative).</li><li>Customer churn prediction (Churn/No Churn).</li></ul></li></ul><h3 id="Practical-Example-Binary-Classification-with-Logistic-Regression">Practical Example: Binary Classification with Logistic Regression</h3><p>Below is a Python example using scikit-learn:</p><h4 id="Problem-Predict-whether-a-person-has-heart-disease-based-on-two-features">Problem: Predict whether a person has heart disease based on two features.</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_classification<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score, confusion_matrix, classification_report<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># Generate synthetic binary classification dataset</span><br>X, y = make_classification(n_samples=<span class="hljs-number">200</span>, n_features=<span class="hljs-number">2</span>, n_informative=<span class="hljs-number">2</span>, n_redundant=<span class="hljs-number">0</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># Split the dataset into training and testing sets</span><br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># Fit logistic regression model</span><br>model = LogisticRegression()<br>model.fit(X_train, y_train)<br><br><span class="hljs-comment"># Make predictions</span><br>y_pred = model.predict(X_test)<br><br><span class="hljs-comment"># Evaluate the model</span><br>accuracy = accuracy_score(y_test, y_pred)<br>print(<span class="hljs-string">&quot;Accuracy:&quot;</span>, accuracy)<br>print(<span class="hljs-string">&quot;Confusion Matrix:\n&quot;</span>, confusion_matrix(y_test, y_pred))<br>print(<span class="hljs-string">&quot;Classification Report:\n&quot;</span>, classification_report(y_test, y_pred))<br><br><span class="hljs-comment"># Plot decision boundary</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>x_min, x_max = X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br>y_min, y_max = X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br>xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="hljs-number">0.1</span>),<br>                     np.arange(y_min, y_max, <span class="hljs-number">0.1</span>))<br>Z = model.predict(np.c_[xx.ravel(), yy.ravel()])<br>Z = Z.reshape(xx.shape)<br><br>plt.contourf(xx, yy, Z, alpha=<span class="hljs-number">0.8</span>)<br>plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y, edgecolor=<span class="hljs-string">&#x27;k&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br>plt.title(<span class="hljs-string">&quot;Logistic Regression Decision Boundary&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Feature 1&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Feature 2&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/DvGwCmw.png" alt=""></p><h2 id="Logistic-Regression-for-Multiclass-Classification">Logistic Regression for Multiclass Classification</h2><p>Logistic regression can be extended to handle <strong>multiclass classification problems</strong> where the target variable has more than two classes. The two common approaches are <strong>One-vs-Rest (OvR)</strong> and <strong>Softmax (Multinomial)</strong> logistic regression.</p><h3 id="One-vs-Rest-OvR">One-vs-Rest (OvR)</h3><h4 id="Overview">Overview:</h4><ul><li>In OvR, a separate binary classifier is trained for each class.</li><li>For a class $ k $, the classifier treats:<ul><li>$ y = k $ as <strong>positive (1)</strong>.</li><li>$ y \neq k $ as <strong>negative (0)</strong>.</li></ul></li><li>Each classifier predicts the probability of the input belonging to its class.</li></ul><h4 id="Prediction">Prediction:</h4><ul><li>For a new data point, the class with the <strong>highest probability</strong> is chosen:<br>$$<br>\hat{y} = \arg\max_{k} P(y = k | x)<br>$$</li></ul><h3 id="Softmax-Multinomial-Logistic-Regression">Softmax (Multinomial) Logistic Regression</h3><p>Softmax logistic regression generalizes binary logistic regression to multiple classes. Instead of fitting separate binary classifiers, it predicts the probability for all classes simultaneously using the <strong>softmax function</strong>.</p><h4 id="Softmax-Function">Softmax Function:</h4><p>$$<br>P(y = k | x) = \frac{e^{X \beta_k}}{\sum_{j=1}^K e^{X \beta_j}}<br>$$<br>Where:</p><ul><li>$ K $: Total number of classes.</li><li>$ \beta_k $: Coefficients for class $ k $.</li><li>$ P(y = k | x) $: Probability of class $ k $ given the input $ x $.</li></ul><h4 id="Prediction-v2">Prediction:</h4><ul><li>For a new data point, the class with the highest softmax probability is chosen:<br>$$<br>\hat{y} = \arg\max_{k} P(y = k | x)<br>$$</li></ul><h3 id="Summary-of-Methods">Summary of Methods:</h3><table><thead><tr><th><strong>Method</strong></th><th><strong>When to Use</strong></th><th><strong>Advantages</strong></th><th><strong>Disadvantages</strong></th></tr></thead><tbody><tr><td><strong>One-vs-Rest</strong></td><td>Small datasets with a limited number of classes.</td><td>Easy to implement, interpretable.</td><td>Can struggle with overlapping classes.</td></tr><tr><td><strong>Softmax</strong></td><td>When normalized probabilities across classes are needed.</td><td>Probabilities are calibrated.</td><td>Computationally expensive.</td></tr></tbody></table><p><strong>Softmax approach</strong> (also called multinomial logistic regression)</p><ol><li><p><strong>C-Class Classification</strong>:</p><ul><li>The goal is to classify the target variable $ y $ into one of $ C $ classes:<br>$$<br>y \in {0, 1, \dots, C-1}<br>$$</li></ul></li><li><p><strong>Discrete Probability Distribution</strong>:</p><ul><li>The probabilities $ \theta_0, \theta_1, \dots, \theta_{C-1} $ represent the likelihood of a data point belonging to each class.</li><li>These probabilities satisfy:<br>$$<br>\theta_i \in [0, 1] \quad \text{and} \quad \sum_{i=0}^{C-1} \theta_i = 1<br>$$</li></ul></li><li><p><strong>Link Function</strong>:</p><ul><li>The relationship between the linear model ($ X\beta $) and the class probabilities is established using the <strong>Softmax function</strong>:<br>$$<br>g(\theta) = \log \left( \frac{\theta_i}{1 - \sum_{u=0}^{C-1} \theta_u} \right) = X^T \beta<br>$$</li></ul></li><li><p><strong>Class Probabilities</strong>:</p><ul><li>For each class $ i $, the probability is computed as:<br>$$<br>P(y = i | X, \beta) = \frac{e^ {X^ T \beta_i}}{1 + \sum_{j=0}^ {C-1} e^ {X^ T \beta_j}}<br>$$</li><li>For the last class $ C-1 $, the probability is:<br>$$<br>P(y = C-1 | X, \beta) = \frac{1}{1 + \sum_{i=0}^{C-2} e^ {X^T \beta_i}}<br>$$</li></ul></li></ol><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">AI: Logistic Regression</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Linear Model Optimization</title>
    <link href="https://karobben.github.io/2024/12/30/AI/linearoptimal/"/>
    <id>https://karobben.github.io/2024/12/30/AI/linearoptimal/</id>
    <published>2024-12-30T20:59:30.000Z</published>
    <updated>2024-12-31T03:33:25.115Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Measure-Information">Measure Information</h2><p>Both Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are measures used to evaluate the quality of statistical models, particularly in the context of selecting the best model size or complexity.</p><ol><li><p><strong>Why Use AIC and BIC?</strong></p><ul><li>When building statistical or machine learning models, we often face the challenge of balancing <strong>model fit</strong> (how well the model explains the data) with <strong>model simplicity</strong> (avoiding overfitting).</li><li>AIC and BIC are metrics that help in selecting the best model by incorporating penalties for the number of parameters used in the model.</li></ul></li><li><p><strong>Akaike Information Criterion (AIC):</strong></p><ul><li>AIC estimates the relative quality of a model for a given dataset.</li><li>Formula:<br>$$<br>\text{AIC} = 2k - 2 \ln(L)<br>$$<ul><li>$ k $: Number of parameters in the model.</li><li>$ L $: Likelihood of the model (how well it fits the data).</li></ul></li><li><strong>Objective</strong>: Choose the model with the <strong>lowest AIC value</strong>, which balances fit and complexity.</li></ul></li><li><p><strong>Bayesian Information Criterion (BIC):</strong></p><ul><li>Similar to AIC, BIC adds a stronger penalty for model complexity to account for overfitting.</li><li>Formula:<br>$$<br>\text{BIC} = k \ln(n) - 2 \ln(L)<br>$$<ul><li>$ n $: Number of observations in the dataset.</li><li>$ k $: Number of parameters in the model.</li></ul></li><li><strong>Objective</strong>: Choose the model with the <strong>lowest BIC value</strong> for a balance between fit and simplicity, especially when sample size $ n $ is large.</li></ul></li><li><p><strong>Key Difference Between AIC and BIC:</strong></p><ul><li>AIC focuses on model quality and is less strict about model size.</li><li>BIC penalizes complexity more heavily, making it more conservative in selecting simpler models.</li></ul></li></ol><p><strong>Applications</strong>:</p><ul><li>Model selection in regression, time-series analysis, and machine learning.</li><li>Comparing models with different numbers of features or parameters.</li><li>Evaluating trade-offs between underfitting and overfitting.</li></ul><p>Would you like a detailed example or visual demonstration of how AIC and BIC are used?</p><table><thead><tr><th><strong>Criterion</strong></th><th><strong>Formula</strong></th><th><strong>Focus</strong></th><th><strong>Penalty for Complexity</strong></th><th><strong>Use Case</strong></th><th><strong>Objective</strong></th></tr></thead><tbody><tr><td><strong>Akaike Information Criterion (AIC)</strong></td><td>$ 2k - 2\ln(L) $</td><td>Model fit vs. simplicity</td><td>Proportional to $ k $</td><td>Choosing models that balance goodness-of-fit and simplicity</td><td>Minimize AIC</td></tr><tr><td><strong>Bayesian Information Criterion (BIC)</strong></td><td>$ k\ln(n) - 2\ln(L) $</td><td>Model fit vs. parsimony</td><td>Stronger penalty with $ \ln(n) $</td><td>Suitable for large datasets and emphasizing simpler models</td><td>Minimize BIC</td></tr><tr><td><strong>Penalty Strength</strong></td><td>Moderate</td><td>High</td><td><strong>Depends on Sample Size ($ n $)</strong></td><td>Larger datasets lead to stricter penalties in BIC</td><td></td></tr><tr><td><strong>Common Application</strong></td><td>Time-series, regression, machine learning</td><td>Model selection across varying complexity</td><td>Multi-model comparison</td><td>Best when balancing underfitting and overfitting</td><td></td></tr></tbody></table><ol><li><p><strong>AIC</strong>:</p><ul><li>Prefers models with a better balance between complexity and fit.</li><li>Less conservative than BIC, suitable for small datasets or exploratory analysis.</li></ul></li><li><p><strong>BIC</strong>:</p><ul><li>Stronger emphasis on simplicity.</li><li>More appropriate for larger datasets or when avoiding overfitting is crucial.</li></ul></li><li><p><strong>Choosing Between AIC and BIC</strong>:</p><ul><li>Use <strong>AIC</strong> if you prioritize model quality over strict simplicity.</li><li>Use <strong>BIC</strong> if simplicity and generalization are more important.</li></ul></li></ol><h3 id="Likelihood">Likelihood</h3><p>When calculating AIC or BIC, the likelihood refers to <strong>how well the model trained on the training data</strong> explains the same training data. The likelihood is not calculated on the test data, as AIC and BIC are measures of model quality on the training dataset itself.</p><h3 id="Likelihood-in-AIC-BIC-Context">Likelihood in AIC/BIC Context:</h3><ol><li><strong>Training Data</strong>:<ul><li>We use the model parameters (e.g., coefficients in regression) estimated from the training data to calculate the likelihood of the training data.</li></ul></li><li><strong>Likelihood Calculation</strong>:<ul><li>For a model trained on the training data, the likelihood is the probability (or density) of the observed training data under the model:<br>$$<br>L(\theta | \text{Training Data}) = \prod_{i=1}^n f(y_i | \theta)<br>$$<br>Where:<ul><li>$ y_i $: Observed target value.</li><li>$ \theta $: Model parameters estimated during training.</li><li>$ f(y_i | \theta) $: Probability density of $ y_i $ under the model.</li></ul></li></ul></li><li><strong>Log-Likelihood for AIC/BIC</strong>:<ul><li>Instead of working with $ L $, we calculate the <strong>log-likelihood</strong> to simplify computations:<br>$$<br>\ln L(\theta | \text{Training Data}) = \sum_{i=1}^n \ln f(y_i | \theta)<br>$$</li></ul></li></ol><h3 id="Steps-to-Calculate-Likelihood-for-AIC-BIC">Steps to Calculate Likelihood for AIC/BIC:</h3><ol><li>Train the Model:<ul><li>Use the training data to estimate the model parameters ($ \theta $).</li></ul></li><li>Calculate Predictions ($ \hat{y}_i $):<ul><li>Predict the mean or central tendency of the model for each training data point.</li></ul></li><li>Calculate Residuals and Likelihood:<ul><li>Assume a distribution for the residuals (e.g., normal distribution).</li><li>For a normal distribution:<br>$$<br>f(y_i | \hat{y}_i, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \hat{y}_i)^ 2}{2\sigma^ 2}\right)<br>$$</li><li>The log-likelihood becomes:<br>$$<br>\ln L = \sum_{i=1}^n \left[ -\frac{1}{2} \ln(2\pi\sigma^2) - \frac{(y_i - \hat{y}_i)^ 2}{2\sigma^ 2} \right]<br>$$</li></ul></li></ol><p>$\sigma$ represents the <strong>standard deviation of the residuals</strong></p><h3 id="Example-Using-Training-Data-to-Calculate-Likelihood">Example: Using Training Data to Calculate Likelihood</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Simulated training data</span><br>X_train = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>y_train = np.array([<span class="hljs-number">1.2</span>, <span class="hljs-number">2.3</span>, <span class="hljs-number">2.8</span>, <span class="hljs-number">4.1</span>, <span class="hljs-number">5.3</span>])<br><br><span class="hljs-comment"># Train a linear regression model</span><br>model = LinearRegression()<br>model.fit(X_train, y_train)<br>y_pred_train = model.predict(X_train)<br><br><span class="hljs-comment"># Calculate residuals and variance</span><br>residuals = y_train - y_pred_train<br>sigma_squared = np.var(residuals, ddof=<span class="hljs-number">1</span>)  <span class="hljs-comment"># Variance of residuals</span><br><br><span class="hljs-comment"># Calculate log-likelihood</span><br>n = <span class="hljs-built_in">len</span>(y_train)<br>log_likelihood = -<span class="hljs-number">0.5</span> * n * np.log(<span class="hljs-number">2</span> * np.pi * sigma_squared) - np.<span class="hljs-built_in">sum</span>((residuals**<span class="hljs-number">2</span>) / (<span class="hljs-number">2</span> * sigma_squared))<br><br><span class="hljs-comment"># AIC and BIC</span><br>k = <span class="hljs-number">2</span>  <span class="hljs-comment"># Number of parameters (intercept + slope)</span><br>aic = <span class="hljs-number">2</span> * k - <span class="hljs-number">2</span> * log_likelihood<br>bic = k * np.log(n) - <span class="hljs-number">2</span> * log_likelihood<br><br>&#123;<span class="hljs-string">&quot;Log-Likelihood&quot;</span>: log_likelihood, <span class="hljs-string">&quot;AIC&quot;</span>: aic, <span class="hljs-string">&quot;BIC&quot;</span>: bic&#125;<br></code></pre></td></tr></table></figure></div><h3 id="Example">Example</h3><p>Source: <a href="https://www.youtube.com/watch?v=HOqHI53x9Go">Model selection with AIC and AICc</a><br><img src="https://imgur.com/K4j8QGS.png" alt=""></p><h2 id="Forward-stagewise-regression-and-Backward-stagewise-regression">Forward stagewise regression and Backward stagewise regression</h2><p><strong>Backward stagewise regression</strong> and <strong>Forward stagewise regression</strong> are methods for variable selection and model fitting, primarily used in regression contexts. They are stepwise procedures for adding or removing predictors in a systematic way to improve model performance or interpretability.</p><h3 id="Backward-Stagewise-Regression"><strong>Backward Stagewise Regression</strong></h3><h4 id="Overview">Overview:</h4><ul><li>Starts with a <strong>full model</strong> (all predictors included).</li><li>Gradually <strong>removes predictors</strong> one by one, based on a criterion (e.g., p-value, AIC, or adjusted $ R^2 $).</li><li>The goal is to find a smaller, simpler model without significantly compromising the fit.</li></ul><h4 id="Procedure">Procedure:</h4><ol><li>Begin with a model containing all predictors.</li><li>Evaluate the significance of each predictor (e.g., using p-values).</li><li>Remove the <strong>least significant predictor</strong> (highest p-value) that exceeds a predefined</li></ol><p>threshold (e.g., $p &gt; 0.05$).</p><ol start="4"><li>Refit the model and repeat the process until all remaining predictors are statistically significant or meet the stopping criteria.</li></ol><h4 id="Advantages">Advantages:</h4><ul><li>Simple and interpretable.</li><li>Useful for removing irrelevant predictors in high-dimensional datasets.</li></ul><h4 id="Disadvantages">Disadvantages:</h4><ul><li>Can miss optimal combinations of predictors.</li><li>Sensitive to multicollinearity among predictors.</li></ul><h3 id="2-Forward-Stagewise-Regression">2. <strong>Forward Stagewise Regression</strong></h3><h4 id="Overview-v2">Overview:</h4><ul><li>Starts with an <strong>empty model</strong> (no predictors included).</li><li>Gradually <strong>adds predictors</strong> one at a time, based on a criterion (e.g., reducing residual sum of squares or improving AIC/BIC).</li><li>The goal is to build a model step-by-step, adding only significant predictors.</li></ul><h4 id="Procedure-v2">Procedure:</h4><ol><li>Begin with an empty model.</li><li>Evaluate all predictors not yet in the model, adding the one that most improves the model fit (e.g., the one with the smallest p-value or largest improvement in $ R^2 $).</li><li>Refit the model and repeat the process until no additional predictors meet the inclusion criteria.</li></ol><h4 id="Advantages-v2">Advantages:</h4><ul><li>Can handle datasets with a large number of predictors.</li><li>Less likely to overfit compared to starting with a full model.</li></ul><h4 id="Disadvantages-v2">Disadvantages:</h4><ul><li>Ignores potential joint effects of predictors (e.g., interactions).</li><li>May miss the best subset of predictors.</li></ul><h3 id="Key-Differences-Between-Backward-and-Forward-Stagewise-Regression">Key Differences Between Backward and Forward Stagewise Regression</h3><table><thead><tr><th>Feature</th><th>Backward Stagewise</th><th>Forward Stagewise</th></tr></thead><tbody><tr><td><strong>Starting Point</strong></td><td>Full model (all predictors).</td><td>Empty model (no predictors).</td></tr><tr><td><strong>Procedure</strong></td><td>Removes predictors iteratively.</td><td>Adds predictors iteratively.</td></tr><tr><td><strong>Use Case</strong></td><td>Small datasets with fewer predictors.</td><td>Large datasets with many predictors.</td></tr><tr><td><strong>Limitations</strong></td><td>May retain redundant predictors.</td><td>May miss joint effects of predictors.</td></tr></tbody></table><h3 id="When-to-Use-Each-Method">When to Use Each Method?</h3><ul><li><p><strong>Backward Stagewise</strong>:</p><ul><li>When you suspect many predictors are irrelevant.</li><li>When computational resources are not a concern (since fitting starts with a large model).</li></ul></li><li><p><strong>Forward Stagewise</strong>:</p><ul><li>When you have a large number of predictors and computational efficiency is critical.</li><li>When you want a simpler starting point and add complexity gradually.</li></ul></li></ul><h3 id="A-Quick-Example">A Quick Example</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Re-import necessary libraries after environment reset</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_regression<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br><br><span class="hljs-comment"># Generate a dataset with 100 samples and 10 features</span><br>np.random.seed(<span class="hljs-number">42</span>)<br>X, y = make_regression(n_samples=<span class="hljs-number">100</span>, n_features=<span class="hljs-number">10</span>, noise=<span class="hljs-number">10</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># Initialize model and variables for Forward Stagewise Regression</span><br>selected_features = []<br>remaining_features = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]))<br>forward_scores = []<br><br><span class="hljs-comment"># Forward Stagewise Regression</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(remaining_features)):<br>    scores = []<br>    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> remaining_features:<br>        <span class="hljs-comment"># Fit a model with the current feature added</span><br>        features_to_test = selected_features + [feature]<br>        model = LinearRegression().fit(X[:, features_to_test], y)<br>        score = model.score(X[:, features_to_test], y)  <span class="hljs-comment"># R^2 score</span><br>        scores.append((score, feature))<br>    <br>    <span class="hljs-comment"># Select the feature with the highest R^2 score</span><br>    scores.sort(reverse=<span class="hljs-literal">True</span>)<br>    best_score, best_feature = scores[<span class="hljs-number">0</span>]<br>    forward_scores.append(best_score)<br>    selected_features.append(best_feature)<br>    remaining_features.remove(best_feature)<br><br><span class="hljs-comment"># Results of Forward Stagewise Regression</span><br>selected_features_forward = selected_features  <span class="hljs-comment"># Save selected features for clarity</span><br><br><span class="hljs-comment"># Backward Stagewise Regression</span><br>selected_features_backward = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]))<br>backward_scores = []<br><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(selected_features_backward) - <span class="hljs-number">1</span>):<br>    scores = []<br>    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> selected_features_backward:<br>        <span class="hljs-comment"># Fit a model with the current feature removed</span><br>        features_to_test = [f <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> selected_features_backward <span class="hljs-keyword">if</span> f != feature]<br>        model = LinearRegression().fit(X[:, features_to_test], y)<br>        score = model.score(X[:, features_to_test], y)  <span class="hljs-comment"># R^2 score</span><br>        scores.append((score, feature))<br>    <br>    <span class="hljs-comment"># Remove the feature with the smallest impact on R^2 score</span><br>    scores.sort(reverse=<span class="hljs-literal">True</span>)<br>    best_score, worst_feature = scores[-<span class="hljs-number">1</span>]<br>    backward_scores.append(best_score)<br>    selected_features_backward.remove(worst_feature)<br><br><span class="hljs-comment"># Plot R^2 scores for Forward and Backward Stagewise Regression</span><br>plt.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))<br><br><span class="hljs-comment"># Forward Stagewise Regression</span><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(forward_scores) + <span class="hljs-number">1</span>), forward_scores, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">&#x27;Forward Stagewise&#x27;</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>)<br><br><span class="hljs-comment"># Backward Stagewise Regression</span><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(backward_scores), <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>), backward_scores, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">&#x27;Backward Stagewise&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br><br><span class="hljs-comment"># Formatting the plot</span><br>plt.title(<span class="hljs-string">&quot;R² Scores During Forward and Backward Stagewise Regression&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Number of Features&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;R² Score&quot;</span>)<br>plt.xticks(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(forward_scores) + <span class="hljs-number">1</span>))<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.tight_layout()<br>plt.show()<br></code></pre></td></tr></table></figure></div><script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script><script type="text/javascript">  google.charts.load('current', { packages: ['corechart'] });  google.charts.setOnLoadCallback(drawChart);  function drawChart() {    var data = google.visualization.arrayToDataTable([          ['Number of Features', 'Forward Stagewise R²', 'Backward Stagewise R²'],          [1, 0.287, null],          [2, 0.511, null],          [3, 0.698, null],          [4, 0.801, null],          [5, 0.918, null],          [6, 0.988, null],          [7, 0.995, null],          [8, 0.997, null],          [9, 0.997, null],          [9, 0.997, 0.787],          [8, null, 0.643],          [7, null, 0.441],          [6, null, 0.285],          [5, null, 0.176],          [4, null, 0.076],          [3, null, 0.045],          [2, null, 0.011],          [1, null, 0.002],        ]);    var options = {      title: 'R² Scores During Forward and Backward Stagewise Regression',      hAxis: { title: 'Number of Features' },      vAxis: { title: 'R² Score', minValue: 0, maxValue: 1 },      legend: { position: 'top' },      colors: ['blue', 'red'],    };    var chart = new google.visualization.LineChart(document.getElementById('chart_div'));    chart.draw(data, options);  }</script><div id="chart_div" style="width: 100%  ; height: 300px"></div><h3 id="Limitations">Limitations</h3><p><strong>Forward and Backward Stagewise Regression</strong> can become computationally expensive and impractical when dealing with a <strong>large number of features (e.g., 1000+ features)</strong> because:</p><ol><li><strong>High Computational Cost</strong>:<ul><li>Both methods involve iteratively adding or removing features, which requires fitting a model at each step. For large datasets, this becomes infeasible.</li></ul></li><li><strong>Potential Overfitting</strong>:<ul><li>With a large number of features, stepwise methods might select features that fit noise in the data rather than actual patterns.</li></ul></li><li><strong>Ignoring Interactions</strong>:<ul><li>These methods do not account for interactions between features, which can lead to suboptimal feature selection.</li></ul></li></ol><p><strong>Alternative Methods for Large Feature Spaces</strong></p><table><thead><tr><th><strong>Method</strong></th><th><strong>Description</strong></th><th><strong>Advantages</strong></th><th><strong>Disadvantages</strong></th><th><strong>Best Use Case</strong></th></tr></thead><tbody><tr><td><strong>Lasso Regression (L1)</strong></td><td>Shrinks coefficients and sets some to exactly zero for feature selection.</td><td>- Efficient for high-dimensional data.<br>- Automatically selects features.<br>- Prevents overfitting.</td><td>- May ignore correlated features.<br>- Requires hyperparameter tuning ($ \lambda $).</td><td>When many features are irrelevant, and sparse solutions are desired.</td></tr><tr><td><strong>Elastic Net</strong></td><td>Combines L1 (Lasso) and L2 (Ridge) regularization.</td><td>- Balances feature selection and handling multicollinearity.<br>- Suitable for correlated features.</td><td>- More complex than Lasso.<br>- Requires tuning of both $ \lambda $ and $ \alpha $.</td><td>When predictors are highly correlated, and feature selection is needed.</td></tr><tr><td><strong>Recursive Feature Elimination (RFE)</strong></td><td>Iteratively removes the least important features based on a chosen model.</td><td>- Works with any estimator (e.g., linear, tree-based).<br>- Provides a rank of feature importance.</td><td>- Computationally expensive.<br>- Sensitive to model choice and training data.</td><td>When model-specific feature ranking is required.</td></tr><tr><td><strong>Principal Component Analysis (PCA)</strong></td><td>Reduces dimensionality by transforming features into uncorrelated components that capture most variance.</td><td>- Handles high-dimensional data well.<br>- Removes multicollinearity.<br>- No need for target variable.</td><td>- Components are linear combinations of features, losing interpretability.<br>- Not ideal for feature selection.</td><td>When reducing dimensionality is more important than interpretability.</td></tr><tr><td><strong>Tree-Based Feature Importance</strong></td><td>Uses models like Random Forest or Gradient Boosting to rank feature importance.</td><td>- Naturally handles non-linearity.<br>- Accounts for feature interactions.<br>- Fast for large datasets.</td><td>- Can be biased toward high-cardinality features.<br>- Does not directly reduce feature count.</td><td>When using tree-based models or ranking feature importance is a priority.</td></tr><tr><td><strong>Mutual Information</strong></td><td>Measures the statistical dependency between features and the target variable.</td><td>- Non-parametric.<br>- Detects non-linear relationships.</td><td>- Computationally expensive for many features.<br>- Does not handle feature interactions.</td><td>When quantifying feature relevance to the target variable without assumptions is needed.</td></tr><tr><td><strong>Feature Clustering</strong></td><td>Groups similar features into clusters and uses cluster representatives for modeling.</td><td>- Reduces redundancy in correlated features.<br>- Scales well with high-dimensional data.</td><td>- May lose specific feature contributions.<br>- Requires a meaningful distance metric.</td><td>When dealing with highly correlated features or datasets with groups of similar features.</td></tr><tr><td><strong>Embedding-Based Methods</strong></td><td>Uses deep learning or models like word2vec to transform features into a lower-dimensional space.</td><td>- Captures complex relationships between features.<br>- Flexible for large feature spaces.</td><td>- Requires advanced techniques and computational resources.<br>- May lose interpretability.</td><td>When handling very high-dimensional data (e.g., text, genomic data) with complex dependencies.</td></tr></tbody></table><h4 id="Recommendations">Recommendations:</h4><ul><li><strong>Lasso Regression</strong>: If feature selection is the goal and the data has many irrelevant features.</li><li><strong>Elastic Net</strong>: If features are highly correlated and Lasso alone may struggle.</li><li><strong>PCA</strong>: When interpretability is less important, and you want to reduce dimensionality.</li><li><strong>Tree-Based Importance</strong>: For datasets where feature importance ranking is needed, especially with tree-based models.</li><li><strong>Feature Clustering</strong>: For correlated features where redundancy needs to be reduced.</li></ul><h2 id="M-Estimators">M-Estimators</h2><p><strong>M-Estimators</strong> (Maximum Likelihood-type Estimators) are a general class of estimators in statistics used for robust parameter estimation. They extend the principle of Maximum Likelihood Estimation (MLE) to allow for more flexibility and robustness, especially in the presence of outliers or non-normal errors.</p><h3 id="What-Are-M-Estimators">What Are M-Estimators?</h3><ol><li><p><strong>Definition</strong>:</p><ul><li>M-Estimators generalize Maximum Likelihood Estimators by minimizing a <strong>loss function</strong> (also called the objective function) over the parameters of interest.</li></ul></li><li><p><strong>Loss Function</strong>:</p><ul><li>The core idea is to minimize a function of residuals:<br>$$<br>\hat{\theta} = \arg\min_{\theta} \sum_{i=1}^n \rho\left(\frac{r_i}{\sigma}\right)<br>$$<br>Where:<ul><li>$ r_i = y_i - f(x_i, \theta) $: Residual (difference between observed and predicted values).</li><li>$ \rho(\cdot) $: A loss function that determines the contribution of residuals.</li><li>$ \sigma $: Scale parameter (controls the spread).</li></ul></li></ul></li><li><p><strong>Goal</strong>:</p><ul><li>Instead of focusing purely on minimizing squared residuals (like in Ordinary Least Squares), M-Estimators allow for more flexible functions to make the estimator <strong>less sensitive to outliers</strong>.</li></ul></li></ol><h3 id="Examples-of-M-Estimators">Examples of M-Estimators</h3><table><thead><tr><th><strong>Type</strong></th><th><strong>Loss Function ($ \rho $)</strong></th><th><strong>Characteristics</strong></th></tr></thead><tbody><tr><td><strong>Ordinary Least Squares (OLS)</strong></td><td>$ \rho( r ) = r^2 $</td><td>Highly sensitive to outliers. Minimizes sum of squared errors.</td></tr><tr><td><strong>Huber Loss</strong></td><td>$\rho( r) = \begin{cases} r^2 &amp; \text{if } |r| \leq c \\ 2c|r| - c^2 &amp; \text{if } |r| &gt; c\end{cases}$</td><td>Combines squared loss (for small residuals) and absolute loss (for large residuals).</td></tr><tr><td><strong>Tukey’s Biweight</strong></td><td>$\rho( r ) = \begin{cases}  c^2\left(1 - \left[1 - \left(\frac{r}{c}\right)^ 2\right]^ 3\right) &amp; \text{if } |r| \leq c \\ c^2 &amp; \text{if } |r| &gt; c \end{cases}$</td><td>Completely ignores residuals larger than a threshold $ c $.</td></tr><tr><td><strong>Huberized Absolute Loss</strong></td><td>$\rho( r) = |r|$</td><td>Linear penalty, robust but less efficient.</td></tr></tbody></table><h3 id="Advantages-of-M-Estimators">Advantages of M-Estimators</h3><ol><li><strong>Robustness to Outliers</strong></li><li><strong>Flexibility</strong></li><li><strong>Generalization of MLE</strong>:<ul><li>MLE is a special case of M-Estimators, making them widely applicable in parametric settings.</li></ul></li></ol><h3 id="When-to-Use-M-Estimators">When to Use M-Estimators?</h3><ol><li><strong>Presence of Outliers</strong>:</li><li><strong>Non-Normal Errors</strong>:</li><li><strong>Heavy-Tailed Distributions</strong>:</li></ol><h3 id="Practical-Example-Using-Huber-Loss">Practical Example: Using Huber Loss</h3><p>Below is an example of applying <strong>Huber Loss</strong> to regression in Python:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> HuberRegressor<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># Simulate data with outliers</span><br>np.random.seed(<span class="hljs-number">42</span>)<br>X = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>y = <span class="hljs-number">3</span> * X.flatten() + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size=X.shape[<span class="hljs-number">0</span>])<br>y[::<span class="hljs-number">10</span>] += <span class="hljs-number">20</span>  <span class="hljs-comment"># Add outliers every 10th point</span><br><br><span class="hljs-comment"># Fit Ordinary Least Squares (OLS) Regression</span><br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br>ols = LinearRegression().fit(X, y)<br><br><span class="hljs-comment"># Fit Huber Regression</span><br>huber = HuberRegressor(epsilon=<span class="hljs-number">1.35</span>).fit(X, y)<br><br><span class="hljs-comment"># Plot the results</span><br>plt.scatter(X, y, color=<span class="hljs-string">&quot;blue&quot;</span>, label=<span class="hljs-string">&quot;Data with Outliers&quot;</span>)<br>plt.plot(X, ols.predict(X), color=<span class="hljs-string">&quot;red&quot;</span>, label=<span class="hljs-string">&quot;OLS Regression Line&quot;</span>)<br>plt.plot(X, huber.predict(X), color=<span class="hljs-string">&quot;green&quot;</span>, label=<span class="hljs-string">&quot;Huber Regression Line&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;Comparison of OLS and Huber Regression&quot;</span>)<br>plt.legend()<br>plt.xlabel(<span class="hljs-string">&quot;X&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;y&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div><script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script><script type="text/javascript">  google.charts.load('current', { packages: ['corechart'] });  google.charts.setOnLoadCallback(drawChart);  function drawChart() {    // Data for OLS and Huber Regression    var data = google.visualization.arrayToDataTable([['X', 'Observed Data', 'OLS Regression', 'Huber Regression'], [0.0, 20.496714153011233, 2.3618406112691552, -0.05895399071098967], [0.10101010101010101, 0.16476600185911838, 2.6554630935183825, 0.24599617373579663], [0.20202020202020202, 1.2537491441612985, 2.9490855757676098, 0.550946338182583], [0.30303030303030304, 2.4321207654989347, 3.242708058016837, 0.8558965026293694], [0.40404040404040403, 0.9779678373978762, 3.5363305402660643, 1.1608466670761555], [0.5050505050505051, 1.2810145582023347, 3.8299530225152916, 1.465796831522942], [0.6060606060606061, 3.39739463368921, 4.123575504764519, 1.7707469959697284], [0.7070707070707071, 2.88864685036503, 4.417197987013745, 2.0756971604165146], [0.8080808080808081, 1.9547680383074721, 4.710820469262973, 2.380647324863301], [0.9090909090909091, 3.2698327708586916, 5.0044429515122, 2.6855974893100876], [1.0101010101010102, 22.566885337490568, 5.298065433761428, 2.990547653756874], [1.1111111111111112, 2.8676035797630766, 5.591687916010654, 3.29549781820366], [1.2121212121212122, 3.878325907929671, 5.8853103982598824, 3.6004479826504467], [1.3131313131313131, 2.0261136947361416, 6.178932880509109, 3.905398147097233], [1.4141414141414141, 2.5175064099112094, 6.472555362758336, 4.210348311544019], [1.5151515151515151, 3.983167016213572, 6.766177845007563, 4.5152984759908055], [1.6161616161616161, 3.835653728150425, 7.059800327256791, 4.820248640437591], [1.7171717171717171, 5.465762484110425, 7.353422809506018, 5.125198804884378], [1.8181818181818181, 4.546521379024243, 7.647045291755245, 5.430148969331165], [1.9191919191919191, 4.345272056240466, 7.940667774004472, 5.73509913377795], [2.0202020202020203, 27.526254829527616, 8.2342902562537, 6.040049298224737], [2.121212121212121, 6.1378600631498275, 8.527912738502927, 6.344999462671523], [2.2222222222222223, 6.734194871354591, 8.821535220752153, 6.64994962711831], [2.323232323232323, 5.5449487834835125, 9.11515770300138, 6.954899791565095], [2.4242424242424243, 6.728344548202091, 9.40878018525061, 7.259849956011883], [2.525252525252525, 7.6866801654674415, 9.702402667499836, 7.564800120458668], [2.6262626262626263, 6.727794301365576, 9.996025149749062, 7.8697502849054555], [2.727272727272727, 8.557516200163853, 10.289647631998289, 8.17470044935224], [2.8282828282828283, 7.88420979492968, 10.583270114247517, 8.479650613799027], [2.929292929292929, 8.49618503808551, 10.876892596496743, 8.784600778245814], [3.0303030303030303, 28.489202478679694, 11.170515078745971, 9.0895509426926], [3.131313131313131, 11.24621757844833, 11.464137560995198, 9.394501107139385], [3.2323232323232323, 9.683472472231763, 11.757760043244426, 9.699451271586172], [3.3333333333333335, 8.9422890710441, 12.051382525493654, 10.00440143603296], [3.4343434343434343, 11.125575215133491, 12.34500500774288, 10.309351600479745], [3.5353535353535355, 9.385216956089582, 12.638627489992109, 10.614301764926532], [3.6363636363636362, 11.117954504095664, 12.932249972241335, 10.919251929373319], [3.7373737373737375, 9.252451088241438, 13.225872454490563, 11.224202093820105], [3.8383838383838382, 10.186965466253085, 13.51949493673979, 11.52915225826689], [3.9393939393939394, 12.015043054050942, 13.813117418989018, 11.834102422713677], [4.040404040404041, 32.85967870120753, 14.106739901238244, 12.139052587160464], [4.141414141414141, 12.595610705432392, 14.40036238348747, 12.444002751607249], [4.242424242424242, 12.611624444884486, 14.693984865736699, 12.748952916054035], [4.343434343434343, 12.729199334713742, 14.987607347985925, 13.053903080500822], [4.444444444444445, 11.854811342965906, 15.281229830235153, 13.358853244947609], [4.545454545454545, 12.916519427968927, 15.57485231248438, 13.663803409394394], [4.646464646464646, 13.47875516843415, 15.868474794733606, 13.96875357384118], [4.747474747474747, 15.299546468643157, 16.162097276982834, 14.273703738287967], [4.848484848484849, 14.889072835023008, 16.45571975923206, 14.578653902734755], [4.94949494949495, 13.085444693122115, 16.74934224148129, 14.883604067181542], [5.05050505050505, 35.47559912090995, 17.042964723730513, 15.188554231628325], [5.151515151515151, 15.069463174129137, 17.336587205979743, 15.493504396075112], [5.252525252525253, 15.080653757269799, 17.630209688228973, 15.7984545605219], [5.353535353535354, 16.67228234944693, 17.9238321704782, 16.103404724968687], [5.454545454545454, 17.394635886132313, 18.217454652727426, 16.40835488941547], [5.555555555555555, 17.597946785782863, 18.511077134976652, 16.713305053862257], [5.656565656565657, 16.13047944647433, 18.80469961722588, 17.018255218309044], [5.757575757575758, 16.96351489687606, 19.09832209947511, 17.32320538275583], [5.858585858585858, 17.907021007161138, 19.39194458172433, 17.628155547202617], [5.959595959595959, 18.854333005910238, 19.68556706397356, 17.933105711649404], [6.0606060606060606, 37.70264394397289, 19.979189546222788, 18.23805587609619], [6.161616161616162, 18.299189508184668, 20.272812028472018, 18.543006040542977], [6.262626262626262, 17.681543813872757, 20.56643451072124, 18.84795620498976], [6.363636363636363, 17.89470246682842, 20.86005699297047, 19.152906369436547], [6.4646464646464645, 20.20646521633359, 21.153679475219697, 19.457856533883334], [6.565656565656566, 21.05320972554052, 21.447301957468927, 19.762806698330124], [6.666666666666667, 19.927989878419666, 21.740924439718153, 20.06775686277691], [6.767676767676767, 21.306563200922326, 22.03454692196738, 20.372707027223694], [6.8686868686868685, 20.96769663110824, 22.328169404216606, 20.67765719167048], [6.96969696969697, 20.263971154485787, 22.621791886465832, 20.982607356117267], [7.070707070707071, 41.573516817629624, 22.915414368715062, 21.287557520564054], [7.171717171717171, 23.053188081617485, 23.209036850964285, 21.592507685010837], [7.2727272727272725, 21.782355779071864, 23.502659333213515, 21.897457849457627], [7.373737373737374, 23.685855777026127, 23.79628181546274, 22.202408013904414], [7.474747474747475, 19.80449732015268, 24.08990429771197, 22.5073581783512], [7.575757575757575, 23.54917523164795, 24.383526779961194, 22.812308342797984], [7.6767676767676765, 23.117350098541202, 24.677149262210424, 23.11725850724477], [7.777777777777778, 23.034325982867465, 24.97077174445965, 23.422208671691557], [7.878787878787879, 23.728124412899138, 25.26439422670888, 23.727158836138344], [7.979797979797979, 21.951825024793045, 25.558016708958103, 24.03210900058513], [8.080808080808081, 44.02275235458673, 25.851639191207333, 24.337059165031917], [8.181818181818182, 24.902567116966292, 26.14526167345656, 24.642009329478704], [8.282828282828282, 26.32637889322636, 26.438884155705786, 24.946959493925487], [8.383838383838384, 24.633244933241507, 26.732506637955016, 25.251909658372277], [8.484848484848484, 24.646051851652267, 27.026129120204242, 25.55685982281906], [8.585858585858587, 25.25581871399122, 27.319751602453472, 25.86180998726585], [8.686868686868687, 26.976008178308135, 27.613374084702695, 26.166760151712634], [8.787878787878787, 26.692387473296044, 27.90699656695192, 26.47171031615942], [8.88888888888889, 26.136906462899628, 28.20061904920115, 26.776660480606207], [8.98989898989899, 27.482964402810325, 28.494241531450378, 27.081610645052994], [9.09090909090909, 47.36980482207531, 28.787864013699604, 27.386560809499777], [9.191919191919192, 28.54440256629047, 29.081486495948834, 27.691510973946567], [9.292929292929292, 27.176734784910522, 29.375108978198057, 27.99646113839335], [9.393939393939394, 27.854156035220416, 29.66873146044729, 28.30141130284014], [9.494949494949495, 28.09274033171633, 29.962353942696513, 28.606361467286924], [9.595959595959595, 27.324363839746667, 30.25597642494574, 28.91131163173371], [9.696969696969697, 29.38702936797367, 30.54959890719497, 29.2162617961805], [9.797979797979798, 29.65499466611928, 30.843221389444196, 29.521211960627284], [9.8989898989899, 29.70208315361216, 31.136843871693426, 29.826162125074074], [10.0, 29.765412866624853, 31.430466353942652, 30.131112289520857]]);    // Chart options    var options = {      title: 'OLS vs Huber Regression',      hAxis: { title: 'X', minValue: 0 },      vAxis: { title: 'y', minValue: 0 },      pointSize: 2,      legend: { position: 'right' },      series: {        0: { color: 'black', pointShape: 'circle' }, // Observed data points        1: { color: 'red', lineWidth: 2, pointSize: 0 },          // OLS Regression line        2: { color: 'green', lineWidth: 2, pointSize: 0 },        // Huber Regression line      },    };    // Render the chart    var chart = new google.visualization.ScatterChart(document.getElementById('chart_div2'));    chart.draw(data, options);  }</script><div id="chart_div2" style="width: 100%; height: 400px;"></div><hr><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Linear Model Optimization</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Regularization</title>
    <link href="https://karobben.github.io/2024/12/30/AI/regularization/"/>
    <id>https://karobben.github.io/2024/12/30/AI/regularization/</id>
    <published>2024-12-30T16:50:04.000Z</published>
    <updated>2024-12-30T18:18:52.803Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Quick-View">Quick View</h2><p><strong>Video Tutorial</strong>:</p><ul><li><a href="https://www.youtube.com/watch?v=Q81RR3yKn30">StatQuest with Josh Starmer: Regularization Part 1: Ridge (L2) Regression</a></li><li><a href="https://www.youtube.com/watch?v=NGf0voTMlcs">StatQuest with Josh Starmer: Regularization Part 2: Lasso (L1) Regression</a></li><li><a href="https://www.youtube.com/watch?v=1dKRdX9bfIo">StatQuest with Josh Starmer: Regularization Part 3: Elastic Net Regression</a></li></ul><h3 id="What-is-Regularization">What is Regularization?</h3><p><strong>Regularization</strong> is a technique used in machine learning and regression to prevent <strong>overfitting</strong> by adding a penalty to the loss function. The penalty discourages overly complex models and large coefficients, helping the model generalize better to unseen data.</p><h3 id="Why-Do-We-Need-Regularization">Why Do We Need Regularization?</h3><ol><li><p><strong>Overfitting</strong>:</p><ul><li>When a model becomes too complex, it memorizes the training data, leading to poor performance on test data.</li><li>Example: In polynomial regression, high-degree polynomials might perfectly fit the training data but fail to generalize.</li></ul></li><li><p><strong>Ill-Conditioned Data</strong>:</p><ul><li>When predictors are highly correlated or there are many predictors relative to observations, the regression model can become unstable.</li></ul></li><li><p><strong>Bias-Variance Tradeoff</strong>:</p><ul><li>Regularization introduces some bias but reduces variance, improving the model’s robustness.</li></ul></li></ol><h3 id="Types-of-Regularization-Why-Ridge-Lasso-and-Elastic-Net">Types of Regularization: Why Ridge, Lasso, and Elastic Net?</h3><p>These are three popular regularization methods used for linear regression:</p><h4 id="1-Ridge-Regression-L2-Regularization">1. <strong>Ridge Regression (L2 Regularization)</strong>:</h4><ul><li><p><strong>Penalty</strong>: Adds the squared magnitude of coefficients to the loss function.<br>$$<br>\text{Loss Function: } \sum_{i=1}^n (y_i - \hat{y}_ i)^2 + \lambda \sum_ {j=1}^p \beta_ j^2<br>$$</p><ul><li>$ \lambda $: Regularization parameter (controls penalty strength).</li><li>$ \beta_j $: Coefficients of predictors.</li></ul></li><li><p><strong>Effect</strong>:</p><ul><li>Shrinks coefficients towards zero, but never makes them exactly zero.</li><li>Reduces the impact of less important predictors without removing them entirely.</li></ul></li><li><p><strong>Use Case</strong>:</p><ul><li>Works well when many predictors are correlated.</li></ul></li></ul><h4 id="2-Lasso-Regression-L1-Regularization">2. <strong>Lasso Regression (L1 Regularization)</strong>:</h4><ul><li><p><strong>Penalty</strong>: Adds the absolute value of coefficients to the loss function.<br>$$<br>\text{Loss Function: } \sum_ {i=1}^n (y_ i - \hat{y}_ i)^2 + \lambda \sum_ {j=1}^p |\beta_ j|<br>$$</p></li><li><p><strong>Effect</strong>:</p><ul><li>Can shrink some coefficients to exactly zero, effectively performing <strong>feature selection</strong>.</li><li>Helps in creating sparse models by keeping only the most relevant predictors.</li></ul></li><li><p><strong>Use Case</strong>:</p><ul><li>Useful when you expect only a subset of predictors to be important.</li></ul></li></ul><h4 id="3-Elastic-Net-Regression">3. <strong>Elastic Net Regression</strong>:</h4><ul><li><p><strong>Penalty</strong>: Combines both L1 (lasso) and L2 (ridge) penalties.<br>$$<br>\text{Loss Function: } \sum_{i=1}^n (y_ i - \hat{y}_ i)^2 + \lambda_ 1 \sum_{j=1}^p |\beta_ j| + \lambda_ 2 \sum_{j=1}^p \beta_ j^2<br>$$</p></li><li><p><strong>Effect</strong>:</p><ul><li>Balances the strengths of Ridge and Lasso regression.</li><li>Retains the ability to perform feature selection (like Lasso) while handling multicollinearity (like Ridge).</li></ul></li><li><p><strong>Use Case</strong>:</p><ul><li>Best when there are many predictors and some are correlated, but feature selection is also desired.</li></ul></li></ul><h3 id="Comparison-of-Regularization-Methods">Comparison of Regularization Methods:</h3><table><thead><tr><th><strong>Method</strong></th><th><strong>Penalty</strong></th><th><strong>Effect on Coefficients</strong></th><th><strong>Use Case</strong></th></tr></thead><tbody><tr><td><strong>Ridge</strong></td><td>$ \beta_j^2 $</td><td>Shrinks coefficients, no zeros.</td><td>Multicollinearity or many predictors.</td></tr><tr><td><strong>Lasso</strong></td><td>$|\beta_j|$</td><td>Shrinks coefficients to zero.</td><td>Feature selection with fewer predictors.</td></tr><tr><td><strong>Elastic Net</strong></td><td>$|\beta_j| + \beta_j^2 $</td><td>Combination of Ridge and Lasso.</td><td>Multicollinearity with feature selection.</td></tr></tbody></table><h3 id="Why-Are-They-Discussed-Together">Why Are They Discussed Together?</h3><ul><li>All three are <strong>extensions of linear regression</strong>.</li><li>They <strong>regularize the model</strong> to prevent overfitting, but they differ in the type of penalty they impose on the coefficients.</li></ul><h2 id="Ridge-Regression">Ridge Regression</h2><h3 id="Ridge-Regression-Loss-Function">Ridge Regression Loss Function</h3><p>The Ridge regression modifies the Ordinary Least Squares (OLS) cost function by adding a penalty (regularization term) to the sum of squared coefficients:</p><p>$$<br>\text{Loss} = \sum_ {i=1}^n \left( y_ i - \hat{y}_ i \right)^2 + \lambda \sum_{j=1}^p \beta_ j^2<br>$$</p><p>Where:</p><ul><li>$ y_i $: Observed target value.</li><li>$ \hat{y}_i $: Predicted value ($ \hat{y}_i = X_i \cdot \beta $).</li><li>$ \beta_j $: Coefficients of the regression model.</li><li>$ \lambda $: Regularization parameter (also called penalty parameter).</li></ul><h3 id="Ridge-Coefficient-Solution">Ridge Coefficient Solution</h3><p>The Ridge regression coefficients are obtained by solving the following optimization problem:</p><p>$$<br>\min_{\beta} \{ \|y - X\beta\|^2 + \lambda \|\beta\|^2  \}<br>$$</p><ol><li><p><strong>Matrix Form</strong>:</p><ul><li>Rewrite the problem in matrix notation:<br>$$<br>\min_{\beta} \{ (y - X\beta)^T (y - X\beta) + \lambda \beta^T \beta \}<br>$$</li></ul></li><li><p><strong>Solution for $ \beta $</strong>:</p><ul><li>Differentiating the loss function with respect to $ \beta $, we get:<br>$$<br>\beta = \left( X^T X + \lambda I \right)^{-1} X^T y<br>$$</li><li>Here:<ul><li>$ X^T X $: Correlation matrix of predictors.</li><li>$ \lambda I $: Regularization term, where $ I $ is the identity matrix.</li><li>$ \lambda $: Controls the trade-off between minimizing the squared error and penalizing large coefficients.</li></ul></li></ul></li></ol><h3 id="Why-Add-lambda-I">Why Add $ \lambda I $?</h3><ul><li>Inverse of $ X^T X $ might not exist if the predictors are highly correlated or there are fewer observations than predictors (multicollinearity).</li><li>Adding $ \lambda I $ ensures that $ X^T X + \lambda I $ is always invertible.</li></ul><h3 id="Finding-the-Optimal-lambda">Finding the Optimal $ \lambda $</h3><ol><li><p><strong>Grid Search with Cross-Validation</strong>:</p><ul><li>Evaluate the model’s performance (e.g., Mean Squared Error) for different values of $ \lambda $.</li><li>Use k-fold cross-validation to select the $ \lambda $ that minimizes validation error.</li></ul></li><li><p><strong>Mathematical Insight</strong>:</p><ul><li>When $ \lambda = 0 $: Ridge reduces to Ordinary Least Squares (OLS).</li><li>As $ \lambda \to \infty $: Coefficients $ \beta \to 0 $ (model becomes very simple).</li></ul></li><li><p><strong>Validation-Based Optimization</strong>:</p><ul><li>Define a range of $ \lambda $ values (e.g., $ \lambda = [0.001, 0.01, 0.1, 1, 10, 100] $).</li><li>For each $ \lambda $, perform cross-validation and select the value with the lowest error.</li></ul></li></ol><h3 id="Example-Finding-lambda-with-Cross-Validation">Example: Finding $ \lambda $ with Cross-Validation</h3><p>Here’s Python code to find the optimal $ \lambda $ using grid search:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score<br><br><span class="hljs-comment"># Define a range of lambda (alpha) values</span><br>alphas = np.logspace(-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">50</span>)  <span class="hljs-comment"># Lambda values from 0.001 to 1000</span><br><span class="hljs-comment"># Compute cross-validated MSE and standard deviation for each alpha</span><br>mse_values = []<br>std_errors = []<br><br><span class="hljs-keyword">for</span> alpha <span class="hljs-keyword">in</span> alphas:<br>    ridge = Ridge(alpha=alpha)<br>    scores = -cross_val_score(ridge, X_train, y_train, scoring=<span class="hljs-string">&quot;neg_mean_squared_error&quot;</span>, cv=<span class="hljs-number">5</span>)<br>    mse_values.append(scores.mean())<br>    std_errors.append(scores.std())<br><br><span class="hljs-comment"># Plot the results with error bars</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.errorbar(alphas, mse_values, yerr=std_errors, fmt=<span class="hljs-string">&#x27;o&#x27;</span>, linestyle=<span class="hljs-string">&#x27;-&#x27;</span>, label=<span class="hljs-string">&#x27;Cross-validated MSE&#x27;</span>, capsize=<span class="hljs-number">3</span>)<br>plt.xscale(<span class="hljs-string">&#x27;log&#x27;</span>)  <span class="hljs-comment"># Log scale for alpha</span><br>plt.xlabel(<span class="hljs-string">&quot;Lambda (α)&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Mean Squared Error (MSE)&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;Finding Optimal Lambda (α) for Ridge Regression with Error Bars&quot;</span>)<br>plt.axvline(alphas[np.argmin(mse_values)], color=<span class="hljs-string">&#x27;red&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">f&quot;Optimal λ = <span class="hljs-subst">&#123;alphas[np.argmin(mse_values)]:<span class="hljs-number">.3</span>f&#125;</span>&quot;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br><br><span class="hljs-comment"># Output optimal lambda</span><br>optimal_lambda_error_bar = alphas[np.argmin(mse_values)]<br>optimal_lambda_error_bar<br></code></pre></td></tr></table></figure></div><pre>0.21209508879201902</pre><p><img src="https://imgur.com/ir98xgb.png" alt=""></p><h3 id="Key-Insights">Key Insights</h3><ol><li><p><strong>Ridge Regression Purpose</strong>:</p><ul><li>Penalizes large coefficients to reduce model complexity and improve generalization.</li></ul></li><li><p><strong>Finding $ \lambda $</strong>:</p><ul><li>Perform grid search with cross-validation to select $ \lambda $ that minimizes validation error.</li></ul></li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Simulate noisier data</span><br>np.random.seed(<span class="hljs-number">42</span>)<br>n_samples = <span class="hljs-number">100</span><br>X = np.random.rand(n_samples, <span class="hljs-number">2</span>)  <span class="hljs-comment"># Two predictors</span><br>X[:, <span class="hljs-number">1</span>] = X[:, <span class="hljs-number">0</span>] + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, size=n_samples)  <span class="hljs-comment"># Add stronger multicollinearity</span><br>y = <span class="hljs-number">4</span> * X[:, <span class="hljs-number">0</span>] + <span class="hljs-number">3</span> * X[:, <span class="hljs-number">1</span>] + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, size=n_samples)  <span class="hljs-comment"># More noise in the data</span><br><br><span class="hljs-comment"># Randomly sample 20% for training</span><br>train_indices = np.random.choice(<span class="hljs-built_in">range</span>(n_samples), size=<span class="hljs-built_in">int</span>(<span class="hljs-number">0.2</span> * n_samples), replace=<span class="hljs-literal">False</span>)<br>test_indices = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples) <span class="hljs-keyword">if</span> i <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> train_indices]<br><br>X_train, y_train = X[train_indices], y[train_indices]<br>X_test, y_test = X[test_indices], y[test_indices]<br><br><span class="hljs-comment"># Ordinary Least Squares Regression</span><br>ols_model = LinearRegression()<br>ols_model.fit(X_train, y_train)<br>y_pred_ols = ols_model.predict(X_test)<br><br><span class="hljs-comment"># Ridge Regression</span><br>ridge_model = Ridge(alpha=<span class="hljs-number">1.0</span>)  <span class="hljs-comment"># Alpha is equivalent to λ</span><br>ridge_model.fit(X_train, y_train)<br>y_pred_ridge = ridge_model.predict(X_test)<br><br><span class="hljs-comment"># Evaluate models</span><br>mse_ols = mean_squared_error(y_test, y_pred_ols)<br>mse_ridge = mean_squared_error(y_test, y_pred_ridge)<br><br><span class="hljs-comment"># Return updated MSE and coefficients</span><br>mse_results_updated = &#123;<br>    <span class="hljs-string">&quot;Mean Squared Error (OLS)&quot;</span>: mse_ols,<br>    <span class="hljs-string">&quot;Mean Squared Error (Ridge)&quot;</span>: mse_ridge,<br>    <span class="hljs-string">&quot;OLS Coefficients&quot;</span>: ols_model.coef_,<br>    <span class="hljs-string">&quot;Ridge Coefficients&quot;</span>: ridge_model.coef_,<br>&#125;<br><br>mse_results_updated<br><br><br><span class="hljs-comment"># Correct the regression line plotting using predicted results</span><br><br><span class="hljs-comment"># Generate predictions for the entire feature range for consistent straight lines</span><br>X_range = np.linspace(X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>(), X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>(), <span class="hljs-number">100</span>).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>X_range_full = np.hstack([X_range, X_range + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, size=X_range.shape)])<br><br><span class="hljs-comment"># Predict the regression lines for OLS and Ridge models</span><br>y_ols_line = ols_model.predict(X_range_full)<br>y_ridge_line = ridge_model.predict(X_range_full)<br><br><span class="hljs-comment"># Plot regression results</span><br>plt.figure(figsize=(<span class="hljs-number">14</span>, <span class="hljs-number">6</span>))<br><br><span class="hljs-comment"># OLS Regression</span><br>plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>plt.scatter(X_test[:, <span class="hljs-number">0</span>], y_test, color=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&quot;Test Set&quot;</span>, alpha=<span class="hljs-number">0.7</span>)<br>plt.scatter(X_train[:, <span class="hljs-number">0</span>], y_train, color=<span class="hljs-string">&#x27;orange&#x27;</span>, label=<span class="hljs-string">&quot;Training Set&quot;</span>, alpha=<span class="hljs-number">0.7</span>)<br>plt.plot(X_range[:, <span class="hljs-number">0</span>], y_ols_line, color=<span class="hljs-string">&#x27;red&#x27;</span>, label=<span class="hljs-string">&quot;OLS Regression Line&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;OLS Regression&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Feature 1&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Target (y)&quot;</span>)<br>plt.legend()<br><br><span class="hljs-comment"># Ridge Regression</span><br>plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>plt.scatter(X_test[:, <span class="hljs-number">0</span>], y_test, color=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&quot;Test Set&quot;</span>, alpha=<span class="hljs-number">0.7</span>)<br>plt.scatter(X_train[:, <span class="hljs-number">0</span>], y_train, color=<span class="hljs-string">&#x27;orange&#x27;</span>, label=<span class="hljs-string">&quot;Training Set&quot;</span>, alpha=<span class="hljs-number">0.7</span>)<br>plt.plot(X_range[:, <span class="hljs-number">0</span>], y_ridge_line, color=<span class="hljs-string">&#x27;purple&#x27;</span>, label=<span class="hljs-string">&quot;Ridge Regression Line&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;Ridge Regression&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Feature 1&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Target (y)&quot;</span>)<br>plt.legend()<br><br>plt.tight_layout()<br>plt.show()<br><br><span class="hljs-comment"># Plot feature contributions (coefficients) as bar plots</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>x_labels = [<span class="hljs-string">&#x27;Feature 1&#x27;</span>, <span class="hljs-string">&#x27;Feature 2&#x27;</span>]<br><br><span class="hljs-comment"># OLS Coefficients</span><br>plt.bar(x_labels, ols_model.coef_, label=<span class="hljs-string">&#x27;OLS Coefficients&#x27;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br><br><span class="hljs-comment"># Ridge Coefficients</span><br>plt.bar(x_labels, ridge_model.coef_, label=<span class="hljs-string">&#x27;Ridge Coefficients&#x27;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>)<br><br><span class="hljs-comment"># Add title and labels</span><br>plt.title(<span class="hljs-string">&quot;Comparison of Feature Contributions (OLS vs Ridge)&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Coefficient Value&quot;</span>)<br>plt.legend()<br>plt.show()<br></code></pre></td></tr></table></figure></div><pre>{'Mean Squared Error (OLS)': 3.747535481239866, 'Mean Squared Error (Ridge)': 3.7344119726941427, 'OLS Coefficients': array([4.0641917 , 3.31246222]), 'Ridge Coefficients': array([2.93270253, 2.91932805])}</pre><p><img src="https://imgur.com/tUDhjah.png" alt=""><br><img src="https://imgur.com/dJzQZyh.png" alt=""></p><p>In this specific example, ridge regression slight reduced the mean squared error by reducing the contribution of <strong>feature 1</strong>. Contribution of the <strong>feature 1</strong> and <strong>feature 2</strong> are almost the same (Blue color in barplot). The linear regression plot was updated by removing the effects of <strong>feature 2</strong>.</p><h2 id="Compare-3-Methods">Compare 3 Methods</h2><p>Code continue from above</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> ElasticNet<br><br><span class="hljs-comment"># Perform Elastic Net Regression</span><br>elastic_net_model = ElasticNet(alpha=<span class="hljs-number">0.1</span>, l1_ratio=<span class="hljs-number">0.5</span>)  <span class="hljs-comment"># Alpha controls regularization strength, l1_ratio balances Lasso and Ridge</span><br>elastic_net_model.fit(X_train, y_train)<br>y_pred_elastic_net = elastic_net_model.predict(X_test)<br><br><span class="hljs-comment"># Evaluate Elastic Net model</span><br>mse_elastic_net = mean_squared_error(y_test, y_pred_elastic_net)<br><br><span class="hljs-comment"># Plot feature contributions (coefficients) for Elastic Net</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.bar(x_labels, ols_model.coef_, label=<span class="hljs-string">&quot;OLS Coefficients&quot;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&quot;red&quot;</span>)<br>plt.bar(x_labels, ridge_model.coef_, label=<span class="hljs-string">&quot;Ridge Coefficients&quot;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&quot;blue&quot;</span>)<br>plt.bar(x_labels, lasso_model.coef_, label=<span class="hljs-string">&quot;Lasso Coefficients&quot;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&quot;green&quot;</span>)<br>plt.bar(x_labels, elastic_net_model.coef_, label=<span class="hljs-string">&quot;Elastic Net Coefficients&quot;</span>, alpha=<span class="hljs-number">0.7</span>, color=<span class="hljs-string">&quot;purple&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;Comparison of Feature Contributions (OLS, Ridge, Lasso, Elastic Net)&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Coefficient Value&quot;</span>)<br>plt.legend()<br>plt.show()<br><br><span class="hljs-comment"># Return MSE and coefficients for Elastic Net</span><br>mse_results_elastic_net = &#123;<br>    <span class="hljs-string">&quot;Mean Squared Error (OLS)&quot;</span>: mse_ols,<br>    <span class="hljs-string">&quot;Mean Squared Error (Ridge)&quot;</span>: mse_ridge,<br>    <span class="hljs-string">&quot;Mean Squared Error (Lasso)&quot;</span>: mse_lasso,<br>    <span class="hljs-string">&quot;Mean Squared Error (Elastic Net)&quot;</span>: mse_elastic_net,<br>    <span class="hljs-string">&quot;OLS Coefficients&quot;</span>: ols_model.coef_,<br>    <span class="hljs-string">&quot;Ridge Coefficients&quot;</span>: ridge_model.coef_,<br>    <span class="hljs-string">&quot;Lasso Coefficients&quot;</span>: lasso_model.coef_,<br>    <span class="hljs-string">&quot;Elastic Net Coefficients&quot;</span>: elastic_net_model.coef_,<br>&#125;<br><br>mse_results_elastic_net<br></code></pre></td></tr></table></figure></div><pre>{'Mean Squared Error (OLS)': 3.747535481239866, 'Mean Squared Error (Ridge)': 3.7344119726941427, 'Mean Squared Error (Lasso)': 3.681549054209485, 'Mean Squared Error (Elastic Net)': 3.810867636824328, 'OLS Coefficients': array([4.0641917 , 3.31246222]), 'Ridge Coefficients': array([2.93270253, 2.91932805]), 'Lasso Coefficients': array([3.3579283 , 2.97769008]), 'Elastic Net Coefficients': array([2.72149455, 2.71825096])}</pre><p><img src="https://imgur.com/ExhrW4P.png" alt=""></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Regularization is a way to make sure our model doesn&#39;t become too complicated. It ensures the model doesn’t overfit the training data while still making good predictions on new data. Think of it as adding a &#39;&lt;b&gt;rule&lt;/b&gt;&#39; or &#39;&lt;b&gt;constraint&lt;/b&gt;&#39; that prevents the model from relying too much on any specific feature or predictor.</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>pyrosetta</title>
    <link href="https://karobben.github.io/2024/12/20/Bioinfor/pyrosetta/"/>
    <id>https://karobben.github.io/2024/12/20/Bioinfor/pyrosetta/</id>
    <published>2024-12-21T00:06:54.000Z</published>
    <updated>2024-12-23T21:35:44.191Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Loop-Regenerate-Codes-From-ChatGPT">Loop Regenerate (Codes From ChatGPT)</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> init, Pose, get_fa_scorefxn<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.loops <span class="hljs-keyword">import</span> Loops, Loop<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.loops.loop_mover.perturb <span class="hljs-keyword">import</span> LoopMover_Perturb_KIC<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.loops.loop_mover.refine <span class="hljs-keyword">import</span> LoopMover_Refine_KIC, LoopMover_Refine_CCD<br><br><span class="hljs-comment">#from pyrosetta.rosetta.core.import_pose import pose_from_pdbstring as pose_from_pdb</span><br><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span>  pose_from_pdb<br><br><span class="hljs-comment"># 1. Initialize PyRosetta</span><br>init()<br><br><span class="hljs-comment"># 2. Load Your Protein Pose</span><br>pose = pose_from_pdb( <span class="hljs-string">&quot;test.pdb&quot;</span>)<br><br>n_chains = pose.num_chains()<br><span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>    start_res = pose.chain_begin(chain_index)<br>    end_res = pose.chain_end(chain_index)<br>    print(<span class="hljs-string">f&quot;Chain <span class="hljs-subst">&#123;chain_index&#125;</span>: residues <span class="hljs-subst">&#123;start_res&#125;</span> to <span class="hljs-subst">&#123;end_res&#125;</span>&quot;</span>)<br><br>loops_by_chain = &#123;&#125;<br><br><span class="hljs-comment"># Iterate over chains</span><br>n_chains = pose.num_chains()<br><span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>    start_res = pose.chain_begin(chain_index)<br>    end_res = pose.chain_end(chain_index)<br><br>    <span class="hljs-comment"># Extract secondary structure substring for this chain</span><br>    chain_secstruct = secstruct[start_res-<span class="hljs-number">1</span>:end_res]<br><br>    loop_regions = []<br>    current_loop_start = <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># Identify loop regions as stretches of &#x27;L&#x27;</span><br>    <span class="hljs-keyword">for</span> i, s <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(chain_secstruct, start=start_res):<br>        <span class="hljs-keyword">if</span> s == <span class="hljs-string">&#x27;L&#x27;</span>:<br>            <span class="hljs-keyword">if</span> current_loop_start <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                current_loop_start = i<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> current_loop_start <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                loop_regions.append((current_loop_start, i-<span class="hljs-number">1</span>))<br>                current_loop_start = <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># Check if a loop extends to the end of the chain</span><br>    <span class="hljs-keyword">if</span> current_loop_start <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        loop_regions.append((current_loop_start, end_res))<br><br>    <span class="hljs-comment"># Extract sequences for each loop region</span><br>    <span class="hljs-comment"># Store them in a dictionary keyed by chain index</span><br>    chain_loops = []<br>    <span class="hljs-keyword">for</span> (loop_start, loop_end) <span class="hljs-keyword">in</span> loop_regions:<br>        <span class="hljs-comment"># Extract the sequence of the loop</span><br>        loop_seq = <span class="hljs-string">&quot;&quot;</span>.join([pose.residue(r).name1() <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(loop_start, loop_end+<span class="hljs-number">1</span>)])<br>        chain_loops.append(&#123;<br>            <span class="hljs-string">&quot;start&quot;</span>: loop_start,<br>            <span class="hljs-string">&quot;end&quot;</span>: loop_end,<br>            <span class="hljs-string">&quot;sequence&quot;</span>: loop_seq<br>        &#125;)<br><br>    loops_by_chain[chain_index] = chain_loops<br><br>n_chains = pose.num_chains()<br><span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>    start_res = pose.chain_begin(chain_index)<br>    end_res = pose.chain_end(chain_index)<br>    print(<span class="hljs-string">f&quot;Chain <span class="hljs-subst">&#123;chain_index&#125;</span>: residues <span class="hljs-subst">&#123;start_res&#125;</span> to <span class="hljs-subst">&#123;end_res&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 3. Define the Loop(s) You Want to Remodel</span><br><span class="hljs-comment"># Suppose you want to remodel the loop from residues 45 to 55.</span><br><span class="hljs-comment"># Choose a cut point (ideally inside the loop), typically near the middle.</span><br>loop_start = <span class="hljs-number">593</span><br>loop_end = <span class="hljs-number">608</span><br>cutpoint = <span class="hljs-number">601</span><br><br>loops = Loops()<br>loops.add_loop( Loop(loop_start, loop_end, cutpoint) )<br><br><span class="hljs-comment"># 4. Set Up a Scorefunction</span><br>scorefxn = get_fa_scorefxn()<br><br><span class="hljs-comment"># 5. Set Up the Loop Remodeling Protocol</span><br><span class="hljs-comment"># You have multiple options: </span><br><span class="hljs-comment"># Example: Use KIC Perturb and then Refine</span><br>perturb_mover = LoopMover_Perturb_KIC(loops)<br>perturb_mover.set_scorefxn(scorefxn)<br><br>refine_mover = LoopMover_Refine_KIC(loops)<br>refine_mover.set_scorefxn(scorefxn)<br><br><span class="hljs-comment"># Alternatively, you might use CCD refinement:</span><br><span class="hljs-comment"># refine_mover = LoopMover_Refine_CCD(loops)</span><br><span class="hljs-comment"># refine_mover.set_scorefxn(scorefxn)</span><br><br><span class="hljs-comment"># 6. Optionally: Set Up Monte Carlo or Repeats</span><br><span class="hljs-comment"># Often you do multiple trials and pick the best model.</span><br><br><span class="hljs-comment"># 7. Apply the Movers</span><br><span class="hljs-comment"># First do perturbation</span><br>perturb_mover.apply(pose)<br><br><span class="hljs-comment"># Then refine</span><br>refine_mover.apply(pose)<br><br><span class="hljs-comment"># After this, you should have a remodeled loop region.</span><br><span class="hljs-comment"># You can save the resulting structure to a PDB file:</span><br>pose.dump_pdb(<span class="hljs-string">&quot;remodeled_loop.pdb&quot;</span>)<br></code></pre></td></tr></table></figure></div><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/eJG1S0x.png" alt="Raw loop"></td><td style="text-align:left">Raw loop</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/Lu1BdXo.png" alt="Predicted loop"></td><td style="text-align:left">Predicted loop by ussing ImmuneBuilder. The Predicted results has some trouble in the CDRH3 region. And if we place it in the corrected position and it has crush.</td></tr><tr><td style="text-align:center"><img src="https://imgur.com/l41Fw8X.png" alt="Reconstructed loop"></td><td style="text-align:left">Rosetta reconstructed loop by using the code above. Rosetta takes lots of time to reconstruct the loop and the result is terrible. The loop inseted into a very wired and unlikly position</td></tr></tbody></table><h2 id="Loop-Regenerate-Codes-From-Tutorial">Loop Regenerate (Codes From Tutorial)</h2><p><img src="https://imgur.com/1Op8NKe.png" alt=""></p><p>In the Tutorial 9.01, it use 2 structure: 1) the complete structure and 2) the structure has gap. The missing parts is range from 29~31. It not only deleted 5 residues, but also split it into 2 chains.</p><p>Because it was in a separate chain, the index 28 and 29 is the C terminal and N terminal in the chain with gap. The selected residues is 28 and 32 when they are in the original structure</p><h3 id="How-it-works-in-antibody-CDRH3">How it works in antibody CDRH3</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Notebook setup</span><br><span class="hljs-keyword">import</span> pyrosettacolabsetup; pyrosettacolabsetup.install_pyrosetta()<br><span class="hljs-keyword">import</span> pyrosetta; pyrosetta.init()<br><br><span class="hljs-comment"># py3Dmol setup (if there&#x27;s an error, make sure you have &#x27;py3Dmol&#x27; and &#x27;ipywidgets&#x27; pip installed)</span><br><span class="hljs-keyword">import</span> glob<br><span class="hljs-keyword">import</span> logging<br>logging.basicConfig(level=logging.INFO)<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pyrosetta.distributed<br><span class="hljs-keyword">import</span> pyrosetta.distributed.io <span class="hljs-keyword">as</span> io<br><span class="hljs-keyword">import</span> pyrosetta.distributed.viewer <span class="hljs-keyword">as</span> viewer<br><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> pose_from_pdb<br><br>input_pose = pose_from_pdb(<span class="hljs-string">&#x27;/mnt/Data/PopOS/Data_Ana/Wu/PigAntiBodies/AB_regine/data/14-1_ImmuneCorrect_partial.pdb&#x27;</span>)<br>input_pose_no_loop = pose_from_pdb(<span class="hljs-string">&#x27;/mnt/Data/PopOS/Data_Ana/Wu/PigAntiBodies/AB_regine/data/14-1_ImmuneCorrect_partial_Noloop.pdb&#x27;</span>)<br><br><br>helix_selector = pyrosetta.rosetta.core.select.residue_selector.SecondaryStructureSelector(<span class="hljs-string">&quot;H&quot;</span>)<br>loop_selector = pyrosetta.rosetta.core.select.residue_selector.SecondaryStructureSelector(<span class="hljs-string">&quot;L&quot;</span>)<br><br>modules = [<br>    viewer.setBackgroundColor(color=<span class="hljs-string">&quot;black&quot;</span>),<br>    viewer.setStyle(residue_selector=helix_selector, cartoon_color=<span class="hljs-string">&quot;blue&quot;</span>, label=<span class="hljs-literal">False</span>, radius=<span class="hljs-number">0</span>),<br>    viewer.setStyle(residue_selector=loop_selector, cartoon_color=<span class="hljs-string">&quot;yellow&quot;</span>, label=<span class="hljs-literal">False</span>, radius=<span class="hljs-number">0</span>),<br>    viewer.setZoomTo(residue_selector=loop_selector)<br>]<br><br><span class="hljs-comment">#view = viewer.init(input_pose, window_size=(800, 600), modules=modules).show()</span><br><span class="hljs-comment">#view = viewer.init(input_pose_no_loop, window_size=(800, 600), modules=modules).show()</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Chain_num</span>(<span class="hljs-params">pose</span>):</span><br>    n_chains = pose.num_chains()<br>    <span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>        start_res = pose.chain_begin(chain_index)<br>        end_res = pose.chain_end(chain_index)<br>        print(<span class="hljs-string">f&quot;Chain <span class="hljs-subst">&#123;chain_index&#125;</span>: residues <span class="hljs-subst">&#123;start_res&#125;</span> to <span class="hljs-subst">&#123;end_res&#125;</span>&quot;</span>)<br><br>Chain_num(input_pose)<br>Chain_num(input_pose_no_loop)<br><br>Start = input_pose_no_loop.chain_end(<span class="hljs-number">1</span>)<br>Len = input_pose.chain_end(<span class="hljs-number">1</span>) - input_pose_no_loop.chain_end(<span class="hljs-number">2</span>)<br>End = Start + Len<br>Miss_Seq = <span class="hljs-string">&quot;&quot;</span>.join([input_pose.residue(i).name1() <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Start, End)])<br><br><span class="hljs-comment">##The c terminus of one helix</span><br>print(input_pose_no_loop.residue(Start).name())<br><span class="hljs-comment">#The N terminus of the other helix</span><br>print(input_pose_no_loop.residue(Start+<span class="hljs-number">1</span>).name())<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mutate_position</span>(<span class="hljs-params">pose,position,mutate</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;A simple function to mutate an amino acid given a pose number&#x27;&#x27;&#x27;</span><br>    mr = pyrosetta.rosetta.protocols.simple_moves.MutateResidue()<br>    mr.set_target(position)<br>    mr.set_res_name(mutate)<br>    mr.apply(pose)<br><br><br><span class="hljs-comment">##Mutate both 28 and 29 to ALA</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">In tutorial, they mutated the residues into ALA. It says is make a pose that can be applied by GenKic. I am not sure it is because of the GenKic algorithem or GenKic function. I possible could be that GenKic function doesn&#x27;t accept the resideus from either side do the termianl. So, we need to remove them and add them back. Because my goal is get a better conformation of the loop, I just relpace it with itself to do the lateral test.</span><br><span class="hljs-string">Also, I tried to use ALA at the begining, too. The folding resutls are not promissing.</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>Resi1 = input_pose_no_loop.residue(Start).name3()<br>Resi2 = input_pose_no_loop.residue(Start+<span class="hljs-number">1</span>).name3()<br><br>mutate_position(input_pose_no_loop,Start,Resi1)<br>mutate_position(input_pose_no_loop,Start+<span class="hljs-number">1</span>,Resi2)<br><span class="hljs-keyword">assert</span>(input_pose_no_loop.residue(Start).name() == Resi1)<br><span class="hljs-keyword">assert</span>(input_pose_no_loop.residue(Start+<span class="hljs-number">1</span>).name() == Resi2)<br><br><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> Pose<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">slice_pose</span>(<span class="hljs-params">p,start,end</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Take a pose object and return from start, end</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    sliced = Pose()<br>    <span class="hljs-keyword">if</span> end &gt; p.size() <span class="hljs-keyword">or</span> start &gt; p.size():<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;end/start slice is longer than total lenght of pose &#123;&#125; &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(start,end)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start,end+<span class="hljs-number">1</span>):<br>        sliced.append_residue_by_bond(p.residue(i))<br>    <span class="hljs-keyword">return</span> sliced<br><br><span class="hljs-comment">##Pose object 1 - helix_AB all the way up to residue 28</span><br>helix_ab_pose = slice_pose(input_pose_no_loop,<span class="hljs-number">1</span>,Start)<br><span class="hljs-comment">##Pose object 2 - helix C and the reaminder of the pose</span><br><span class="hljs-comment">#helix_c_pose = slice_pose(input_pose_no_loop,Start+1,input_pose_no_loop.size())</span><br>helix_c_pose = slice_pose(input_pose_no_loop,Start+<span class="hljs-number">1</span>,input_pose_no_loop.chain_end(<span class="hljs-number">2</span>))<br><br><span class="hljs-comment"># We&#x27;re just going to quicky add in pdb info so that our viewing commands work</span><br>add_pdb_info_mover = pyrosetta.rosetta.protocols.simple_moves.AddPDBInfoMover()<br>add_pdb_info_mover.apply(helix_ab_pose)<br>add_pdb_info_mover.apply(helix_c_pose)<br><span class="hljs-comment"># Here&#x27;s the second part</span><br><span class="hljs-comment">#view = viewer.init(helix_c_pose, window_size=(800, 600), modules=modules).show()</span><br><span class="hljs-comment"># Here&#x27;s the first object</span><br><span class="hljs-comment">#view = viewer.init(helix_ab_pose, window_size=(800, 600), modules=modules).show()</span><br><br><span class="hljs-comment"># Here&#x27;s the second object</span><br><span class="hljs-comment">#view = viewer.init(helix_c_pose, window_size=(800, 600), modules=modules).show()</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">crudely_connect_w_loop</span>(<span class="hljs-params">n_term_pose,c_term_pose,connect_with</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    The function will take two poses and join them with a loop</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Keep in mind this is just joined as far as the pose is concerned. The bond angles and lenghts will be sub-optimal</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    one_to_three = &#123;<br>    <span class="hljs-string">&#x27;A&#x27;</span>: <span class="hljs-string">&#x27;ALA&#x27;</span>,<br>    <span class="hljs-string">&#x27;C&#x27;</span>: <span class="hljs-string">&#x27;CYS&#x27;</span>,<br>    <span class="hljs-string">&#x27;D&#x27;</span>: <span class="hljs-string">&#x27;ASP&#x27;</span>,<br>    <span class="hljs-string">&#x27;E&#x27;</span>: <span class="hljs-string">&#x27;GLU&#x27;</span>,<br>    <span class="hljs-string">&#x27;F&#x27;</span>: <span class="hljs-string">&#x27;PHE&#x27;</span>,<br>    <span class="hljs-string">&#x27;G&#x27;</span>: <span class="hljs-string">&#x27;GLY&#x27;</span>,<br>    <span class="hljs-string">&#x27;H&#x27;</span>: <span class="hljs-string">&#x27;HIS&#x27;</span>,<br>    <span class="hljs-string">&#x27;I&#x27;</span>: <span class="hljs-string">&#x27;ILE&#x27;</span>,<br>    <span class="hljs-string">&#x27;K&#x27;</span>: <span class="hljs-string">&#x27;LYS&#x27;</span>,<br>    <span class="hljs-string">&#x27;L&#x27;</span>: <span class="hljs-string">&#x27;LEU&#x27;</span>,<br>    <span class="hljs-string">&#x27;M&#x27;</span>: <span class="hljs-string">&#x27;MET&#x27;</span>,<br>    <span class="hljs-string">&#x27;N&#x27;</span>: <span class="hljs-string">&#x27;ASN&#x27;</span>,<br>    <span class="hljs-string">&#x27;P&#x27;</span>: <span class="hljs-string">&#x27;PRO&#x27;</span>,<br>    <span class="hljs-string">&#x27;Q&#x27;</span>: <span class="hljs-string">&#x27;GLN&#x27;</span>,<br>    <span class="hljs-string">&#x27;R&#x27;</span>: <span class="hljs-string">&#x27;ARG&#x27;</span>,<br>    <span class="hljs-string">&#x27;S&#x27;</span>: <span class="hljs-string">&#x27;SER&#x27;</span>,<br>    <span class="hljs-string">&#x27;T&#x27;</span>: <span class="hljs-string">&#x27;THR&#x27;</span>,<br>    <span class="hljs-string">&#x27;V&#x27;</span>: <span class="hljs-string">&#x27;VAL&#x27;</span>,<br>    <span class="hljs-string">&#x27;Y&#x27;</span>: <span class="hljs-string">&#x27;TYR&#x27;</span>,<br>    <span class="hljs-string">&#x27;W&#x27;</span>: <span class="hljs-string">&#x27;TRP&#x27;</span>&#125;<br><br>    pose_a = Pose()<br>    pose_a.assign(n_term_pose)<br><br>    pose_b = Pose()<br>    pose_b.assign(c_term_pose)<br><br>    <span class="hljs-comment"># Setup CHEMICAL MANAGER TO MAKE NEW RESIDUES</span><br>    chm = pyrosetta.rosetta.core.chemical.ChemicalManager.get_instance()<br>    rts = chm.residue_type_set(<span class="hljs-string">&#x27;fa_standard&#x27;</span>)<br>    get_residue_object = <span class="hljs-keyword">lambda</span> x: pyrosetta.rosetta.core.conformation.ResidueFactory.create_residue(<br>        rts.name_map(x))<br><br>    <span class="hljs-comment"># Will keep track of indexing of rebuilt loop</span><br>    rebuilt_loop = []<br><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Iterate through string turning each letter into a residue object and then</span><br><span class="hljs-string">    appending it to the N term pose&#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">for</span> one_letter <span class="hljs-keyword">in</span> connect_with:<br>        resi = get_residue_object(one_to_three[one_letter])<br>        pose_a.append_residue_by_bond(resi, <span class="hljs-literal">True</span>)<br>        pose_a.set_omega(pose_a.total_residue(), <span class="hljs-number">180.</span>)<br>        rebuilt_loop.append(pose_a.total_residue())<br><br>    <span class="hljs-comment">##ADD the C term pose to the end of the loop we just appended</span><br>    <span class="hljs-keyword">for</span> residue_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, pose_b.total_residue()+<span class="hljs-number">1</span>):<br>        pose_a.append_residue_by_bond(<br>            pose_b.residue(residue_index))<br><br>    print(<span class="hljs-string">&quot;Joined NTerm and CTerm pose with loop &#123;&#125; at residues &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(connect_with,rebuilt_loop))<br>    <span class="hljs-keyword">return</span> pose_a<br><br><span class="hljs-comment">#Returns a pose that is connected, but sub-optimal geometry</span><br>gk_input_pose = crudely_connect_w_loop(helix_ab_pose,helix_c_pose,Miss_Seq)<br><span class="hljs-comment">#gk_input_pose = crudely_connect_w_loop(helix_ab_pose,helix_c_pose,Miss_Seq)</span><br><br>print(Miss_Seq)<br><span class="hljs-keyword">for</span> chain <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>,input_pose_no_loop.num_chains()+<span class="hljs-number">1</span>):<br>    print(chain)<br>    gk_input_pose.append_pose_by_jump(input_pose_no_loop.split_by_chain(chain), gk_input_pose.total_residue())<br><br>Chain_num(helix_ab_pose)<br>Chain_num(gk_input_pose)<br>Chain_num(input_pose)<br><br><br><br><span class="hljs-keyword">from</span> additional_scripts.GenKic <span class="hljs-keyword">import</span> GenKic<br><br><span class="hljs-comment">##All that GenKic needs is the loop residue list</span><br>loop_residues = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Start,End+<span class="hljs-number">2</span>)]<br>gk_object = GenKic(loop_residues)<br><br><span class="hljs-comment">##Let&#x27;s set the closure attempt to 500000</span><br>gk_object.set_closure_attempts(<span class="hljs-number">500000</span>)<br>gk_object.set_min_solutions(<span class="hljs-number">10</span>)<br><br><br><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> ScoreFunction<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_bb_only_sfxn</span>():</span><br>    scorefxn = ScoreFunction()<br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.fa_atr, <span class="hljs-number">1</span>)    <span class="hljs-comment"># full-atom attractive score</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.fa_rep, <span class="hljs-number">0.55</span>)    <span class="hljs-comment"># full-atom repulsive score</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.hbond_sr_bb, <span class="hljs-number">1</span>)    <span class="hljs-comment"># short-range hbonding</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.hbond_lr_bb, <span class="hljs-number">1</span>)    <span class="hljs-comment"># long-range hbonding</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.rama_prepro, <span class="hljs-number">0.45</span>)    <span class="hljs-comment"># ramachandran score</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.omega, <span class="hljs-number">0.4</span>)    <span class="hljs-comment"># omega torsion score</span><br>    scorefxn.set_weight(pyrosetta.rosetta.core.scoring.p_aa_pp, <span class="hljs-number">0.625</span>)<br>    <span class="hljs-keyword">return</span> scorefxn<br><br><span class="hljs-comment">##Grab BB Only SFXN</span><br>bb_only_sfxn = get_bb_only_sfxn()<br><br><span class="hljs-comment">##Pass it to GK</span><br>gk_object.set_scorefxn(bb_only_sfxn)<br><br>gk_object.set_selector_type(<span class="hljs-string">&#x27;lowest_energy_selector&#x27;</span>)<br><span class="hljs-comment">#First lets set alll mainchain omega values to 180 degrees in our loop. We don&#x27;t want to include residue after the last anchor residue as that could potentially not exist.</span><br><span class="hljs-keyword">for</span> res_num <span class="hljs-keyword">in</span> loop_residues[:-<span class="hljs-number">1</span>]:<br>    gk_object.set_dihedral(res_num, res_num + <span class="hljs-number">1</span>, <span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;N&quot;</span>, <span class="hljs-number">180.1</span>)<br><br><span class="hljs-comment">###Or there is a convienience function within the class that does the same thing</span><br>gk_object.set_omega_angles()<br><br><span class="hljs-keyword">for</span> res_num <span class="hljs-keyword">in</span> loop_residues:<br>    gk_object.randomize_backbone_by_rama_prepro(res_num)<br><span class="hljs-comment">##This will grab the GK instance and apply everything we have set to our pose</span><br>gk_object.get_instance().apply(gk_input_pose)<br><br><span class="hljs-comment">##You can see, we perturbed the loop, but we did not tell GK to close the bond</span><br><span class="hljs-comment">#view = viewer.init(gk_input_pose, window_size=(800, 600), modules=modules).show()</span><br><br>gk_object.close_normal_bond(End,End+<span class="hljs-number">1</span>) <span class="hljs-comment">#or gk_object.close_normal_bond(loop_residues[-2],loop_residues[-1])</span><br>gk_object.get_instance().apply(gk_input_pose)<br><br><span class="hljs-comment">#view = viewer.init(gk_input_pose, window_size=(800, 600), modules=modules).show()</span><br><br><span class="hljs-comment">##The first residue in our loop definition will be confiend to alpha-helical rama space</span><br>gk_object.set_filter_backbone_bin(loop_residues[<span class="hljs-number">0</span>],<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-built_in">bin</span>=<span class="hljs-string">&#x27;ABBA&#x27;</span>)<br><span class="hljs-comment">##The last residue in our loop definition will be confiend to alpha-helical rama space</span><br>gk_object.set_filter_backbone_bin(loop_residues[-<span class="hljs-number">1</span>],<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-built_in">bin</span>=<span class="hljs-string">&#x27;ABBA&#x27;</span>)<br><br>gk_object.set_filter_loop_bump_check()<br><br><br><span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> gk_object.pivot_residues:<br>    gk_object.set_filter_rama_prepro(r,cutoff=<span class="hljs-number">0.5</span>)<br><br><span class="hljs-comment">##Grab GK instance</span><br>gk_instance = gk_object.get_instance()<br><span class="hljs-comment">##apply it to the pose</span><br>gk_instance.apply(gk_input_pose)<br><span class="hljs-comment">##The Input pose with no loop, the reference pose we are trying to recreate and the GK pose</span><br>poses = [input_pose_no_loop, input_pose, gk_input_pose]<br><span class="hljs-comment"># view = viewer.init(poses) + viewer.setStyle()</span><br><span class="hljs-comment"># view()</span><br><br><span class="hljs-comment">#gk_input_pose.dump_pdb(&quot;final_pose.pdb&quot;)</span><br><br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.relax <span class="hljs-keyword">import</span> FastRelax<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.core.kinematics <span class="hljs-keyword">import</span> MoveMap<br><br><span class="hljs-keyword">from</span> pyrosetta.rosetta.protocols.relax <span class="hljs-keyword">import</span> FastRelax<br><br><br><br>gk_input_pose.dump_pdb(<span class="hljs-string">&quot;loop.pdb&quot;</span>)<br><br><span class="hljs-comment"># loop alone</span><br>relax = FastRelax()<br>relax.set_scorefxn(bb_only_sfxn)<br>movemap = MoveMap()<br><span class="hljs-comment">#for res in loop_residues:</span><br><span class="hljs-comment">#  movemap.set_bb(res, True)</span><br>relax.set_movemap(movemap)<br>relax.apply(gk_input_pose)<br><br><br>gk_input_pose.dump_pdb(<span class="hljs-string">&quot;loop_relaxe.pdb&quot;</span>)<br><br><span class="hljs-comment"># relax: Full</span><br>relax = FastRelax()<br>relax.set_scorefxn(bb_only_sfxn)<br>relax.apply(gk_input_pose)<br><br><br>gk_input_pose.dump_pdb(<span class="hljs-string">&quot;loop_full_relaxe.pdb&quot;</span>)<br></code></pre></td></tr></table></figure></div><p>In this script, I am not only test the loop reconstruction, but also add relaxation steps. Here is the results from different methods. It seems like no matter how you try, it is hard to reconstruct this loop in Rosetta.</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/iyjWPH6.png" alt="Reconstructed Loop"></td><td style="text-align:center"><img src="https://imgur.com/ZX450qV.png" alt="Loop-reconstruction and Relaxation"></td></tr><tr><td style="text-align:center"><img src="https://imgur.com/wTwR6P5.png" alt="Relaxation only"></td><td style="text-align:center"><img src="https://imgur.com/BRtUTD1.png" alt="Realxation and then loop rexonstruction"></td></tr></tbody></table><h2 id="How-to-check-the-Chain-and-the-number-of-residues">How to check the Chain and the number of residues</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> init, pose_from_pdb<br><br><span class="hljs-comment"># 1. Initialize PyRosetta</span><br>init()<br><span class="hljs-comment"># 2. Load Your Protein Pose</span><br>pose = pose_from_pdb( <span class="hljs-string">&quot;data/14-1_ImmuneCorrect.pdb&quot;</span>)<br><br><span class="hljs-comment"># 3. Count and print the result</span><br><br>n_chains = pose.num_chains()<br><span class="hljs-keyword">for</span> chain_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_chains+<span class="hljs-number">1</span>):<br>    start_res = pose.chain_begin(chain_index)<br>    end_res = pose.chain_end(chain_index)<br>    print(<span class="hljs-string">f&quot;Chain <span class="hljs-subst">&#123;chain_index&#125;</span>: residues <span class="hljs-subst">&#123;start_res&#125;</span> to <span class="hljs-subst">&#123;end_res&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure></div><pre>Chain 1: residues 1 to 322Chain 2: residues 323 to 493Chain 3: residues 494 to 620Chain 4: residues 621 to 729</pre><h2 id="Get-the-Second-Structure">Get the Second Structure</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pyrosetta <span class="hljs-keyword">import</span> init, pose_from_pdb<br><span class="hljs-keyword">from</span> pyrosetta.rosetta.core.scoring.dssp <span class="hljs-keyword">import</span> Dssp<br><br><span class="hljs-comment"># 1. Initialize PyRosetta</span><br>init()<br><span class="hljs-comment"># 2. Load Your Protein Pose</span><br>pose = pose_from_pdb( <span class="hljs-string">&quot;data/14-1_ImmuneCorrect.pdb&quot;</span>)<br><br><span class="hljs-comment"># Run DSSP to get secondary structure</span><br>dssp = Dssp(pose)<br>secstruct = dssp.get_dssp_reduced_IG_as_L_secstruct()<br></code></pre></td></tr></table></figure></div><pre>LLEEEEELELLLLLLEEEELLEEEEEELLEEELEELLLLLLEEEELLELLEELLLELHHHHHHLLLLLLLLLLLLLLLLEEELLLLLELLLLLLLELLHHHHHHHLLLELLLEEEELLLLLLLLLLEELLLLEHLHLLLLLLELLLLEEEEEELLLLLLLEEEEEELLLLLLEEEEEEEEELLLHHHHHHHHLLLLLLEEEEELLLEEEELLLLLLLLLLLLLLLEEEEEEEEELLLLEEEEEELLLEEEELEEEELLELLLLLEEELLLLEEEEEELEELLLLLEL...</pre><div class="admonition note"><p class="admonition-title">What does it mean?</p><ul><li>H: Alpha-Helix</li><li>E: Beta-Strand</li><li>L: Loop or Irregular Region</li></ul></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">pyrosetta</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Heatmap with GGplot</title>
    <link href="https://karobben.github.io/2024/12/15/R/ggplot-heatmap/"/>
    <id>https://karobben.github.io/2024/12/15/R/ggplot-heatmap/</id>
    <published>2024-12-15T06:23:10.000Z</published>
    <updated>2024-12-15T07:20:26.023Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Data-Ready">Data Ready</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r"><span class="hljs-comment"># Library</span><br>library(ggplot2)<br><br><span class="hljs-comment"># Dummy data</span><br>x &lt;- <span class="hljs-built_in">LETTERS</span>[<span class="hljs-number">1</span>:<span class="hljs-number">20</span>]<br>y &lt;- paste0(<span class="hljs-string">&quot;var&quot;</span>, seq(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>))<br>data &lt;- expand.grid(X=x, Y=y)<br>data$Z &lt;- runif(<span class="hljs-number">400</span>, <span class="hljs-number">0</span>, <span class="hljs-number">5</span>)<br> <br><span class="hljs-comment"># Heatmap </span><br>p &lt;- ggplot(data, aes(X, Y, fill= Z)) + <br>  geom_tile()<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/lxVWCkQ.png" alt="heatmap in ggplot"></p><pre>  X    Y         Z1 A var1 2.56291662 B var1 4.91312173 C var1 0.12522194 D var1 2.66059005 E var1 1.23435786 F var1 4.7347760</pre><h2 id="Cluster-column-and-rows">Cluster column and rows</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(stringr)<br><br>data2 &lt;- reshape(data, idvar=<span class="hljs-string">&#x27;X&#x27;</span>, timevar= <span class="hljs-string">&#x27;Y&#x27;</span>, direction= <span class="hljs-string">&#x27;wide&#x27;</span>)<br>row.names(data2) &lt;- data2$X<br>colnames(data2) &lt;- str_remove(colnames(data2), <span class="hljs-string">&#x27;Z.&#x27;</span>)<br>data2 &lt;- data2[-<span class="hljs-number">1</span>]<br><br>ClusLevel &lt;- <span class="hljs-keyword">function</span>(data2)&#123;<br>    tmp &lt;- hclust(dist(data2))<br>    <span class="hljs-built_in">return</span>(tmp$labels[tmp$order])<br>&#125;<br><br>data$X &lt;- factor(data$X, level = ClusLevel(data2))<br>data$Y &lt;- factor(data$Y, level = ClusLevel(t(data2)))<br><br>p &lt;- ggplot(data, aes(X, Y, fill= Z)) + <br>  geom_tile()<br><br><br></code></pre></td></tr></table></figure></div><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/4Lq2Nhf.png" alt="Heatmap"></th></tr></thead><tbody><tr><td style="text-align:center">The cluster resutls are not very clear. It is expectable because the generated dataset doesn’t has any kind of relationship at all</td></tr></tbody></table><h2 id="Classic-RdYlBu-Palette">Classic RdYlBu Palette</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r"><span class="hljs-comment">## Best color for heatmap</span><br>library(RColorBrewer)<br>colorRampPalette(rev(brewer.pal(n = 7,name = &quot;RdYlBu&quot;))) -&gt; cc<br><br>p + scale_fill_gradientn(colors=cc(<span class="hljs-number">100</span>))<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/PYhCSUc.png" alt=""></p><h2 id="Set-Limits-and-Change-Color">Set Limits and Change Color</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">Heatmape &lt;- <span class="hljs-keyword">function</span>(TB, x, y, fill, minpoint = <span class="hljs-literal">FALSE</span>, midpoint = <span class="hljs-literal">FALSE</span>, maxpoint = <span class="hljs-literal">FALSE</span>,<br>                    legend.name = <span class="hljs-built_in">expression</span>(paste(<span class="hljs-string">&quot;Δ&quot;</span>, <span class="hljs-built_in">log</span>[<span class="hljs-number">10</span>], <span class="hljs-string">&quot;(&quot;</span>, K[D], <span class="hljs-string">&quot;)&quot;</span>, sep = <span class="hljs-string">&#x27;&#x27;</span>)),<br>                    axis.title.x = <span class="hljs-string">&#x27;X&#x27;</span>,<br>                    axis.title.y = <span class="hljs-string">&#x27;Y&#x27;</span>,<br>                    colors = <span class="hljs-built_in">c</span>(<span class="hljs-string">&quot;Firebrick4&quot;</span>, <span class="hljs-string">&quot;white&quot;</span>, <span class="hljs-string">&quot;royalblue4&quot;</span>) <br>                    )&#123;<br>    <span class="hljs-keyword">if</span>(minpoint == <span class="hljs-literal">FALSE</span>)&#123;<br>        minpoint = <span class="hljs-built_in">min</span>(TB[[fill]], na.rm = <span class="hljs-literal">TRUE</span>)<br>    &#125;<br>    <span class="hljs-keyword">if</span>(midpoint == <span class="hljs-literal">FALSE</span>)&#123;<br>        midpoint = mean(<span class="hljs-built_in">c</span>(<span class="hljs-built_in">min</span>(TB[[fill]], na.rm = <span class="hljs-literal">TRUE</span>), <span class="hljs-built_in">min</span>(TB[[fill]], na.rm = <span class="hljs-literal">TRUE</span>)))<br>    &#125;<br>    <span class="hljs-keyword">if</span>(maxpoint == <span class="hljs-literal">FALSE</span>)&#123;<br>        maxpoint = <span class="hljs-built_in">max</span>(TB[[fill]], na.rm = <span class="hljs-literal">TRUE</span>)<br>    &#125;<br>    P &lt;- ggplot(TB, aes(TB[[x]], TB[[y]], fill = TB[[fill]])) + geom_tile() +<br>      scale_fill_gradientn(<br>          colors = colors, <br>          values = scales::rescale(<span class="hljs-built_in">c</span>(minpoint, midpoint, maxpoint)),  <span class="hljs-comment"># Set the key values</span><br>          oob = scales::squish,<br>          na.value = <span class="hljs-string">&quot;gray&quot;</span>,<br>          limit = <span class="hljs-built_in">c</span>(minpoint, maxpoint)) +<br>      labs(x = axis.title.x, y = axis.title.y, fill = legend.name) +<br>      theme_linedraw() + <br>      coord_trans(expand = <span class="hljs-number">0</span>) + <br>      theme(panel.background= element_rect (<span class="hljs-string">&#x27;gray&#x27;</span>,), panel.grid = element_blank())<br>    <span class="hljs-built_in">return</span>(P)<br>&#125;<br><br>Heatmape(data, <span class="hljs-string">&quot;X&quot;</span>, <span class="hljs-string">&quot;Y&quot;</span>, <span class="hljs-string">&quot;Z&quot;</span>) + ggtitle(<span class="hljs-string">&quot;With default scale&quot;</span>)<br>Heatmape(data, <span class="hljs-string">&quot;X&quot;</span>, <span class="hljs-string">&quot;Y&quot;</span>, <span class="hljs-string">&quot;Z&quot;</span>, minpoint=<span class="hljs-number">1.5</span>, midpoint =<span class="hljs-number">2</span>, maxpoint=<span class="hljs-number">2.5</span>) + ggtitle(<span class="hljs-string">&quot;given value range&quot;</span>)<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/kgMkccx.png" alt=""></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Heatmap with GGplot</summary>
    
    
    
    <category term="R" scheme="https://karobben.github.io/categories/R/"/>
    
    <category term="Plot" scheme="https://karobben.github.io/categories/R/Plot/"/>
    
    <category term="GGPLOT" scheme="https://karobben.github.io/categories/R/Plot/GGPLOT/"/>
    
    
    <category term="R" scheme="https://karobben.github.io/tags/R/"/>
    
    <category term="ggplot" scheme="https://karobben.github.io/tags/ggplot/"/>
    
  </entry>
  
  <entry>
    <title>GGplot: Prism style</title>
    <link href="https://karobben.github.io/2024/12/15/R/ggplot-prismStyle/"/>
    <id>https://karobben.github.io/2024/12/15/R/ggplot-prismStyle/</id>
    <published>2024-12-15T06:07:28.000Z</published>
    <updated>2024-12-19T00:47:27.075Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Barplot">Barplot</h2><h3 id="Data-Prepare">Data Prepare</h3><p>This code would use the inner data set <code>chickwts</code> as example to calculate the mean value and sd value to use as bar height and error bar</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(dplyr)<br><br>result &lt;- chickwts %&gt;%<br>  group_by(feed) %&gt;%<br>  summarise(mean = mean(weight), sd = sd(weight))<br></code></pre></td></tr></table></figure></div><p>Data <code>chickwts</code>:</p><pre>  weight      feed1    179 horsebean2    160 horsebean3    136 horsebean4    227 horsebean</pre><p>Data frame after converted:</p><pre># A tibble: 6 × 3  feed       mean    sd  <fct>     <dbl> <dbl>1 casein     324.  64.42 horsebean  160.  38.63 linseed    219.  52.24 meatmeal   277.  64.95 soybean    246.  54.16 sunflower  329.  48.8</pre><h3 id="Plot-the-Plot-and-Define-the-Theme">Plot the Plot and Define the Theme</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(ggplot2)<br><br>Prim_bar &lt;- <span class="hljs-keyword">function</span>(p)&#123;<br>  P &lt;- p + theme(panel.background = element_blank(),<br>          axis.line = element_line(size = <span class="hljs-number">1</span>),<br>          axis.ticks = element_line(colour = <span class="hljs-string">&quot;black&quot;</span>, size = <span class="hljs-number">1</span>),<br>          axis.ticks.length = unit(<span class="hljs-number">.25</span>, <span class="hljs-string">&#x27;cm&#x27;</span>),<br>          axis.text = element_text(size = <span class="hljs-number">15</span>),<br>          axis.text.x = element_text(angle = <span class="hljs-number">45</span>, vjust = <span class="hljs-number">1</span>, hjust = <span class="hljs-number">1</span>),<br>          axis.title = element_text(size = <span class="hljs-number">20</span>),<br>          plot.title = element_text(hjust = <span class="hljs-number">.5</span>, size = <span class="hljs-number">25</span>)) +<br>  scale_y_continuous(expand = <span class="hljs-built_in">c</span>(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0</span>)) +<br>  ggtitle(<span class="hljs-string">&#x27;plot&#x27;</span>)<br>  <span class="hljs-built_in">return</span>(P)<br>&#125;<br><br>p &lt;- ggplot(result, aes(feed, mean)) +<br>    geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = <span class="hljs-number">.3</span>, size = <span class="hljs-number">1</span>) +<br>geom_bar(stat = <span class="hljs-string">&#x27;identity&#x27;</span>, color = <span class="hljs-string">&#x27;black&#x27;</span>, size = <span class="hljs-number">1</span>, width = <span class="hljs-number">.6</span>, fill = <span class="hljs-string">&#x27;Gainsboro&#x27;</span>)<br><br>Prim_bar(p)<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/omoKjMU.png" alt="Apply the Prism Themes to ggplot"></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">GGplot: Prism style</summary>
    
    
    
    <category term="R" scheme="https://karobben.github.io/categories/R/"/>
    
    <category term="Plot" scheme="https://karobben.github.io/categories/R/Plot/"/>
    
    <category term="GGPLOT" scheme="https://karobben.github.io/categories/R/Plot/GGPLOT/"/>
    
    
    <category term="R" scheme="https://karobben.github.io/tags/R/"/>
    
    <category term="Plot" scheme="https://karobben.github.io/tags/Plot/"/>
    
    <category term="ggplot" scheme="https://karobben.github.io/tags/ggplot/"/>
    
  </entry>
  
  <entry>
    <title>OpenMM, Molecular Dynamic Simulation</title>
    <link href="https://karobben.github.io/2024/12/08/Bioinfor/openMM/"/>
    <id>https://karobben.github.io/2024/12/08/Bioinfor/openMM/</id>
    <published>2024-12-08T17:48:03.000Z</published>
    <updated>2024-12-19T02:29:45.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Install">Install</h2><p>More detailed installations: <a href="http://docs.openmm.org/latest/userguide/application/01_getting_started.html">OpenMM User Guide</a></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">conda create -n openmm python=3.9 -y<br>conda activate openmm<br>conda install -c conda-forge openmm<br></code></pre></td></tr></table></figure></div><p><strong>Test the installation</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">python -m openmm.testInstallation<br></code></pre></td></tr></table></figure></div><pre>OpenMM Version: 8.2Git Revision: 53770948682c40bd460b39830d4e0f0fd3a4b868There are 4 Platforms available:1 Reference - Successfully computed forces2 CPU - Successfully computed forces3 CUDA - Successfully computed forces1 warning generated.1 warning generated.4 OpenCL - Successfully computed forcesMedian difference in forces between platforms:Reference vs. CPU: 6.29538e-06Reference vs. CUDA: 6.75176e-06CPU vs. CUDA: 7.49106e-07Reference vs. OpenCL: 6.75018e-06CPU vs. OpenCL: 7.64529e-07CUDA vs. OpenCL: 1.757e-07All differences are within tolerance.</pre><h2 id="Use-the-GPU-in-Simulation">Use the GPU in Simulation</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> openmm <span class="hljs-keyword">import</span> Platform<br><br><span class="hljs-comment"># define the platform</span><br>platform = Platform.getPlatformByName(<span class="hljs-string">&#x27;CUDA&#x27;</span>)<br>properties = &#123;<span class="hljs-string">&#x27;DeviceIndex&#x27;</span>: <span class="hljs-string">&#x27;1&#x27;</span>, <span class="hljs-string">&#x27;Precision&#x27;</span>: <span class="hljs-string">&#x27;mixed&#x27;</span>&#125; <br><br><span class="hljs-comment"># add the parameter in simulation</span><br>simulation = app.Simulation(pdb.topology, system, integrator, platform, properties)<br></code></pre></td></tr></table></figure></div><p>To be notice: Evene though, you implied that the GPU parameter in the code, it still heavily relies on the CPU.</p><h2 id="PDB-fix">PDB fix</h2><p>Before you run the simulation, you may need to fix the PDB first.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> openmm <span class="hljs-keyword">import</span> app<br><span class="hljs-keyword">from</span> openmm.unit <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> pdbfixer <span class="hljs-keyword">import</span> PDBFixer<br><span class="hljs-keyword">from</span> openmm.app <span class="hljs-keyword">import</span> PDBFile<br><br><br><span class="hljs-comment"># repair the PDB</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">PDB_fix</span>(<span class="hljs-params">INPUT, OUPUT</span>):</span><br>    fixer = PDBFixer(filename=INPUT)<br>    fixer.findMissingResidues()<br>    fixer.findNonstandardResidues()<br>    fixer.replaceNonstandardResidues()<br>    fixer.findMissingAtoms()<br>    fixer.addMissingAtoms()<br>    fixer.addMissingHydrogens(<span class="hljs-number">7.0</span>)<br>    <span class="hljs-comment">#fixer.addSolvent(fixer.topology.getUnitCellDimensions())</span><br>    <span class="hljs-comment"># Remove problematic water molecules and add correct TIP3P water</span><br>    fixer.removeHeterogens(keepWater=<span class="hljs-literal">True</span>)<br>    PDBFile.writeFile(fixer.topology, fixer.positions, <span class="hljs-built_in">open</span>(OUPUT, <span class="hljs-string">&#x27;w&#x27;</span>))<br><br>PDB_fix(<span class="hljs-string">&#x27;best.pdb&#x27;</span>, <span class="hljs-string">&#x27;best_fix.pdb&#x27;</span>)<br></code></pre></td></tr></table></figure></div><h2 id="Anchor-a-protein">Anchor a protein</h2><p><mark>Code was not tested because my protein has hydrophobic surface and it would crush in the water environment</mark></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Anchor protein 1 by restraining its atoms</span><br>anchor_force = mm.CustomExternalForce(<span class="hljs-string">&#x27;0.5 * k * (x^2 + y^2 + z^2)&#x27;</span>)<br>anchor_force.addPerParticleParameter(<span class="hljs-string">&#x27;k&#x27;</span>)<br><br><span class="hljs-comment"># Add position restraints to protein 1</span><br><span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> pdb.topology.atoms():<br>    <span class="hljs-keyword">if</span> atom.residue.chain.<span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>]:  <span class="hljs-comment"># Assume protein 1 is in chain A</span><br>        anchor_force.addParticle(atom.index, [<span class="hljs-number">1000</span>])  <span class="hljs-comment"># High force constant</span><br><br>system.addForce(anchor_force)<br></code></pre></td></tr></table></figure></div><h2 id="Pull-Protein-Force-Apply">Pull Protein Force Apply</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Apply a pulling force to protein 2</span><br>pulling_force = mm.CustomExternalForce(<span class="hljs-string">&#x27;k_pull * (x - x0)^2&#x27;</span>)<br>pulling_force.addPerParticleParameter(<span class="hljs-string">&#x27;k_pull&#x27;</span>)<br>pulling_force.addPerParticleParameter(<span class="hljs-string">&#x27;x0&#x27;</span>)<br><br><span class="hljs-comment"># Add pulling force to atoms of protein 2 (e.g., chain B)</span><br><span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> pdb.topology.atoms():<br>    <span class="hljs-keyword">if</span> atom.residue.chain.<span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>]:  <span class="hljs-comment"># Assume protein 2 is in chain B</span><br>        pulling_force.addParticle(atom.index, [-<span class="hljs-number">1</span>, <span class="hljs-number">1.0</span>])  <span class="hljs-comment"># Adjust constants</span><br><br>system.addForce(pulling_force)<br></code></pre></td></tr></table></figure></div><h2 id="Change-Record">Change Record</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">simulation.reporters.append(app.StateDataReporter(<br>    <span class="hljs-string">&#x27;best_fix_sld_tr.csv&#x27;</span>, <span class="hljs-number">10</span>, step=<span class="hljs-literal">True</span>, potentialEnergy=<span class="hljs-literal">True</span>, temperature=<span class="hljs-literal">True</span>))<br>simulation.reporters.append(app.PDBReporter(<span class="hljs-string">&#x27;best_fix_sld_tr.pdb&#x27;</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure></div><h2 id="Save-the-structure-as-cif">Save the structure as cif</h2><p>During the simulation, you may wants to add lots of wather molecular. When the number of molecular over than 100,000, pdb format can’t handle it anymore. So you want to save it as <code>cif</code> format.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;best_fix_sld.cif&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    app.PDBxFile.writeFile(modeller.topology, modeller.positions, f)<br></code></pre></td></tr></table></figure></div><h2 id="Save-the-structure-from-the-simulation">Save the structure from the simulation</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># After finishing your MD steps:</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;final_structure.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br></code></pre></td></tr></table></figure></div><h2 id="In-Action-Protein-in-Water">In Action: Protein in Water</h2><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/JnRSG6l.gif" alt=""></td><td style="text-align:left">With the help of openMM toolkit, you can simulate the protein in water easily. Here is the code from the <a href="https://openmm.github.io/openmm-cookbook/latest/notebooks/tutorials/protein_in_water.html">document</a>.</td></tr></tbody></table><p>In the simulation, the main codes is explained:</p><ol><li>read the pdb file with <code>PDBFile</code></li><li>Specify the force filed.</li><li>Clean water and add water as a period box. In this step, you can add the water in the force filed based on the size of the filed. Our you can test small filed. The filed is a period box (replicated infinitely in all directions) which means when the object moves to the end of one side, it will not run out of the box, but coming back from <strong>against face</strong>.</li><li>Setup the integrator:<ul><li><code>forcefield.createSystem</code>:<ul><li><code>modeller.topology</code>: you’ll add you molecular (protein)</li><li><code>nonbondedMethod=PME</code>: <font title='ChatGPT o1' color=gray>specifies how long-range electrostatic interactions. Simply cutting them off at a certain radius can introduce errors. <strong>PME</strong> (Particle Mesh Ewald) uses a combination of direct space calculations (for short distances) and reciprocal space calculations (using fast Fourier transforms) to accurately handle these interactions.</font></li><li><code>nonbondedCutoff=1.0*nanometer</code>: <font title='ChatGPT o1' color=gray>When using a cutoff-based approach (like nonbondedCutoff=1.0*nanometer), the simulation engine directly calculates the vdW (Lennard-Jones) interactions only between pairs of atoms that are within that 1 nm cutoff distance. If two atoms are farther apart than 1 nm, their vdW interactions are not explicitly computed.</font></li></ul></li><li><code>integrator</code>: This is the place to given the value of the Tm and time scale.</li></ul></li><li><code>simulation.minimizeEnergy()</code>: Start to minimize local Energy.</li><li>Setup report: set up the report to record the energy change and save the trajectory.</li><li>Simulate in the <mark>NVT equillibration</mark> and <mark>NPT production MD</mark> condition.<ul><li><code>1*bar</code>: bar is a standard measure of pressure, and 1 bar is approximately equal to atmospheric pressure at sea level.</li></ul></li></ol><div class="admonition note"><p class="admonition-title">What is NVT and NPT?</p><p><font title='ChatGPT o1' color=gray>In the NVT (constant Number of particles, Volume, and Temperature) ensemble, the system is thermally equilibrated at a fixed volume to achieve a stable temperature distribution. This step ensures that any initial structural distortions and non-equilibrium distributions of kinetic energy dissipate, providing a well-relaxed starting point. Following NVT equilibration, the system is often subjected to an NPT (constant Number of particles, Pressure, and Temperature) ensemble, where both temperature and pressure are maintained constant. This allows the simulation box volume to fluctuate to the pressure target, enabling the system’s density and structure to equilibrate under more experimentally relevant conditions. The transition from NVT to NPT thus facilitates a smooth pathway from initial equilibration to realistic production conditions, offering a balanced and physically representative environment for subsequent analyses of structural, thermodynamic, and dynamic properties.</font></p></div><table><thead><tr><th>Feature</th><th>NVT Equilibration</th><th>NPT Production MD</th></tr></thead><tbody><tr><td>Ensemble</td><td>Canonical (NVT)</td><td>Isothermal–Isobaric (NPT)</td></tr><tr><td>Variables Held Fixed</td><td>Number of particles (N), Volume (V), Temperature (T)</td><td>Number of particles (N), Pressure §, Temperature (T)</td></tr><tr><td>Volume Adjustment</td><td>Fixed volume</td><td>Volume fluctuates to maintain target pressure</td></tr><tr><td>Pressure Control</td><td>Not controlled, can fluctuate</td><td>Actively controlled via a barostat</td></tr><tr><td>Typical Use</td><td>Initial temperature equilibration after energy minimization</td><td>Production runs to simulate conditions resembling experimental environments</td></tr><tr><td>Realism</td><td>Less physically representative of ambient conditions (volume fixed)</td><td>More realistic: system adapts to pressure, resulting in stable density</td></tr><tr><td>Common Duration</td><td>Shorter (tens to hundreds of picoseconds)</td><td>Longer (nanoseconds to microseconds) for data collection</td></tr><tr><td>Outcome</td><td>Thermally equilibrated structure at given T</td><td>Equilibrium structure and dynamics at given P and T, suitable for analysis</td></tr></tbody></table><h2 id="Protein-Relaxation-Test">Protein Relaxation Test</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sys <span class="hljs-keyword">import</span> stdout<br><span class="hljs-keyword">from</span> openmm.app <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> openmm <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> openmm.unit <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># Input files</span><br>pdb_filename = <span class="hljs-string">&#x27;best_fix.pdb&#x27;</span>  <span class="hljs-comment"># Your starting protein structure (from cryo-EM)</span><br>forcefield_files = [<span class="hljs-string">&#x27;amber14-all.xml&#x27;</span>, <span class="hljs-string">&#x27;amber14/tip3pfb.xml&#x27;</span>]  <span class="hljs-comment"># Force fields</span><br>ionic_strength = <span class="hljs-number">0.15</span>*molar<br><br><span class="hljs-comment"># Load the PDB</span><br>pdb = PDBFile(pdb_filename)<br><br><span class="hljs-comment"># Create a forcefield object</span><br>forcefield = ForceField(*forcefield_files)<br><br><span class="hljs-comment"># Create a model of the system with solvent</span><br><span class="hljs-comment"># Add a water box around the protein (10 Å padding)</span><br>modeller = Modeller(pdb.topology, pdb.positions)<br><span class="hljs-comment">#modeller.addSolvent(forcefield, model=&#x27;tip3p&#x27;, boxSize=Vec3(10,10,20)*nanometer, ionicStrength=ionic_strength)</span><br>modeller.addSolvent(forcefield, padding=<span class="hljs-number">1.0</span>*nanometer, ionicStrength=ionic_strength)<br><br><span class="hljs-comment"># Create the system</span><br>system = forcefield.createSystem(<br>    modeller.topology,<br>    nonbondedMethod=PME,<br>    nonbondedCutoff=<span class="hljs-number">1.0</span>*nanometer,<br>    constraints=HBonds,<br>    hydrogenMass=<span class="hljs-number">4</span>*amu<br>)<br><br><span class="hljs-comment"># Add a thermostat and barostat for later (NPT)</span><br>temperature = <span class="hljs-number">300</span>*kelvin<br>pressure = <span class="hljs-number">1</span>*bar<br>friction = <span class="hljs-number">1</span>/picosecond<br>timestep = <span class="hljs-number">0.002</span>*picoseconds<br><br>system.addForce(MonteCarloBarostat(pressure, temperature))<br><br><span class="hljs-comment"># Create integrator (for equilibration and production)</span><br>integrator = LangevinIntegrator(temperature, friction, timestep)<br><br><span class="hljs-comment"># Create simulation object</span><br>platform = Platform.getPlatformByName(<span class="hljs-string">&#x27;CUDA&#x27;</span>)  <span class="hljs-comment"># or &#x27;CUDA&#x27;/&#x27;OpenCL&#x27; if available</span><br>simulation = Simulation(modeller.topology, system, integrator, platform)<br>simulation.context.setPositions(modeller.positions)<br><br><span class="hljs-comment"># Minimization</span><br>print(<span class="hljs-string">&quot;Minimizing...&quot;</span>)<br>simulation.minimizeEnergy(maxIterations=<span class="hljs-number">1000</span>)<br><br><span class="hljs-comment"># save the last structure from simulation</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_mini.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br>    <br><span class="hljs-comment"># NVT Equilibration: Remove barostat and fix volume for initial temp equilibration</span><br><span class="hljs-comment"># (Optional step: you can also start directly with NPT if you prefer)</span><br>forces = &#123; force.__class__.__name__: force <span class="hljs-keyword">for</span> force <span class="hljs-keyword">in</span> system.getForces() &#125;<br>system.removeForce(<span class="hljs-built_in">list</span>(forces.keys()).index(<span class="hljs-string">&#x27;MonteCarloBarostat&#x27;</span>))<br>simulation.context.reinitialize(preserveState=<span class="hljs-literal">True</span>)<br><br>simulation.context.setVelocitiesToTemperature(temperature)<br>print(<span class="hljs-string">&quot;Equilibrating under NVT conditions...&quot;</span>)<br>simulation.reporters.append(StateDataReporter(stdout, <span class="hljs-number">1000</span>, step=<span class="hljs-literal">True</span>, potentialEnergy=<span class="hljs-literal">True</span>, temperature=<span class="hljs-literal">True</span>))<br>simulation.reporters.append(DCDReporter(<span class="hljs-string">&#x27;relax.dcd&#x27;</span>, <span class="hljs-number">1000</span>))  <span class="hljs-comment"># Save a frame every 1000 steps</span><br><br>print(<span class="hljs-string">&#x27;start simulation&#x27;</span>)<br>simulation.step(<span class="hljs-number">50000</span>)  <span class="hljs-comment"># ~100 ps of NVT equilibration (adjust as needed)</span><br><br><span class="hljs-comment"># save the last structure from simulation</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_NVT.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br><br><span class="hljs-comment"># Re-introduce NPT conditions (barostat)</span><br>system.addForce(MonteCarloBarostat(pressure, temperature))<br>simulation.context.reinitialize(preserveState=<span class="hljs-literal">True</span>)<br><br>print(<span class="hljs-string">&quot;Equilibrating under NPT conditions...&quot;</span>)<br><span class="hljs-comment"># Remove old reporters and add a new one</span><br>simulation.step(<span class="hljs-number">50000</span>)  <span class="hljs-comment"># Another ~100 ps (adjust as needed)</span><br><br><span class="hljs-comment"># save the last structure from simulation</span><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_NPT.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br><br><span class="hljs-comment"># Now we have an equilibrated system at NPT.</span><br><span class="hljs-comment"># This is where you might start your production run.</span><br><br>production_steps = <span class="hljs-number">250000</span>  <span class="hljs-comment"># ~500 ps of production (adjust as needed)</span><br><span class="hljs-comment">#simulation.reporters.append(PDBReporter(&#x27;output_production.pdb&#x27;, 5000)) # Save frames every 10 ps</span><br>simulation.reporters.append(StateDataReporter(<span class="hljs-string">&#x27;production_log.csv&#x27;</span>, <span class="hljs-number">1000</span>, step=<span class="hljs-literal">True</span>, time=<span class="hljs-literal">True</span>, potentialEnergy=<span class="hljs-literal">True</span>,<br>                                               kineticEnergy=<span class="hljs-literal">True</span>, totalEnergy=<span class="hljs-literal">True</span>, temperature=<span class="hljs-literal">True</span>,<br>                                               volume=<span class="hljs-literal">True</span>, density=<span class="hljs-literal">True</span>))<br><br>print(<span class="hljs-string">&quot;Running Production MD...&quot;</span>)<br>simulation.step(production_steps)<br>print(<span class="hljs-string">&quot;Done!&quot;</span>)<br><br>state = simulation.context.getState(getPositions=<span class="hljs-literal">True</span>)<br>positions = state.getPositions()<br><span class="hljs-comment"># Write out the final structure to a PDB file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;relax_test_final.pdb&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    PDBFile.writeFile(simulation.topology, positions, f)<br></code></pre></td></tr></table></figure></div><p>In the final simulation, your protein may on the “edge” of the box. So, we need to adjust the relative position of the protein</p><table><thead><tr><th style="text-align:center">Before Recenter</th><th style="text-align:center">After Recenter</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/1obBq2B.png" alt=""></td><td style="text-align:center"><img src="https://imgur.com/kyKbB8I.png" alt=""></td></tr></tbody></table><h2 id="Recenter-the-Protein">Recenter the Protein</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mdtraj <span class="hljs-keyword">as</span> md<br><br><span class="hljs-comment"># Load the trajectory and topology</span><br>traj = md.load(<span class="hljs-string">&#x27;relax.dcd&#x27;</span>, top=<span class="hljs-string">&#x27;relax_test_mini.cif&#x27;</span>)<br>traj = traj.image_molecules()<br><br><span class="hljs-comment"># Re-center coordinates so the protein is centered in the box</span><br>centered_traj = traj.center_coordinates()<br><span class="hljs-comment"># Save the re-centered trajectory as a multi-model PDB (one MODEL per frame)</span><br><span class="hljs-comment">#centered_traj.save_pdb(&#x27;centered_system.pdb&#x27;)</span><br>centered_traj.save_dcd(<span class="hljs-string">&#x27;centered_system.dcd&#x27;</span>)<br></code></pre></td></tr></table></figure></div><h2 id="Trouble-Shot">Trouble Shot</h2><h3 id="No-template-found-for-residue-30730-HOH">No template found for residue 30730 (HOH)</h3><p>Citation: <a href=""></a><br>Error Code:</p><pre>ValueError: No template found for residue 30730 (HOH).  The set of atoms matches HOH, but the bonds are different.</pre><p>Bug reason: <a href="https://github.com/openmm/openmm/issues/3393">The PDB format doesn’t support models with more than 100,000 atoms.</a></p><p>How to solve: save the output as <code>cif</code> format by using <code>PDBxFile</code></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="DIFF"><figure class="iseeu highlight /diff"><table><tr><td class="code"><pre><code class="hljs diff"><span class="hljs-deletion">- with open(&#x27;best_fix_sld.pdb&#x27;, &#x27;w&#x27;) as f:</span><br><span class="hljs-addition">+ with open(&#x27;best_fix_sld.cif&#x27;, &#x27;w&#x27;) as f:</span><br><span class="hljs-deletion">-    app.PDBFile.writeFile(modeller.topology, modeller.positions, f)</span><br><span class="hljs-addition">+    app.PDBxFile.writeFile(modeller.topology, modeller.positions, f)</span><br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">OpenMM, Molecular Dynamic Simulation</summary>
    
    
    
    <category term="Python" scheme="https://karobben.github.io/categories/Python/"/>
    
    <category term="Bio" scheme="https://karobben.github.io/categories/Python/Bio/"/>
    
    
    <category term="openMM" scheme="https://karobben.github.io/tags/openMM/"/>
    
    <category term="Molecular Dynamic Simulation" scheme="https://karobben.github.io/tags/Molecular-Dynamic-Simulation/"/>
    
  </entry>
  
</feed>
