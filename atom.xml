<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Karobben</title>
  
  <subtitle>Engjoy~</subtitle>
  <link href="https://karobben.github.io/atom.xml" rel="self"/>
  
  <link href="https://karobben.github.io/"/>
  <updated>2024-10-09T22:52:46.766Z</updated>
  <id>https://karobben.github.io/</id>
  
  <author>
    <name>Karobben</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Softmax</title>
    <link href="https://karobben.github.io/2024/10/09/AI/softmax/"/>
    <id>https://karobben.github.io/2024/10/09/AI/softmax/</id>
    <published>2024-10-09T22:46:15.000Z</published>
    <updated>2024-10-09T22:52:46.766Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Softmax">Softmax</h2><p>Key idea: $f_c(x) =$ posterior probability of cass $c$</p><ul><li>A perceptron has a one-hot output vector, in which $f_c(x) = 1$ if the<br>neural net thinks $c$ is the most likely value of $y$, and 0 otherwise</li><li>A softmax computes $f_c(x) \approx Pr(Y =c |x)$. The conditions for this to be true are:<ul><li>It needs to satisfy the axioms of probability:<br>$$ 0 \leq f_c(x) \leq 1, \quad \sum_{c=1}^{v} f_c(x) = 1$$</li><li>The weight matrix, $W$, is trained using a loss function that encourages $f(x)$ to approximate posterior probability of the labels on some training dataset:<br>$$f_c(x) \approx \Pr(Y = c|x)$$</li></ul></li></ul><h3 id="Softmax-satisfies-the-axioms-of-probability">Softmax satisfies the axioms of probability</h3><ul><li><p>Axiom #1, probabilities are non-negative $(f_k(x) \geq 0)$. There are many ways to do this, but one way that works is to choose:</p><p>$$<br>f_c(x) \propto \exp(w_c^T x + b_c)<br>$$</p></li><li><p>Axiom #2, probabilities should sum to one $(\sum_{k=1}^{v} f_k(x) = 1)$. This can be done by normalizing:</p></li></ul><p>$$<br>f(x) = [f_1(x), …, f_v(x)]^T<br>$$<br>$$<br>f_c(x) = \frac{\exp(w_c^T x + b_c)}{\sum_{k=0}^{v-1} \exp(w_k^T x + b_k)}<br>$$</p><p>where $w_k^T$ is the $k^{th}$ row of the matrix $W$.</p><h3 id="The-logistic-sigmoid-function">The logistic sigmoid function</h3><p>For a two-class classifier, we don’t really need the vector label. If we define $w = w_2 - w_1$ and $b = b_2 - b_1$, then the softmax simplifies to:</p><p>$$<br>f(Wx + b) =<br>\begin{bmatrix}<br>\text{Pr}(Y = 1|x) \\<br>\text{Pr}(Y = 2|x)<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\frac{1}{1+e^ {-(w^ Tx+b)}} \\<br>\frac{e^ {-(w^ Tx+b)}}{1+e^ {-(w^ Tx+b)}}<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\sigma(w^Tx + b) \\<br>1 - \sigma(w^Tx + b)<br>\end{bmatrix}<br>$$</p><p>… so instead of the softmax, we use a scalar function called the logistic sigmoid function:</p><p>$$<br>\sigma(z) = \frac{1}{1+e^{-z}}<br>$$</p><p>This function is called sigmoid because it is S-shaped.</p><p>For $z \to -\infty$, $\sigma(z) \to 0$</p><p>For $z \to +\infty$, $\sigma(z) \to 1$</p><h3 id="Gradient-descent">Gradient descent</h3><p>Suppose we have training tokens $(x_i, y_i)$, and we have some initial class vectors $w_1$ and $w_2$. We want to update them as</p><p>$$<br>w_1 \leftarrow w_1 - \eta \frac{\partial \mathcal{L}}{\partial w_1}<br>$$</p><p>$$<br>w_2 \leftarrow w_2 - \eta \frac{\partial \mathcal{L}}{\partial w_2}<br>$$</p><p>…where $\mathcal{L}$ is some loss function. What loss function makes sense?</p><h3 id="Zero-one-loss-function">Zero-one loss function</h3><p>The most obvious loss function for a classifier is its classification error rate,</p><p>$$<br>\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \ell(\hat{f}(x_i), y_i)<br>$$</p><p>Where $\ell(\hat{y}, y)$ is the zero-one loss function,</p><p>$$<br>\ell(f(x), y) =<br>\begin{cases}<br>0 &amp; \text{if } f(x) = y \\<br>1 &amp; \text{if } f(x) \neq y<br>\end{cases}<br>$$</p><p>The problem with zero-one loss is that it’s not differentiable.</p><h3 id="A-loss-function-that-learns-probabilities">A loss function that learns probabilities</h3><p>Suppose we have a softmax output, so we want $f_c(x) \approx \Pr(Y = c|x)$. We can train this by learning $W$ and $b$ to maximize the probability of the training corpus. If we assume all training tokens are independent, we get:</p><p>$$<br>W, b = \underset{W,b}{\text{argmax}} \prod_{i=1}^{n} \Pr(Y = y_i|x_i) = \underset{W,b}{\text{argmax}} \sum_{i=1}^{n} \ln \Pr(Y = y_i|x_i)<br>$$</p><p>But remember that $f_c(x) \approx \Pr(Y = c|x)$! Therefore, maximizing the log probability of training data is the same as minimizing the cross entropy between the neural net and the ground truth:</p><p>$$<br>W, b = \underset{W,b}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} \mathcal{L}_ i, \quad \mathcal{L}_ i = - \log f_ {y_ i}(x_ i)<br>$$</p><h3 id="Cross-entropy">Cross-entropy</h3><p>This loss function:</p><p>$$<br>\mathcal{L} = - \ln f_{y}(x)<br>$$</p><p>is called cross-entropy. It measures the difference in randomness between:</p><ul><li>Truth: $Y = y$ with probability 1.0, $\ln(1.0) = 0$, minus the</li><li>Neural net estimate: $Y = y$ with probability $f_{y}(x)$.</li></ul><p>Thus</p><p>$$<br>\mathcal{L} = 0 - \ln f_{y}(x)<br>$$</p><h3 id="Gradient-of-the-cross-entropy-of-the-softmax">Gradient of the cross-entropy of the softmax</h3><p>Since we have these definitions:</p><p>$$<br>\mathcal{L} = - \ln f_{y}(x), \quad f_{y}(x) = \frac{\exp(z_{y})}{\sum_{k=1}^{v} \exp(z_{k})}, \quad z_{c} = w_c^T x + b_c<br>$$</p><p>Then:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w_c} = \left( \frac{\partial \mathcal{L}}{\partial z_c} \right) \left( \frac{\partial z_c}{\partial w_c} \right) = \left( \frac{\partial \mathcal{L}}{\partial z_c} \right) x<br>$$</p><p>…where:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial z_c} =<br>\begin{cases}<br>f_{c}(x_i) - 1 &amp; c = y \\<br>f_{c}(x_i) &amp; c \neq y<br>\end{cases}<br>$$</p><h3 id="Similarity-to-linear-regression">Similarity to linear regression</h3><p>For linear regression, we had:</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w} = \epsilon x, \quad \epsilon = f(x) - y<br>$$</p><p>For the softmax classifier with cross-entropy loss, we have</p><p>$$<br>\frac{\partial \mathcal{L}}{\partial w_c} = \epsilon_c x<br>$$</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y \text{ (output should be 1)} \\<br>f_c(x_i) &amp; \text{otherwise (output should be 0)}<br>\end{cases}<br>$$</p><h3 id="Similarity-to-perceptron">Similarity to perceptron</h3><p>Suppose we have a training token $(x, y)$, and we have some initial class vectors $w_c$. Using softmax and cross-entropy loss, we can update the weight vectors as</p><p>$$<br>w_c \leftarrow w_c - \eta \epsilon_c x<br>$$</p><p>…where</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y_i \\<br>f_c(x_i) &amp; \text{otherwise}<br>\end{cases}<br>$$</p><p>In other words, like a perceptron,</p><p>$$<br>\epsilon_c =<br>\begin{cases}<br>\epsilon_c &lt; 0 &amp; c = y_i \\<br>\epsilon_c &gt; 0 &amp; \text{otherwise}<br>\end{cases}<br>$$</p><h3 id="Outline">Outline</h3><ul><li><p>Softmax:<br>$$ f_c(x) = \frac{\exp(w_c^T x + b_c)}{\sum_{k=1}^{v} \exp(w_k^T x + b_k)} \approx \Pr(Y = c|x) $$</p></li><li><p>Cross-entropy:<br>$$ \mathcal{L} = - \ln f_{y}(x) $$</p></li><li><p>Derivative of the cross-entropy of a softmax:<br>$$ \frac{\partial \mathcal{L}}{\partial w_c} = \epsilon_c x, \quad \epsilon_c =<br>\begin{cases}<br>f_c(x_i) - 1 &amp; c = y \text{ (output should be 1)} \\<br>f_c(x_i) &amp; \text{otherwise (output should be 0)}<br>\end{cases} $$</p></li><li><p>Gradient descent:<br>$$ w_c \leftarrow w_c - \eta \epsilon_c x $$</p></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Softmax</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Support Vector Machine</title>
    <link href="https://karobben.github.io/2024/09/29/AI/supportvectormachine/"/>
    <id>https://karobben.github.io/2024/09/29/AI/supportvectormachine/</id>
    <published>2024-09-30T02:41:26.000Z</published>
    <updated>2024-10-09T22:55:34.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-Support-Vector-Machine">What is Support Vector Machine</h2><p>SVM was developed in the 1990s by Vladimir Vapnik and his colleagues. The development of SVM was rooted in statistical learning theory. It introduced the concept of finding the maximum margin hyperplane to separate classes effectively, with extensions to handle non-linear data through kernel functions. SVM gained popularity due to its ability to create powerful classifiers, especially in high-dimensional feature spaces.</p><h3 id="Compare-to-Random-Forest">Compare to Random Forest</h3><p>Random Forest is an ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting, handling both linear and non-linear data well. It’s good for large datasets and provides feature importance but is less interpretable.</p><p>SVM finds the optimal hyperplane to separate classes by maximizing the margin. It works well for smaller, high-dimensional datasets but is computationally expensive for large datasets and harder to interpret.</p><h3 id="Compare-to-Linear-Regression">Compare to Linear Regression</h3><p>The decision function of <strong>Support Vector Machine (SVM)</strong> looks very similar to a <strong>linear function</strong>—and indeed, it shares common elements with <strong>linear regression</strong>. However, the main differences lie in their objectives and the way they handle data:</p><h4 id="Similarities">Similarities</h4><ul><li><strong>Linear Function Form</strong>: Both SVM and Linear Regression use a linear function of the form:<br>$$<br>f(x) = w_1 x_1 + w_2 x_2 + \cdots + w_k x_k + b<br>$$<br>Where $ w_i $ are the weights, $ x_i $ are the features, and $ b $ is the bias term.</li><li><strong>Weight Optimization</strong>: Both models optimize the weights ($ w $) to achieve their goals.</li></ul><h4 id="Key-Differences">Key Differences</h4><ol><li><p><strong>Objective Function</strong>:</p><ul><li><strong>Linear Regression</strong>: The goal is to <strong>minimize the error</strong> (typically the mean squared error) between predicted and actual values. It aims to find the line (or hyperplane) that best fits the data points by minimizing the difference between predictions and true values.</li><li><strong>SVM</strong>: The goal is to <strong>maximize the margin</strong> between different classes. SVM seeks to find a hyperplane that not only separates the classes but does so with the largest possible gap between the nearest points of each class (called <strong>support vectors</strong>). This makes the decision boundary as robust as possible against errors or noise.</li></ul></li><li><p><strong>Loss Function</strong>:</p><ul><li><strong>Linear Regression</strong>: Uses <strong>squared loss</strong> to penalize errors, which means that even small deviations contribute to the overall loss.</li><li><strong>SVM</strong>: Uses a <strong>hinge loss function</strong> for classification, which penalizes misclassifications and ensures a margin of separation. The loss function focuses more on correctly classifying data points with maximum confidence.</li></ul></li><li><p><strong>Problem Type</strong>:</p><ul><li><strong>Linear Regression</strong>: Primarily used for <strong>regression</strong> problems, where the goal is to predict a continuous output.</li><li><strong>SVM</strong>: Primarily used for <strong>classification</strong> (though it can be adapted for regression as <strong>SVR</strong>), where the goal is to classify data points into different categories. In SVM, the function output is interpreted using a sign function, where:<br>$$<br>f(x) = w^T x + b \Rightarrow \text{classify as } \begin{cases}<br>+1, &amp; \text{if } f(x) &gt; 0 \\<br>-1, &amp; \text{if } f(x) &lt; 0<br>\end{cases}<br>$$</li></ul></li><li><p><strong>Margin and Support Vectors</strong>:</p><ul><li><strong>Linear Regression</strong>: There is no concept of a <strong>margin</strong> or <strong>support vectors</strong> in linear regression. It simply finds the line of best fit for all data points.</li><li><strong>SVM</strong>: Introduces the concept of <strong>margin</strong>, which is the distance between the hyperplane and the closest data points from each class. These closest points are called <strong>support vectors</strong>, and they are crucial to defining the decision boundary.</li></ul></li><li><p><strong>Use of Kernels (Non-linearity)</strong>:</p><ul><li><strong>Linear Regression</strong>: Strictly a linear model. To handle non-linearity, you would have to explicitly add polynomial features or transform the features.</li><li><strong>SVM</strong>: Supports <strong>kernel tricks</strong> (such as polynomial or radial basis function kernels) to project data into higher dimensions, allowing it to separate data that isn’t linearly separable in its original space. This feature makes SVM more powerful for complex, non-linear classification problems.</li></ul></li></ol><h4 id="Summary">Summary</h4><ul><li><strong>Linear Regression</strong>: Minimizes prediction error for a best-fit line, used for regression.</li><li><strong>SVM</strong>: Maximizes the margin to find an optimal separating hyperplane, used for classification.</li><li>While both use linear functions, SVM is fundamentally about <strong>classification and margin maximization</strong>, whereas linear regression focuses on <strong>minimizing the difference between predicted and actual continuous values</strong>. SVM also handles non-linearity more effectively through kernels, making it more versatile for complex datasets.</li></ul><h2 id="Overview-of-SVM">Overview of SVM</h2><ul><li>Decision Boundary: $w^T x + b$.</li><li>Classification: $f(x) = sign(w^T x + b)$</li><li>Cost function: Training error cost + $\lambda$ penalty</li></ul><table><thead><tr><th><strong>Number of Features</strong></th><th><strong>Decision Boundary Equation</strong></th><th><strong>Classification Equation</strong></th></tr></thead><tbody><tr><td>1 Feature</td><td>$ w_1 x_1 + b = 0 $</td><td>$ f(x) = \text{sign}(w_1 x_1 + b) $</td></tr><tr><td>2 Features</td><td>$ w_1 x_1 + w_2 x_2 + b = 0 $</td><td>$ f(x) = \text{sign}(w_1 x_1 + w_2 x_2 + b) $</td></tr><tr><td>$ k $ Features</td><td>$ w_1 x_1 + w_2 x_2 + \cdots + w_k x_k + b = 0 $</td><td>$ f(x) = \text{sign}(w_1 x_1 + w_2 x_2 + \cdots + w_k x_k + b) $</td></tr></tbody></table><div class="admonition question"><p class="admonition-title">What does $w^T$ mean</p><p><strong>Explanation</strong>:</p><ul><li>A vector $ w $ is typically represented as a column vector, meaning it has multiple rows and a single column.</li><li>$ w^T $ is the <strong>transpose</strong> of $ w $, which means converting a column vector into a row vector, or vice versa.</li></ul><p><strong>Mathematical Notation</strong>:</p><ul><li>If $ w $ is a column vector with elements:$$w = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}$$</li><li>Then the <strong>transpose</strong> $ w^T $ is w row vector:$$w^T = \begin{bmatrix} w_1 &amp; w_2 &amp; \cdots &amp; w_n \end{bmatrix}$$In <strong>SVM</strong> or <strong>machine learning</strong>, the transpose is often used to indicate a <strong>dot product</strong> operation when combined with another vector or matrix. For example, if you have: $w^T x $, it means you're calculating the <strong>dot product</strong> of vector $ w $ and vector $ x $, which is a scalar value used in calculating distances, projections, or in constructing decision boundaries in algorithms like SVM.</li></ul></div><h3 id="Features">Features</h3><p>$$<br>f(\mathbf{x}) = \text{sign}(\mathbf{w}^T \mathbf{x} + b)<br>$$</p><p>where</p><p>$$<br>\mathbf{x} = \begin{bmatrix} x_0 \ x_1 \end{bmatrix}, \quad \mathbf{w} = \begin{bmatrix} w_0 \ w_1 \end{bmatrix}, \quad \text{and} \quad b \text{ is a scalar.}<br>$$</p><p><strong>Boundary</strong></p><p>The boundary condition is given by:</p><p>$$<br>\begin{bmatrix} w_0 &amp; w_1 \end{bmatrix} \begin{bmatrix} x_0 \ x_1 \end{bmatrix} + b = 0<br>$$</p><p>Solving for $ x_1 $:</p><p>$$<br>w_0 x_0 + w_1 x_1 + b = 0<br>$$</p><p>$$<br>x_1 = -\frac{w_0}{w_1} x_0 - \frac{b}{w_1}<br>$$</p><p><strong>Classification</strong></p><p>The classification function is:</p><p>$$<br>y = \begin{cases}<br>1 &amp; \text{if } x_1 \geq -\frac{w_0}{w_1}x_0 - \frac{b}{w_1} \\<br>-1 &amp; \text{if } x_1 &lt; -\frac{w_0}{w_1}x_0 - \frac{b}{w_1}<br>\end{cases}<br>$$</p><h2 id="Training-Cost">Training Cost</h2><p>The training cost in SVM refers to the computational and resource-related costs involved in training the model, which is an important consideration when choosing an algorithm, especially for larger datasets. SVM’s training cost is influenced by its optimization problem, which involves finding the hyperplane that maximizes the margin while correctly classifying the training data (or with minimal misclassification for soft margins).</p><h3 id="Training-Cost-in-SVM">Training Cost in SVM</h3><ol><li><p><strong>Optimization Complexity</strong>:</p><ul><li>SVM training involves <strong>solving a quadratic optimization problem</strong> to find the best hyperplane.</li><li>This process is complex and takes more computation, especially with <strong>non-linear kernels</strong>.</li></ul></li><li><p><strong>Time Complexity</strong>:</p><ul><li><strong>Linear SVM</strong>: Training time is between $O(n * d)$ and $O(n^2 * d)$, where $ n $ is the number of data points and $ d $ is the number of features.</li><li><strong>Non-linear Kernel SVM</strong>: Training complexity is approximately $O(n^2)$ to $O(n^3)$, making it very expensive for large datasets.</li></ul></li><li><p><strong>Memory Usage</strong>:</p><ul><li>With kernels, SVM stores a <strong>kernel matrix</strong> of size $ n \times n $, which uses a lot of memory if $ n $ is large.</li></ul></li><li><p><strong>Support Vectors</strong>:</p><ul><li>More <strong>support vectors</strong> means more computation during both training and prediction. Complex datasets often need more support vectors.</li></ul></li></ol><h3 id="Why-Care-About-Training-Cost">Why Care About Training Cost?</h3><ul><li><strong>Scalability</strong>: SVM can become impractical for <strong>large datasets</strong> due to the high cost in terms of time and memory.</li><li><strong>Resources</strong>: It requires substantial <strong>CPU and memory</strong>, limiting its use on resource-constrained systems.</li><li><strong>Algorithm Selection</strong>: For small to medium datasets, SVM works well. For large datasets, other methods like <strong>Random Forest</strong> or <strong>SGD</strong> may be better.</li></ul><h3 id="Reducing-Training-Cost">Reducing Training Cost</h3><ol><li><strong>Linear SVM</strong>: Use for linearly separable data—it has lower complexity.</li><li><strong>Approximations</strong>: Use <strong>SGDClassifier</strong> or <strong>kernel approximations</strong> for faster training.</li><li><strong>Data Subset</strong>: Train on a <strong>smaller subset</strong> of data to speed up training.</li></ol><h3 id="Hinge-Loss">Hinge Loss</h3><table><thead><tr><th>Condition</th><th>Cost Function</th><th>Description</th></tr></thead><tbody><tr><td>$y_i \neq \text{sign}(\hat{y}_i)$</td><td>$C(y_ i, \hat{y}_ i) = |y_ i| + 1$</td><td>Large</td></tr><tr><td>$y_i = \text{sign}(\hat{y}_i)$ close</td><td>$C(y_ i, \hat{y}_ i) = |y_ i| + 1$</td><td>Medium</td></tr><tr><td>$y_i = \text{sign}(\hat{y}_i)$ away</td><td>$C(y_ i, \hat{y}_ i) = 0$</td><td>No cost</td></tr><tr><td>General Cost Function</td><td>$C(y_ i, \hat{y}_ i) = \max(0, 1 - y_ i \cdot \hat{y}_ i)$</td><td>-</td></tr></tbody></table><h2 id="Train-a-SVM">Train a SVM</h2><h3 id="Training-Error">Training Error</h3><ul><li>$ \frac{1}{N} \sum_{i=1}^N C(y_i, \hat{y}_ i)$<ul><li>$ = \frac{1}{N} \sum_{i=1}^N \max(0, 1 - y_ i \cdot \hat{y}_ i) $</li><li>$ = \frac{1}{N} \sum_{i=1}^N \max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b)) $</li></ul></li></ul><h3 id="Cost-Function">Cost Function</h3><p>$$ S(\mathbf{w}, b; \lambda) = \frac{1}{N} \sum_{i=1}^N [\max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b))] + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w} $$</p><h3 id="Stochastic-Gradient-Descent">Stochastic Gradient Descent</h3><p>In training a Support Vector Machine (SVM), the primary objective is to minimize the cost function. This cost function often includes terms that measure the classification error and possibly a regularization term. The minimization of the cost function aims to find the best hyperplane that separates the classes while also considering the margin maximization between different classes and controlling model complexity to prevent overfitting.</p><p>$$<br>\mathbf{u} = \begin{bmatrix} \mathbf{w} \ b \end{bmatrix}<br>$$</p><p><strong>Minimize cost function:</strong></p><p>$$<br>g(\mathbf{u}) = \left[ \frac{1}{N} \sum_{i=1}^N g_i(\mathbf{u}) \right] + g_0(\mathbf{u})<br>$$</p><p>where:</p><p>$$<br>g_i(\mathbf{u}) = \max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b))<br>$$</p><p>and:</p><p>$$<br>g_0(\mathbf{u}) = \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}<br>$$</p><p><strong>Iteratively, at step $(n)$:</strong></p><ul><li>Compute descent direction $p^{(n)}$ and step size $\eta$</li><li>Ensure that $g(u^{(n)} + \eta p^{(n)}) \leq g(u^{(n)})$</li><li>Update $u^{(n+1)} = u^{(n)} + \eta p^{(n)}$</li></ul><p><strong>Descent direction:</strong></p><p>$$<br>p^{(n)} = -\nabla g(\mathbf{u}^{(n)})<br>$$</p><p>$$<br>= -\left( \frac{1}{N} \sum_{i=1}^N \nabla g_i(\mathbf{u}) + \nabla g_0(\mathbf{u}) \right)<br>$$</p><p><strong>Estimation through mean of batch:</strong></p><p>$$<br>p^{(n)}_ {N_ b} = -\left( \frac{1}{N_b} \sum_ {j \in \text{batch}} \nabla g_ j(\mathbf{u}) + \nabla g_ 0(\mathbf{u}) \right)<br>$$</p><p><strong>Epoch</strong></p><ul><li>One pass on training set of size $N$</li><li>Each step sees a batch of $N_b$ items</li><li>The dataset is covered in $\frac{N}{N_b}$ steps</li><li>Step size in epoch $e$: $\eta^{(e)} = \frac{m}{e + l}$<ul><li>Constants $m$ and $l$: tune on small subsets</li></ul></li></ul><p><strong>Season</strong></p><ul><li>Constant number of iterations, much smaller than epochs</li><li>Each step sees a batch of $N_b$ items</li><li>Step size in season $s$: $\eta^{(s)} = \frac{m}{s + l}$<ul><li>Constants $m$ and $l$: tune on small subsets</li></ul></li></ul><h3 id="Full-SGD">Full SGD</h3><ul><li><p><strong>Vector u and its gradient:</strong><br>$$ \mathbf{u} = \begin{bmatrix} u_1 \\ \vdots \\ u_d \end{bmatrix}, \quad \nabla g = \begin{bmatrix} \frac{\partial g}{\partial u_1} \\ \vdots \\ \frac{\partial g}{\partial u_d} \end{bmatrix} $$</p></li><li><p><strong>Batches of 1 sample at each training step:</strong><br>$$ N_b = 1 $$</p></li><li><p><strong>Gradient of g(u):</strong><br>$$ \nabla g(\mathbf{u}) = \nabla \left( \max(0, 1 - y_i \cdot (\mathbf{w}^T \mathbf{x}_i + b)) + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w} \right) $$</p></li><li><p><strong>Update rules for a and b:</strong><br>$$ \begin{bmatrix} \mathbf{w}^{(n+1)} \ b^{(n+1)} \end{bmatrix} = \begin{bmatrix} \mathbf{w}^{(n)} \ b^{(n)} \end{bmatrix} - \eta \begin{bmatrix} \nabla_{\mathbf{w}} \ \nabla_{b} \end{bmatrix} $$</p></li><li><p><strong>Condition for correct classification away from the boundary:</strong><br>$$ y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) \geq 1. \quad \text{Correct, away from boundary} $$<br>$$ \nabla_ {\mathbf{w}} (0 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = \lambda \mathbf{w}, \quad \nabla_ {b}(0 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = 0 $$</p></li><li><p><strong>Condition for classification close to the boundary or incorrect:</strong><br>$$ y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) &lt; 1. \quad \text{Correct, close to boundary, or incorrect} $$<br>$$ \nabla_ {\mathbf{w}} (1 - y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = -y_i \mathbf{x}_ i + \lambda \mathbf{w} $$<br>$$ \nabla_ {b} (1 - y_ i \cdot (\mathbf{w}^T \mathbf{x}_ i + b) + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}) = -y_i $$</p></li></ul><h3 id="Stops">Stops</h3><p>Stop when</p><ul><li>predefined number of seasons or epochs</li><li>error on held-out data items is smaller than some threshold</li><li>other criteria</li></ul><p><strong>Regularization Constant $ \lambda $</strong></p><ul><li><p>Regularization constant $ \lambda $ in $ g(\mathbf{u}) = \frac{1}{2} \lambda \mathbf{w}^T \mathbf{w} $. Try at different scales (e.g., $ \lambda \in {10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1} $)</p></li><li><p><strong>Procedure for Cross-Validation:</strong></p><ul><li>Split dataset into Test Set and Train Set for cross-validation.</li><li>For each $ \lambda_i $ in set to try, iteratively:<ul><li>Generate a new Fold from Train Set with a Cross-Validation Train Set and Validation Set.</li><li>Using testing $ \lambda_i $, apply Stochastic Gradient Descent (SGD) on Cross-Validation Train Set to find $ \mathbf{w} $ and $ b $.</li><li>Evaluate $ \mathbf{w} $, $ b $, $ \lambda_i $ on Validation Set and record error for current Fold.</li><li>Cross-validation error for chosen $ \lambda_i $ is average error over all the Folds.</li></ul></li><li>Using $ \lambda $ with the lowest cross-validation error, apply SGD on whole training set to get final $ \mathbf{w} $ and $ b $.</li></ul></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Support vector machien is a very commonly used in machine learning</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Random Forest</title>
    <link href="https://karobben.github.io/2024/09/29/AI/randomforest/"/>
    <id>https://karobben.github.io/2024/09/29/AI/randomforest/</id>
    <published>2024-09-30T01:11:25.000Z</published>
    <updated>2024-10-09T22:59:13.565Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Basic-Architectures">Basic Architectures</h2><ol><li><strong>Bootstrap Sampling</strong>: Random subsets of the training data are created with replacement, known as bootstrap samples. Each subset is used to train an individual decision tree.</li><li><strong>Feature Randomness</strong>: For each tree, a random subset of features is considered at each split, reducing correlation among trees and improving generalization.</li><li><strong>Multiple Decision Trees</strong>: Multiple decision trees are grown independently using the bootstrap samples. Each tree makes a prediction for a given input.</li><li><strong>Ensemble Output</strong>: For classification, the output is typically based on majority voting across all trees, while for regression, the final output is an average of all tree predictions.</li></ol><h2 id="Limitations">Limitations</h2><ul><li>Many different trees can lead to similar classifications</li><li>The algorithm to build a decision tree grows each branch just deeply enough to perfectly classify the training examples<ul><li>potential overfit</li></ul></li><li>Randomness in identification of splits: features, thresholds<ul><li>better splits may have not been considered</li></ul></li><li>Addressed through Random Forests</li></ul><h2 id="Basic-of-Random-Forest">Basic of Random Forest</h2><h3 id="Tree-Expanding">Tree Expanding</h3><p>Random forest is build on number of decision trees. For avoiding the overfit, early stop is needed during the tree expanding.</p><ul><li>It stop when<ul><li>depth(branch_node) &gt;= max_depth (manual)</li><li>size(dataset) &lt;= min_leave_size (manual)</li><li>all elements in dataset in same class</li></ul></li></ul><h3 id="Decision-Making">Decision Making</h3><p>The best decision made could be evaluate by the “entropy”.</p><ol><li>2 classes:<br>Here are the formulas from the image converted into Markdown format:</li></ol><ul><li>$ \text{Entropy}(S) = -P(A) \log_2 P(A) - P(B) \log_2 P(B) $<ul><li>$ N = |A| + |B| $</li><li>$ P(A) = \frac{|A|}{N}, \quad P(B) = \frac{|B|}{N} $</li></ul></li></ul><ol start="2"><li>C classes:</li></ol><ul><li>$ \text{Entropy}(S) = - \sum_{i=1}^C P_i \log_2 P_i $<ul><li>Each class $ i $ with probability $ P_i $.</li></ul></li></ul><h3 id="Information-Gain">Information Gain</h3><p><strong>Goal</strong>: The goal is to maximize Information Gain at each split, which corresponds to choosing features that result in subsets with the least entropy, making the data more pure (less mixed) after the split. In Random Forest and decision tree learning, the feature with the highest Information Gain is selected for splitting at each node.</p><ul><li><strong>Definition</strong>: Information Gain measures the reduction in entropy (or uncertainty) after splitting a dataset. It helps to determine the best feature for splitting the data.</li><li><strong>Entropy Before Split</strong>: The initial dataset $ S $ has an entropy $ \text{Entropy}(S) $, which quantifies the impurity or randomness in the dataset.</li><li><strong>Entropy After Split</strong>: When the dataset is split into subsets $ S_l $ and $ S_r $, each subset has its own entropy: $ \text{Entropy}(S_l) $ and $ \text{Entropy}(S_r) $.</li><li><strong>Weighted Entropy</strong>: The weighted average of the entropy of the subsets after the split is given by:<br>$$<br>\text{Entropy}_{\text{after split}} = \frac{|S_l|}{|S|} \text{Entropy}(S_l) + \frac{|S_r|}{|S|} \text{Entropy}(S_r)<br>$$</li><li><strong>Information Gain Calculation</strong>: The Information Gain for the feature $ x^{(i)} $ is calculated as the difference between the entropy before and after the split:<br>$$<br>\text{Information Gain} = \text{Entropy}(S) - \text{Entropy}_{\text{after split}}<br>$$</li></ul><p>The symbols $ |S| $, $ |S_l| $, and $ |S_r| $ represent the <strong>cardinalities</strong> or the <strong>sizes</strong> of the respective sets, meaning they indicate the <strong>number of elements</strong> in each set.</p><h3 id="Missing-Values">Missing Values</h3><ul><li>In splits, if an item misses the feature value that decide where it goes<ul><li>Estimate it based on other examples: mode or mean</li></ul></li><li>Consider only the examples in the corresponding branch</li></ul><h2 id="Decision-Tree-in-Random-Forest">Decision Tree in Random Forest</h2><p>For get the best “Decision” in each branch, iterating through all possible splits at each node can be computationally expensive, especially for large datasets and numerous features. However, decision trees (and Random Forests) use several optimization techniques to find the best split efficiently, while managing computational cost:</p><h3 id="1-Feature-and-Threshold-Selection-Strategy">1. <strong>Feature and Threshold Selection Strategy</strong></h3><ul><li><strong>Greedy Algorithm</strong>: Decision tree algorithms commonly use a greedy approach to split at each node. They do not explore all possible trees but instead make the locally optimal choice (the split that maximizes Information Gain or minimizes entropy) at each step. While this doesn’t guarantee a globally optimal tree, it is computationally efficient.</li><li><strong>Threshold Optimization</strong>: Rather than testing every possible threshold for each feature, the algorithm often considers a subset of thresholds. If the feature is numeric, thresholds are typically evaluated at points between consecutive, sorted feature values.</li></ul><h3 id="2-Random-Forest-Feature-Subsampling">2. <strong>Random Forest Feature Subsampling</strong></h3><ul><li>In <strong>Random Forests</strong>, at each node, only a random subset of features is considered for splitting, rather than evaluating all features. This greatly reduces the number of calculations needed, enhances computational efficiency, and decorrelates the trees in the ensemble (increasing robustness).</li></ul><h3 id="3-Heuristics-to-Reduce-Computation">3. <strong>Heuristics to Reduce Computation</strong></h3><ul><li><strong>Best First Split</strong>: During the process, the split that gives the maximum Information Gain is stored, and the search for a better split continues until the end of the subset of considered thresholds. If no better split is found, the stored one is selected.</li><li><strong>Stopping Conditions</strong>: To further reduce resource usage, decision tree growth is often constrained by stopping criteria such as maximum depth, minimum number of samples per leaf, or if a split provides insufficient improvement.</li></ul><h3 id="4-Approximations-for-Efficiency">4. <strong>Approximations for Efficiency</strong></h3><ul><li><strong>Bin-based Thresholds</strong>: For numerical features, rather than considering every possible value as a split point, values can be grouped into bins. The potential split thresholds are then defined based on these bins.</li><li><strong>Pre-sorting Features</strong>: In some implementations, features are pre-sorted, so determining potential split points for numeric features can be faster.</li></ul><h3 id="5-Iterative-Splitting-and-Best-Split-Finding">5. <strong>Iterative Splitting and Best Split Finding</strong></h3><ul><li>For categorical features, the split can be done in subsets if there are many categories, or by considering binary splits. For numerical features, it evaluates splits between values.</li><li>Yes, it does involve iteration, but the optimizations listed above ensure that this iteration is performed in a manageable and efficient way without explicitly iterating through every possible split for all features.</li></ul><h3 id="Balancing-Efficiency-and-Quality-of-Decision-Trees"><strong>Balancing Efficiency and Quality of Decision Trees</strong></h3><p>The combination of the above techniques allows decision trees to strike a balance between:</p><ol><li><strong>Finding Good Splits</strong>: Even if the splits aren’t absolutely perfect, they are often good enough to form a strong decision tree.</li><li><strong>Limiting Resource Waste</strong>: Efficient search heuristics and optimizations are used to reduce the exhaustive computational cost.</li></ol><h3 id="In-Random-Forest">In Random Forest:</h3><ol><li><strong>Choose $ m = \sqrt{|x|} $ Features at Random</strong><ul><li>Instead of evaluating all features for a potential split, a random subset of features is selected to reduce computation. The number of features selected ($ m $) is proportional to the square root of the total number of features ($ |x| $).</li><li>This is a common technique in Random Forests to decorrelate individual decision trees and make the algorithm <mark>computationally efficient</mark>. It prevents overfitting by introducing randomness and limits the number of features under consideration at each node.</li></ul></li><li><strong>Identify Candidate Splits for the Selected Feature $ x^{(i)} $</strong><ul><li><strong>Feature Sorting</strong>:<ul><li>The feature values ($ x^{(i)} $) can be sorted to determine the best thresholds for splitting the dataset.</li></ul></li><li><strong>Class Boundaries as Thresholds</strong>:<ul><li>The sorted feature values are evaluated to find boundaries between different classes.</li></ul></li><li><strong>Sort Data Items According to Feature Value</strong>:<ul><li>All data points are sorted by their value for the feature $ x^{(i)} $. This allows easy identification of candidate split points.</li></ul></li><li><strong>Adjacent Pairs in Different Classes</strong>:<ul><li>The algorithm looks for adjacent pairs of data points where one belongs to a different class than the other. This suggests a potential decision boundary.</li><li>These pairs, $ (item_0, item_1) $, are identified since they may represent a significant change in class, making them good candidates for splitting.</li></ul></li><li><strong>Threshold Midway Between $ item_0 $ and $ item_1 $</strong>:<ul><li>The threshold for the split is placed midway between these adjacent items from different classes. This ensures that the split captures the difference between the classes as effectively as possible.</li></ul></li></ul></li><li><strong>Randomly Select $ k $ Thresholds</strong><ul><li>To further limit the number of potential splits to evaluate, the algorithm randomly selects $ k $ thresholds from the identified candidate thresholds. This further reduces computational cost while maintaining a good chance of finding an effective split.</li><li>This random sampling balances computational efficiency with the quality of the splits, ensuring that the decision tree doesn’t become too computationally expensive.</li></ul></li><li>Summary<br>The image explains a process that helps reduce the number of potential splits evaluated at each node:<ol><li><strong>Random Subset of Features</strong>: Only a random $ m $ features are considered.</li><li><strong>Identifying Thresholds</strong>: For each selected feature, potential split thresholds are identified by analyzing class boundaries.</li><li><strong>Random Selection of Split Points</strong>: A random subset of the identified thresholds is evaluated.<br>These steps are taken to avoid an exhaustive search, reduce computational resources, and prevent overfitting, particularly in Random Forests where multiple trees are built.</li></ol></li></ol><h3 id="Step-by-Step-Explanation-with-Equations-and-Code">Step-by-Step Explanation with Equations and Code</h3><h4 id="Step-1-Choosing-m-sqrt-x-Features-at-Random">Step 1: Choosing $ m = \sqrt{|x|} $ Features at Random</h4><ul><li>Suppose you have $ |x| $ features in your dataset.</li><li>To decide the split, randomly select $ m $ features to evaluate, where:<br>$$<br>m =  \sqrt{|x|}<br>$$</li></ul><h5 id="Python-Code-Representation">Python Code Representation:</h5><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># data from: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database?resource=download</span><br><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>data = pd.read_csv(<span class="hljs-string">&#x27;/home/yiran/Downloads/diabetes.csv&#x27;</span>)<br>features = d.columns[:-<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># Total number of features</span><br>num_features = <span class="hljs-built_in">len</span>(features)<br><br><span class="hljs-comment"># Choose m features at random</span><br>m = <span class="hljs-built_in">int</span>(np.sqrt(num_features))<br>selected_features = np.random.choice(features, m, replace=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure></div><h4 id="Step-2-Identify-Candidate-Splits-for-a-Feature-x-i">Step 2: Identify Candidate Splits for a Feature $ x^{(i)} $</h4><p>For each feature selected, you need to determine possible thresholds to split the data.</p><h5 id="Steps-in-Code">Steps in Code:</h5><ol><li><p><strong>Sort Feature Values</strong>:</p><ul><li>For the selected feature $ x^{(i)} $, sort the data points by their feature values.</li></ul></li><li><p><strong>Identify Boundaries Between Classes</strong>:</p><ul><li>Find the pairs of data points that belong to different classes.</li><li>Calculate candidate thresholds midway between these pairs.</li></ul></li></ol><h5 id="Equations-for-Finding-Thresholds">Equations for Finding Thresholds:</h5><ul><li><p>Let $ x^{(i)}_j $ represent the feature value for data point $ j $.</p></li><li><p>Sort all values of feature $ x^{(i)} $:<br>$$<br>x^{(i)}_1, x^{(i)}_2, \ldots, x^{(i)}_n \quad \text{where } x^{(i)}_1 &lt; x^{(i)}_2 &lt; \ldots &lt; x^{(i)}_n<br>$$</p></li><li><p>Identify the adjacent pairs that belong to different classes. For a pair of adjacent items $ (x^ {(i)}_ j, x^ {(i)}_ {j+1}) $ from different classes, the candidate threshold $ t_j $ is given by:<br>$$<br>t_ j = \frac{x^ {(i)}_ j + x^ {(i)}_ {j+1}}{2}<br>$$</p></li></ul><h5 id="Python-Code-Representation-v2">Python Code Representation:</h5><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Assume we have a data structure `data` which contains features and labels</span><br><span class="hljs-comment"># We are focusing on the selected feature xi</span><br><br>data = TB.T.to_dict()<br>data = [data[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> data]<br><br><span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> selected_features:<br>    <span class="hljs-comment"># Sort data based on the selected feature&#x27;s value</span><br>    sorted_data = <span class="hljs-built_in">sorted</span>(data, key=<span class="hljs-keyword">lambda</span> d: d[feature])<br>    <span class="hljs-comment"># Find candidate thresholds</span><br>    candidate_thresholds = []<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sorted_data) - <span class="hljs-number">1</span>):<br>        <span class="hljs-comment"># If the class label changes between adjacent items</span><br>        <span class="hljs-keyword">if</span> sorted_data[j][<span class="hljs-string">&#x27;Outcome&#x27;</span>] != sorted_data[j + <span class="hljs-number">1</span>][<span class="hljs-string">&#x27;Outcome&#x27;</span>]:<br>            <span class="hljs-comment"># Find the midpoint between two adjacent feature values</span><br>            threshold = (sorted_data[j][feature] + sorted_data[j + <span class="hljs-number">1</span>][feature]) / <span class="hljs-number">2</span><br>            candidate_thresholds.append(threshold)<br></code></pre></td></tr></table></figure></div><div class="admonition note"><p class="admonition-title">note</p><p>In the code <code>sorted_data = sorted(data, key=lambda d: d[feature])</code></p><pre>feature_value_pairs = [(d, d[feature]) for d in data]# Step 2: Sort based on the feature valuefeature_value_pairs_sorted = sorted(feature_value_pairs, key=lambda pair: pair[1])# Step 3: Extract the sorted data pointssorted_data = [pair[0] for pair in feature_value_pairs_sorted]</pre></div><h4 id="Step-3-Randomly-Select-k-Thresholds">Step 3: Randomly Select $ k $ Thresholds</h4><ul><li>Randomly pick $ k $ thresholds from the candidate thresholds identified in the previous step to reduce computation.</li></ul><h5 id="Equation">Equation:</h5><ul><li>Suppose $ T = { t_1, t_2, \ldots, t_p } $ is the set of candidate thresholds.</li><li>Select $ k $ thresholds randomly from $ T $:<br>$$<br>T_{\text{selected}} = { t_{i_1}, t_{i_2}, \ldots, t_{i_k} }, \quad \text{where } i_j \in {1, \ldots, p}<br>$$</li></ul><h5 id="Python-Code-Representation-v3">Python Code Representation:</h5><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><br>some_predefined_k = <span class="hljs-number">15</span><br><span class="hljs-comment"># Number of thresholds to randomly select</span><br>k = <span class="hljs-built_in">min</span>(<span class="hljs-built_in">len</span>(candidate_thresholds), some_predefined_k)<br><br><span class="hljs-comment"># Randomly select k thresholds from candidate_thresholds</span><br>selected_thresholds = np.random.choice(candidate_thresholds, k, replace=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure></div><h4 id="Step-4-Compute-Information-Gain-for-Each-Split">Step 4: Compute Information Gain for Each Split</h4><ul><li>Iterate through the selected thresholds to compute the Information Gain and select the best one.</li></ul><h5 id="Equation-for-Information-Gain">Equation for Information Gain:</h5><ul><li><p>For a given threshold $ t $, split the data into two subsets:<br>$$<br>S_l = { x \in S : x^{(i)} \le t }, \quad S_r = { x \in S : x^{(i)} &gt; t }<br>$$</p></li><li><p>Compute the weighted entropy after the split:<br>$$<br>\text{Entropy}_{\text{after}} = \frac{|S_l|}{|S|}\text{Entropy}(S_l) + \frac{|S_r|}{|S|}\text{Entropy}(S_r)<br>$$</p></li><li><p>Compute the Information Gain:<br>$$<br>\text{Information Gain} = \text{Entropy}(S) - \text{Entropy}_{\text{after}}<br>$$</p></li></ul><h5 id="Python-Code-Representation-v4">Python Code Representation:</h5><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">entropy</span>(<span class="hljs-params">data</span>):</span><br>    <span class="hljs-comment"># Assume `data` has a function to calculate entropy</span><br>    labels = [d[<span class="hljs-string">&#x27;Outcome&#x27;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> data]<br>    <span class="hljs-comment"># Count the occurrences of each label</span><br>    label_counts = Counter(labels)<br>    total_count = <span class="hljs-built_in">len</span>(data)<br>    <span class="hljs-comment"># Calculate the entropy</span><br>    ent = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> label_counts.values():<br>        <span class="hljs-comment"># Calculate the probability of each label</span><br>        p = count / total_count<br>        <span class="hljs-comment"># Add to entropy, using the formula -p * log2(p)</span><br>        <span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0</span>:<br>            ent -= p * np.log2(p)<br>    <span class="hljs-keyword">return</span> ent<br><br>best_gain = -np.inf<br>best_threshold = <span class="hljs-literal">None</span><br><br><span class="hljs-keyword">for</span> threshold <span class="hljs-keyword">in</span> selected_thresholds:<br>    <span class="hljs-comment"># Split the data into left and right based on the threshold</span><br>    left_split = [d <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> sorted_data <span class="hljs-keyword">if</span> d[feature] &lt;= threshold]<br>    right_split = [d <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> sorted_data <span class="hljs-keyword">if</span> d[feature] &gt; threshold]<br>    <br>    <span class="hljs-comment"># Calculate the weighted entropy of the two subsets</span><br>    p_left = <span class="hljs-built_in">len</span>(left_split) / <span class="hljs-built_in">len</span>(sorted_data)<br>    p_right = <span class="hljs-built_in">len</span>(right_split) / <span class="hljs-built_in">len</span>(sorted_data)<br>    <br>    entropy_after = p_left * entropy(left_split) + p_right * entropy(right_split)<br>    gain = entropy(sorted_data) - entropy_after<br>    <br>    <span class="hljs-comment"># Update the best gain and threshold</span><br>    <span class="hljs-keyword">if</span> gain &gt; best_gain:<br>        best_gain = gain<br>        best_threshold = threshold<br></code></pre></td></tr></table></figure></div><h4 id="Summary">Summary</h4><ol><li><strong>Select Features Randomly</strong>: $ m = \sqrt{|x|} $ features are selected randomly to evaluate.</li><li><strong>Determine Candidate Thresholds</strong>: For each feature, the data is sorted, and class boundaries are used to identify potential split points.</li><li><strong>Random Threshold Selection</strong>: From the candidate thresholds, a random subset is chosen to reduce computational cost.</li><li><strong>Calculate Information Gain</strong>: Evaluate the Information Gain for each threshold to find the best split.</li></ol><p>These steps ensure that the decision tree algorithm efficiently finds good splits without exhaustively considering all possible splits. The randomness helps reduce computational costs and enhances the model’s robustness, especially in Random Forests.</p><h2 id="Another-Example">Another Example</h2><ol><li><a href="https://github.com/HaoranTang/Applied-Machine-Learning/blob/main/ClassifyingImages.ipynb">Applied-Machine-Learning/ClassifyingImages.ipynb</a></li><li>Quick ChatGPT example:</li></ol><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Step 1: Import Libraries</span><br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><br><span class="hljs-comment"># Step 2: Load and Prepare Data</span><br><span class="hljs-comment"># We&#x27;ll use the Iris dataset as an example</span><br>iris = load_iris()<br>X, y = iris.data, iris.target<br><br><span class="hljs-comment"># Split the data into training and test sets (80% train, 20% test)</span><br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># Step 3: Train the Random Forest Classifier</span><br>clf = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, random_state=<span class="hljs-number">42</span>)  <span class="hljs-comment"># 100 trees in the forest</span><br>clf.fit(X_train, y_train)<br><br><span class="hljs-comment"># Step 4: Make Predictions and Evaluate</span><br>y_pred = clf.predict(X_test)<br><br><span class="hljs-comment"># Calculate the accuracy</span><br>accuracy = accuracy_score(y_test, y_pred)<br>print(<span class="hljs-string">f&#x27;Accuracy: <span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.2</span>f&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure></div><hr><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Random Forest</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>DLVO theory: Atom Interaction</title>
    <link href="https://karobben.github.io/2024/08/16/LearnNotes/DLVOtheory/"/>
    <id>https://karobben.github.io/2024/08/16/LearnNotes/DLVOtheory/</id>
    <published>2024-08-16T21:27:54.000Z</published>
    <updated>2024-08-16T22:53:35.344Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DLVO-Theory">DLVO Theory</h2><table><thead><tr><th style="text-align:center"><img src="https://ars.els-cdn.com/content/image/3-s2.0-B0080431526016223-gr3.gif" alt="DLVO theory"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.sciencedirect.com/science/article/abs/pii/B0080431526016223">© J.H. Adair; 2001</a></td></tr></tbody></table><p>DLVO theory is named after Derjaguin, Landau, Verwey, and Overbeek, who developed it in the 1940s. It describes the forces between charged surfaces interacting through a liquid medium. The theory combines two main types of forces:</p><ol><li><strong>Van der Waals forces:</strong> These are attractive forces that arise from induced electrical interactions between molecules or atoms.</li><li><strong>Electrostatic double-layer forces:</strong> These are repulsive forces that occur due to the overlap of electrical double layers surrounding charged particles.</li></ol><p>The balance between these forces determines whether particles will aggregate (if the attractive forces dominate) or remain stable in suspension (if the repulsive forces dominate). This theory is widely used in colloid chemistry, environmental science, and materials science to understand and predict the stability of colloidal dispersions.</p><p>The DLVO theory describes the interaction energy $ U_{total} $ between two colloidal particles as the sum of the Van der Waals attraction $ U_{VdW} $ and the electrostatic repulsion $ U_{elec} $. The general form of the DLVO potential is given by:</p><p>$$ U_{total}(h) = U_{VdW}(h) + U_{elec}(h) $$</p><p>where $ h $ is the distance between the surfaces of the particles.</p><h3 id="Van-der-Waals-Attraction-U-VdW">Van der Waals Attraction ($ U_{VdW} $)</h3><p>The Van der Waals attraction energy between two spherical particles of radius $ R $ at a separation distance $ h $ is given by:</p><p>$$ U_{VdW}(h) = - \frac{A}{6} \left( \frac{2R^2}{h(2R + h)} + \frac{2R^2}{(2R + h)^2} + \ln \left( \frac{h}{2R + h} \right) \right) $$</p><ul><li>$ A $: Hamaker constant (typically around $ 10^{-20} $ J for biological systems)</li><li>$ R $: Radius of the amino acid (approx. $ 0.5 $ nm or $ 0.5 \times 10^{-9} $ m)</li><li>$ h $: Separation distance between the particles</li></ul><div class="admonition note"><p class="admonition-title">What if the atoms are different?</p><p>$R^2= R_1 * R_2$$2R = R_1 + R_2$</p></div><h3 id="Electrostatic-Repulsion-U-elec">Electrostatic Repulsion ($ U_{elec} $)</h3><p>The electrostatic repulsion energy between two spherical particles with surface potential $ \psi_0 $ and radius $ R $ in a medium with Debye length $ \kappa^{-1} $ (which is related to the ionic strength of the medium) is given by:</p><p>$$ U_{elec}(h) = 2 \pi \epsilon R \psi_0^2 \ln \left( 1 + \exp(-\kappa h) \right) $$</p><ul><li>$ \epsilon $: Permittivity of the medium (water, typically $ 80 \times 8.854 \times 10^{-12} $ F/m)</li><li>$ \psi_0 $: Surface potential (approx. $ 25 $ mV or $ 25 \times 10^{-3} $ V)</li><li>$ \kappa $: Inverse Debye length (for a Debye length of $ 1 $ nm, $ \kappa \approx 10^9 $ m$^{-1}$)</li><li>$ h $: Separation distance between the particles</li></ul><h3 id="Total-Interaction-Energy">Total Interaction Energy</h3><p>Combining these two expressions, the total interaction energy is:</p><p>$$ U_{total}(h) = - \frac{A}{6} \left( \frac{2R^2}{h(2R + h)} + \frac{2R^2}{(2R + h)^2} + \ln \left( \frac{h}{2R + h} \right) \right) + 2 \pi \epsilon R \psi_0^2 \ln \left( 1 + \exp(-\kappa h) \right) $$</p><p>This equation allows us to predict whether the colloidal particles will repel each other and remain stable in suspension or attract each other and aggregate, depending on the balance of the attractive and repulsive forces.</p><h3 id="Separation-Distance-h">Separation Distance (h)</h3><ul><li>If the radii $ R_1 $ and $ R_2 $ of two spherical particles are known, and the center-to-center distance between them is $ D $, the separation distance $ h $ is calculated as:<br>$$<br>h = D - (R_1 + R_2)<br>$$</li><li>For identical particles with the same radius $ R $, it simplifies to:<br>$$<br>h = D - 2R<br>$$</li></ul><p>In the code snippet provided, the parameters can be categorized into <strong>constant parameters</strong> (those that remain the same across different residues) and <strong>variable parameters</strong> (those that may change depending on the specific residues or the system under consideration).</p><h2 id="Parameters">Parameters</h2><h3 id="Constant-Parameters">Constant Parameters:</h3><ol><li><p><strong>$ A $</strong> (Hamaker constant):</p><ul><li><strong>Value:</strong> $ 1 \times 10^{-20} $ J</li><li><strong>Description:</strong> This is a material-specific constant that depends on the nature of the interacting particles and the medium. For biological molecules in water, it’s often taken as a constant.</li></ul></li><li><p><strong>$ \epsilon $</strong> (Permittivity of the medium):</p><ul><li><strong>Value:</strong> $ 80 \times 8.854 \times 10^{-12} $ F/m (Permittivity of water)</li><li><strong>Description:</strong> The permittivity of the medium (usually water in biological contexts) is a constant based on the dielectric properties of the solvent.</li></ul></li><li><p><strong>$ \kappa $</strong> (Inverse Debye length):</p><ul><li><strong>Value:</strong> $ 1 \times 10^9 $ m$^{-1}$</li><li><strong>Description:</strong> The inverse Debye length is related to the ionic strength of the medium and is often considered constant under specific conditions, such as physiological ionic strength.</li></ul></li></ol><h3 id="Variable-Parameters">Variable Parameters:</h3><ol><li><p><strong>$ R $</strong> (Radius of the amino acid):</p><ul><li><strong>Value:</strong> $ 0.5 \times 10^{-9} $ m (0.5 nm)</li><li><strong>Description:</strong> The radius could vary slightly between different amino acids, especially when considering side chains. The value used here is an approximation and might need adjustment for specific residues.</li></ul></li><li><p><strong>$ \psi_0 $</strong> (Surface potential):</p><ul><li><strong>Value:</strong> $ 25 \times 10^{-3} $ V (25 mV)</li><li><strong>Description:</strong> The surface potential can vary depending on the charge state of the amino acid side chains. For example, charged residues like lysine or aspartic acid will have different surface potentials compared to neutral residues like alanine.</li></ul></li><li><p><strong>$ h $</strong> (Separation distance):</p><ul><li><strong>Value:</strong> Range from $ 0.1 \times 10^{-9} $ m to $ 10 \times 10^{-9} $ m</li><li><strong>Description:</strong> The separation distance between two residues or atoms is the primary variable in these calculations, often determined by the 3D structure of the protein or molecular complex being studied.</li></ul></li></ol><h2 id="Who-to-Know-the-R-and-h">Who to Know the R and h?</h2><p>To calculate the radius of amino acids such as valine (V) and phenylalanine (F), you’re generally referring to an approximation of the <strong>van der Waals (VDW) radius</strong> or the <strong>effective radius</strong> of the entire amino acid side chain. This radius can be used in models like DLVO theory to represent the size of the interacting particle.</p><h3 id="Methods-to-Determine-the-Radius">Methods to Determine the Radius:</h3><ol><li><p><strong>Van der Waals Radius of Atoms:</strong></p><ul><li>The van der Waals radius is an inherent property of atoms and can be summed up to approximate the radius of a molecule or side chain. For example, the VDW radius for carbon is about 1.7 Å, and for hydrogen, it’s about 1.2 Å.</li></ul></li><li><p><strong>Effective Radius from Crystal Structures:</strong></p><ul><li>If you have a crystal structure or molecular model, you can measure the effective radius of the side chain by considering the spatial extent of the side chain atoms. This is often done using software tools that can calculate the solvent-accessible surface area (SASA) or by directly measuring distances in a molecular viewer.</li></ul></li><li><p><strong>Using Approximate Values from Literature:</strong></p><ul><li>For many applications, approximate radii for amino acids are available in the literature based on their typical side chain sizes.</li></ul></li></ol><h3 id="Approximate-Radii-for-Valine-V-and-Phenylalanine-F">Approximate Radii for Valine (V) and Phenylalanine (F):</h3><ul><li><p><strong>Valine (V):</strong></p><ul><li>Valine has a branched, non-polar side chain. Its effective radius is often approximated as <strong>~3.0 Å (0.3 nm)</strong>.</li></ul></li><li><p><strong>Phenylalanine (F):</strong></p><ul><li>Phenylalanine has a larger, aromatic side chain. Its effective radius is typically around <strong>~3.5-4.0 Å (0.35-0.4 nm)</strong>.</li></ul></li></ul><p>These values are not exact but are generally used in theoretical calculations.</p><h3 id="Calculation-Example">Calculation Example:</h3><p>If you need to calculate the interaction between valine (V) and phenylalanine (F), you could use these approximate radii:</p><ul><li><strong>Valine (V):</strong> $ R_V \approx 0.3 $ nm</li><li><strong>Phenylalanine (F):</strong> $ R_F \approx 0.35 $ nm</li></ul><p>The separation distance $ h $ would then be calculated based on the center-to-center distance $ D $ between the residues:</p><p>$$<br>h = D - (R_V + R_F)<br>$$</p><p>If $ D $ is known (from a crystal structure or a model), this formula gives the separation distance $ h $ between the residues’ surfaces.</p><h3 id="Tools-for-More-Accurate-Measurements">Tools for More Accurate Measurements:</h3><ul><li><strong>Molecular Visualization Software (e.g., PyMOL, Chimera):</strong> You can load a protein structure and measure the distance between specific atoms or calculate the van der Waals surface.</li><li><strong>Computational Tools:</strong> Software packages like CHARMM, AMBER, or GROMACS can provide detailed calculations based on molecular dynamics or energy minimization, giving more precise values for radii in specific contexts.</li></ul><h2 id="Python-Script">Python Script</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">van_der_waals</span>(<span class="hljs-params">h, A, R</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Calculate Van der Waals attraction energy.&quot;&quot;&quot;</span><br>    term1 = (<span class="hljs-number">2</span> * R**<span class="hljs-number">2</span>) / (h * (<span class="hljs-number">2</span> * R + h))<br>    term2 = (<span class="hljs-number">2</span> * R**<span class="hljs-number">2</span>) / (<span class="hljs-number">2</span> * R + h)**<span class="hljs-number">2</span><br>    term3 = np.log(h / (<span class="hljs-number">2</span> * R + h))<br>    <span class="hljs-keyword">return</span> - (A / <span class="hljs-number">6</span>) * (term1 + term2 + term3)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">electrostatic_repulsion</span>(<span class="hljs-params">h, epsilon, R, psi_0, kappa</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Calculate electrostatic repulsion energy.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * np.pi * epsilon * R * psi_0**<span class="hljs-number">2</span> * np.log(<span class="hljs-number">1</span> + np.exp(-kappa * h))<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dlvo_total</span>(<span class="hljs-params">h, A, R, epsilon, psi_0, kappa</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Calculate total DLVO interaction energy.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> van_der_waals(h, A, R) + electrostatic_repulsion(h, epsilon, R, psi_0, kappa)<br><br><span class="hljs-comment"># Parameters</span><br>A = <span class="hljs-number">1e-20</span>  <span class="hljs-comment"># Hamaker constant in J</span><br>R = <span class="hljs-number">1e-7</span>   <span class="hljs-comment"># Radius of particles in m</span><br>epsilon = <span class="hljs-number">80</span> * <span class="hljs-number">8.854e-12</span>  <span class="hljs-comment"># Permittivity of water in F/m</span><br>psi_0 = <span class="hljs-number">25e-3</span>  <span class="hljs-comment"># Surface potential in V</span><br>kappa = <span class="hljs-number">1e8</span>    <span class="hljs-comment"># Inverse Debye length in 1/m</span><br>h = np.linspace(<span class="hljs-number">5e-10</span>, <span class="hljs-number">1e-7</span>, <span class="hljs-number">400</span>)  <span class="hljs-comment"># Separation distance in m</span><br><br><span class="hljs-comment"># Calculate DLVO potential</span><br>U_vdw = van_der_waals(h, A, R)<br>U_elec = electrostatic_repulsion(h, epsilon, R, psi_0, kappa)<br>U_total = dlvo_total(h, A, R, epsilon, psi_0, kappa)<br><br><span class="hljs-comment"># Plot results</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.plot(h * <span class="hljs-number">1e9</span>, U_vdw, label=<span class="hljs-string">&#x27;Van der Waals Attraction&#x27;</span>, linestyle=<span class="hljs-string">&#x27;-.&#x27;</span>)<br>plt.plot(h * <span class="hljs-number">1e9</span>, U_elec, label=<span class="hljs-string">&#x27;Electrostatic Repulsion&#x27;</span>, linestyle=<span class="hljs-string">&#x27;-.&#x27;</span>)<br>plt.plot(h * <span class="hljs-number">1e9</span>, U_total, label=<span class="hljs-string">&#x27;Total DLVO Potential&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Separation Distance (nm)&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Interaction Energy (J)&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;DLVO Theory Interaction Energy&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/GMxKF8T.png" alt=""></p><h2 id="Simplified-System">Simplified System</h2><h3 id="Empirical-Formula-in-the-Context-of-Your-Problem"><strong>Empirical Formula in the Context of Your Problem</strong></h3><p>A general empirical formula for the interaction energy $ \Delta G_{i,j} $ between two residues $ H_i $ and $ A_j $ might look like this:</p><p>$$<br>\Delta G_{i,j} = V_{\text{LJ}}(r_{ij}) + V_{\text{Coulomb}}(r_{ij})<br>$$</p><p>Where:</p><ul><li>$ r_{ij} $ is the distance between residue $ H_i $ and residue $ A_j $.</li><li>$ V_{\text{LJ}}(r_{ij}) $ represents the van der Waals interaction.</li><li>$ V_{\text{Coulomb}}(r_{ij}) $ represents the electrostatic interaction.</li></ul><h3 id="Lennard-Jones-Potential-van-der-Waals-Interactions"><strong>Lennard-Jones Potential (van der Waals Interactions)</strong></h3><p>The Lennard-Jones potential is a commonly used empirical formula to describe the van der Waals forces between two non-bonded atoms or molecules. It has the form:</p><p>$$<br>V_{\text{LJ}}( r ) = 4\epsilon \left[ \left(\frac{\sigma}{r}\right)^{12} - \left(\frac{\sigma}{r}\right)^{6} \right]<br>$$</p><ul><li>$ V_{\text{LJ}}( r ) $ is the potential energy as a function of distance $ r $ between two particles.</li><li>$ \epsilon $ is the depth of the potential well, representing the strength of the interaction.</li><li>$ \sigma $ is the distance at which the potential energy is zero (often related to the size of the atoms/molecules).</li><li>$ r $ is the distance between the two particles.</li></ul><p>The term $ \left(\frac{\sigma}{r}\right)^{12} $ represents the repulsive interaction at short distances (due to Pauli exclusion principle), and the term $ \left(\frac{\sigma}{r}\right)^{6} $ represents the attractive van der Waals forces at longer distances.</p><h3 id="Coulomb’s-Law-Electrostatic-Interactions"><strong>Coulomb’s Law (Electrostatic Interactions)</strong></h3><p>Coulomb’s law describes the electrostatic interaction between two charged particles:</p><p>$$<br>V_{\text{Coulomb}}( r ) = \frac{k_e \cdot q_1 \cdot q_2}{r}<br>$$</p><ul><li>$ V_{\text{Coulomb}}( r ) $ is the potential energy between two charges.</li><li>$ k_e $ is Coulomb’s constant ($ 8.9875 \times 10^9 , \text{N} \cdot \text{m}<sup>2/\text{C}</sup>2 $ in vacuum).</li><li>$ q_1 $ and $ q_2 $ are the charges of the two interacting particles.</li><li>$ r $ is the distance between the two charges.</li></ul><h3 id="Python-Script-v2">Python Script</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> prody <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Load the protein structure</span><br>structure = parsePDB(<span class="hljs-string">&#x27;your_structure.pdb&#x27;</span>)<br><br><span class="hljs-comment"># Select the side chains of the residues of interest</span><br>residue1_sidechain = structure.select(<span class="hljs-string">&#x27;resid 10 and sidechain&#x27;</span>)<br>residue2_sidechain = structure.select(<span class="hljs-string">&#x27;resid 20 and sidechain&#x27;</span>)<br><br><span class="hljs-comment"># Function to calculate interaction energy considering only side chains</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_interaction_energy</span>(<span class="hljs-params">residue1, residue2, cutoff=<span class="hljs-number">5.0</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Calculate the van der Waals and electrostatic interaction energy </span><br><span class="hljs-string">    between two residues&#x27; side chains using a simple empirical formula.</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    :param residue1: ProDy atom group for the first residue side chain</span><br><span class="hljs-string">    :param residue2: ProDy atom group for the second residue side chain</span><br><span class="hljs-string">    :param cutoff: Distance cutoff for interaction (in Å)</span><br><span class="hljs-string">    :return: Tuple of (vdW energy, electrostatic energy)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># van der Waals parameters (simplified example)</span><br>    epsilon = <span class="hljs-number">0.1</span>  <span class="hljs-comment"># Depth of the potential well (kcal/mol)</span><br>    sigma = <span class="hljs-number">3.5</span>  <span class="hljs-comment"># Distance at which the potential is zero (Å)</span><br>    <br>    <span class="hljs-comment"># Coulomb constant (for electrostatic energy calculation)</span><br>    k_e = <span class="hljs-number">8.9875517873681764e9</span>  <span class="hljs-comment"># N m² C⁻² (can be adjusted for unit compatibility)</span><br>    <br>    <span class="hljs-comment"># Simplified charges for electrostatic calculation</span><br>    charge1 = np.<span class="hljs-built_in">sum</span>([atom.getCharge() <span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> residue1])<br>    charge2 = np.<span class="hljs-built_in">sum</span>([atom.getCharge() <span class="hljs-keyword">for</span> atom <span class="hljs-keyword">in</span> residue2])<br>    <br>    vdW_energy = <span class="hljs-number">0.0</span><br>    electrostatic_energy = <span class="hljs-number">0.0</span><br><br>    <span class="hljs-comment"># Calculate pairwise interactions considering only side chains</span><br>    <span class="hljs-keyword">for</span> atom1 <span class="hljs-keyword">in</span> residue1:<br>        <span class="hljs-keyword">for</span> atom2 <span class="hljs-keyword">in</span> residue2:<br>            distance = np.linalg.norm(atom1.getCoords() - atom2.getCoords())<br>            <span class="hljs-keyword">if</span> distance &lt; cutoff:<br>                <span class="hljs-comment"># van der Waals energy (Lennard-Jones potential)</span><br>                vdW_energy += <span class="hljs-number">4</span> * epsilon * ((sigma / distance)**<span class="hljs-number">12</span> - (sigma / distance)**<span class="hljs-number">6</span>)<br>                <span class="hljs-comment"># Electrostatic energy (Coulomb&#x27;s law)</span><br>                electrostatic_energy += k_e * (charge1 * charge2) / distance<br>    <span class="hljs-keyword">return</span> vdW_energy, electrostatic_energy<br><br><span class="hljs-comment"># Calculate interaction energy</span><br>vdW_energy, electrostatic_energy = calculate_interaction_energy(residue1_sidechain, residue2_sidechain)<br>print(<span class="hljs-string">f&quot;van der Waals Energy: <span class="hljs-subst">&#123;vdW_energy:<span class="hljs-number">.2</span>f&#125;</span> kcal/mol&quot;</span>)<br>print(<span class="hljs-string">f&quot;Electrostatic Energy: <span class="hljs-subst">&#123;electrostatic_energy:<span class="hljs-number">.2</span>f&#125;</span> kcal/mol&quot;</span>)<br></code></pre></td></tr></table></figure></div><details><summary>Codes for the plot</summary><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">h = np.linspace(<span class="hljs-number">3.3</span>, <span class="hljs-number">10</span>, <span class="hljs-number">400</span>)  <span class="hljs-comment"># Separation distance in m</span><br><span class="hljs-comment"># Plot results</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.plot(h, [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> h], linestyle = <span class="hljs-string">&#x27;-.&#x27;</span>)<br>plt.plot(h, [calculate_interaction_energy(i)  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> h], color = <span class="hljs-string">&#x27;salmon&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Separation Distance (Åm)&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Interaction Energy&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Lennard-Jones Potential&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div></details><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/wmcPi2K.png" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">In this plot, it shows the change of the Lennard-Jones Potential with the change of the distance when the $\sigma = 3.5$</td></tr></tbody></table><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">DLVO theory describes the forces between charged surfaces interacting through a liquid medium. The theory combines two main types of forces</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="other" scheme="https://karobben.github.io/categories/Notes/other/"/>
    
    
    <category term="Physics" scheme="https://karobben.github.io/tags/Physics/"/>
    
  </entry>
  
  <entry>
    <title>Kernel Density Estimation (KDE)</title>
    <link href="https://karobben.github.io/2024/08/16/AI/KernelDensityEstimation/"/>
    <id>https://karobben.github.io/2024/08/16/AI/KernelDensityEstimation/</id>
    <published>2024-08-16T20:41:30.000Z</published>
    <updated>2024-10-09T23:01:25.526Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kernel-Density-Estimation-KDE">Kernel Density Estimation (KDE)</h2><p><strong>Kernel Density Estimation (KDE)</strong> is a non-parametric method to estimate the probability density function (PDF) of a random variable based on a finite set of data points. Unlike parametric methods, which assume that the underlying data follows a specific distribution (like normal, exponential, etc.), KDE makes no such assumptions and can model more complex data distributions.</p><h3 id="How-KDE-Works">How KDE Works:</h3><ol><li><p><strong>Kernel Function</strong>: The kernel function is a smooth, continuous, symmetric function that is centered on each data point. The most commonly used kernel is the Gaussian (normal) kernel, but other kernels like Epanechnikov, triangular, and uniform can also be used.</p></li><li><p><strong>Bandwidth (Smoothing Parameter)</strong>: The bandwidth is a crucial parameter that controls the smoothness of the KDE. It determines the width of the kernel functions. A smaller bandwidth leads to a more sensitive, less smooth estimate, while a larger bandwidth produces a smoother, less sensitive estimate.</p></li><li><p><strong>Summation of Kernels</strong>: KDE constructs the overall density estimate by summing the contributions of each kernel function across all data points. Each data point contributes a small “bump” to the estimate, and the sum of these bumps forms the estimated density function.</p></li></ol><h3 id="KDE-Formula">KDE Formula:</h3><p>Given a set of $ n $ data points $ x_1, x_2, \ldots, x_n $, the KDE at a point $ x $ is calculated as:</p><p>$$<br>\hat{f}(x) = \frac{1}{n \cdot h} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)<br>$$</p><p>Where:</p><ul><li>$ \hat{f}(x) $ is the estimated density at point $ x $.</li><li>$ n $ is the number of data points.</li><li>$ h $ is the bandwidth.</li><li>$ K $ is the kernel function.</li><li>$ x_i $ are the observed data points.</li></ul><h3 id="Example-of-KDE">Example of KDE:</h3><p>Imagine you have a dataset of people’s heights. Rather than assuming the heights follow a specific distribution (like normal), KDE allows you to estimate the distribution directly from the data, which may reveal subtle features like bimodal distributions (e.g., a mix of two distinct groups).</p><h3 id="Advantages-of-KDE">Advantages of KDE:</h3><ul><li><strong>Flexible</strong>: KDE doesn’t assume any specific form of the distribution, making it suitable for complex and unknown distributions.</li><li><strong>Smooth Estimation</strong>: It provides a smooth estimate of the density function, which can be more informative than histograms.</li></ul><h3 id="Disadvantages-of-KDE">Disadvantages of KDE:</h3><ul><li><strong>Choice of Bandwidth</strong>: The performance of KDE heavily depends on the choice of bandwidth. Too small a bandwidth can lead to overfitting, while too large a bandwidth can oversmooth important features.</li><li><strong>Computationally Intensive</strong>: KDE can be computationally intensive, especially for large datasets and high-dimensional data.</li></ul><h3 id="Applications-of-KDE">Applications of KDE:</h3><ul><li><strong>Data Visualization</strong>: KDE is often used to visualize the distribution of data, particularly in one-dimensional and two-dimensional cases.</li><li><strong>Anomaly Detection</strong>: KDE can be used to detect outliers by identifying areas of low probability density.</li><li><strong>Density-Based Clustering</strong>: In clustering methods like DBSCAN, KDE can help define regions of high density.</li></ul><h2 id="How-Do-It-in-Python">How Do It in Python</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KernelDensity<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br><span class="hljs-comment"># Step 2: Prepare Your Data</span><br><span class="hljs-comment"># Example list of values</span><br>data_list = [<span class="hljs-number">1.2</span>, <span class="hljs-number">2.3</span>, <span class="hljs-number">2.4</span>, <span class="hljs-number">2.5</span>, <span class="hljs-number">3.1</span>, <span class="hljs-number">3.6</span>, <span class="hljs-number">3.8</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">4.2</span>, <span class="hljs-number">5.0</span>] * <span class="hljs-number">30</span><br><span class="hljs-comment"># Convert the list to a NumPy array and reshape it for the model</span><br>data = np.array(data_list).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-comment"># Step 3: Fit the Kernel Density Estimation Model</span><br><span class="hljs-comment"># Fit the KDE model</span><br>kde = KernelDensity(kernel=<span class="hljs-string">&#x27;gaussian&#x27;</span>, bandwidth=<span class="hljs-number">0.2</span>).fit(data)<br><span class="hljs-comment"># Step 4: (Optional) Plot the Estimated Density</span><br><span class="hljs-comment"># Define a range of values</span><br>x_range = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">6</span>, <span class="hljs-number">1000</span>).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-comment"># Estimate density for the entire range</span><br>log_density = kde.score_samples(x_range)<br>density = np.exp(log_density)<br><span class="hljs-comment"># Plot the density</span><br>sns.kdeplot(data_list, bw_adjust=<span class="hljs-number">0.5</span>)<br>plt.plot(x_range, density)<br>plt.title(<span class="hljs-string">&quot;Kernel Density Estimation&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Value&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Density&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure></div><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/Tr6nMEI.png" alt="KDE plot"></th></tr></thead><tbody><tr><td style="text-align:center"></td></tr></tbody></table><h2 id="Save-and-Load-the-Model">Save and Load the Model</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pickle<br><br><span class="hljs-comment"># Fit the KDE model (assuming you have already done this)</span><br><span class="hljs-comment"># kde = KernelDensity(kernel=&#x27;gaussian&#x27;, bandwidth=0.2).fit(data)</span><br><br><span class="hljs-comment"># Save the model to a file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;kde_model.pkl&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>    pickle.dump(kde, file)<br><br><span class="hljs-comment"># Load the model from the file</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;kde_model.pkl&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>    loaded_kde = pickle.load(file)<br><br><span class="hljs-comment"># Value for which you want to estimate the density</span><br>value = <span class="hljs-number">3.5</span><br><br><span class="hljs-comment"># Estimate the density using the loaded model</span><br>log_density = loaded_kde.score_samples([[value]])<br>density = np.exp(log_density)<br>print(<span class="hljs-string">f&quot;Density of the value <span class="hljs-subst">&#123;value&#125;</span> using loaded model: <span class="hljs-subst">&#123;density[<span class="hljs-number">0</span>]:<span class="hljs-number">.6</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Kernel Density Estimation (KDE)</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="Regression" scheme="https://karobben.github.io/categories/Machine-Learning/Regression/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Regression" scheme="https://karobben.github.io/tags/Regression/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Taylor Series and Its Applications in Machine Learning</title>
    <link href="https://karobben.github.io/2024/08/09/AI/taylorseries/"/>
    <id>https://karobben.github.io/2024/08/09/AI/taylorseries/</id>
    <published>2024-08-09T22:40:44.000Z</published>
    <updated>2024-10-09T23:03:07.711Z</updated>
    
    <content type="html"><![CDATA[<p>The Taylor Series is a fundamental mathematical tool that finds applications across various domains, including machine learning. In this post, we’ll explore what the Taylor Series is, how it is used in machine learning, and the significant impact it can have on <strong>optimizing</strong> machine learning models. Here are some good videos to explain the basic of the Taylor Series: <a href="https://www.youtube.com/watch?v=3d6DsjIBzJ4">Taylor series | Chapter 11, Essence of calculus</a>, <a href="https://www.youtube.com/watch?v=LkLVMJQAj6A">Visualization of the Taylor Series</a>, <a href="https://www.youtube.com/watch?v=EYjBnnUJTP8">3 Applications of Taylor Series: Integrals, Limits, &amp; Series</a>, and <a href="https://www.youtube.com/watch?v=eX1hvWxmJVE">Dear Calculus 2 Students, This is why you’re learning Taylor Series</a></p><h2 id="What-is-the-Taylor-Series"><strong>What is the Taylor Series?</strong></h2><p>The Taylor Series is a mathematical concept that allows us to <strong>approximate complex functions</strong> using an infinite sum of terms, calculated from the derivatives of the function at a specific point. It essentially breaks down a function into a <strong>polynomial</strong> that closely approximates the function near a given point.</p><p>The general formula for the Taylor Series of a function $ f(x) $ around a point $ a $ is:</p><p>$$<br>f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!} (x - a)^n<br>$$</p><p>Where $ f^{(n)}(a) $ represents the $ n $-th derivative of $ f(x) $ at point $ a $, and $ n! $ is the factorial of $ n $.</p><p>This approximation is particularly useful when dealing with functions that are difficult to compute directly, as it allows us to work with simpler polynomial expressions instead.</p><h2 id="Taylor-Series-for-the-Cosine-Function"><strong>Taylor Series for the Cosine Function</strong></h2><p><img src="https://imgur.com/20cgEEk.png" alt="Taylor Series for the Cosine Function"></p><p>The cosine function, $ \cos(x) $, is a smooth and periodic function (<mark>line in black</mark>) that oscillates between -1 and 1. While it can be computed directly using trigonometric tables or built-in functions in programming languages, these computations can be resource-intensive, especially for small embedded systems or in scenarios requiring real-time processing.</p><p>The Taylor Series provides a way to approximate $ \cos(x) $ using a polynomial expansion around a specific point, typically $ x = 0 $ (the Maclaurin series, a special case of the Taylor Series). The Taylor Series for $ \cos(x) $ is given by:</p><p>$$<br>\cos(x) = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \dots<br>$$</p><p>This series can be written as:</p><p>$$<br>\cos(x) = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!} x^{2n}<br>$$</p><p>Where:</p><ul><li>$ (-1)^n $ alternates the sign of each term.</li><li>$ (2n)! $ is the factorial of $ 2n $, ensuring that the series converges.</li><li>$ x^{2n} $ means that only even powers of $ x $ are used, reflecting the symmetry of the cosine function.</li></ul><h3 id="How-the-Approximation-Works"><strong>How the Approximation Works</strong></h3><p>The beauty of the Taylor Series lies in its ability to approximate $ \cos(x) $ with just a few terms, depending on the desired accuracy.</p><ul><li><p><strong>First Few Terms</strong>: If you only take the first two terms (up to $ x^2 $), the approximation is:<br>$$<br>\cos(x) \approx 1 - \frac{x^2}{2}<br>$$<br>This provides a reasonable approximation for $ \cos(x) $ when $ x $ is close to 0, capturing the initial downward curve of the cosine function.</p></li><li><p><strong>Adding More Terms</strong>: As you include more terms (e.g., $ x^4 $, $ x^6 $), the approximation becomes increasingly accurate, even for values of $ x $ further from 0. Each additional term refines the curve, making the polynomial more closely match the actual cosine function.</p></li></ul><h3 id="Practical-Use-and-Computational-Benefits"><strong>Practical Use and Computational Benefits</strong></h3><p>In practical applications, such as in computer graphics, signal processing, or physics simulations, using the Taylor Series to approximate $ \cos(x) $ can significantly reduce computational cost. Instead of performing the full trigonometric calculation, which might involve iterative or complex operations, a system can compute a few polynomial terms, which are far less demanding.</p><ul><li><p><strong>Example</strong>: In embedded systems where processing power is limited, calculating $ \cos(x) $ using the Taylor Series with just a few terms can save time and energy, which is crucial in battery-powered devices.</p></li><li><p><strong>Trade-off</strong>: There is always a trade-off between the number of terms used and the accuracy of the approximation. For most practical purposes, using 4 to 6 terms provides a good balance between accuracy and computational efficiency.</p></li></ul><h3 id="Beyond-Cosine-General-Use-in-Trigonometric-Functions"><strong>Beyond Cosine: General Use in Trigonometric Functions</strong></h3><p>The approach used to approximate $ \cos(x) $ can also be applied to other trigonometric functions like $ \sin(x) $ and $ \tan(x) $. Each of these functions has its own Taylor Series expansion, enabling similar approximations and computational savings.</p><h2 id="Applications-of-Taylor-Series-in-Machine-Learning"><strong>Applications of Taylor Series in Machine Learning</strong></h2><p>While the Taylor Series is a powerful mathematical tool on its own, its applications in machine learning are particularly noteworthy, especially in the context of optimization algorithms and model behavior analysis.</p><h3 id="1-Gradient-Descent-and-Optimization"><strong>1. Gradient Descent and Optimization</strong></h3><p>In machine learning, gradient descent is a widely used optimization technique that minimizes a loss function by iteratively adjusting model parameters. The Taylor Series plays a crucial role in understanding and improving this process.</p><ul><li><p><strong>Basic Gradient Descent</strong>:</p><ul><li>Gradient descent uses the first-order Taylor approximation of the loss function to update parameters. However, the basic gradient descent approach can be slow and sensitive to the choice of the learning rate, often requiring careful tuning to avoid issues like overshooting or slow convergence.</li></ul></li><li><p><strong>Newton’s Method Using Taylor Series</strong>:</p><ul><li>By incorporating the second-order Taylor expansion of the loss function, Newton’s method leverages the Hessian matrix (a matrix of second derivatives) to make more informed updates. This results in faster and more stable convergence, especially near the optimum, although it comes at the cost of increased computational complexity.</li></ul></li></ul><p><strong>Before vs. After Applying the Taylor Series</strong>:</p><ul><li><strong>Before</strong>: Gradient descent can be slow and sensitive, requiring many iterations to reach a solution.</li><li><strong>After</strong>: Newton’s method, using the Taylor Series, accelerates convergence and provides more stability, particularly in challenging optimization landscapes.</li></ul><h3 id="2-Understanding-Model-Behavior"><strong>2. Understanding Model Behavior</strong></h3><p>The Taylor Series also helps in linearizing non-linear models, which is essential for understanding how small changes in input features affect the model’s output.</p><ul><li><strong>Linearization of Non-Linear Models</strong>: By approximating non-linear functions (like activation functions in neural networks) with a Taylor Series, we can analyze the local behavior of these functions. This is particularly useful for sensitivity analysis, where understanding the impact of small input perturbations is crucial for model robustness.</li></ul><h3 id="3-Regularization-and-Generalization"><strong>3. Regularization and Generalization</strong></h3><p>Regularization techniques, which are used to prevent overfitting in machine learning models, can also be viewed through the lens of the Taylor Series. By penalizing higher-order terms in the Taylor expansion, regularization methods like L2 regularization (Ridge) help in controlling model complexity and improving generalization.</p><h2 id="Real-World-Example-Logistic-Regression-and-Taylor-Series"><strong>Real-World Example: Logistic Regression and Taylor Series</strong></h2><p>To illustrate the practical application of the Taylor Series in machine learning, consider a logistic regression model used to classify emails as spam or not. The model uses a sigmoid function to predict probabilities, and the goal is to minimize the binary cross-entropy loss function.</p><ul><li><p><strong>Without Taylor Series</strong>: Using basic gradient descent, the model may take many iterations to converge, with convergence being highly dependent on the chosen learning rate.</p></li><li><p><strong>With Taylor Series (Newton’s Method)</strong>: By applying the Taylor Series, specifically the second-order approximation, the model can achieve faster and more stable convergence, even if each iteration is more computationally intensive.</p></li></ul><p>In this case, applying the Taylor Series through Newton’s method can drastically reduce the number of iterations required to reach an optimal solution, highlighting the power of this mathematical tool in machine learning optimization.</p><h2 id="Conclusion"><strong>Conclusion</strong></h2><p>The Taylor Series is more than just a mathematical concept; it’s a powerful tool that underpins several key techniques in machine learning. From optimizing models with gradient descent to understanding the behavior of complex functions, the Taylor Series enables us to make more accurate and efficient decisions in model training and evaluation. Whether you’re dealing with logistic regression or deep learning, understanding and applying the Taylor Series can significantly enhance your machine learning practice.</p><p>By incorporating second-order information through the Taylor Series, you can achieve faster convergence, better stability, and a deeper understanding of your models, ultimately leading to more robust and effective machine learning solutions.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">The Taylor Series is a mathematical tool that approximates complex functions with polynomials, playing a crucial role in machine learning optimization. It enhances gradient descent by incorporating second-order information, leading to faster and more stable convergence. Additionally, it aids in linearizing non-linear models and informs regularization techniques. This post explores the significance of the Taylor Series in improving model training efficiency and understanding model behavior. $$\cos(x) = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!} x^{2n}$$</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="Math" scheme="https://karobben.github.io/categories/Machine-Learning/Math/"/>
    
    
    <category term="Math" scheme="https://karobben.github.io/tags/Math/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>FoldX</title>
    <link href="https://karobben.github.io/2024/07/06/Bioinfor/foldx/"/>
    <id>https://karobben.github.io/2024/07/06/Bioinfor/foldx/</id>
    <published>2024-07-06T22:11:38.000Z</published>
    <updated>2024-07-23T02:49:21.088Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Run-the-FoldX">Run the FoldX</h2><p>In this example, I am using the <strong>7ekb</strong> as example</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># download the pdb</span><br>wget https://files.rcsb.org/view/7ekb.pdb<br><span class="hljs-comment"># Repair the PDB. After repaired it, you&#x27;ll get the 7ekb_Repair.pdb for the next step</span><br>FoldX --<span class="hljs-built_in">command</span>=RepairPDB --pdb=7ekb.pdb<br><span class="hljs-comment"># calculate the free energy of the PDB  </span><br>FoldX --<span class="hljs-built_in">command</span>=Stability  --pdb=7ekb_Repair.pdb<br></code></pre></td></tr></table></figure></div><h2 id="RepairPDB">RepairPDB</h2><h3 id="Why-Repair-PDB">Why Repair PDB?</h3><p>According to ChatGPT4o, <code>RepairPDB</code> command in FoldX is a crucial step to ensure the quality and integrity of your PDB file before performing stability calculations or other analyses. Also, you could found more information from <a href="https://foldxsuite.crg.eu/command/RepairPDB">document</a></p><ol><li><p><strong>Fix Structural Issues</strong>:</p><ul><li><strong>Correcting Errors</strong>: PDB files obtained from experiments like X-ray crystallography or cryo-EM often have missing atoms, residues, or other structural issues that can affect downstream analyses. <code>RepairPDB</code> fixes these issues to ensure a complete and accurate structure.</li><li><strong>Adding Missing Atoms</strong>: The command can add missing atoms, such as hydrogen atoms, which are essential for energy calculations.</li></ul></li><li><p><strong>Standardizing the Structure</strong>:</p><ul><li><strong>Normalization</strong>: <code>RepairPDB</code> standardizes the structure to ensure that all residues and atoms are in the correct format and positions. This includes correcting bond lengths and angles to standard values.</li><li><strong>Removing Non-standard Residues</strong>: It can remove or correct non-standard residues and ligands that might interfere with calculations.</li></ul></li><li><p><strong>Improving Energy Calculations</strong>:</p><ul><li><strong>Optimizing Geometry</strong>: The command optimizes the geometry of the protein, ensuring that the atomic positions are energetically favorable. This leads to more accurate stability and free energy calculations.</li><li><strong>Minimizing Steric Clashes</strong>: It identifies and resolves steric clashes (where atoms are too close to each other), which can distort energy calculations.</li></ul></li><li><p><strong>Ensuring Compatibility</strong>:</p><ul><li><strong>Consistency</strong>: Running <code>RepairPDB</code> ensures that your PDB file is compatible with FoldX’s algorithms, reducing the risk of errors during subsequent steps.</li></ul></li></ol><div class="admonition question"><p class="admonition-title">How does the Output looks like?</p></div><pre>Residue LYSH222 has high Energy, we mutate it to itselfRepair Residue ID= LYSH222BackHbond       =               -317.22SideHbond       =               -137.87Energy_VdW      =               -476.42Electro         =               -15.23Energy_SolvP    =               628.80Energy_SolvH    =               -624.90Energy_vdwclash =               15.60energy_torsion  =               9.33backbone_vdwclash=              143.43Entropy_sidec   =               245.04Entropy_mainc   =               632.72water bonds     =               0.00helix dipole    =               -0.35loop_entropy    =               0.00cis_bond        =               4.50disulfide       =               -13.95kn electrostatic=               -0.25partial covalent interactions = 0.00Energy_Ionisation =             1.07Entropy Complex =               0.00-----------------------------------------------------------Total          =   -49.10</pre><p>It took me <mark>2m 48s</mark>. It only work in single thread and cannot move on multiple threads. I guess because it works by following the order of the AA and the <code>Total</code> is depending on the previous values. So, it can’t work on multiple threads.</p><p>Here is the result of before and after repairing. The RMS=0.01 which means it almost the same. But the slightly different are mainly focus on the loos area. In the picture present below, the left panel with green structure is the raw pdb file from PDB database. The light blue structure on the right is the corrected by FoldX. Red structure is antigen. As I marked on the left panel, 2 beta-sheets and 1 alpha helix are deleted and become random loop. Those area from the antibody are very closing to the antigen. So, technically, random loop would make more sense to me.</p><p><img src="https://imgur.com/i8aPrTy.png" alt=""></p><h2 id="Stability-Calculations">Stability Calculations</h2><p>After repaired the PDB file, you can get the result immediately.</p><pre>   ********************************************   ***                                      ***   ***             FoldX 5.1 (c)            ***   ***                                      ***   ***     code by the FoldX Consortium     ***   ***                                      ***   ***     Jesper Borg, Frederic Rousseau   ***   ***    Joost Schymkowitz, Luis Serrano   ***   ***    Peter Vanhee, Erik Verschueren    ***   ***     Lies Baeten, Javier Delgado      ***   ***       and Francois Stricher          ***   *** and any other of the 9! permutations ***   ***   based on an original concept by    ***   ***   Raphael Guerois and Luis Serrano   ***   ********************************************Stability >>>1 models read: 7ekb_Repair.pdbBackHbond       =               -332.04SideHbond       =               -163.29Energy_VdW      =               -481.14Electro         =               -17.42Energy_SolvP    =               626.91Energy_SolvH    =               -633.28Energy_vdwclash =               13.20energy_torsion  =               9.65backbone_vdwclash=              144.57Entropy_sidec   =               259.27Entropy_mainc   =               634.24water bonds     =               0.00helix dipole    =               -0.40loop_entropy    =               0.00cis_bond        =               4.50disulfide       =               -13.95kn electrostatic=               -0.41partial covalent interactions = 0.00Energy_Ionisation =             1.14Entropy Complex =               0.00-----------------------------------------------------------Total          =   -93.01FINISHING STABILITY ANALYSIS OPTIONYour file run OKEnd time of FoldX: Sat Jul  6 17:23:18 2024Total time spend: 0.85 seconds.</pre><h2 id="Mutation-Energy-Change-Calculation">Mutation Energy Change Calculation</h2><p>With FoldX, you can predicted the mutations effects when you have the wild type structure. The command <code>BuildModel</code> could generate the new pdb structure with the ‘mutate_file’ you write. Here is an example of <code>mutate_file</code>:</p><pre>AA4P,FD4P;AA4F,QD4F;</pre><p>In this example, it would generate 2 new structures. For the first one, in chain A, the mutation is A4P, in chain D, the mutation is F4P. The “;” means the first mutate process is done. It would read the second line to create the another mutation file. You don’t need to calculate the mutation energy difference between before and after again. Because all they are saved in the file (*.fxout) as <code>tsv</code> format.</p><p>For running it, you just need to run like:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">FoldX --<span class="hljs-built_in">command</span>=BuildModel --pdb=protein_Repair.pdb --mutant-file=individual_list.txt<br></code></pre></td></tr></table></figure></div><p>An interesting thing is for the mutate-file, it has to start with “individual_list” or ‘mutate_file’ or you’ll get error. After the job is down, you’ll got 4 outputs: “Average_<em>.fxout&quot;, &quot;Dif_</em>.fxout”, “PdbList_<em>.fxout&quot;, and &quot;Raw_</em>.fxout”. Here is some details about those outputs:</p><ol><li><p><strong>Average_*.fxout</strong>:</p><ul><li>This file contains the average energy values from multiple runs of the BuildModel command. FoldX often performs multiple simulations to generate an average value to improve reliability and account for variability in the calculations.</li><li>It includes averaged energy terms like van der Waals interactions, electrostatics, solvation, and total energy for the mutated model.</li></ul></li><li><p><strong>Dif_*.fxout</strong>:</p><ul><li>This file contains the differences in energy values between the wild-type and mutated proteins.</li><li>It shows the ΔΔG (difference in free energy change) due to the introduced mutation(s), which helps in understanding the stability change caused by the mutation.</li></ul></li><li><p><strong>PdbList_*.fxout</strong>:</p><ul><li>This file lists the PDB files generated during the mutation process.</li><li>It includes the names of the mutated PDB files that FoldX generated, which you can further analyze or visualize using molecular visualization tools.</li></ul></li><li><p><strong>Raw_*.fxout</strong>:</p><ul><li>This file contains the raw energy values for each individual run of the BuildModel command.</li><li>It provides detailed energy components for each simulation, such as van der Waals interactions, electrostatics, solvation, and other energy terms for the mutated model.</li></ul></li></ol><div class="admonition note"><p class="admonition-title">Notice</p><ol><li>For the <code>mutate_file</code>, you can't add any extra expressions like space in it.</li><li>According to the <a href="https://pdb101.rcsb.org/learn/guide-to-understanding-pdb-data/protein-primary-sequences">PDB document</a>, the SEQRES and ATOM records may include only a portion of the molecule. So if your sequence was extracted from the PDB file, the numbering of it may incorrect. For achieve the correct position, you may like to extract extract the sequence from cif format.</li></ol></div><h3 id="BuildModel-in-Action">BuildModel in Action</h3><p>Here, the test data is from Qi Wen Teo<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> CR9114 (<strong>4FQI</strong>) as example. I randomly choose 4FQI as the standard. In the paper, they mutated the resi through 93 to 102 (kabat numbering) which is 97 to 110. So, we could do it with a <code>mutate_file</code>. For FoldX, it only recognize the digital numbering. But in antibody (show below) sometimes was numbered by kabat numbering or something similar methods. So it may contain numbering like 100A, 100B, etc. They can’t recognized by FoldX and we need to renumbering them. Pymol is very complicated in this kind of task. But Biopython could handle it very well. You could using the script from <a href="https://github.com/Karobben/Bio_tools">Karobben/Bio_tools</a> with code: <code>python PDBreNumbering.py -i 4FQI_Repair.pdb -o renumbered.pdb</code></p><pre>ATOM   4942  CD1 TYR H 100     -17.095  54.149 -23.690  1.00 17.83      C    C  ATOM   4943  CD2 TYR H 100     -15.503  54.016 -21.927  1.00 17.51      C    C  ATOM   4944  CE1 TYR H 100     -16.055  54.246 -24.606  1.00 18.76      C    C  ATOM   4945  CE2 TYR H 100     -14.431  54.115 -22.848  1.00 20.56      C    C  ATOM   4946  CZ  TYR H 100     -14.740  54.248 -24.173  1.00 21.30      C    C  ATOM   4947  OH  TYR H 100     -13.735  54.378 -25.146  1.00 22.06      C    O  ATOM   4948  N   TYR H 100A    -19.396  56.114 -18.971  1.00 19.55      C    N  ATOM   4949  CA  TYR H 100A    -20.277  56.072 -17.797  1.00 21.40      C    C  ATOM   4950  C   TYR H 100A    -21.609  56.741 -18.067  1.00 25.46      C    C  ATOM   4951  O   TYR H 100A    -22.655  56.288 -17.527  1.00 25.98      C    O  ATOM   4952  CB  TYR H 100A    -19.611  56.821 -16.587  1.00 18.28      C    C  ATOM   4953  CG  TYR H 100A    -18.192  56.412 -16.276  1.00 19.12      C    C  ATOM   4954  CD1 TYR H 100A    -17.753  55.092 -16.396  1.00 21.46      C    C  </pre><p>Script to create the <code>mutate_file</code>. In this script, the target region is from number 97-110 and the sequence is “ARHGNYYYYSGMDV”.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">WT = <span class="hljs-built_in">list</span>(<span class="hljs-string">&quot;ARHGNYYYYSGMDV&quot;</span>)<br>All20 = <span class="hljs-built_in">list</span>(<span class="hljs-string">&quot;ARNDCEQGHILKMFPSTWYV&quot;</span>)<br>Num = <span class="hljs-number">96</span><br>sublist = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> WT:<br>    Num += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> ii <span class="hljs-keyword">in</span> All20:<br>        <span class="hljs-keyword">if</span> i != ii:<br>             sublist += [<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;i&#125;</span>H<span class="hljs-subst">&#123;Num&#125;</span><span class="hljs-subst">&#123;ii&#125;</span>;&quot;</span>]<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;individual_list.txt&quot;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> F:<br>    F.write(<span class="hljs-string">&quot;\n&quot;</span>.join(sublist))<br></code></pre></td></tr></table></figure></div><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># repair the PDB</span><br>FoldX --<span class="hljs-built_in">command</span>=RepairPDB --pdb=4FQI.pdb<br><span class="hljs-comment"># renumbering the resi for FoldX</span><br>python PDBreNumbering.py -i 4FQI_Repair.pdb -o renumbered.pdb<br><span class="hljs-comment"># calculate the results </span><br>FoldX --<span class="hljs-built_in">command</span>=BuildModel --pdb=renumbered.pdb --mutant-file=individual_list.txt<br></code></pre></td></tr></table></figure></div><p>After that, the result is saved in the file <code>Raw_renumbered.fxout</code>. The table was started at line 9. We could use R to sorting and compare the experiment result. For the experiment result, you can download from <a href="https://github.com/nicwulab/CR9114_LC_CDRH3_screen/blob/main/result/CDRH3_KD_table_summary.csv">nicwulab/CR9114_LC_CDRH3_screen</a></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="R"><figure class="iseeu highlight /r"><table><tr><td class="code"><pre><code class="hljs r">library(ggplot2)<br>library(reshape2)<br>library(stringr)<br><br>TB &lt;- read.csv(<span class="hljs-string">&#x27;Raw_renumbered.fxout&#x27;</span>, skip = <span class="hljs-number">8</span>, sep = <span class="hljs-string">&#x27;\t&#x27;</span>)<br>TB$Type &lt;- <span class="hljs-string">&quot;Mute&quot;</span><br>TB$Type[grep(<span class="hljs-string">&quot;WT_&quot;</span>, TB$Pdb)] &lt;- <span class="hljs-string">&quot;WT&quot;</span><br>TB &lt;- TB[<span class="hljs-built_in">c</span>(<span class="hljs-string">&#x27;Pdb&#x27;</span>, <span class="hljs-string">&#x27;total.energy&#x27;</span>, <span class="hljs-string">&#x27;Type&#x27;</span>)]<br>TB$Pdb &lt;- str_remove(TB$Pdb, <span class="hljs-string">&quot;WT_&quot;</span>)<br>TBM &lt;- reshape(TB, idvar = <span class="hljs-string">&#x27;Pdb&#x27;</span>, timevar = <span class="hljs-string">&#x27;Type&#x27;</span>, direction = <span class="hljs-string">&#x27;wide&#x27;</span>)<br>colnames(TBM) &lt;- str_remove(colnames(TBM), <span class="hljs-string">&#x27;total.energy.&#x27;</span>)<br><br>Anno &lt;- read.csv(<span class="hljs-string">&#x27;individual_list.txt&#x27;</span>, header = <span class="hljs-built_in">F</span>)<br>TBM$Anno &lt;- str_remove(Anno$V1, <span class="hljs-string">&quot;;&quot;</span>)<br>TBM$Diff = TBM$Mute - TBM$WT<br><br>library(scales)<br>library(readr)<br>library(tidyr)<br>library(dplyr)<br>library(gridExtra)<br><br>aa_level &lt;- rev(<span class="hljs-built_in">c</span>(<span class="hljs-string">&#x27;E&#x27;</span>,<span class="hljs-string">&#x27;D&#x27;</span>,<span class="hljs-string">&#x27;R&#x27;</span>,<span class="hljs-string">&#x27;K&#x27;</span>,<span class="hljs-string">&#x27;H&#x27;</span>,<span class="hljs-string">&#x27;Q&#x27;</span>,<span class="hljs-string">&#x27;N&#x27;</span>,<span class="hljs-string">&#x27;S&#x27;</span>,<span class="hljs-string">&#x27;T&#x27;</span>,<span class="hljs-string">&#x27;P&#x27;</span>,<span class="hljs-string">&#x27;G&#x27;</span>,<span class="hljs-string">&#x27;C&#x27;</span>,<span class="hljs-string">&#x27;A&#x27;</span>,<span class="hljs-string">&#x27;V&#x27;</span>,<span class="hljs-string">&#x27;I&#x27;</span>,<span class="hljs-string">&#x27;L&#x27;</span>,<span class="hljs-string">&#x27;M&#x27;</span>,<span class="hljs-string">&#x27;F&#x27;</span>,<span class="hljs-string">&#x27;Y&#x27;</span>,<span class="hljs-string">&#x27;W&#x27;</span>,<span class="hljs-string">&#x27;_&#x27;</span>))<br><br>df &lt;- read_csv(<span class="hljs-string">&#x27;CDRH3_KD_table_summary.csv&#x27;</span>) %&gt;%<br>  filter(grepl(<span class="hljs-string">&#x27;CR9114&#x27;</span>,ID)) %&gt;%<br>  mutate(log10_Kd=log10(Kd)) %&gt;%<br>  filter((log10_Kd &lt; -<span class="hljs-number">8</span> &amp; p.value &lt; <span class="hljs-number">0.2</span>) | (log10_Kd &gt;= -<span class="hljs-number">8</span>)) %&gt;%<br>  mutate(Mutation=gsub(<span class="hljs-string">&#x27;CR9114_&#x27;</span>,<span class="hljs-string">&quot;&quot;</span>,ID)) %&gt;%<br>  filter(Mutation != <span class="hljs-string">&#x27;WT&#x27;</span>) %&gt;%<br>  mutate(resi=str_sub(Mutation,<span class="hljs-number">1</span>,-<span class="hljs-number">2</span>)) %&gt;%<br>  mutate(aa=str_sub(Mutation,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>)) %&gt;%<br>  filter(aa %in% aa_level) %&gt;%<br>  mutate(aa=factor(aa,levels=aa_level)) %&gt;%<br>  complete(resi, aa) %&gt;%<br>  mutate(Pos=str_sub(resi,<span class="hljs-number">2</span>,-<span class="hljs-number">1</span>)) %&gt;%<br>  mutate(Pos=<span class="hljs-built_in">as.numeric</span>(<span class="hljs-built_in">as.character</span>(Pos))) %&gt;%<br>  arrange(Pos) %&gt;%<br>  mutate(resi=factor(resi,levels=unique(resi))) %&gt;%<br>  mutate(log10_Kd=case_when(str_sub(resi,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)==aa ~ log10(<span class="hljs-number">5.19e-10</span>), <span class="hljs-literal">TRUE</span> ~ log10_Kd)) %&gt;%<br>  mutate(Mutation=paste(resi,aa,sep=<span class="hljs-string">&#x27;&#x27;</span>)) %&gt;%<br>  select(Mutation, resi, Pos, aa, log10_Kd)<br><br>df$Pos = df$Pos + <span class="hljs-number">96</span><br>df$Anno &lt;- paste(gsub(<span class="hljs-string">&quot;[0-9]&quot;</span>, <span class="hljs-string">&quot;&quot;</span>, df$resi), df$Pos, df$aa, sep = <span class="hljs-string">&#x27;&#x27;</span>)<br><br>remove_second_letter &lt;- <span class="hljs-keyword">function</span>(x) &#123;<br>  paste0(substr(x, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), substr(x, <span class="hljs-number">3</span>, nchar(x)))<br>&#125;<br><br>TBM$Anno &lt;- sapply( TBM$Anno, remove_second_letter)<br>TBM$log10_K &lt;- df$log10_Kd[match(TBM$Anno, df$Anno)]<br>TBMF &lt;- TBM[!<span class="hljs-built_in">is.na</span>(TBM$log10_K),]<br><br>ggplot(TBMF, aes(Diff, log10_K )) + geom_point() + <br>  geom_smooth(method = <span class="hljs-string">&#x27;lm&#x27;</span>) + <br>  theme_bw()<br><br>ggplot(TBMF[TBMF$Diff &lt;= <span class="hljs-number">2</span>,], aes(Diff, log10_K )) + geom_point() + <br>  geom_smooth(method = <span class="hljs-string">&#x27;lm&#x27;</span>) +<br>  geom_vline( xintercept = <span class="hljs-number">0</span>, linetype = <span class="hljs-number">4</span>) + <br>  geom_hline( yintercept = -<span class="hljs-number">9.28</span>, linetype = <span class="hljs-number">4</span>) + <br>  theme_bw()<br></code></pre></td></tr></table></figure></div><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/zstiOTG.png" alt=""></td><td style="text-align:center"><img src="https://imgur.com/Ceu7diL.png" alt=""></td></tr></tbody></table><p>According to this plot, the correlation between experiments and the prediction is terrible. I think the main reason is because all those positions are located on CDRH3 region which not only they are random loop, but also the key site to determine the binding affinity of the antibody. So, the prediction result would be extrimly hard. But I think the result is not totally useless. At least when the Δ G of the complex predicted became more stable ($\Delta_ {mutate} - \Delta_ {wt} &lt; 0$), most of experiments results are very closing to the wild type.</p><table><thead><tr><th>Mute</th><th>WT</th><th>Anno</th><th>Diff</th><th>log10_K</th></tr></thead><tbody><tr><td>-111.068</td><td>-110.525</td><td>D109E</td><td>-0.543</td><td>-9.444906</td></tr><tr><td>-110.702</td><td>-110.512</td><td>D109Q</td><td>-0.190</td><td>-9.422508</td></tr><tr><td>-110.532</td><td>-110.525</td><td>D109C</td><td>-0.007</td><td>-9.343902</td></tr><tr><td>-111.447</td><td>-110.503</td><td>D109M</td><td>-0.944</td><td>-9.296702</td></tr><tr><td>-112.243</td><td>-110.512</td><td>G100M</td><td>-1.731</td><td>-9.222573</td></tr><tr><td>-111.185</td><td>-110.503</td><td>D109T</td><td>-0.682</td><td>-9.180450</td></tr><tr><td>-110.902</td><td>-110.857</td><td>S106M</td><td>-0.045</td><td>-9.170053</td></tr><tr><td>-111.330</td><td>-110.525</td><td>D109R</td><td>-0.805</td><td>-9.156767</td></tr><tr><td>-112.243</td><td>-110.504</td><td>D109Y</td><td>-1.739</td><td>-9.136677</td></tr><tr><td>-111.697</td><td>-110.893</td><td>G100I</td><td>0.804</td><td>-9.114074</td></tr><tr><td>-112.226</td><td>-110.512</td><td>G100K</td><td>-1.714</td><td>-9.100179</td></tr><tr><td>-111.195</td><td>-110.525</td><td>G100C</td><td>-0.670</td><td>-9.075721</td></tr><tr><td>-113.282</td><td>-110.512</td><td>G100R</td><td>-2.770</td><td>-9.057495</td></tr><tr><td>-111.521</td><td>-111.362</td><td>Y104F</td><td>-0.159</td><td>-9.040850</td></tr><tr><td>-110.971</td><td>-110.611</td><td>G107F</td><td>-0.360</td><td>-9.038579</td></tr><tr><td>-111.158</td><td>-110.503</td><td>D109F</td><td>-0.655</td><td>-9.028260</td></tr><tr><td>-110.824</td><td>-110.503</td><td>D109L</td><td>-0.321</td><td>-9.026410</td></tr><tr><td>-112.106</td><td>-110.820</td><td>G100N</td><td>-1.286</td><td>-8.995671</td></tr><tr><td>-111.164</td><td>-110.470</td><td>V110L</td><td>-0.694</td><td>-8.978111</td></tr><tr><td>-111.718</td><td>-110.818</td><td>G100H</td><td>-0.900</td><td>-8.869666</td></tr><tr><td>-111.044</td><td>-110.873</td><td>S106N</td><td>-0.171</td><td>-8.545155</td></tr><tr><td>-110.944</td><td>-110.837</td><td>N101H</td><td>-0.107</td><td>-8.423659</td></tr></tbody></table><h3 id="How-it-work">How it work?</h3><p>Here is the corrected version of your text:</p><p>According to the documentation: This is the workhorse of the FoldX mutation engine. This command ensures that whenever you are mutating a protein, you always move the same neighbors in the WT and in the mutant, producing for each mutant PDB a corresponding PDB for its WT. Each mutation will move different neighbors, and therefore you need different WT references.</p><p>From the experience above, I think it works under the assumption that the structure of the protein won’t change due to the point mutation. As shown below, even though the amino acid changed from <strong>Y</strong> to <strong>R</strong>, the position remains unchanged, and the RMSD is 0. So, I think it would be more reliable when this amino acid is in the alpha helix or beta sheet. When a point mutation happens in these regions, the rough structure remains relatively the same. However, when the mutation occurs in the loop region, the result would be less reliable. According to Yuan M, et al.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>, when the mutation only happens in the V gene of the antibody, the correct ratio was about 70%. But according to the test above, the result is inconsistent and lacks convinciveness.</p><p><img src="https://imgur.com/JpXnrlY.png" alt=""></p><h2 id="Interface-Analysis">Interface Analysis</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># after repair the PDB</span><br>FoldX --<span class="hljs-built_in">command</span>=AnalyseComplex --pdb=4FQI_Repair.pdb --analyseComplexChains=A,B,H,L<br><span class="hljs-comment"># output: Indiv_energies_4FQI_Repair_AC.fxout  Interaction_4FQI_Repair_AC.fxout  Interface_Residues_4FQI_Repair_AC.fxout  Summary_4FQI_Repair_AC.fxout</span><br></code></pre></td></tr></table></figure></div><ul><li><strong>Indiv_energies_4FQI_Repair_AC</strong>:<br>This file contains individual energy contributions of residues to the overall stability of the protein complex. To be notice, the total energy of the complex is not equals to the sum of individual total energy.</li><li><strong>Interaction_4FQI_Repair_AC</strong>:<br>It records detailed interaction between each chain pairs. I think <strong>Interaction Energy</strong> is one of most important results from it.</li><li><strong>Summary_4FQI_Repair_AC.fxout</strong>:<br>It contains only a few important columns from the <em>Interaction_4FQI_Repair_AC.fxout</em>.</li><li><strong>Interface_Residues_4FQI_Repair_AC.fxout</strong>:<br>It records all residues in the interface in a list.</li></ul><div class="admonition note"><p class="admonition-title">Notice</p><p>Before running <code>AnalyseComplex</code>, you should renumber the residues as well. Residue numbers like 100A and 100B in an antibody can both be shown as 100, leading to multiple entries like YH100 in the list. After renumbering the whole PDB with the script above, the results would become &quot;YH103, YH104, YH105&quot;.Additionally, the <code>Interface_residues</code> results may contain some unusual entries such as <strong>oA11</strong>. Technically, this means that in chain A, position 11, there is a non-typical residue, such as a Zn ion.</p></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p><a href="https://www.sciencedirect.com/science/article/pii/S2211124723014225">Teo Q W, Wang Y, Lv H, et al. Stringent and complex sequence constraints of an IGHV1-69 broadly neutralizing antibody to influenza HA stem[J]. Cell reports, 2023, 42(11).</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Yuan M, Feng Z, Lv H, et al. Widespread impact of immunoglobulin V-gene allelic polymorphisms on antibody reactivity[J]. Cell Reports, 2023, 42(10). <a href="#fnref2" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">FoldX</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/"/>
    
    
    <category term="Protein Structure" scheme="https://karobben.github.io/tags/Protein-Structure/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
  </entry>
  
  <entry>
    <title>Nanopre and PacBio based Genome Assembly</title>
    <link href="https://karobben.github.io/2024/06/30/Bioinfor/LongReads/"/>
    <id>https://karobben.github.io/2024/06/30/Bioinfor/LongReads/</id>
    <published>2024-06-30T20:50:52.000Z</published>
    <updated>2024-07-05T16:33:10.980Z</updated>
    
    <content type="html"><![CDATA[<p>Related Papers:</p><ul><li>Rayamajhi N, Cheng C H C, Catchen J M. Evaluating Illumina-, Nanopore-, and PacBio-based genome assembly strategies with the bald notothen, Trematomus borchgrevinki[J]. G3, 2022, 12(11): jkac192. <a href="https://academic.oup.com/g3journal/article/12/11/jkac192/6651842">Paper</a></li><li>van Rengs W M J, Schmidt M H W, Effgen S, et al. A chromosome scale tomato genome built from complementary PacBio and Nanopore sequences alone reveals extensive linkage drag during breeding[J]. The Plant Journal, 2022, 110(2): 572-588. <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/tpj.15690">Paper</a></li><li>Murigneux V, Rai S K, Furtado A, et al. Comparison of long-read methods for sequencing and assembly of a plant genome[J]. GigaScience, 2020, 9(12): giaa146. <a href="https://academic.oup.com/gigascience/article/9/12/giaa146/6042729">Paper</a></li></ul><h2 id="Evaluating-Illumina-Nanopore-and-PacBio-based-genome-assembly-strategies-with-the-bald-notothen-Trematomus-borchgrevinki">Evaluating Illumina-, Nanopore-, and PacBio-based genome assembly strategies with the bald notothen, Trematomus borchgrevinki</h2><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left">In this paper, they did long reads assembly and Long-reads, short-reads hybrid assembly comparing. The experiment organism is “Trematomus borchgrevinki” (<strong>fish</strong>), a cold specialized Antarctic notothenioid fish with an estimated genome size of 1.28 Gb</td><td style="text-align:center"><img src="https://www.researchgate.net/profile/Gregory-Sloop/publication/364608485/figure/fig6/AS:11431281091299426@1666380222030/Trematomus-bernacchii-Courtesy-of-Zureks-and-Wikimedia-Commons.png" alt=""><br><a href="https://www.researchgate.net/publication/364608485_The_Cardiovascular_System_of_Antarctic_Icefish_Appears_to_Have_Been_Designed_to_Utilize_Hemoglobinless_Blood?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Gregory Sloop</a></td></tr></tbody></table><p>Sequencing Size:</p><ul><li>Nanopore: 24.29 Gb</li><li>PacBio: 118.42 Gb</li></ul><p><strong>Hybrid</strong> assemblies can generate <mark>higher contiguity</mark> they tend to suffer from lower quality. <strong>long-read-only assemblies</strong> can be optimized for <mark>contiguity</mark> by subsampling length-restricted raw reads. Long-read contig assembly is the current <strong>best choice</strong> and that assemblies from phase I and phase II were of lower quality.</p><p>Strategies:</p><ul><li>Long-reads and short-reads hybrid: quickmerge<ol><li>Long-reads assembly independently: <code>Canu</code> and <code>WTDBG2</code> assembly, assessed with <code>QUAST</code></li><li>2 rounds of polishing with <code>Pilon</code>. (First round: SNPs adn indels, Second round: local reassembly)</li><li>Gap filling with <code>PBJELLY</code></li></ol></li><li>Long-reads only was assembly by variaties of tools. The yacrd (Marijon et al. 2020) it the tool to identify potential <strong>chimeric reads</strong><ol><li><code>WTDBG2</code> was used to do the assembly</li></ol></li></ul><p>For long-reads, comparing to short-reads assembled genome, it has high continuity but also more number of duplicated BUSCO genes. Chimeric reads are exist. In this paper, they also applied the subsampling to deleted chimeric reads. By cooperate with the limiting reads lengths, the PacBio reads assembly results could be improved. The number of contigs dropped from 10,848 to 4,409 with only 70 Gb of data (generated by sampling minimum and maximum read lengths of 10 and 40 kb)</p><p>In this paper, the data shows that the assembly results from ONT reads are not as good as those from PacBio reads. However, because they used very different methods for pre-processing reads and assembly, the results are somewhat incomparable. Therefore, we can only conclude that the PacBio pipeline is more advanced.</p><h2 id="Comparison-of-long-read-methods-for-sequencing-and-assembly-of-a-plant-genome">Comparison of long-read methods for sequencing and assembly of a plant genome</h2><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left">This paper targets <em><strong>Macadamia jansenii</strong></em>, a type of tree. The PacBio data surprised others because it has higher coverage and longer reads than the typical ONT data. Therefore, <strong>cross-comparison is meaningless</strong>. However, they assembled the genome using <strong>multiple tools</strong>, so the <strong>internal data-type comparison</strong> is still valuable.</td><td style="text-align:center"><img src="https://www.researchgate.net/profile/Ian-Cock/publication/264458556/figure/fig1/AS:392523655729155@1470596340564/Macadamia-integriflora-leaves-and-flowers-photographed-accessed-from-Wikipedia-Commons.png" alt="Macadamia integriflora"><a href="https://www.researchgate.net/publication/264458556_Evaluation_of_the_potential_of_Macadamia_integriflora_extracts_as_antibacterial_food_agents?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Ian Edwin Cock</a></td></tr></tbody></table><table><thead><tr><th>Category</th><th>ONT</th><th>PacBio</th><th>BGI</th></tr></thead><tbody><tr><td><strong>Mean Reads Length</strong></td><td>7,962</td><td>20,575</td><td>2 × 100</td></tr><tr><td><strong>Assembly</strong></td><td>Redbean v2.5, Flye v2.5, and Canu</td><td>Redbean v2.5, Flye v2.5, and Canu</td><td>SuperPlus v1.0, Supernova v2.1.1, TGS-GapCloser</td></tr><tr><td><strong>Data Size</strong></td><td>24.9 Gb</td><td>65.2 Gb</td><td>74.5 Gb</td></tr><tr><td><strong>Largest Contigs</strong></td><td>9,683,794</td><td>23,824,472</td><td>517,998</td></tr><tr><td><strong>Scaffold</strong></td><td>5,332</td><td>5,446</td><td>5,065</td></tr><tr><td><strong>Scaffold N50</strong></td><td>3.52</td><td>3.50</td><td>3.54</td></tr><tr><td><strong>Contigs</strong></td><td>6,022</td><td>5,717</td><td>19,954</td></tr><tr><td><strong>Contigs N50(M)</strong></td><td>1.04</td><td>1.60</td><td>0.036</td></tr><tr><td><strong>BUSCO</strong></td><td>1,963 (92.5)</td><td>1,983 (93.5)</td><td>1,873 (88.3)</td></tr></tbody></table><p>Hybrid assembly:</p><ol><li>MaSuRCA v3.3.3: Illumina + ONT/PacBio</li><li>Flye v2.5 to perform the final assembly</li></ol><p>Diploid de novo genome assembly: PacBio reads was performed with FALCON v1.3.0</p><p>Assembly Evaluation: QUAST v5.0.2;  publicly available reference genome of M. integrifolia v2 (Genbank accession: GCA_900631585.1); subjected to BUSCO v3.0.2 with the eudicotyledons_odb10 database (2,121 genes).</p><p>In this result, the PacBio sequences dominate everything. This is because ultra-long ONT reads were not used here. Consequently, not only the length of the reads but also the accuracy and coverage of the ONT reads are lower than those of the PacBio. This comparison is extremely uneven. By <strong>comparing different long-reads assembly tools</strong> (Redbean, Flye, Falcon, Canu, Raven), the <mark>Rave</mark> is the best for both PacBio and ONT data. An interesting thing is, according to the paper, Rave supports the GPU-accelerate. But in this research, they only given 12 threads for Rave though, technically, we could give more than 1,000 of threads if we have a professional GPU.</p><h2 id="A-chromosome-scale-tomato-genome-built-from-complementary-PacBio-and-Nanopore-sequences-alone-reveals-extensive-linkage-drag-during-breeding">A chromosome scale tomato genome built from complementary PacBio and Nanopore sequences alone reveals extensive linkage drag during breeding</h2><pre class="mermaid">graph LR;　　Nanopore-->|NECAT|Assembly1;　　PacBio-->|Hifiasm|Assembly2;    Assembly1-->|quickmerge| Sinlge;    Assembly2-->|quickmerge| Sinlge</pre><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left">In this research, they target into the cultivated <strong>tomato</strong> (Solanum lycopersicum). They applied PacBio HiFi and ONT Nanopore sequencing to develop <strong>independent</strong>. After then, they <strong>merged the HiFi and ONT assemblies</strong> to generate a long-read-only assembly where all 12 chromosomes were represented as 12 contiguous sequences (N50 = 68.5 Mbp).</td><td style="text-align:center"><img src="https://www.researchgate.net/profile/Gbenga-Orunmolase/publication/371350058/figure/fig1/AS:11431281165942236@1686127227760/Tomato-Solanum-lycopersicum.jpg" alt=""><br><a href="https://www.researchgate.net/publication/371350058_MICROORGANISMS_ASSOCIATED_WITH_SOFT_ROT_OF_TOMATOES?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Gbenga Emmanuel Orunmolase</a></td></tr></tbody></table><table><thead><tr><th></th><th>ONT</th><th>PacBio HiFi</th></tr></thead><tbody><tr><td><strong>Data Size</strong></td><td>100~ Gb (x2)</td><td>20~ Gb (x2)</td></tr><tr><td><strong>Assembled Contigs</strong></td><td>100~300</td><td>700~2000</td></tr><tr><td><strong>Computational Costs</strong></td><td>3 days with 256 threads (NECAT)</td><td>2 hours with 128 threads (Hifiasm)</td></tr></tbody></table><p>Although the <strong>ONT has fewer contigs</strong>, it has a lower BUSCO (complete) percentage due to uncorrectable base errors. For the <mark>saturating test</mark>, they found that for PacBio HiFi reads, <strong>20 Gb</strong> would be enough to finish a good assembly with <strong>Hifiasm</strong>. For longer ONT reads, <strong>50 Gb</strong> could do a similar job with <strong>NECAT</strong>. After that, they conducted the <mark>merge test</mark> because they found partial complementarity of the assemblies as the breakpoints were different. After merging the two results, they obtained 12 super contigs, which correspond to the 12 chromosomes. Along with these 12 super contigs, they also obtained 54 contigs that could not be assembled into the 12 chromosomes; these could be chloroplast, mitochondrial, rDNA, and satellite repeat-derived sequences.</p><h3 id="MbTMV-assembly-pipeline-Merge-ONT-and-PacBio-results">MbTMV assembly pipeline (Merge ONT and PacBio results)</h3><ol><li>Assembly result polishing</li><li>nucmer (part of mummer v.4.0.0rc1) with the -l parameter to prevent invalid contig links</li><li>quickmerge<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> was used to merge 2 assemblies with the parameter -c 7.0.</li></ol><p>A very interesting thing is they use a customized script to convert the Salsa2 output to Hi-C file and plot the contact plot with jucibox</p><hr><p>A recent comparison pointed out that PacBio HiFi reads tend to lead to better assembly of the barley (Hordeum vulgare) genome than ONT (Mascher et al., 2021)</p><style>pre {//  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Chakraborty M, Baldwin-Brown J G, Long A D, et al. Contiguous and accurate de novo assembly of metazoan genomes with modest long read coverage[J]. Nucleic acids research, 2016, 44(19): e147-e147. <a href="https://github.com/mahulchak/quickmerge">GitHub</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">Nanopre and PacBio based Genome Assembly</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/WGS/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="Genome" scheme="https://karobben.github.io/tags/Genome/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/tags/WGS/"/>
    
  </entry>
  
  <entry>
    <title>Juicer: a One-Click System for Analyzing Loop-Resolution Hi-C Experiments</title>
    <link href="https://karobben.github.io/2024/06/27/Bioinfor/juicer/"/>
    <id>https://karobben.github.io/2024/06/27/Bioinfor/juicer/</id>
    <published>2024-06-27T20:55:58.000Z</published>
    <updated>2024-07-05T19:41:51.202Z</updated>
    
    <content type="html"><![CDATA[<p>Prerequisite :</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">conda install bwa               <span class="hljs-comment"># for short reads alignment</span><br>conda install samtools          <span class="hljs-comment"># for reading the align results </span><br></code></pre></td></tr></table></figure></div><p>Resources:</p><ul><li>Paper: Durand NC, Shamim MS, Machol I, Rao SSP, Huntley MH, Lander ES, et al. Juicer Provides a One-Click System for Analyzing Loop-Resolution Hi-C Experiments. Cell Syst. 2016;3:95–8.</li><li>GitHub Source code: <a href="https://github.com/aidenlab/juicer">aidenlab/juicer</a></li><li>Example Pipeline: <a href="https://github.com/ENCODE-DCC/hic-pipeline">ENCODE-DCC/hic-pipeline</a></li><li>Forums: <a href="https://groups.google.com/g/3d-genomics">3d-genomics; google</a>. They suggest to talk and ask on the google group rather than the github issue because you could got faster responds there.</li></ul><p>For working on <strong>hic-pipeline</strong>, if you want to run it in local machine, make sure that <code>docker</code> is installed. I don’t have docker installed, so, I’ll giving this try up.</p><p>When you got the mistake and want run again, make sure remove those directories first.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">rm -rf /home/wenkanl2/Tomas/20240614_DuckGenome/myJuicerDir/aligned<br></code></pre></td></tr></table></figure></div><h2 id="Juicer-in-Action">Juicer in Action</h2><p>According to the <a href="https://github.com/aidenlab/juicer/wiki/Usage">documentation</a>, there are 5 steps for running this juicer:</p><ol><li>Download genome fasta file, put in references folder</li><li>Run <code>bwa index</code> on the fasta file</li><li>At the same time, run <a href="https://github.com/theaidenlab/juicer/blob/master/misc/generate_site_positions.py">generate_site_positions.py</a> on the fasta file + your restriction enzyme (see <a href="https://github.com/theaidenlab/juicer/wiki/Pre#restriction-site-file-format">this site about the restriction site file format</a>)</li><li>Once generate_site_positions is done, run <code>awk 'BEGIN&#123;OFS="\t"&#125;&#123;print $1, $NF&#125;' mygenome_myenzyme.txt &gt; mygenome.chrom.sizes</code> (where mygenome is your genome, like hg19, and myenzyme is your enzyme, like MboI)</li><li>Run juicer.sh with the flags <code>-z &lt;path to genome fasta file&gt;</code>, <code>-p &lt;path to mygenome.chrom.sizes&gt;</code>, and  <code>-y &lt;path to  mygenome_myenzyme.txt&gt;</code></li></ol><h3 id="1-Prepare-Your-Data">1. Prepare Your Data</h3><p>The data download from NCBI is not applicable for this pipeline. We need to adapt the name of each reads. According to the error codes <test>(-: Aligning files matching <em>/myJuicerDir/fastq/</em>_R*.fastq*</test>, we could know that the name of the reads should be <code>*_R*.fastq*</code>. Specified, according to the test data, the name of the paired ends reads should be: <code>*_R1*.fastq*</code> and <code>*_R2*.fastq</code>. So, make sure you have the correct name for each of reads.</p><h3 id="3-Generate-Restriction-Site">3. Generate Restriction Site</h3><p>It seams like you’d like to naming your ref genome first. For example, it automatically supplies the <strong>hg19</strong> and <strong>hg38</strong>. If you list the <code>restriction_site</code> directory, it has <code>hg19_MboI.txt</code></p><p>Before running the pipeline, we need to ready the <code>restriction_site</code> file, too. Here is a script from juicer to help us to generate it: <code>misc/generate_site_positions.py</code>. It works as below. To be notice, the helpers said that <test>Usage: ./generate_site_positions.py <restriction enzyme> <genome> [location]</test>. But the genome here means the name of the genome. In the example, I give it <em><strong>ZJU1.0</strong></em>. The third parameter <code>[location]</code> is the location of the genome fasta file. With the code below, they would output the file <code>ZJU1.0_HindIII.txt</code></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">python generate_site_positions.py HindIII ZJU1.0 GCF_015476345.1_ZJU1.0_genomic.fna<br>mv ZJU1.0_HindIII.txt ../restriction_sites/<br></code></pre></td></tr></table></figure></div><p>Here is the few supported name of restriction enzymes:</p><pre>    'HindIII'     : 'AAGCTT',    'DpnII'       : 'GATC',    'MboI'        : 'GATC',    'Sau3AI'      : 'GATC',    'Arima'       : [ 'GATC', 'GANTC' ],</pre><h3 id="4-Create-Chromosome-Size-File">4. Create Chromosome Size File</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;BEGIN&#123;OFS=&quot;\t&quot;&#125;&#123;print $1, $NF&#125;&#x27;</span> restriction_sites/ZJU1.0_HindIII.txt &gt; ZJU1.0.chrom.sizes<br></code></pre></td></tr></table></figure></div><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># HiCCUPS</span><br>scripts/common/juicer_tools hiccups --ignore_sparsity aligned/inter_30.hic aligned/inter_30_loops<br><span class="hljs-comment"># APA: </span><br>scripts/common/juicer_tools apa aligned/inter_30.hic aligned/inter_30_loops apa_results<br></code></pre></td></tr></table></figure></div><p>In the test data, it generally takes 90GB RAM and 7 GB of GPU RAM</p><h3 id="5-Run-with-the-New-Parameters">5. Run with the New Parameters</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">&lt;myJuicerDir&gt;/scripts/juicer.sh -D &lt;myJuicerDir&gt; <br></code></pre></td></tr></table></figure></div><h3 id="Result">Result</h3><p>I didn’t processing the Juicer successfully yet. It was always exit at post data processing. I get the aligned result successfully. But it seems like failed to find the loop and get the <code>apa_resutls</code>.</p><p>So, according to the ChatGPT4o, we expected to get the results below after juicer:</p><ol><li>Contact Maps: These are heatmap-like visualizations showing the frequency of interactions between different regions of the genome.</li><li>.hic Files: The primary output format of Juicer, .hic files contain the processed Hi-C data, which can be visualized using tools like Juicebox.</li><li>Statistics and Quality Metrics; Genome-Wide Interaction Profiles; Contact Frequency Plots; and Visualizations in Juicebox</li></ol><p>In the <code>aligned</code> directory, we got 2 <code>.hic</code> file, one is <code>inter_30.hic</code>, another is <code>inter.hic</code>. According to ChatGPT4o, <code>inter.hic</code> typically contains the raw or minimally processed interaction data. <code>inter_30.hic</code> contains interaction data that has been <strong>normalized</strong> and possibly <strong>filtered</strong> to remove noise and low-quality interactions. The “30” in the name usually refers to a specific bin size (e.g., 30 kb). And typically, the <code>inter_30.hic</code> file or another similarly named file with a specific bin size (e.g., <code>inter_5.hic</code>, <code>inter_10.hic</code>) is considered the final, high-quality result suitable for detailed analysis.</p><h2 id="Troubleshooting">Troubleshooting</h2><pre>HiCCUPS:GPUs are not installed so HiCCUPs cannot be run(-: Postprocessing successfully completed, maps too sparse to annotate or GPUs unavailable (-:***! Error! Either inter.hic or inter_30.hic were not createdEither inter.hic or inter_30.hic were not created.  Check  for results</pre><p>Check if cuda is installed appropriate. If so, Check if it is in your working environment.</p><p>How to add it in your working environment: (edit your <code>~/.bashrc</code> or <code>~/.zshrc</code> file)</p><pre>export PATH="/usr/local/cuda-8.0/bin:$PATH"export LD_LIBRARY_PATH="/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH"</pre><p>Or you could add the <code>--cpu</code> flag on file <code>scripts/common/juicer_postprocessing.sh</code></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="DIFF"><figure class="iseeu highlight /diff"><table><tr><td class="code"><pre><code class="hljs diff"><span class="hljs-deletion">- if hash nvcc 2&gt;/dev/null</span><br><span class="hljs-deletion">- then</span><br><span class="hljs-deletion">-    $&#123;juicer_tools_path&#125; hiccups $&#123;hic_file_path&#125; $&#123;hic_file_path%.*&#125;&quot;_loops&quot;</span><br><span class="hljs-addition">+    $&#123;juicer_tools_path&#125; hiccups --cpu $&#123;hic_file_path&#125; $&#123;hic_file_path%.*&#125;&quot;_loops&quot;</span><br><span class="hljs-deletion">-    if [ $? -ne 0 ]; then</span><br><span class="hljs-deletion">-echo &quot;***! Problem while running HiCCUPS&quot;;</span><br><span class="hljs-deletion">-exit 1</span><br><span class="hljs-deletion">-    fi</span><br><span class="hljs-deletion">-else</span><br><span class="hljs-deletion">-    echo &quot;GPUs are not installed so HiCCUPs cannot be run&quot;;</span><br><span class="hljs-deletion">-fi</span><br></code></pre></td></tr></table></figure></div><p>When the <code>if hash nvcc 2&gt;/dev/null</code> detected that the <code>nvcc</code> doesn’t in the environment, it would exit. So, you may like to delete the entail if statement.</p><pre>HiCCUPS:Picked up _JAVA_OPTIONS: -Xmx150000m -Xms150000mReading file: /home/wenkanl2/Tomas/20240614_DuckGenome/myJuicerDir/aligned/inter_30.hicNo valid configurations specified, using default settingsWarning Hi-C map is too sparse to find many loops via HiCCUPS.Exiting. To disable sparsity check, use the --ignore_sparsity flag.</pre><p>As it suggests, you need to add the <code>--ignore_sparsity</code> flag. But, again, you can only make this change by alter <code>scripts/common/juicer_postprocessing.sh</code></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="DIFF"><figure class="iseeu highlight /diff"><table><tr><td class="code"><pre><code class="hljs diff">if hash nvcc 2&gt;/dev/null<br>then<br><span class="hljs-deletion">-    $&#123;juicer_tools_path&#125; hiccups $&#123;hic_file_path&#125; $&#123;hic_file_path%.*&#125;&quot;_loops&quot;</span><br><span class="hljs-addition">+    $&#123;juicer_tools_path&#125; hiccups  --ignore_sparsity $&#123;hic_file_path&#125; $&#123;hic_file_path%.*&#125;&quot;_loops&quot;</span><br>    if [ $? -ne 0 ]; then<br>echo &quot;***! Problem while running HiCCUPS&quot;;<br>exit 1<br>    fi<br>else<br>    echo &quot;GPUs are not installed so HiCCUPs cannot be run&quot;;<br>fi<br></code></pre></td></tr></table></figure></div><p>0 loops</p><pre>100% 0 loops written to file: ...HiCCUPS complete</pre><p>According to this <a href="https://groups.google.com/g/3d-genomics/c/9f5UUhuS8O4/m/RTE1YVTKAgAJ">post answer by Neva Durand</a>, it could be the data is too sparse.</p><blockquote><p>Yes, it’s an order of magnitude too few reads to find loops. You need to do deeper sequencing  / more replicates and then combine them. You need at least 1 billion reads. Otherwise your experiments simply don’t have the depth to determine loops (with any algorithm).</p></blockquote><pre>Not including fragment mapError while reading graphs file: java.io.FileNotFoundException: /home/wenkanl2/Tomas/20240614_DuckGenome/myJuicerDir/aligned/inter_30_hists.m (No such file or directory)Start preprocessWriting headerWriting bodyjava.lang.RuntimeException: No reads in Hi-C contact matrices. This could be because the MAPQ filter is set too high (-q) or because all reads map to the same fragment.at juicebox.tools.utils.original.Preprocessor$MatrixZoomDataPP.mergeAndWriteBlocks(Preprocessor.java:1650)at juicebox.tools.utils.original.Preprocessor$MatrixZoomDataPP.access$000(Preprocessor.java:1419)at juicebox.tools.utils.original.Preprocessor.writeMatrix(Preprocessor.java:832)at juicebox.tools.utils.original.Preprocessor.writeBody(Preprocessor.java:582)at juicebox.tools.utils.original.Preprocessor.preprocess(Preprocessor.java:346)at juicebox.tools.clt.old.PreProcessing.run(PreProcessing.java:116)at juicebox.tools.HiCTools.main(HiCTools.java:96)real2m7.365suser0m33.647ssys0m49.450sPicked up _JAVA_OPTIONS: -Xmx150000m -Xms150000mError reading datasetnulljava.io.EOFExceptionat htsjdk.tribble.util.LittleEndianInputStream.readFully(LittleEndianInputStream.java:138)at htsjdk.tribble.util.LittleEndianInputStream.readLong(LittleEndianInputStream.java:80)at htsjdk.tribble.util.LittleEndianInputStream.readDouble(LittleEndianInputStream.java:100)at juicebox.data.DatasetReaderV2.readFooter(DatasetReaderV2.java:470)at juicebox.data.DatasetReaderV2.read(DatasetReaderV2.java:235)at juicebox.tools.utils.original.NormalizationVectorUpdater.updateHicFile(NormalizationVectorUpdater.java:78)at juicebox.tools.clt.old.AddNorm.run(AddNorm.java:84)at juicebox.tools.HiCTools.main(HiCTools.java:96)real0m0.706suser0m1.229ssys0m0.399s/home/wenkanl2/Tomas/20240614_DuckGenome/myJuicerDir/scripts/common/juicer_postprocessing.sh: option requires an argument -- gUsage: /home/wenkanl2/Tomas/20240614_DuckGenome/myJuicerDir/scripts/common/juicer_postprocessing.sh [-h] -j <juicer_tools_file_path> -i <hic_file_path> -m <bed_file_dir> -g <genome ID>***! Error! Either inter.hic or inter_30.hic were not createdEither inter.hic or inter_30.hic were not created.  Check  for results</pre><h2 id="Other-Pipelines-for-Hi-C-Data">Other Pipelines for Hi-C Data</h2><p><img src="https://s3.amazonaws.com/4dn-dcic-public/static-pages/hicpipeline.png" alt="© 4dn"></p><ul><li><a href="https://data.4dnucleome.org/resources/data-analysis/hi_c-processing-pipeline">Hi-C Processing Pipeline</a></li><li><a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0831-x">HiC-Pro: an optimized and flexible pipeline for Hi-C data processing</a></li><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4706059/">HiCUP: pipeline for mapping and processing Hi-C data</a></li><li><a href="https://www.bioinformatics.babraham.ac.uk/projects/hicup/">Babraham Bioinformatics: HiCUP (Hi-C User Pipeline)</a></li><li><a href="https://www.encodeproject.org/hic/">HiC Data Standards and Processing Pipeline</a></li><li><a href="https://nf-co.re/hic/2.1.0">nf-core/hic</a></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}test {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Juicer: a One-Click System for Analyzing Loop-Resolution Hi-C Experiments</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/WGS/"/>
    
    
    <category term="Software" scheme="https://karobben.github.io/tags/Software/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="Genome" scheme="https://karobben.github.io/tags/Genome/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/tags/WGS/"/>
    
  </entry>
  
  <entry>
    <title>NextDenovo: an efficient error correction and accurate assembly tool for noisy long reads</title>
    <link href="https://karobben.github.io/2024/06/26/Bioinfor/NextDenovo/"/>
    <id>https://karobben.github.io/2024/06/26/Bioinfor/NextDenovo/</id>
    <published>2024-06-26T16:11:48.000Z</published>
    <updated>2024-06-30T21:16:55.880Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="https://nextdenovo.readthedocs.io/en/latest/QSTART.html#quick-start">Quick Start</a></li><li><a href="https://nextdenovo.readthedocs.io/en/latest/TEST1.html">Tutorial</a></li><li>Other related Long reads Assembly Tools, check the end of the post, for example: Falcon (Chin et al. 2016), Canu (Koren et al. 2017), WTDBG2 (Ruan and Li 2020), or Flye (Kolmogorov et al. 2019)</li></ul><h2 id="NextDenovo">NextDenovo</h2><p>Paper: <a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-024-03252-4">Hu J, Wang Z, Sun Z, et al. NextDenovo: an efficient error correction and accurate assembly tool for noisy long reads[J]. Genome Biology, 2024, 25(1): 107.</a></p><h3 id="Background">Background</h3><ul><li>Third-generation long-read:<br>PacBio has high-fidelity (HiFi) reads but they are relatively short (~ 15 kb). So, it is unable to span long tandem or highly homologous multi-copy repeats like centromeres. ONT sequencing can generate &gt; 100-kb “ultra-long” reads.</li><li>CTA and ATC:<br>“correction then assembly” (CTA, an assembler first corrects errors in the reads and then uses the corrected reads for assembly) and “assembly then correction” (ATC, an assembler uses error-prone reads to assemble the genome and then corrects errors in the assembled genome) are commonly used in assembly. CTA is much slower. But in terms of the assembly of segmental duplications/repeats, and especially for large plant genome assemblies, the CTA-based strategy usually has an enhanced ability to distinguish different gene copies and produce more accurate and continuous assemblies. <mark>NextDenovo is the tool of CTA-based assembly tool</mark></li></ul><h3 id="Steps">Steps</h3><ol><li><p>Detecting Overlapping Reads</p><ul><li><strong>Initial Detection</strong>: Detects overlapping reads (Fig. 1A).</li><li><strong>Filtering</strong>: Filters out alignments caused by repeats.</li><li><strong>Splitting</strong>: Splits chimeric seeds based on overlapping depth (Fig. 1B).</li></ul></li><li><p>Rough Correction with KSC Algorithm</p><ul><li><strong>Algorithm Used</strong>: Kmer score chain (KSC) algorithm, used in NextPolish [19], for initial rough correction (Fig. 1C).</li></ul></li><li><p>Handling Repeated Regions</p><ul><li><strong>Detection of Low-Score Regions (LSRs)</strong>: Uses a heuristic algorithm during traceback within the KSC algorithm.</li><li><strong>Accurate Correction</strong>:<ul><li>Combines partial order alignment (POA) [20] and KSC.</li><li>Collects subsections spanning LSRs and generates kmer sets at flanking sequences.</li><li>Filters subsections with lower kmer scores.</li><li>Creates pseudo-LSR seeds from top-ranked subsections using a greedy POA consensus algorithm.</li><li>Maps and corrects pseudo-LSR seeds multiple times for accuracy.</li><li>Integrates corrected LSRs back into the primary corrected seed (Fig. 1D).</li></ul></li></ul></li><li><p>Pairwise Overlapping and Dovetail Alignments</p><ul><li><strong>Two Rounds of Overlapping</strong>:<ul><li><strong>First Round</strong>: Uses rapid detection parameters.</li><li><strong>Second Round</strong>: Applies rigorous parameters for accurate alignments.</li></ul></li><li><strong>Graph Construction</strong>:<ul><li>Constructs a directed string graph.</li><li>Removes transitive edges using the “best overlap graph” (BOG) algorithm.</li><li>For repeat nodes, edges are only removed if below specific thresholds to maintain connectivity.</li><li>Removes tips and resolves bubbles.</li></ul></li></ul></li><li><p>Progressive Graph Cleaning</p><ul><li><strong>Simplifying Subgraphs</strong>:<ul><li>Uses a progressive cleaning strategy with increasingly stringent thresholds.</li><li>Breaks paths at nodes with multiple connections.</li><li>Outputs contigs from broken linear paths.</li></ul></li><li><strong>Reducing Misassemblies</strong>:<ul><li>Maps all seeds to contigs.</li><li>Breaks contigs at lower mapping depth regions (LDRs) (Fig. 1E).</li></ul></li></ul></li></ol><h3 id="Key-Algorithms-and-Techniques">Key Algorithms and Techniques</h3><ul><li><strong>KSC Algorithm</strong>: Used for initial rough correction and handling LSRs.</li><li><strong>Heuristic and Accurate Algorithms</strong>: For detecting and correcting LSRs.</li><li><strong>BOG Algorithm</strong>: For removing transitive edges in the graph.</li></ul><h2 id="Error-Correction">Error Correction</h2><p>NextDenovo is 1.63 times faster on real data compared to Consent, Canu, and Necat. As the read length increases, the time required for correction also increases. However, NextDenovo and Necat demonstrated only slight increases, while Canu exhibited a significant increase in processing time</p><h2 id="Installation">Installation</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Prerequirement</span><br>pip install paralleltask<br><br><span class="hljs-comment"># Install from github </span><br>git <span class="hljs-built_in">clone</span> git@github.com:Nextomics/NextDenovo.git<br><span class="hljs-built_in">cd</span> NextDenovo &amp;&amp; make<br><br><span class="hljs-comment"># Test</span><br>nextDenovo test_data/run.cfg<br></code></pre></td></tr></table></figure></div><h2 id="Run">Run</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">nextDenovo run.cfg<br></code></pre></td></tr></table></figure></div><p>Example of <code>run.cfg</code></p><pre>[General]job_type = local job_prefix = nextDenovotask = allrewrite = yesdeltmp = yesparallel_jobs = 22input_type = rawread_type = ont # clr, ont, hifiinput_fofn = input.fofninput_fofn2 = input2.fofnworkdir = HG002_NA24385_son_assemble[correct_option]read_cutoff = 1kgenome_size = 3g # estimated genome sizesort_options = -m 50g -t 30minimap2_options_raw = -t 8pa_correction = 5correction_options = -p 30[assemble_option]minimap2_options_cns = -t 8nextgraph_options = -a 1</pre><h2 id="Result">Result</h2><ul><li>Sequence: <code>01_rundir/03.ctg_graph/nd.asm.fasta</code></li><li>Statistics: <code>01_rundir/03.ctg_graph/nd.asm.fasta.stat</code></li></ul><p>Assembly data: 109G+98G<br>RAM utility: about 400GB. (You can also make it run with 64 RAM but it would takes much loger time to finish)<br>Time: about 2 days.</p><h2 id="After-Assembly">After Assembly</h2><h3 id="Compare-the-result-from-the-SKLA1-0-by-MUMmer">Compare the result from the SKLA1.0 by MUMmer</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">~/software/mummer-4.0.0rc1/mummer  -l 200 -threads 30 -qthreads 30  -mum -b -c data/NextDenovo_result.fa  data/SKLA1.0.chrall.fa &gt; result/NextDenovo_SKLA1.0.mums<br></code></pre></td></tr></table></figure></div><p><img src="https://imgur.com/nsV3ADI.png" alt="mummer"></p><p>According to this result, the first three chromosomes from bottom to top are chr1, chr2, and chr3. The x-axis, from left to right, is sorted by the length of the contigs. As we can see, the first contig represents the full length of chr3. Contigs 2, 3, and 7 represent chr1, while contigs 6, 8, and 9 are three pieces of chr2. Another very interesting result is that, except for chr2, both chr1 and chr3 are complemented and reversed.</p><h2 id="NextPolish">NextPolish</h2><p>NextPolish was also recomand. You can download and install by following the instruction form <a href="https://github.com/Nextomics/NextPolish">github</a>. But I am not that luck to install it in my Ubuntu server. It come with the error:</p><pre>gcc -g -Wall -Wno-unused-function -O2 -DHAVE_PTHREAD -DUSE_MALLOC_WRAPPERS bwashm.o bwase.o bwaseqio.o bwtgap.o bwtaln.o bamlite.o bwape.o kopen.o pemerge.o maxk.o bwtsw2_core.o bwtsw2_main.o bwtsw2_aux.o bwt_lite.o bwtsw2_chain.o fastmap.o bwtsw2_pair.o main.o -o bwa -L. -lbwa -lm -lz -lpthread -lrt/usr/bin/ld: ./libbwa.a(rope.o):/raid/home/wenkanl2/BioTools/NextPolish/util/bwa/rle.h:33: multiple definition of `rle_auxtab'; ./libbwa.a(bwtindex.o):/raid/home/wenkanl2/BioTools/NextPolish/util/bwa/rle.h:33: first defined here/usr/bin/ld: ./libbwa.a(rle.o):/raid/home/wenkanl2/BioTools/NextPolish/util/bwa/rle.h:33: multiple definition of `rle_auxtab'; ./libbwa.a(bwtindex.o):/raid/home/wenkanl2/BioTools/NextPolish/util/bwa/rle.h:33: first defined herecollect2: error: ld returned 1 exit statusmake[2]: *** [Makefile:30: bwa] Error 1make[2]: Leaving directory '/raid/home/wenkanl2/BioTools/NextPolish/util/bwa'make[1]: *** [Makefile:19: bwa_] Error 2make[1]: Leaving directory '/raid/home/wenkanl2/BioTools/NextPolish/util'make: *** [Makefile:18: all] Error 2</pre><p>So, I tied install it with bioconda:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">conda install NextPolish<br></code></pre></td></tr></table></figure></div><pre>nextpolish-1.4.1           |  py311h99925d8_3         1.7 MB  bioconda</pre><p>And them, it installed the version 1.4.1. Next, I tried the test: <code>nextPolish test_data/run.cfg</code> and it finished the test correctly:</p><pre>Type           Length (bp)            Count (#)N10                60501                   1N20                60501                   1N30                60501                   1N40                60501                   1N50                60501                   1N60                51048                   2N70                51048                   2N80                51048                   2N90                51048                   2Min.               51048                   -Max.               60501                   -Ave.               55774                   -Total             111549                   2</pre><h3 id="Options-for-NextPolish">Options for NextPolish</h3><p>[sgs_option]: Polishing using short reads only<br>[lgs_option]: Polishing using long reads only</p><h3 id="In-Action">In Action:</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># prepare the short reads information</span><br>ls data/WGS/*.fastq &gt; sgs.fofn<br><span class="hljs-comment"># Start running</span><br>nohup nextPolish Polish.cfg &gt; Polish.log &amp;<br></code></pre></td></tr></table></figure></div><p>cfg file for this experiment:</p><pre>[General]job_type = localjob_prefix = nextPolishtask = defaultrewrite = yesrerun = 3parallel_jobs = 20multithread_jobs = 20genome = result/NextDenovo_result.fagenome_size = autoworkdir = ./01_rundirpolish_options = -p {multithread_jobs}[sgs_option]sgs_fofn = ./sgs.fofnsgs_options = -max_depth 100</pre><p>In this config file, it given the number of parallel jobs as 20 and multithread jobs as 20, which means the max threads allocated would be 20*20 = 400. So,be sure about that you have that much of threads for calculation or it would make the whole processes slower than normal.</p><h2 id="Other-Long-Reads-Assembly-Tools">Other Long-Reads Assembly Tools</h2><h3 id="LongStitch-Enhancing-Genome-Assembly-with-Long-Reads">LongStitch: Enhancing Genome Assembly with Long Reads</h3><p>LongStitch<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> is a powerful computational tool designed to improve the quality of genome assemblies by utilizing long-read sequencing data. It addresses common issues in genome assembly, such as errors and gaps, that are often introduced during the assembly of short-read sequences.</p><p>It was published at 2021. Until the July 2024, it got 34 citations. It is an open source software and deposit in <a href="https://github.com/bcgsc/longstitch">GitHub</a> with 42 starts.</p><p>Key features of LongStitch include:</p><ul><li><strong>Error Correction</strong>: By aligning long reads to the existing genome assembly, LongStitch identifies and corrects misassemblies, leading to a more accurate genomic representation.</li><li><strong>Scaffolding</strong>: LongStitch leverages long reads to link contigs into scaffolds, significantly enhancing the continuity and completeness of the genome assembly.</li><li><strong>High-Quality Output</strong>: The resulting assemblies are more comprehensive and accurate, making them invaluable for further genomic analysis and research.</li></ul><p>LongStitch’s ability to handle repetitive regions and complex genomic structures makes it an essential tool for researchers aiming to achieve high-quality genome assemblies.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Coombe L, Li J X, Lo T, et al. LongStitch: high-quality genome assembly correction and scaffolding using long reads[J]. BMC bioinformatics, 2021, 22: 1-13. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">NextDenovo is a string graph-based de novo assembler for long reads (CLR, HiFi and ONT)</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="Genome" scheme="https://karobben.github.io/tags/Genome/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/tags/WGS/"/>
    
  </entry>
  
  <entry>
    <title>IgCaller</title>
    <link href="https://karobben.github.io/2024/06/24/Bioinfor/IgCaller/"/>
    <id>https://karobben.github.io/2024/06/24/Bioinfor/IgCaller/</id>
    <published>2024-06-24T21:45:27.000Z</published>
    <updated>2024-06-25T17:34:50.131Z</updated>
    
    <content type="html"><![CDATA[<p>IgCaller is used extensively in immunology research to study B-cell receptor diversity and antibody generation mechanisms. Clinically, it helps identify clonal B-cell expansions, monitor minimal residual disease in leukemias and lymphomas, and analyze antibody responses to vaccines. Additionally, it supports therapeutic antibody development by identifying candidate antibodies from strong immune responses.</p><p>It is an <a href="https://github.com/ferrannadeu/IgCaller">open-source</a> tool designed to study human B cell Ig gene rearrangements. According to the documentation, it only supports the human hg19 or hg38 genome as the input reference, so the application of this tool is limited to humans. It requires selecting specific areas of the genome.</p><p>I am currently working on nonhuman Ig. I may update with more details later when I work with human Ig.</p><p>The basic use only requires the short reads aligned BAM file:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">IgCaller -I /path/to/IgCaller/IgCaller_reference_files/ -V hg19 -C ensembl -T /path/to/bams/tumor.bam -N /path/to/bams/normal.bam -R /path/to/reference/genome_hg19.fa -o /path/to/IgCaller/outputs/<br></code></pre></td></tr></table></figure></div><ul><li>Output: IgCaller returns a set of tab-separated files:<ul><li>tumor_sample_output_filtered.tsv: High confidence rearrangements passing the defined filters.</li><li>tumor_sample_output_IGH.tsv: File containing all IGH rearrangements.</li><li>tumor_sample_output_IGK.tsv: File containing all IGK rearrangements.</li><li>tumor_sample_output_IGL.tsv: File containing all IGL rearrangements.</li><li>tumor_sample_output_class_switch.tsv: File containing all CSR rearrangements.</li><li>tumor_sample_output_oncogenic_IG_rearrangements.tsv: File containing all oncogenic IG rearrangements (translocations, deletions, inversions, and gains) identified genome-wide.</li></ul></li></ul><p>More details:<br>Nadeu, F., Mas-de-les-Valls, R., Navarro, A. et al. IgCaller for reconstructing immunoglobulin gene rearrangements and oncogenic translocations from whole-genome sequencing in lymphoid neoplasms. Nature Communications 11, 3390 (2020). <a href="https://doi.org/10.1038/s41467-020-17095-7">https://doi.org/10.1038/s41467-020-17095-7</a>.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Reconstructing immunoglobulin gene rearrangements and oncogenic translocations from WGS, WES, and capture NGS data</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="Genome" scheme="https://karobben.github.io/tags/Genome/"/>
    
    <category term="Antibody" scheme="https://karobben.github.io/tags/Antibody/"/>
    
    <category term="Immunology" scheme="https://karobben.github.io/tags/Immunology/"/>
    
  </entry>
  
  <entry>
    <title>MUMmer: Rapidly Genomes Alignment</title>
    <link href="https://karobben.github.io/2024/06/21/Bioinfor/MUMmer/"/>
    <id>https://karobben.github.io/2024/06/21/Bioinfor/MUMmer/</id>
    <published>2024-06-21T23:29:31.000Z</published>
    <updated>2024-06-24T21:55:49.415Z</updated>
    
    <content type="html"><![CDATA[<p>Installation:</p><p>In linux, you could simply install it by apt install mummer. The version of the mummer is around 3.1. The version from Bioconda is little bit new than the apt source but still kind of old. If you install teh MUMmer from those 2 source, you’ll meet error when the reads is too long.</p><h2 id="NUCmer">NUCmer:</h2><p>Test target: Chicken Genome (GRCg6a) and Duck Genome (SKLA1.0)<br>It could be RAM monster when you compare 2 Genome directly. Especially when you want to use the multiple threads, the RAM would be occupied very quick and the program would be killed.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">nucmer -maxmatch -c 100 -p output_prefix reference.fasta query.fasta<br>show-coords -rcl output_prefix.delta &gt; output_prefix.coords<br>cat output_prefix.coords<br>grep -Ev <span class="hljs-string">&quot;^$|=|\[&quot;</span> result/test.coords| grep <span class="hljs-string">&quot;^ &quot;</span>| awk <span class="hljs-string">&#x27;BEGIN &#123; OFS=&quot;\t&quot;; print &quot;Ref_Start&quot;, &quot;Ref_End&quot;, &quot;Query_Start&quot;, &quot;Query_End&quot;, &quot;Length1&quot;, &quot;Length2&quot;, &quot;Identity&quot;, &quot;Ref&quot;, &quot;Query&quot; &#125; &#123;OFS=&quot;\t&quot;;  print $1, $2, $4, $5, $7, $8, $10, $12, $13&#125;&#x27;</span> &gt; test.coords<br><br></code></pre></td></tr></table></figure></div><p>But the problem is there are no such parameter for NUCmer to limited the use of RAM. The only way to solving this problem is split the genome into single sequences and processing one by one.</p><pre>[1]    70706 killed     nucmer --maxmatch -c 100 -p result/Chicken-SKLA1.0</pre><p><img src="https://imgur.com/iXM6Kr2.png" alt="btop"></p><p>I was tried to extract the first chromosome (196,202,544 bp, 192M) from the Chicken align against the Duck genome (1.2 Gb) and it takes 41 GB RAM if you given only 1 thread. If you run it without given threads, it would run with 3 threads and the RAM would increased into 70 GB. So, it seams it is save to run with about 7 threads with this size of data. But after 1h 12min, it was killed because of the increased demand of RAM. If you use single thread, the RAM would increased in to 77 after about 1h and 12min. Finally, it takes 13h 3m.</p><h3 id="MUMmer">MUMmer</h3><p>MUMmer is focusing on the difference between the reference and the Subject. It output is very sample. It only contains the start, end, and the length of the reference. Based on this, we could know that they are forward or reverse-complemented. It doesn’t has the responded position for Subject chromosome. So, because of the low dimension information, it requires very low RAM and works very efficient. It could finishing calculation in a very short time.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">mummer -mum -b -c ref.fa sub.fa  &gt; ref.mums<br>mummerplot --postscript --prefix=ref_qry ref.mums<br>gnuplot ref_qry.gp<br></code></pre></td></tr></table></figure></div><p>For visualization, you could use the package provide tools. You could also convert it into tables and visualize it with your favorite tools. So, after convert the mummer result into <code>tsv</code> by a python script, we could visualize the result with ggplot.</p><p><img src="https://imgur.com/KcnMXqg.png" alt="mummer plots"></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">MUMmer is a system for rapidly aligning entire genomes</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="Software" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/Software/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="Genome" scheme="https://karobben.github.io/tags/Genome/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/tags/WGS/"/>
    
  </entry>
  
  <entry>
    <title>Whole Genome Sequencing (WGS)</title>
    <link href="https://karobben.github.io/2024/06/17/Bioinfor/wholegenomesequencing/"/>
    <id>https://karobben.github.io/2024/06/17/Bioinfor/wholegenomesequencing/</id>
    <published>2024-06-17T20:34:03.000Z</published>
    <updated>2024-07-17T15:02:46.483Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Whole-Genome-Sequencing-WGS">Whole Genome Sequencing (WGS)</h2><blockquote><p>Whole-genome sequencing (WGS) is a comprehensive method for analyzing entire genomes. Genomic information has been instrumental in identifying inherited disorders, characterizing the mutations that drive cancer progression, and tracking disease outbreaks. Rapidly dropping sequencing costs and the ability to produce large volumes of data with today’s sequencers make whole-genome sequencing a powerful tool for genomics research. (<a href="https://www.illumina.com/techniques/sequencing/dna-sequencing/whole-genome-sequencing.html">Illumina</a>)</p></blockquote><h2 id="Illumina-WGS">Illumina WGS</h2><p>KEY WHOLE-GENOME SEQUENCING METHODS</p><ul><li>Large whole-genome sequencing</li><li>Small whole-genome sequencing</li><li>De novo sequencing<ul><li>Targeting to species without reference genome</li></ul></li><li>Phased sequencing</li><li>Human whole-genome sequencing<ul><li>optimized for human</li></ul></li><li>Long-reads sequencing</li></ul><h2 id="Examples-from-Publications">Examples from Publications</h2><h3 id="SKLA1-0-Duck">SKLA1.0 (Duck)</h3><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left">A chromosome-scale Beijing duck assembly (SKLA1.0<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>) by integrating Nanopore, Bionano, and Hi-C data covers 40 chromosomes, improves the contig N50 of the previous duck assembly with highest contiguity (ZJU1.0) of more than a 5.79-fold and contains a complete genomic map of the MHC.</td><td style="text-align:center"><img src="https://www.researchgate.net/profile/Inma-Aznar/publication/51580858/figure/fig19/AS:669632922923023@1536664338755/Mallard-Anas-platyrhynchos-Photo-John-Carey.png" alt="Duck"><a href="https://www.researchgate.net/publication/51580858_A_review_of_Ireland's_waterbirds_with_emphasis_on_wintering_migrants_and_reference_to_H5N1_avian_influenza?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Olivia Crowe</a></td></tr></tbody></table><p>Species: Anas platyrhynchos (a native breed in China, using a hierarchical and hybrid approach)</p><p>Reads types: Nanopore, Bionano, and Hi-C data.</p><ul><li>71-fold normal and 24-fold ultra-long Nanopore reads</li><li>117-fold 150bp paired-end Illumina reads for polishing</li><li>216-fold optical map reads and 234-fold PE150 Hi-C</li></ul><p>Result:</p><ul><li>40 chromosomes, improves the contig N50 of the previous duck assembly with highest contiguity (ZJU1.0) of more than a 5.79-fold</li><li>a complete genomic map of the MHC</li></ul><p>Solved challenges:</p><ul><li>traditional assembly tools have not enabled proper genomic draft of highly repetitive and GC-rich sequences, such as the MHC</li></ul><p>Something I don’t understand:</p><ul><li>C18 Duck?</li><li>heterozygosity estimation: why they do it? How could it help on the genome assembly?</li><li>What is BUSCO score?</li></ul><h4 id="Steps-for-Genome-assembly">Steps for Genome assembly:</h4><ol><li><strong>Estimate Genome Heterozygosity</strong><ul><li>Before starting the assembly, the genome heterozygosity of the C18 duck was estimated. The heterozygosity was found to be as low as 0.58% (Additional file 1: Table S1 and Additional file 2: Fig. S1-S3).</li></ul></li><li><strong>Assemble Genome with <mark>Nanopore Reads</mark></strong><ul><li>Using 71-fold normal and 24-fold ultra-long Nanopore reads, the duck genome was assembled into 151 contigs covering a total length of 1.22 Gb with a contig N50 of 32.81 Mb (Additional file 1: Table S2-S3).</li><li><a href="https://github.com/Nextomics/NextDenovo"><strong>NextDenovo</strong></a>: Clean and assembly</li></ul></li><li><strong>Polish Contigs with <mark>Illumina Reads</mark></strong><ul><li>The 151 contigs were then polished with 912 million 150-bp Illumina pair-end reads, corrected, and integrated with high-quality optical maps (Additional file 1: Table S4-S5). This effort generated 69 scaffolds with a scaffold N50 of 72.53 Mb (Additional file 1: Table S6).</li><li><strong>Nextpolish-1.2.3</strong><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>: polished three rounds</li></ul></li><li><strong>Use <mark>Hi-C Data</mark> for Scaffold Ordering</strong><ul><li>A total of 274 Gb PE150 Hi-C data was used to order and orient the duck scaffolds, correct mis-joined sections, and merge overlaps, resulting in 40 super-scaffolds (Additional file 1: Table S7).</li><li><strong>Trimmomatic-0.36</strong><sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>; <strong>Juicer software-1.5</strong><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>; <strong>3d-DNA package-180922</strong><sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>; <strong>Juicebox-1.13.01</strong><sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup></li></ul></li><li><strong>Perform Gap Filling</strong><ul><li>Gap filling was performed using 95-fold corrected Nanopore reads to remove gaps, generating the final duck assembly (SKLA1.0), representing 1.16 Gb of the genomic sequence, approximately 99.11% of the estimated genome size (Table 1).</li><li><strong>Gapcloser-0.56</strong><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup></li></ul></li><li><strong>Chromosome Coverage and Comparison</strong><ul><li>Since the duck contains 80 chromosomes (diploid, 2n=80), it was inferred that this duck assembly had covered all chromosomes except W (Additional file 1: Table S8). The SKLA1.0 assembly was compared with the previous duck BGI_duck_1.0 assembly, the ZJU1.0 assembly, and two high-quality avian reference genomes (chicken GRCg6a and zebra finch bTaeGut1.4.pri). These analyses indicated that the SKLA1.0 assembly represents a major improvement over the previous assemblies in terms of contiguity, completeness, and chromosome size. The contiguity and completeness of SKLA1.0 is also higher than that of the zebra finch bTaeGut1.4.pri and the chicken GRCg6a (Fig. 1a–d and Table 1).</li></ul></li></ol><h4 id="After-Assembly">After Assembly</h4><ul><li>Funannotate pipeline and the <strong>GETA pipeline</strong> together with a manual curation of key gene families: 17,896 duck coding genes. Quality was validated by number of coding genes, # of transcripts, # of gaps, and <strong>BUSCO</strong> score.</li><li>Visualization: Bionano map-<a href="https://bionanogenomics.com/support/software-downloads">SOLVE</a></li></ul><h3 id="ASM2904224v1-Greater-scaup-Aythya-marila">ASM2904224v1: Greater scaup (Aythya marila)</h3><table><thead><tr><th style="text-align:left"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:left">First high-quality chromosome-level genome assembly of A. marila<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>, with a final genome size of 1.14 Gb, scaffold N50 of 85.44 Mb, and contig N50 of 32.46 Mb. A total of 154.94 Mb of repetitive sequences were identified. 15,953 protein-coding genes were predicted in the genome, and 98.96% of genes were functionally annotated.</td><td style="text-align:center"><img src="https://www.researchgate.net/profile/Inma-Aznar/publication/51580858/figure/fig19/AS:669632922923023@1536664338755/Mallard-Anas-platyrhynchos-Photo-John-Carey.png" alt="Duck"><a href="https://www.researchgate.net/publication/51580858_A_review_of_Ireland's_waterbirds_with_emphasis_on_wintering_migrants_and_reference_to_H5N1_avian_influenza?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">© Olivia Crowe</a></td></tr></tbody></table><ol><li><strong>Nanopore long reads</strong>, errors corrected using <strong>Illumina short reads</strong></li><li>Quality: Final size: 1.14 Gb, scaffold N50 of 85.44 Mb, and contig N50 of 32.46 Mb.</li><li>106 contigs were clustered and ordered onto 35 chromosomes based on <strong>Hi-C data</strong>, covering approximately 98.28% of the genome</li><li>BUSCO assessment showed that 97.0% of the highly conserved genes</li></ol><p>Source: muscle tissue of wild male.</p><ul><li>60.77 GB for Illumina HiSeq 4000: Illumina® TruSeq® Nano DNA Library Prep kits to generate sequencing libraries of genomic DNA</li><li>122.55 GB from PromethION platform (91.36 fold of the greater scaup’s genome)</li><li>63.43 Gb for Hi-C data</li></ul><h4 id="Genome-Assembly">Genome Assembly</h4><ol><li><strong>Quality Control</strong>:<ul><li>K = 17, the estimated genome size was 1,341.4 Mb, the heterozygosity was 0.47%, and the proportion of repetitive sequences was 42.28%</li><li>jellyfish (v2.2.7)</li></ul></li><li><strong>Assembly</strong>:<ul><li>assemble the genome with Oxford nanopore technologies (ONT) long reads</li><li>NextDenovo (v2.4.0)</li></ul></li><li><strong>Polish</strong>:<ul><li>increase the precision of single base with Illumina short reads</li><li>NextPolish12 (v1.3.1)<sup class="footnote-ref"><a href="#fn2" id="fnref2:1">[2:1]</a></sup></li></ul></li><li><strong>Scaffold Ordering</strong>:<ul><li>mount the contigs in preliminarily assembly onto chromosomes based on the signal strength after Hi-C data</li><li>ALLHiC (v0.9.8)<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> and Juicebox (v1.11.08)</li></ul></li></ol><table><thead><tr><th style="text-align:center">HiC Results for the global heat map of all the chromosomes.</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41597-023-02142-x/MediaObjects/41597_2023_2142_Fig1_HTML.png" alt=""></td></tr><tr><td style="text-align:center"><a href="https://www.nature.com/articles/s41597-023-02142-x">© Shengyang Zhou</a></td></tr></tbody></table><h4 id="After-Assembly-v2">After Assembly</h4><ol><li>Assessment<ul><li>Burrows-Wheeler aligner14 (BWA) (v0.7.8) to map Illumina reads to the genome with matching rate was approximately 98.80%.</li><li>Merqury15 (v1.3) was ran to evaluate assembly quality value (QV), and a high QV (42.14)</li><li>Benchmarking Universal Single-Copy Orthologs16 (BUSCO) (v5.4.4) (use option “–augustus”) and Core Eukaryotic Genes Mapping Approach17 (CEGMA) (v2.5) were also used to assess the integrity</li><li>238 of 248 core eukaryotic genes were detected using CEGMA</li></ul></li><li>Comparison<ul><li>Mummer18 (v4.0.0) was used to identify the synteny between A. marila and tufted duck19 (Aythya fuligula) genomes to determine orthologous chromosome pairs, and we used TBtools20 (v1.112) to draw the synteny between their chromosomes.</li></ul></li><li>Annotation of repetitive sequences<ul><li>de novo prediction: Tandem Repeats Finder21 (TRF) (v4.09) to detect tandem repeat sequences</li><li>RepeatModeler (v2.0.3), RepeatScout22 (v1.0.6) and RECON (v1.08) were used to build a database of transposable element (TE)</li><li>RepeatProteinMask and RepeatMasker (v4.1.2-p1) were used for homology prediction with Repbase database23 and Dfam database24, the species parameter used was chicken.</li></ul></li><li>Gene structure prediction<ul><li><strong>Prediction Methods</strong>:<ul><li><strong>Ab Initio Prediction</strong>: Used software Augustus (v3.3.2), GlimmerHMM (v3.0.4), and Geneid (v1.4.5).</li><li><strong>Homology-Based Prediction</strong>: Utilized genomes and annotation files from six related species (Anser cygnoides, Anas platyrhynchos, Aythya fuligula, Cygnus olor, Cygnus atratus, Gallus gallus) downloaded from NCBI.</li><li><strong>RNA-Seq Prediction</strong>: Processed raw data from six transcriptomic samples using fastp (v0.23.1), assembled paired-end reads with SPAdes (v3.15.3), identified candidate coding regions using TransDecoder (v5.5.0), and clustered sequences using CD-hit (v4.8.1).</li></ul></li><li><strong>Integration</strong>:<ul><li><strong>Matching and Splicing</strong>: Protein sequences from related species were matched to the A. marila genome using Spaln (v2.4.6) and accurately spliced with GeneWise (v2.4.1).</li><li><strong>Gene Set Generation</strong>: Combined homology-based, RNA-Seq, and ab initio predictions using EvidenceModeler (EVM) (v1.1.1) and incorporated masked repeats.</li></ul></li></ul></li><li><strong>Databases and Tools</strong>:<ul><li><strong>DIAMOND</strong>: Used for sequence alignment against SwissProt, TrEMBL, NR (Non-Redundant Protein Sequence Database), Gene Ontology (GO), and Kyoto Encyclopedia of Genes and Genomes Orthology (KO) databases, with an e-value cutoff of 1e-5.</li><li><strong>InterPro</strong>: Utilized for classifying proteins into families and predicting domains and important sites using InterProScan (v5.53).</li></ul></li><li>Filtering and Verification of Gene Set for A. marila<ol><li><strong>Ortholog Identification</strong>:<ul><li><strong>OrthoFinder</strong>: Used to identify orthologs among A. marila and six related species.</li><li>Resulted in 4,086 unassigned genes, of which 3,421 were not annotated in any database.</li></ul></li><li><strong>Filtering Process</strong>:<ul><li>Most unannotated genes (3,417/3,421) were predicted by at least one de novo prediction software, with only four supported by other evidence.</li><li>Removal of these unassigned genes did not affect the BUSCO test results, indicating they may not represent real genes.</li></ul></li><li><strong>Final Gene Set</strong>:<ul><li>After filtering out unassigned genes without annotations and 159 prematurely terminated genes, 15,953 genes remained, including 182 partial genes.</li><li>98.96% of the final gene set was annotated.</li></ul></li></ol></li></ol><h3 id="Pig-Sscrofa11-1">Pig Sscrofa11.1</h3><p>Warr A, Affara N, Aken B, et al. An improved pig reference genome sequence to enable pig genetics and genomics research[J]. Gigascience, 2020, 9(6): giaa051.</p><p>TJ Tabasco<br>corrected and assembled using Falcon (v.0.4.0)<br>65-fold coverage (176 Gb) of the genome<br>3,206 contigs with a contig N50 of 14.5 Mb.</p><p>Compare<br>contigs were mapped to the previous draft assembly (Sscrofa10.2) using Nucmer<br>gap closure using PBJelly</p><table><thead><tr><th>Statistic</th><th>Sscrofa10.2</th><th>Sscrofa11</th><th>Sscrofa11.1</th><th>USMARCv1.0</th><th>GRCh38.p13</th></tr></thead><tbody><tr><td>Total sequence length</td><td>2,808,525,991</td><td>2,456,768,445</td><td>2,501,912,388</td><td>2,755,438,182</td><td>3,099,706,404</td></tr><tr><td>Total ungapped length</td><td>2,519,152,092</td><td>2,454,899,091</td><td>2,472,047,747</td><td>2,623,130,238</td><td>2,948,583,725</td></tr><tr><td>No. of scaffolds</td><td>9,906</td><td>626</td><td>706</td><td>14,157</td><td>472</td></tr><tr><td>Gaps between scaffolds</td><td>5,323</td><td>24</td><td>93</td><td>0</td><td>349</td></tr><tr><td>No. of unplaced scaffolds</td><td>4,562</td><td>583</td><td>583</td><td>14,136</td><td>126</td></tr><tr><td>Scaffold N50</td><td>576,008</td><td>88,231,837</td><td>88,231,837</td><td>131,458,098</td><td>67,794,873</td></tr><tr><td>Scaffold L50</td><td>1,303</td><td>9</td><td>9</td><td>9</td><td>16</td></tr><tr><td>No. of unspanned gaps</td><td>5,323</td><td>24</td><td>93</td><td>0</td><td>349</td></tr><tr><td>No. of spanned gaps</td><td>233,116</td><td>79</td><td>413</td><td>661</td><td>526</td></tr><tr><td>No. of contigs</td><td>243,021</td><td>705</td><td>1,118</td><td>14,818</td><td>998</td></tr><tr><td>Contig N50</td><td>69,503</td><td>48,231,277</td><td>48,231,277</td><td>6,372,407</td><td>57,879,411</td></tr><tr><td>Contig L50</td><td>8,632</td><td>15</td><td>15</td><td>104</td><td>18</td></tr><tr><td>No. of chromosomes*</td><td>*21</td><td>19</td><td>*21</td><td>*21</td><td>24</td></tr></tbody></table><p>pig (Sscrofa10.2, Sscrofa11.1, USMARCv1.0), human (GRCh38.p13)</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Hu J, Song L, Ning M, Niu X, Han M, Gao C, Feng X, Cai H, Li T, Li F, Li H, Gong D, Song W, Liu L, Pu J, Liu J, Smith J, Sun H, Huang Y. A new chromosome-scale duck genome shows a major histocompatibility complex with several expanded multigene families. BMC Biol. 2024 Feb 5;22(1):31. doi: 10.1186/s12915-024-01817-0. PMID: 38317190; PMCID: PMC10845735. <a href="https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-024-01817-0#Sec26">Paper</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Hu J, Fan JP, Sun ZY, Liu SL. NextPolish: a fast and efficient genome polishing tool for long-read assembly. Bioinformatics. 2020;36:2253–5. <a href="#fnref2" class="footnote-backref">↩︎</a> <a href="#fnref2:1" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Bolger AM, Lohse M, Usadel B. Trimmomatic: a flexible trimmer for Illumina sequence data. Bioinformatics. 2014;30:2114–20. <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>Durand NC, Shamim MS, Machol I, Rao SSP, Huntley MH, Lander ES, et al. Juicer Provides a One-Click System for Analyzing Loop-Resolution Hi-C Experiments. Cell Syst. 2016;3:95–8. <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>Dudchenko O, Batra SS, Omer AD, Nyquist SK, Hoeger M, Durand NC, et al. De novo assembly of the Aedes aegypti genome using Hi-C yields chromosome-length scaffolds. Science. 2017;356:92–5. <a href="#fnref5" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p>Durand NC, Robinson JT, Shamim MS, Machol I, Mesirov JP, Lander ES, et al. Juicebox Provides a Visualization System for Hi-C Contact Maps with Unlimited Zoom. Cell Syst. 2016;3:99–101. <a href="#fnref6" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p>Xu MY, Guo LD, Gu SQ, Wang O, Zhang R, Peters BA, et al. TGS-GapCloser: A fast and accurate gap closer for large genomes with low coverage of error-prone long reads. Gigascience. 2020;9:giaa094–104. <a href="#fnref7" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p>Zhou S, Xia T, Gao X, et al. A high-quality chromosomal-level genome assembly of Greater Scaup (Aythya marila)[J]. Scientific Data, 2023, 10(1): 254. <a href="#fnref8" class="footnote-backref">↩︎</a></p></li><li id="fn9" class="footnote-item"><p>Zhang, X., Zhang, S., Zhao, Q., Ming, R. &amp; Tang, H. Assembly of allele-aware, chromosomal-scale autopolyploid genomes based on Hi-C data. Nature Plants 5, 833–845 (2019). <a href="#fnref9" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">Whole Genome Sequencing (WGS)</summary>
    
    
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Biology/"/>
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/categories/Biology/Bioinformatics/WGS/"/>
    
    
    <category term="Bioinformatics" scheme="https://karobben.github.io/tags/Bioinformatics/"/>
    
    <category term="NGS" scheme="https://karobben.github.io/tags/NGS/"/>
    
    <category term="Genome" scheme="https://karobben.github.io/tags/Genome/"/>
    
    <category term="WGS" scheme="https://karobben.github.io/tags/WGS/"/>
    
  </entry>
  
  <entry>
    <title>Simulated Annealing (SA)</title>
    <link href="https://karobben.github.io/2024/05/30/AI/SAnnealing/"/>
    <id>https://karobben.github.io/2024/05/30/AI/SAnnealing/</id>
    <published>2024-05-30T21:34:39.000Z</published>
    <updated>2024-08-16T21:01:46.659Z</updated>
    
    <content type="html"><![CDATA[<p>Video tutorial: <a href="https://www.youtube.com/watch?v=FyyVbuLZav8">Challenging Luck, 2021</a></p><p>Practicing Python code: <a href="https://github.com/challengingLuck/youtube/blob/master/sudoku/sudoku.py">challengingLuck: Using Annealing Algorithm to Solve the Sudo Challenge</a></p><blockquote><p>Simulated Annealing (SA) is a probabilistic technique used for finding an approximate solution to an optimization problem. It is particularly useful for problems where the search space is large and complex, and other methods might get stuck in local optima. Here’s a structured way to start learning about the Simulated Annealing algorithm:</p></blockquote><p>$$ P(E, E’, T) = \begin{cases}<br>1 &amp; \text{if } E’ &lt; E \\<br>\exp\left(\frac{-(E’ - E)}{T}\right) &amp; \text{if } E’ \ge E<br>\end{cases}<br>$$</p><p>This idea is similar to DNA annealing during PCR. During the temperature drop after the high-temperature mutation, the DNA gradually returns to the double strand with its reverse complement strand or the primer due to the decrease in entropy. Unlike DNA annealing, Simulated Annealing (SA) introduces random values to replace the original ones. After that, the “Energy” is calculated, and only when the new score is lower than the previous one is the new value accepted, continuing the iteration until the lowest value is found. It works like this:</p><p>Calculate the initial <code>E</code> → randomly mutate the value and calculate the new <code>E'</code> → if <code>E' ≤ E</code>, then accept the mutated element; otherwise, try another value → if <code>E'</code> meets the lowest <code>E</code>, stop; otherwise, continue until no smaller <code>E'</code> can be found.</p><p>As a result, you could expect that it would <mark>waste a lot of resources on exploration</mark> and easily <mark>fall into local optima</mark>.</p><p>An example of the SA apply in sudo challenge</p><p><img src="https://imgur.com/k4jbsQK.gif" alt="Simulated annealing in Sudo"><br><img src="https://imgur.com/uWGjeSm.png" alt="Simulated annealing in Sudo"><br>In this example, it actually <mark>failed</mark> to get the result because it <strong>fall into local optimal</strong>. It is a very good example to show the capability and limitations of the SA.</p><h2 id="SA-and-Stochastic-gradient-descent">SA and Stochastic gradient descent</h2><p>$$<br>w_{t+1} = w_t - \eta \nabla f(w_t; x_i)<br>$$</p><ul><li>$w$ is the parameter, weight matrix, for example. While $w_t$ is the old one and the $w_{t-1}$ is updated parameter.</li><li>$\eta$ is the learning rate</li><li>$x$ is the input</li></ul><blockquote><p><strong>Stochastic Gradient Descent (SGD)</strong> is an optimization algorithm used primarily for training machine learning models. It iteratively updates the model parameters by computing the gradient of the loss function using a randomly selected subset (mini-batch) of the training data, rather than the entire dataset. This randomness introduces variability in the updates, which helps escape local optima and speeds up convergence. The learning rate controls the step size of each update, determining how far the parameters move in the direction of the negative gradient. By continuously adjusting the parameters, SGD aims to minimize the loss function and improve the model’s performance.</p></blockquote><p>So, it is very similar to Stochastic gradient descent (SGD). But for SGD, there are a learning process from the data. SGD is primally based on the exploitation. But in SA, exploration has more contribution compared with SGD because it hugely relies on random generation first and evaluating later.</p><h2 id="SA-and-Genetic-Algorithm-GA">SA and Genetic Algorithm (GA)</h2><blockquote><p>What is GA?<br>Genetic Algorithms (GA) are evolutionary algorithms inspired by the principles of natural selection and genetics. They work by evolving a population of candidate solutions through successive generations. Each generation undergoes selection, where the fittest individuals are chosen based on their performance. These selected individuals then undergo crossover (recombination) to produce offspring that combine their parents’ characteristics. Additionally, mutation introduces random changes to some individuals to maintain genetic diversity within the population. This process of selection, crossover, and mutation allows GAs to explore a wide search space and balance between exploring new solutions and exploiting the best solutions found so far. This diversity helps GAs avoid getting trapped in local optima, making them effective for solving complex optimization problems, including those that are non-differentiable and non-convex.</p></blockquote><p>From a personal point of view, GA is like an upgraded version of SA. SA works at the level of a single individual, while GA operates at the population level. Similar to SA, GA evaluates and selects the “fitness scores” of each individual. The next generation introduces many random mutations, just like SA. However, unlike SA, GA also includes “crossover” steps, which can help enrich the “better phenotypes”.</p><table><thead><tr><th>Feature</th><th>Simulated Annealing (SA)</th><th>Stochastic Gradient Descent (SGD)</th><th>Genetic Algorithm (GA)</th></tr></thead><tbody><tr><td><strong>Approach</strong></td><td>Probabilistic, accepts worse solutions occasionally</td><td>Deterministic, updates in the direction of the gradient</td><td>Evolutionary, uses selection, crossover, and mutation</td></tr><tr><td><strong>Objective Function</strong></td><td>Non-differentiable and non-convex functions</td><td>Differentiable functions</td><td>Non-differentiable and non-convex functions</td></tr><tr><td><strong>Exploration vs. Exploitation</strong></td><td>Balances both, reduces acceptance of worse solutions over time</td><td>Primarily exploitation with some exploration via mini-batches</td><td>Balances both, uses population diversity to explore the search space</td></tr><tr><td><strong>Cooling Schedule / Learning Rate</strong></td><td>Uses a cooling schedule to reduce probability of accepting worse solutions</td><td>Uses a learning rate to control step size of updates</td><td>Uses selection pressure to favor better solutions and mutation rate to introduce diversity</td></tr><tr><td><strong>Population-Based</strong></td><td>No</td><td>No</td><td>Yes, operates on a population of solutions</td></tr><tr><td><strong>Escape Local Optima</strong></td><td>Yes, by accepting worse solutions with a probability</td><td>Limited, may get stuck in local optima</td><td>Yes, by maintaining a diverse population</td></tr><tr><td><strong>Gradient Requirement</strong></td><td>No</td><td>Yes</td><td>No</td></tr><tr><td><strong>Applications</strong></td><td>Combinatorial and continuous optimization without gradients</td><td>Training machine learning models, especially in deep learning</td><td>Optimization problems, including scheduling, design, and artificial intelligence</td></tr><tr><td><strong>Natural Inspiration</strong></td><td>Annealing in metallurgy</td><td>Gradient descent in calculus</td><td>Natural selection and genetics</td></tr><tr><td><strong>Operators</strong></td><td>Acceptance probability based on temperature</td><td>Gradient-based updates</td><td>Selection, crossover (recombination), and mutation</td></tr></tbody></table><blockquote><p>table from: ChatGPT4o</p></blockquote><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Simulated Annealing (SA) is a probabilistic technique used for finding an approximate solution to an optimization problem. It is particularly useful for problems where the search space is large and complex, and other methods might get stuck in local optima.</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="Math" scheme="https://karobben.github.io/categories/Machine-Learning/Math/"/>
    
    
    <category term="Math" scheme="https://karobben.github.io/tags/Math/"/>
    
    <category term="Algorithm" scheme="https://karobben.github.io/tags/Algorithm/"/>
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>AI Tools for Protein Structures</title>
    <link href="https://karobben.github.io/2024/05/29/AI/protein3dml/"/>
    <id>https://karobben.github.io/2024/05/29/AI/protein3dml/</id>
    <published>2024-05-29T19:28:45.000Z</published>
    <updated>2024-10-11T22:11:38.468Z</updated>
    
    <content type="html"><![CDATA[<h2 id="trRosetta">trRosetta</h2><p>They inverted this network to generate new protein sequences from scratch, aiming to design proteins with structures and functions not found in <a href="http://nature.By">nature.By</a> conducting <strong>Monte Carlo sampling</strong> in sequence space and optimizing the predicted structural features, they managed to produce a variety of new protein sequences.</p><h2 id="RFdiffusion">RFdiffusion</h2><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/iCXildL.png" alt="RFdiffsion"></td><td style="text-align:left">Watson, Joseph L., et al<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> published the RFdiffusion at <a href="https://github.com/RosettaCommons/RFdiffusion">github</a> in 2023. It fine-tune the <strong>RoseTTAFold</strong><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> and designed for tasks like: protein <strong>monomer</strong> design, protein <strong>binder</strong> design, <strong>symmetric oligomer</strong> design, <strong>enzyme active site</strong> scaffolding and symmetric <strong>motif scaffolding</strong> for therapeutic and <strong>metal-binding</strong> protein design. It is a very powerful tool according to the paper. It is based on the Denoising diffusion probabilistic models (<strong>DDPMs</strong>) which is a powerful class of machine learning models demonstrated to generate new photorealistic images in response to text prompts<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>.</td></tr></tbody></table><p>They use the ProteinMPNN<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> network to subsequently design sequences encoding theses structure. The diffusion model is based on the <strong>DDPMs</strong>. It can not only design a protein from generation, but also able to predict multiple types of interactions as is shown of the left. It was based on the RoseTTAFold.</p><p><strong>Compared with AF2</strong></p><ul><li>AlphaFold2 is like a very smart detective that can figure out the 3D shape of a protein just by looking at its amino acid sequence. On the other hand, RFdiffusion is more like an architect that designs entirely new proteins with specific properties. Instead of just figuring out shapes, it creates new proteins that can do things like bind to specific molecules or perform certain reactions. This makes it incredibly useful for designing new therapies and industrial enzymes.</li></ul><h2 id="ImmuneBuilder">ImmuneBuilder</h2><p><a href="https://www.nature.com/articles/s42003-023-04927-7">ImmuneBuilder: Deep-Learning models for predicting the structures of immune proteins</a></p><h3 id="Method-of-ABodyBuilder2">Method of ABodyBuilder2</h3><blockquote><ul><li>First, the heavy and light chain sequences are fed into four separate deep-learning models to predict an ensemble of structures. The closest structure to the average is then selected and refined using OpenMM<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup> to remove clashes and other stereo-chemical errors. The same pipeline is used for NanoBodyBuilder2 and TCRBuilder2.</li><li><strong>Training data set</strong>: 7084 structures from SAbDab. Filtering: No missing residues and resolution ≤ 3.5 Å</li><li><strong>Architect</strong>: The architecture of the deep learning model behind ABodyBuilder2 is an antibody-specific version of the structure module in <strong>AlphaFold-Multimer</strong> with several modifications</li><li><strong>Frame Aligned Point Error (FAPE) loss</strong> (like AFM)</li></ul></blockquote><p>A set of deep learning models trained to accurately predict the structure of antibodies (ABodyBuilder2), nanobodies (NanoBodyBuilder2) and T-Cell receptors (TCRBuilder2). ImmuneBuilder generates structures with state of the art accuracy while being much faster than AlphaFold2.</p><p>Experience it online: <a href="https://colab.research.google.com/github/brennanaba/ImmuneBuilder/blob/main/notebook/ImmuneBuilder.ipynb">Google Colab</a><br>GitHub: <a href="https://github.com/oxpig/ImmuneBuilder">oxpig/ImmuneBuilder</a></p><p>They have built three models</p><ul><li><strong>ABodyBuilder2</strong>, an antibody-specific model</li><li><strong>NanoBodyBuilder2</strong>, a nanobody-specific model</li><li><strong>TCRBuilder2</strong>, a TCR-specific model.</li></ul><p>It compared the performance with other similar tools:</p><ul><li>homology modelling method; <strong>ABodyBuilder</strong><sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup></li><li>general protein structure prediction method: <strong>AlphaFold-Multimer</strong><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup></li><li>antibody-specific methods: ABlooper<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> (ABL), IgFold<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> (IgF) and EquiFold<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup> (EqF)</li></ul><p>How: compare 34 antibody structures recently added</p><table class="data last-table"><thead class="c-article-table-head"><tr><th class="u-text-left "><p>Method</p></th><th class="u-text-left "><p>CDR-H1</p></th><th class="u-text-left "><p>CDR-H2</p></th><th class="u-text-left "><p>CDR-H3</p></th><th class="u-text-left "><p>Fw-H</p></th><th class="u-text-left "><p>CDR-L1</p></th><th class="u-text-left "><p>CDR-L2</p></th><th class="u-text-left "><p>CDR-L3</p></th><th class="u-text-left "><p>Fw-L</p></th></tr></thead><tbody><tr><td class="u-text-left "><p>ABodyBuilder (ABB)</p></td><td class="u-text-left "><p>1.53</p></td><td class="u-text-left "><p>1.09</p></td><td class="u-text-left "><p>3.46</p></td><td class="u-text-left "><p>0.65</p></td><td class="u-text-left "><p>0.71</p></td><td class="u-text-left "><p>0.55</p></td><td class="u-text-left "><p>1.18</p></td><td class="u-text-left "><p>0.59</p></td></tr><tr><td class="u-text-left "><p>ABlooper (ABL)</p></td><td class="u-text-left "><p>1.18</p></td><td class="u-text-left "><p>0.96</p></td><td class="u-text-left "><p>3.34</p></td><td class="u-text-left "><p>0.63</p></td><td class="u-text-left "><p>0.78</p></td><td class="u-text-left "><p>0.63</p></td><td class="u-text-left "><p>1.08</p></td><td class="u-text-left "><p>0.61</p></td></tr><tr><td class="u-text-left "><p>IgFold (IgF)</p></td><td class="u-text-left "><p>0.86</p></td><td class="u-text-left "><p>0.77</p></td><td class="u-text-left "><p>3.28</p></td><td class="u-text-left "><p>0.58</p></td><td class="u-text-left "><p>0.55</p></td><td class="u-text-left "><p>0.43</p></td><td class="u-text-left "><p>1.12</p></td><td class="u-text-left "><p>0.60</p></td></tr><tr><td class="u-text-left "><p>EquiFold (EqF)</p></td><td class="u-text-left "><p>0.86</p></td><td class="u-text-left "><p>0.80</p></td><td class="u-text-left "><p>3.29</p></td><td class="u-text-left "><p>0.56</p></td><td class="u-text-left "><p>0.47</p></td><td class="u-text-left "><p>0.41</p></td><td class="u-text-left "><p>0.93</p></td><td class="u-text-left "><p><b>0.54</b></p></td></tr><tr><td class="u-text-left "><p>AlphaFold-M (AFM)</p></td><td class="u-text-left "><p>0.86</p></td><td class="u-text-left "><p><b>0.68</b></p></td><td class="u-text-left "><p>2.90</p></td><td class="u-text-left "><p>0.55</p></td><td class="u-text-left "><p>0.47</p></td><td class="u-text-left "><p><b>0.40</b></p></td><td class="u-text-left "><p><b>0.83</b></p></td><td class="u-text-left "><p><b>0.54</b></p></td></tr><tr><td class="u-text-left "><p>ABodyBuilder2 (AB2)</p></td><td class="u-text-left "><p><b>0.85</b></p></td><td class="u-text-left "><p>0.78</p></td><td class="u-text-left "><p><b>2.81</b></p></td><td class="u-text-left "><p><b>0.54</b></p></td><td class="u-text-left "><p><b>0.46</b></p></td><td class="u-text-left "><p>0.44</p></td><td class="u-text-left "><p>0.87</p></td><td class="u-text-left "><p>0.57</p></td></tr></tbody></table><ul><li>What is an acceptable RMSD<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup>?</li></ul><div class="admonition note"><p class="admonition-title">What is an acceptable RMSD?</p><p>The experimental error in protein structures generated via X-ray crystallography has been estimated to be around <strong>0.6Å</strong> for regions with organised secondary structures (such as the antibody frameworks) and around <strong>1Å</strong> for protein loops.</p></div><h3 id="Side-Chain-Prediction">Side Chain Prediction</h3><p>ABlooper and IgFold only predict the position of backbones, leaving the side chain to OpenMM<sup class="footnote-ref"><a href="#fn5" id="fnref5:1">[5:1]</a></sup> and Rosetta<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup>, while EquiFold, AlphaFold-Multimer and ABodyBuilder2, all of which output all-atom structures.</p><h2 id="equifold">equifold</h2><p>Designing proteins to achieve specific functions often requires in silico modeling of their properties at high throughput scale and can significantly benefit from fast and accurate protein structure prediction. We introduce EquiFold, a new end-to-end differentiable, SE(3)-equivariant, all-atom protein structure prediction model. EquiFold uses a novel coarse-grained representation of protein structures that does not require multiple sequence alignments or protein language model embeddings, inputs that are commonly used in other state-of-the-art structure prediction models. Our method relies on geometrical structure representation and is substantially smaller than prior state-of-the-art models. In preliminary studies, EquiFold achieved comparable accuracy to AlphaFold but was orders of magnitude faster. The combination of high speed and accuracy make EquiFold suitable for a number of downstream tasks, including protein property prediction and design.</p><p><a href="https://github.com/Genentech/equifold">https://github.com/Genentech/equifold</a></p><h2 id="IgFold">IgFold</h2><p>Official repository for IgFold: Fast, accurate antibody structure prediction from deep learning on massive set of natural antibodies.</p><p>The code and pre-trained models from this work are made available for non-commercial use (including at commercial entities) under the terms of the JHU Academic Software License Agreement. For commercial inquiries, please contact Johns Hopkins Tech Ventures at <a href="mailto:awichma2@jhu.edu">awichma2@jhu.edu</a>.</p><p>Try antibody structure prediction in Google Colab.</p><p><a href="https://github.com/Graylab/IgFold">https://github.com/Graylab/IgFold</a></p><p>!!! Personal experience<br>I feel that the IgFold is kind of horrible in CDRH3 regions. It predicted CDRH3 loop in an erect conformation incorrectly. It is worse than ABodyBuilder2. It is even slower than ABodyBuilder2, too. It only takes the perfect Fab sequences. Any longer seqeunces would end up as a mass.</p><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Watson J L, Juergens D, Bennett N R, et al. De novo design of protein structure and function with RFdiffusion[J]. Nature, 2023, 620(7976): 1089-1100. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Baek M, et al. Accurate prediction of protein structures and interactions using a 3-track network. Science. July 2021. <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Ramesh, A. et al. Zero-shot text-to-image generation. in Proc. 38th International Conference on Machine Learning Vol. 139 (eds Meila, M. &amp; Zhang, T.) 8821–8831 (PMLR, 2021). <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>Dauparas J, Anishchenko I, Bennett N, et al. Robust deep learning–based protein sequence design using ProteinMPNN[J]. Science, 2022, 378(6615): 49-56. <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>Eastman, P. et al. OpenMM 7: rapid development of high-performance algorithms for molecular dynamics. PLoS Comput. Biol. 13, e1005659 (2017). <a href="#fnref5" class="footnote-backref">↩︎</a> <a href="#fnref5:1" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p>Leem, J., Dunbar, J., Georges, G., Shi, J. &amp; Deane, C. M. ABodyBuilder: automated antibody structure prediction with data-driven accuracy estimation. MAbs 8, 1259–1268 (2016). <a href="#fnref6" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p>Evans, R. et al. Protein complex prediction with AlphaFold-Multimer. bioRxiv (2021). <a href="#fnref7" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p>Abanades, B., Georges, G., Bujotzek, A. &amp; Deane, C. M. ABlooper: fast accurate antibody CDR loop structure prediction with accuracy estimation. Bioinformatics 38, 1877–1880 (2022). <a href="#fnref8" class="footnote-backref">↩︎</a></p></li><li id="fn9" class="footnote-item"><p>Ruffolo, J. A., Chu, L.-S., Mahajan, S. P. &amp; Gray, J. J. Fast, accurate antibody structure prediction from deep learning on massive set of natural antibodies. Nat. Commun. 14, 2389 (2023). <a href="#fnref9" class="footnote-backref">↩︎</a></p></li><li id="fn10" class="footnote-item"><p>Lee, J. H. et al. Equifold: Protein structure prediction with a novel coarse-grained structure representation. bioRxiv (2022). <a href="#fnref10" class="footnote-backref">↩︎</a></p></li><li id="fn11" class="footnote-item"><p>Eyal, E., Gerzon, S., Potapov, V., Edelman, M. &amp; Sobolev, V. The limit of accuracy of protein modeling: influence of crystal packing on protein structure. J. Mol. Biol. 351, 431–442 (2005).Return to ref 35 in article <a href="#fnref11" class="footnote-backref">↩︎</a></p></li><li id="fn12" class="footnote-item"><p>Alford, R. F. et al. The Rosetta all-atom energy function for macromolecular modeling and design. J. Chem. Theory Comput. 13, 3031–3048 (2017). <a href="#fnref12" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">AI Tools for Protein Structures</summary>
    
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/categories/Machine-Learning/"/>
    
    <category term="LM" scheme="https://karobben.github.io/categories/Machine-Learning/LM/"/>
    
    <category term="Protein" scheme="https://karobben.github.io/categories/Machine-Learning/LM/Protein/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="AI" scheme="https://karobben.github.io/tags/AI/"/>
    
    <category term="3D" scheme="https://karobben.github.io/tags/3D/"/>
    
    <category term="Protein Structure" scheme="https://karobben.github.io/tags/Protein-Structure/"/>
    
  </entry>
  
  <entry>
    <title>Birds Ig</title>
    <link href="https://karobben.github.io/2024/05/23/LearnNotes/birdig/"/>
    <id>https://karobben.github.io/2024/05/23/LearnNotes/birdig/</id>
    <published>2024-05-23T18:37:09.000Z</published>
    <updated>2024-06-06T05:38:51.270Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Specific-in-Birds">Specific in Birds</h2><p>Until 2024/5/23, the only annotated bird in IMGT is Chicken. According to the IMGT, chicken heavy chain gene has only <a href="https://www.imgt.org/IMGTrepertoire/index.php?section=LocusGenes&amp;repertoire=genetable&amp;species=Chicken&amp;group=IGHV">1 functional V gene </a>, <a href="https://www.imgt.org/IMGTrepertoire/index.php?section=LocusGenes&amp;repertoire=genetable&amp;species=Chicken&amp;group=IGHV">3 functional D genes</a>, and <a href="https://www.imgt.org/IMGTrepertoire/index.php?section=LocusGenes&amp;repertoire=genetable&amp;species=Chicken&amp;group=IGHJ">1 J gene</a>. This is hugely difference from human or other mammals.</p><p>Birds have three classes of antibody: <strong>IgM</strong> (the only class in all vertebrates), <strong>IgY</strong>, and <strong>IgA</strong>. Not been directly determined but inferred from size estimates of the intact molecules of <strong>IgM</strong> and <strong>IgA</strong>, they could form <mark>polypeptide chains</mark><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>. In addition, ducks have a smaller form of IgY, called IgY (ΔFc).</p><ul><li>IgM is the only class of antibody that is found in all vertebrate. IgM is larger than that of a true tetrameric IgM, such as occurs in teleost fish<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> and thus both are likely to be pentameric.</li><li>IgY is the major low-molecular weight form of antibody found circulating in birds, where it has sometimes been referred to as IgG. An IgY-like molecule is likely to have been the evolutionary precursor of both IgG and IgE immunoglobulins<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>.</li></ul><h2 id="Chicken">Chicken</h2><p>The first avian genomic MHC map was the chicken minimal and essential one on chromosome 16. This map, spanning 92 kb and harboring 19 genes, was then extended to be 242 kb containing 46 genes<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup><sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>.</p><blockquote><p><a href="https://www.imgt.org/IMGTrepertoire/LocusGenes/locusdesc/chicken/IGH/Galgal_IGHdesc.html">Chicken (Gallus gallus) IGH locus on chromosome 31</a></p><ul><li>The chicken (Gallus gallus) IGH locus is on chromosome 31. The orientation of the locus is reverse (REV).</li><li>The chicken (Gallus gallus) IGH locus spans 116 kilobases (kb), from 10 kb upstream of the most 5’ gene in the locus, IGHV1-83 §, to 10 kb downstream of the most 3’ gene in the locus, IGHJ (F). The Locus representation encompasses 120 kb.</li><li>The chicken (Gallus gallus) IGH locus consists of 94 IGHV genes belonging to 1 IGHV subgroup, 4 IGHD belonging to 1 IGHD set, 1 IGHJ gene belonging to 1 IGHJ set and 3 IGHC genes.</li><li>The IGHV genes span 97 kilobases (kb), the IGHD genes span 7251 bases (b) and the IGHJ gene span 12 kilobases (kb).<br>source: Imgt</li></ul></blockquote><p><a href="https://www.imgt.org/ligmdb/view?id=IMGT000014">https://www.imgt.org/ligmdb/view?id=IMGT000014</a></p><p>The generation of antibody binding-site diversity is very well understood for the chicken:</p><ul><li>Parvari R, Avivi A, Lentner F, Ziv E, Tel-Or S, Burstein Y, et al. Chicken immunoglobulin gamma-heavy chains: limited VH gene repertoire, combinatorial diversification by D gene segments and evolution of the heavy chain locus. EMBO J. 1988;7:739–44.</li><li>Reynaud CA, Anquez V, Dahan A, Weill JC. A single rearrangement event generates most of the chicken immunoglobulin light chain diversity. Cell. 1985;40:283–91.</li><li>Reynaud CA, Anquez V, Grimal H, Weill JC. A hyperconversion mechanism generates the chicken light chain preimmune repertoire. Cell. 1987;48:379–88.</li><li>Reynaud CA, Dahan A, Anquez V, Weill JC. Somatic hyperconversion diversifies the single Vh gene of the chicken with a high incidence in the D region. Cell. 1989;59:171–83.</li></ul><h2 id="Duck">Duck</h2><p>The newest Duck genome is available at BMC: <a href="https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-024-01817-0">Jiaxiang Hu, et al; 2024</a><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>. The old one ZJU1.0<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> has (<a href="https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_015476345.1/">GCF_015476345.1</a>) 33 chromosomes (not included sexual and mitochondira). For the SKLA1.0 (<a href="https://www.ncbi.nlm.nih.gov/datasets/genome/GCA_037218355.1/">GCA_037218355.1</a>), it covers 40 chromosomes (included in Z chromosome). For the ZJU1.0, they used 115 SMRT cells were sequenced with PacBio RS II. In SJKA1.0, they integrated Nanopore, Bionano, and Hi-C data. It also contains a complete genomic map of the MHC.</p><p>In E. W´ojcik and E. Smalec’s work in 2017<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup>, they described 1-13 autosomes. In there karyotyping analysis result, the got about 84 dots for Anas platyrhynchos.</p><table><thead><tr><th style="text-align:center"><img src="https://imgur.com/YLYWzF0.png" alt="karyotype of Anas platyrhynchos"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://www.researchgate.net/publication/312024031_Constitutive_heterochromatin_in_chromosomes_of_duck_hybrids_and_goose_hybrids">© E. W´ojcik; 2017</a></td></tr></tbody></table><p>Anseriform birds (ducks and their relatives) are the closest relatives of the chickens which was well understood and be studied.</p><p>Ducks have the same hematopoietic tissues as chickens, including bone marrow, gut associated lymphoid tissue, spleen, thymus and the Bursa of Fabricius, a specialized organ for B lymphoid development. However, there is one notable difference. Ducks have lymph nodes, which are completely absent in chickens<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup>.</p><blockquote><ol><li>a single functional rearrangement of the variable (V) region, like chicken.</li><li>generate diversity through gene conversion from a pool of pseudogenes.</li><li>V region element and the pseudogenes appear to consist of a single gene family (The same as the Chicken)</li><li>Further analysis of 26 heavy chain joining (JH) and 27 light chain JL segments shows there is use of a single J segment in ducks.<br>From: Lundqvist M L, et al.<sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1:1]</a></sup></li></ol></blockquote><p>The overwhelming evidence is that all birds express only a single class of immunoglobulin light (L) chain<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup><sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup> most closely related to the λ chain of the mammals <sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup><sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup>. The suggestion of additional classes of L chain in the birds <sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup> has not been substantiated at the amino acid sequence or genetic level.</p><h3 id="Duck-IgY">Duck IgY</h3><p>Ducks IgY:</p><ul><li>secreted form</li><li>a receptor form with a hydrophobic membrane-spanning C-terminus</li><li>truncated form termed IgY(ΔFc)</li></ul><p>Difference and similarities in other species:</p><ul><li>No truncated form: Chickens express only the full-length and membrane-receptor forms of IgY<sup class="footnote-ref"><a href="#fn3" id="fnref3:1">[3:1]</a></sup>.</li><li>Has truncated form: A small form of IgY (5.7S) is produced by some species of turtles<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup>.</li></ul><p>Duck immune response inept:</p><ul><li>lacking: precipitation, agglutination, complement fixation and opsonization<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup><sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup> (related the lacks an Fc region).</li></ul><h3 id="Duck-IgA">Duck IgA</h3><p><strong>IgA</strong> has been described to date only in mammals and birds</p><ul><li><p>secretions of the gut, respiratory and reproductive tracts, as well as in tears, bile and (in mammals) the milk</p></li><li><p>IgA of mammals is typically a dimer.</p></li><li><p><strong>Duck IGH</strong>:</p><ol><li>a single family of VH sequences</li><li>a single expressed JH element</li><li>JH is immediately downstream of a D segment</li></ol></li><li><p><strong>Duck IGL</strong>:</p><ol><li>there are few functional coding VL and JL elements in the germline</li><li>diversification most likely arises in large measure from gene conversion events from an extensive suite of germline VL-related sequences; (Genomic Southern blot analysis showed that there is a large family of germline VL-related sequences in the mallard duck<sup class="footnote-ref"><a href="#fn11" id="fnref11:1">[11:1]</a></sup>)</li><li>It is not known whether there is a single functional V and J element in the duck L chain locus, however, that is the simplest explanation of the results.</li><li>muscovy duck were at least two functional VL genes<sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup>.</li><li>a single family of VL sequences</li></ol></li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Lundqvist M L, Middleton D L, Radford C, et al. Immunoglobulins of the non-galliform birds: antibody expression and repertoire in the duck[J]. Developmental &amp; Comparative Immunology, 2006, 30(1-2): 93-100. <a href="#fnref1" class="footnote-backref">↩︎</a> <a href="#fnref1:1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Leslie GA, Clem LW. Phylogeny of immunoglobulin structure and function. 3. Immunoglobulins of the chicken. J Exp Med. 1969;130:1337–52. <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Parvari R, Avivi A, Lentner F, Ziv E, Tel-Or S, Burstein Y, et al. Chicken immunoglobulin gamma-heavy chains: limited VH gene repertoire, combinatorial diversification by D gene segments and evolution of the heavy chain locus. EMBO J. 1988;7:739–44. <a href="#fnref3" class="footnote-backref">↩︎</a> <a href="#fnref3:1" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>Magor KE, Higgins DA, Middleton DL, Warr GW. One gene encodes the heavy chains for three different forms of IgY in the duck. J Immonol. 1994;153:5549–55. <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>Kaufman J, Milne S, Göbel TW, Walker BA, Jacob JP, Auffray C, et al. The chicken B locus is a minimal essential major histocompatibility complex. Nature. 1999;401:923–5. <a href="#fnref5" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p>Shiina T, Briles WE, Goto RM, Hosomichi K, Yanagiya K, Shimizu S, et al. Extended gene map reveals tripartite motif, C-type lectin, and Ig superfamily type genes within a subregion of the chicken MHC-B affecting infectious disease. J Immunol. 2007;178:7162–72. <a href="#fnref6" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p>Hu J, Song L, Ning M, et al. A new chromosome-scale duck genome shows a major histocompatibility complex with several expanded multigene families[J]. BMC biology, 2024, 22(1): 31. <a href="#fnref7" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p>Li J, Zhang J, Liu J, et al. A new duck genome reveals conserved and convergently evolved chromosome architectures of birds and mammals[J]. Gigascience, 2021, 10(1): giaa142. <a href="#fnref8" class="footnote-backref">↩︎</a></p></li><li id="fn9" class="footnote-item"><p><a href="https://www.researchgate.net/publication/312024031_Constitutive_heterochromatin_in_chromosomes_of_duck_hybrids_and_goose_hybrids">Wójcik E, Smalec E. Constitutive heterochromatin in chromosomes of duck hybrids and goose hybrids[J]. Poultry Science, 2017, 96(1): 18-26.<br></a> <a href="#fnref9" class="footnote-backref">↩︎</a></p></li><li id="fn10" class="footnote-item"><p>Flajnik MF, Miller KM, Du Pasquier L. Evolution of the immune system. In: Paul WE, editor. Fundamental immunology. 5th ed. Philadelphia: Lippincott Williams and Wilkins; 2003. p. 519–70. <a href="#fnref10" class="footnote-backref">↩︎</a></p></li><li id="fn11" class="footnote-item"><p>Magor KE, Higgins DA, Middleton DL, Warr GW. cDNA sequence and organization of the immunoglobulin light chain gene of the duck, Anas platyrhynchos. Dev Comp Immunol. 1994;18:523–31. <a href="#fnref11" class="footnote-backref">↩︎</a> <a href="#fnref11:1" class="footnote-backref">↩︎</a></p></li><li id="fn12" class="footnote-item"><p>Reynaud CA, Dahan A, Weill JC. Complete sequence of a chicken lambda light chain immunoglobulin derived from the nucleotide sequence of its mRNA. Proc Natl Acad Sci USA. 1983;80:4099–103. <a href="#fnref12" class="footnote-backref">↩︎</a></p></li><li id="fn13" class="footnote-item"><p>Grant JA, Sanders B, Hood L. Partial amino acid sequences of chicken and turkey immunoglobulin light chains. Homology with mammalian lambda chains. Biochemistry. 1971;10:3123–32. <a href="#fnref13" class="footnote-backref">↩︎</a></p></li><li id="fn14" class="footnote-item"><p>Kubo RT, Rosenblum IY, Benedict AA. The unblocked N-terminal sequence of chicken IgG lambda-like light chains. J Immunol. 1970;105:534–6. <a href="#fnref14" class="footnote-backref">↩︎</a></p></li><li id="fn15" class="footnote-item"><p>Leslie GA. Evidence for a second avian light chain isotype. Immunochemistry. 1977;14:149–51. <a href="#fnref15" class="footnote-backref">↩︎</a></p></li><li id="fn16" class="footnote-item"><p>Leslie GA, Clem LW. Phylogeny of immunoglobulin structure and function, VI. 17S, 7.5S and 5.7S anti-DNP of the turtle, Pseudamys scripta. J Immunol. 1972;108:1656–64. <a href="#fnref16" class="footnote-backref">↩︎</a></p></li><li id="fn17" class="footnote-item"><p>Grey HM. Duck immunoglobulins. II. Biologic and immunochemical studies. J Imunol. 1967;98:820–6. <a href="#fnref17" class="footnote-backref">↩︎</a></p></li><li id="fn18" class="footnote-item"><p>Humphrey BD, Calvert CC, Klasing KC. The ratio of full length IgY to truncated IgY in immune complexes affects macrophage phagocytosis and the acute phase response of mallard ducks (Anas platyrhynchos) Dev Comp Immunol. 2004;28:665–72. <a href="#fnref18" class="footnote-backref">↩︎</a></p></li><li id="fn19" class="footnote-item"><p>McCormack WT, Carlson LM, Tjoelker LW, Thompson CB. Evolutionary comparison of the avian IgL locus: combinatorial diversity plays a role in the generation of the antibody repertoire in some avian species. Int Immunol. 1989;1:332–41. <a href="#fnref19" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">The avian immune system exhibits a unique immunoglobulin (Ig) system characterized by distinct features absent in other vertebrates. Birds possess a specialized IgY, which serves as the functional equivalent to mammalian IgG and IgE, but with significant structural and functional differences. Unlike mammalian systems, birds utilize a limited number of germline gene segments and rely on gene conversion within the bursa of Fabricius to generate antibody diversity. This mechanism allows for a rapid and diverse immune response, showcasing the evolutionary adaptation of birds to their ecological niches and pathogen challenges.</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Notes/Biology/"/>
    
    <category term="Immunity" scheme="https://karobben.github.io/categories/Notes/Biology/Immunity/"/>
    
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Antibody" scheme="https://karobben.github.io/tags/Antibody/"/>
    
    <category term="Immunology" scheme="https://karobben.github.io/tags/Immunology/"/>
    
  </entry>
  
  <entry>
    <title>Antibody 12/23 rule</title>
    <link href="https://karobben.github.io/2024/05/18/LearnNotes/ab1223rule/"/>
    <id>https://karobben.github.io/2024/05/18/LearnNotes/ab1223rule/</id>
    <published>2024-05-18T14:49:22.000Z</published>
    <updated>2024-05-29T19:31:31.797Z</updated>
    
    <content type="html"><![CDATA[<h2 id="How-Does-Antibody-Fragments-Jointed-Together">How Does Antibody Fragments Jointed Together?</h2><p>We all know that antibodies are composed of V, D, and J segments, which originate from different locations on the chromosome. But how are they connected together after post-transcriptional modification? The 12/23 rule is the fundamental mechanism that ensures proper recombination of these segments.</p><blockquote><p>The V region, or V domain, of an immunoglobulin heavy or light chain is encoded by more than one gene segment. For the light chain, the V domain is encoded by two separate DNA segments. The first segment encodes the first 95–101 amino acids of the light chain and is termed a V gene segment because it encodes most of the V domain. The second segment encodes the remainder of the V domain (up to 13 amino acids) and is termed a joining or J gene segment<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</p></blockquote><h2 id="What-Is-12-23-Rule">What Is 12/23 Rule</h2><p>Video tutorial: <a href="https://www.youtube.com/watch?v=QTOBSFJWogE">Daniel Levy; 2013. VDJ Gene Recombination</a></p><p>The 12/23 rule is a principle in V(D)J recombination, a process crucial for generating the diversity of antibodies and T-cell receptors. It states that recombination can only occur between gene segments flanked by recombination signal sequences (RSS) with spacers of 12 base pairs (bp) and 23 bp. This ensures proper alignment and prevents inappropriate recombination events, maintaining the integrity and functionality of the immune system’s response.</p><table><thead><tr><th style="text-align:center"><img src="https://www.ncbi.nlm.nih.gov/books/NBK27140/bin/CH4F5.jpg" alt="12/23 rule illustration"></th></tr></thead><tbody><tr><td style="text-align:center">© Charles A. Janeway</td></tr></tbody></table><p>This image illustrates the 12/23 rule in the context of V(D)J recombination.</p><ol><li><p><strong>Recombination Signal Sequences (RSS)</strong>:</p><ul><li><strong>Heptamer</strong>: A conserved sequence of 7 base pairs.</li><li><strong>Nonamer</strong>: A conserved sequence of 9 base pairs.</li><li><strong>Spacer</strong>: The region between the heptamer and nonamer, either 12 or 23 base pairs long.</li></ul></li><li><p><strong>V(D)J Recombination Process</strong>:</p><ul><li><strong>Segments</strong>: V (Variable), D (Diversity), and J (Joining) segments.</li><li><strong>Rule Application</strong>: The 12/23 rule ensures that only segments with RSS of different spacers (12 and 23 bp) recombine, facilitating the correct assembly of these segments.</li></ul></li></ol><p>The image visually represents how the rule guides the alignment and recombination of V, D, and J gene segments, which is essential for generating the diversity of antibodies and T-cell receptors.</p><h2 id="How-It-Worked">How It Worked</h2><p>The heptamer and nonamer is the recombination signal sequences (RSSs). The (RAG1-RAG2)2 endonuclease complex (RAG) specifically recognizes and cleaves a pair of RSSs<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p><table><thead><tr><th style="text-align:center"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0959440X18301143-gr3.jpg" alt=""></th></tr></thead><tbody><tr><td style="text-align:center">© Ru H<sup class="footnote-ref"><a href="#fn2" id="fnref2:1">[2:1]</a></sup></td></tr></tbody></table><h2 id="How-Does-It-Applied">How Does It Applied</h2><h3 id="IgDetective">IgDetective</h3><p><a href="https://github.com/Immunotools/IgDetective">IgDetective</a>, published by Vikram Sirupurapu and Yana Safonova<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>, is a tool for detecting and naming antibodies based on the 12/23 rule. This tool leverages the stringent application of the 12/23 rule among mammals, making it suitable primarily for mammalian species.</p><ul><li>Test: not suitable for birds like chicken</li></ul><h3 id="williamdlees-Digger">williamdlees-Digger</h3><p>This is another <a href="https://github.com/williamdlees/digger">open source</a> tool developed by William D. Lees<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>, aimed at annotating the positions of V/D/J genes on newly assembled genomes. According to the results shown in the <a href="https://williamdlees.github.io/digger/_build/html/examples/human_igh.html">documentation</a>, it has very high accuracy.</p><p>Prerequisite:</p><ul><li>a set of known core coding region allele sequences</li><li>Position-weighted matrices</li></ul><p>Some technic details worthy to know:</p><ul><li>The search for V-sequences uses the parameters gapopen 5, gapextend 5, penalty -1, word_size 11.</li><li>D and J searches, word_size is reduced to 7 to reflect the shorter sequences</li><li>D sequences the evalue is set to 100 rather than the default of 10 to widen the search.<br>How it find the heptamers and nonamers:</li></ul><table><thead><tr><th>Type</th><th>Functionality criteria</th></tr></thead><tbody><tr><td>V</td><td>RSS nonamer and heptamer pass PWM thresholds, and match canonical consensus, if defined.<br>L-PART1 and L-PART2 pass PWM thresholds and splice to form a coding sequence with no STOP-CODONs that is in-frame with the V sequence.<br>V-REGION is in-frame and has first and second cysteines at the correct positions in the IMGT alignment.<br>No STOP-CODONs are present in the V-REGION before the second cysteine.</td></tr><tr><td>D</td><td>RSS nonamers and heptamers match canonical consensus, if defined.</td></tr><tr><td>J</td><td>RSS nonamer and heptamer pass PWM thresholds, and match canonical consensus, if defined.<br>The J-motif is found at the expected position relative to the end of the J sequence.<br>The donor splice is found at the expected position, given the length of the matched sequence.</td></tr></tbody></table><p><strong>Limitation:</strong> It only supports <mark>IMGT well-annotated species</mark> because it relies on germline annotation from the IMGT database.<br><strong>Merits:</strong> Easy used and to be understood (write by python).</p><table><thead><tr><th style="text-align:center"><img src="https://williamdlees.github.io/digger/_build/html/_images/igh_results.jpg" alt="Digger Results"></th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://williamdlees.github.io/digger/_build/html/examples/human_igh.html">© William D. Leeszs</a></td></tr></tbody></table><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="code"><pre><code class="hljs bash">extract_refs -L IGH <span class="hljs-string">&quot;Gallus gallus&quot;</span><br>fix_macaque_gaps Gallus_gallus_IGHV_gapped.fasta \<br>    Gallus_gallus_IGHV_gapped_fixed.fasta IGH<br>cat Gallus_gallus_IGHV.fasta Gallus_gallus_IGHD.fasta Gallus_gallus_IGHJ.fasta \<br>    &gt; Gallus_gallus_IGHVDJ.fasta<br><br>parse_imgt_annotations --save_sequence IMGT000014.fasta \<br>   <span class="hljs-string">&quot;https://www.imgt.org/ligmdb/view.action?format=IMGT&amp;id=IMGT000014&quot;</span> \<br>      IMGT000014_genes.csv IGH<br><br>cat IMGT000014.fasta &gt; Mmul_051212.fasta<br><br><br>mkdir motifs<br><span class="hljs-built_in">cd</span> motifs<br>parse_imgt_annotations \<br>        <span class="hljs-string">&quot;https://www.imgt.org/ligmdb/view?format=IMGT&amp;id=IMGT000014&quot;</span> \<br>        IMGT000014_genes.csv IGH<br>calc_motifs IGH IMGT000014_genes.csv<br><span class="hljs-built_in">cd</span> ..<br><br>makeblastdb -<span class="hljs-keyword">in</span> Gallus_gallus_IGHV.fasta -dbtype nucl<br>makeblastdb -<span class="hljs-keyword">in</span> Gallus_gallus_IGHD.fasta -dbtype nucl<br>makeblastdb -<span class="hljs-keyword">in</span> Gallus_gallus_IGHJ.fasta -dbtype nucl<br><br>blastn -db Gallus_gallus_IGHV.fasta -query Mmul_051212.fasta -out mmul_IGHV.out \<br> -outfmt 7 -gapopen 5 -gapextend 5 -penalty -1 -word_size 11<br>blastn -db Gallus_gallus_IGHD.fasta -query Mmul_051212.fasta -out mmul_IGHD.out \<br> -outfmt 7 -gapopen 5 -gapextend 5 -penalty -1 -word_size 4 -evalue 100<br>blastn -db Gallus_gallus_IGHJ.fasta -query Mmul_051212.fasta -out mmul_IGHJ.out \<br> -outfmt 7 -gapopen 5 -gapextend 5 -penalty -1 -word_size 4<br><br><br><br>blastresults_to_csv mmul_IGHV.out mmul_ighvdj_<br>blastresults_to_csv mmul_IGHD.out mmul_ighvdj_ -a<br>blastresults_to_csv mmul_IGHJ.out mmul_ighvdj_ -a<br><br>find_alignments Gallus_gallus_IGHVDJ.fasta \<br>       Mmul_051212.fasta \<br>       <span class="hljs-string">&quot;mmul_ighvdj_nw_*.csv&quot;</span> \<br>       -ref imgt,Gallus_gallus_IGHVDJ.fasta \<br>       -align Gallus_gallus_IGHV_gapped_fixed.fasta \<br>       -motif_dir motifs \<br>       Mmul_051212.csv<br><br>digger ../../../Duck/data/GCA_015476345.1_ZJU1.0_genomic.fna \<br>   -v_ref Homo_sapiens_IGHV.fasta \<br>   -d_ref Homo_sapiens_IGHD.fasta \<br>   -j_ref Homo_sapiens_IGHJ.fasta \<br>   -v_ref_gapped Homo_sapiens_IGHV_gapped.fasta \<br>   -ref imgt,Homo_sapiens_IGHVDJ.fasta \<br>   -species Chicken  \<br>   -locus IGH \<br>   IMGT000035.csv<br><br><br></code></pre></td></tr></table></figure></div><h2 id="How-It-Really-Looks-Like-in-Human-Genome">How It Really Looks Like in Human Genome</h2><p>I randomly checked a few sequences from the homo genome and found that those regions from different V, D, and J genes are very similar. It could because that the 12/23 rule is not very stringent but flexible. But it could also caused by they are prevented to be recombined.</p><p><img src="https://imgur.com/jqUHyK3.png" alt=""></p><style>pre {  background-color:#38393d;  color: #5fd381;}</style><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Janeway C, Travers P, Walport M, et al. Immunobiology: the immune system in health and disease[M]. New York: Garland Pub., 2001. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Ru H, Zhang P, Wu H. Structural gymnastics of RAG-mediated DNA cleavage in V (D) J recombination[J]. Current opinion in structural biology, 2018, 53: 178-186. <a href="#fnref2" class="footnote-backref">↩︎</a> <a href="#fnref2:1" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Sirupurapu V, Safonova Y, Pevzner P A. Gene prediction in the immunoglobulin loci[J]. Genome research, 2022, 32(6): 1152-1169. <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>Lees W D, Saha S, Yaari G, et al. Digger: directed annotation of immunoglobulin and T cell receptor V, D, and J gene sequences and assemblies[J]. Bioinformatics, 2024, 40(3): btae144. <a href="#fnref4" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">The 12/23 rule is fundamental in the V(D)J recombination process</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Biology" scheme="https://karobben.github.io/categories/Notes/Biology/"/>
    
    <category term="Immunity" scheme="https://karobben.github.io/categories/Notes/Biology/Immunity/"/>
    
    
    <category term="Biology" scheme="https://karobben.github.io/tags/Biology/"/>
    
    <category term="Antibody" scheme="https://karobben.github.io/tags/Antibody/"/>
    
    <category term="Immunology" scheme="https://karobben.github.io/tags/Immunology/"/>
    
  </entry>
  
  <entry>
    <title>Multi-layer Neural Nets</title>
    <link href="https://karobben.github.io/2024/02/18/AI/ai-multilayer/"/>
    <id>https://karobben.github.io/2024/02/18/AI/ai-multilayer/</id>
    <published>2024-02-19T05:49:17.000Z</published>
    <updated>2024-05-31T23:00:51.048Z</updated>
    
    <content type="html"><![CDATA[<h2 id="From-linear-to-nonlinear-classifiers">From linear to nonlinear classifiers</h2><ul><li>Linear classifier<ul><li>a linear classifier computes $f(x) = argmax\ Wx$</li><li>The resulting classifier divides the x-space into Voronoi regions: convex regions with piece-wise linear boundaries</li></ul></li><li>Nonlinear classifier<ul><li>Not all classification problems have convex decision regions with PWL boundaries!</li><li>Here’s an example problem in which class 0 (blue) includes values of x near [0.8,0]<sup>T</sup>, but it also includes some values of x near [0.4,0.9]<sup>T</sup></li><li>You can’t compute this function using: $f(x) = argmax\ Wx$</li></ul></li><li>The solution: Piece-wise linear functions<ul><li>Nonlinear classifiers, can be learned using piece-wise linear classification boundaries</li><li>Nonlinear regression problems, can be learned using piece-wise linear regression</li><li>In the limit, as the number of pieces goes to infinity, the approximation approaches the desired solution</li></ul></li></ul><h2 id="Introduction">Introduction</h2><p>Video tutorial: <a href="https://www.youtube.com/watch?v=ErnWZxJovaM&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=1">Intro to Deep Learning; Apr. 29, 2024</a><br>Slides PDF: <a href="http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L1.pdf">Slides</a></p><h3 id="Perceptron-and-Neural-Network">Perceptron and Neural Network</h3><p>For multi Output Perceptron:</p><table><thead><tr><th style="text-align:center">Multi Output Perceptron</th><th style="text-align:center">Single Layer Neural Network</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/tL6UjLX.png" alt=""></td><td style="text-align:center"><img src="https://imgur.com/4YyvzOt.png" alt=""></td></tr><tr><td style="text-align:center">$$z_i = w_{0,i} + \sum^m_{j=1} x_j w_{j,i}$$</td><td style="text-align:center">$$ z_i = w_{0,i}^{(1)} + \sum_{j=1}^{m} x_j w_{j,i}^{(1)} $$  $$ \hat{y}_ i = g \left( w_ {0,i}^ {(2)} + \sum_ {j=1}^ {d_ 1} g(z_ j) w_ {j,i}^ {(2)} \right) $$</td></tr><tr><td style="text-align:center"><a href="https://www.youtube.com/watch?v=ErnWZxJovaM&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=1">© Alexander Amini</a></td><td style="text-align:center"><a href="https://www.youtube.com/watch?v=ErnWZxJovaM&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=1">© Alexander Amini</a></td></tr></tbody></table><p>By comparing them based on this illustration, we can see that the Perceptron and neural network architectures are very similar. The difference lies in the output parts. For a Perceptron, after the perceptron learns the $z$, the results are based directly on $g(z)$. However, in a neural network, after the model learns $z$, it still needs to learn the $w^{(2)}$, and the result is based on both $z$ and $w^{(2)}$. In this case, $z$ becomes a hidden layer.</p><p>For Deep neural network, we just simply increasing the layers of hidden layer which is $z_ n → z_ {n, m}$.</p><h2 id="Quantifying-Loss">Quantifying Loss</h2><p>By following the function above, we know that for a single layer neural net work with single output, $\hat{y} = g \left( w^ {(2)} + \sum_ {j=1}^ {d_ 1} g(z_ j) w_ {j}^ {(2)} \right) $ or just $\hat{y} = g(x^{(i)}; W)$. So, we could define that the loss: $\mathcal{L}(f(x^{(i)}; \mathbf{W}), y^{(i)})$. Hence, the Empirical Loss which measure the total loss should be:</p><p>$$<br>J(W) = \frac{1}{n} \sum^n_ {i=1} \mathcal{L}(f(x^{(i)}; \mathbf{W}), y^{(i)})<br>$$</p><p>According to the classification test or regression test, we could selected tow types of basic loss function:</p><p><strong>Binary Cross-Entropy Loss:</strong></p><ul><li>$ \mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \left[ y^{(i)} \log(f(x^{(i)})) + (1 - y^{(i)}) \log(1 - f(x^{(i)})) \right] $</li></ul><p><strong>Mean Squared Error Loss:</strong></p><ul><li>$ \mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} \left( y^{(i)} - f(x^{(i)}) \right)^2 $</li></ul><h2 id="Training">Training</h2><p>The logic of training is very simple and clear: we want to find the weight that <mark>achieve the lowest loss</mark>.</p><p>We random pick initial value of $w$ and updated when we find a new $w$ which could achieve lower loss. By doing this, we could compute the gradient: $ \frac{\partial J(W)}{\partial W} $</p><p>The way of update the weight is very similar to perceptron:</p><ul><li>$W \leftarrow W - \eta \frac{\partial J(w)}{\partial w} $</li></ul><h3 id="Backpropagation">Backpropagation</h3><p>Backpropagation is a key algorithm in training neural networks, which utilizes the chain rule to compute the gradient of the loss function with respect to each weight in the network. Let’s break down the images and the concepts step-by-step:</p><p>Backpropagation, short for “backward propagation of errors,” is a fundamental algorithm used to train artificial neural networks. It is based on the concept of gradient descent and helps in minimizing the error by adjusting the weights of the network. Here’s a step-by-step explanation and a guide on how to calculate it:</p><h3 id="Understanding-Backpropagation">Understanding Backpropagation</h3><ol><li><p><strong>Forward Pass:</strong></p><ul><li>Input data is passed through the neural network layer by layer to obtain the output.</li><li>Each layer performs a weighted sum of inputs, applies an activation function, and passes the result to the next layer.</li></ul></li><li><p><strong>Loss Calculation:</strong></p><ul><li>The network’s output is compared to the actual target output using a loss function (e.g., Mean Squared Error, Cross-Entropy Loss).</li><li>The difference between the predicted output and the actual output is the error.</li></ul></li><li><p><strong>Backward Pass (Backpropagation):</strong></p><ul><li>The error is propagated back through the network to update the weights.</li><li>This involves computing the gradient of the loss function with respect to each weight in the network.</li><li>Gradients indicate the direction and magnitude of the change required in the weights to minimize the error.</li></ul></li></ol><h3 id="Steps-in-Backpropagation">Steps in Backpropagation</h3><ol><li><p><strong>Initialization:</strong></p><ul><li>Initialize the weights and biases of the network with small random values.</li></ul></li><li><p><strong>Forward Pass:</strong></p><ul><li>For each layer $ l $, compute the input $ z^l $ and output $ a^l $:<ul><li>$z^l = W^l a^{l-1} + b^l$</li><li>$a^l = \sigma(z^l)$</li></ul></li><li>Here, $ W^l $ are the weights, $ b^l $ are the biases, $ \sigma $ is the activation function, and $ a^{l-1} $ is the output from the previous layer (the first $a$ is $x$ which is the input).</li></ul></li><li><p><strong>Compute Loss:</strong></p><ul><li>Compute the loss $ L $ using a suitable loss function.</li></ul></li><li><p><strong>Backward Pass:</strong></p><ul><li>Calculate the gradient of the loss with respect to the output of the last layer $ \delta^L $:<ul><li>$\delta^L = \nabla_a L \cdot \sigma’(z^L)$</li></ul></li><li>For each layer $ l $ from $ L-1 $ to 1, compute:<br>-$\delta^l = (\delta^{l+1} \cdot W^{l+1}) \cdot \sigma’(z^l)$</li><li>Update the weights and biases:<ul><li>$W^l = W^l - \eta \cdot \delta^l \cdot (a^{l-1}) ^T$</li><li>$b^l = b^l - \eta \cdot \delta^l$</li></ul></li><li>Here, $ \eta $ is the learning rate, and $ \sigma’ $ is the derivative of the activation function.</li></ul></li></ol><h3 id="Calculation">Calculation</h3><p>To actually calculate backpropagation, you need to:</p><ol><li><strong>Initialize weights and biases.</strong></li><li><strong>Perform a forward pass</strong> to compute the activations for each layer.</li><li><strong>Compute the loss</strong> using the output from the forward pass and the actual target values.</li><li><strong>Perform a backward pass</strong> to compute the gradients of the loss with respect to each weight.</li><li><strong>Update the weights and biases</strong> using the computed gradients and the learning rate.</li></ol><details><summary> Example Code (Python):</summary><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid_derivative</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> - x)<br><br><span class="hljs-comment"># Example input and output</span><br>x = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])<br>y = np.array([[<span class="hljs-number">0</span>], [<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>], [<span class="hljs-number">0</span>]])<br><br><span class="hljs-comment"># Initialize weights and biases</span><br>np.random.seed(<span class="hljs-number">42</span>)<br>W1 = np.random.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>b1 = np.random.rand(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>W2 = np.random.rand(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>b2 = np.random.rand(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># Learning rate</span><br>eta = <span class="hljs-number">0.1</span><br><br><span class="hljs-comment"># Training loop</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>):<br>    <span class="hljs-comment"># Forward pass</span><br>    z1 = np.dot(x, W1) + b1<br>    a1 = sigmoid(z1)<br>    z2 = np.dot(a1, W2) + b2<br>    a2 = sigmoid(z2)<br>    <span class="hljs-comment"># Loss calculation</span><br>    loss = <span class="hljs-number">0.5</span> * (y - a2)**<span class="hljs-number">2</span><br>    <span class="hljs-comment"># Backward pass</span><br>    delta2 = (a2 - y) * sigmoid_derivative(a2)<br>    delta1 = np.dot(delta2, W2.T) * sigmoid_derivative(a1)<br>    <span class="hljs-comment"># Update weights and biases</span><br>    W2 -= eta * np.dot(a1.T, delta2)<br>    b2 -= eta * np.<span class="hljs-built_in">sum</span>(delta2, axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-literal">True</span>)<br>    W1 -= eta * np.dot(x.T, delta1)<br>    b1 -= eta * np.<span class="hljs-built_in">sum</span>(delta1, axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-literal">True</span>)<br><br>print(<span class="hljs-string">&quot;Final output after training:&quot;</span>)<br>print(a2)<br></code></pre></td></tr></table></figure></div><p>This code demonstrates the basic steps of backpropagation in a simple neural network. By running this code, you can observe how the network learns to approximate the XOR function over time.</p></details><p>In a perceptron, weights and biases are updated by multiplying the error (loss) by the input and learning rate, and then adding this value to the current weights. This approach works because the weights for each input are independent, and the perceptron does not form a network. However, in a neural network, nearly every weight can influence every output. As a result, we cannot simply update the weights based on the error alone. Instead, we need to calculate the contribution of each weight to the overall error and adjust the weights accordingly. This process of calculating each weight’s contribution and updating them is known as backpropagation.</p><h3 id="Overview-of-the-Process">Overview of the Process</h3><ol><li><strong>Forward Pass</strong>: The input $ x $ is passed through the network to compute the output $ \hat{y} $.</li><li><strong>Loss Calculation</strong>: The loss function $ J(W) $ calculates the difference between the predicted output $ \hat{y} $ and the actual output.</li><li><strong>Backward Pass</strong>: Gradients are computed by propagating the error backward through the network, adjusting the weights to minimize the loss.</li></ol><p>The goal is to understand how a small change in one weight (e.g., $ w_2 $) affects the final loss $ J(W) $.</p><p>For the weight $ w_1 $, the gradient involves additional intermediate steps. Specifically:<br>$ \frac{\partial J(W)}{\partial w_1} = \frac{\partial J(W)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial z_1} \times \frac{\partial z_1}{\partial w_1} $</p><p>This decomposition shows that the gradient of the loss with respect to $ w_1 $ depends on:</p><ul><li>The gradient of the loss with respect to the output $ \hat{y} $</li><li>The gradient of $ \hat{y} $ with respect to the intermediate variable $ z_1 $</li><li>The gradient of $ z_1 $ with respect to the weight $ w_1 $</li></ul><h3 id="Why-Backpropagation">Why Backpropagation?</h3><p>Backpropagation efficiently computes these gradients using the chain rule. The key points are:</p><ul><li><strong>Efficiency</strong>: By reusing intermediate results (e.g., the gradient of the loss with respect to $ \hat{y} $), backpropagation avoids redundant calculations.</li><li><strong>Modularity</strong>: Gradients are computed layer by layer, allowing for modular network designs where each layer can be independently understood and modified.</li><li><strong>Training</strong>: These gradients are used to update the weights in a way that minimizes the loss function, allowing the network to learn from data.</li></ul><h3 id="Summary">Summary</h3><p>Backpropagation applies the chain rule to compute gradients of the loss function with respect to each weight in the network. These gradients are essential for updating the weights during training, thereby enabling the network to learn. Understanding the chain rule and how it applies to neural networks is crucial for grasping backpropagation.</p><h2 id="Batches">Batches</h2><p>Running backpropagation can be computationally expensive when calculating (\frac{\partial J(W)}{\partial w_1}) with a large training dataset. It is easy to run out of memory if too many threads are used. To mitigate this, one approach is to use a single data point to compute (\frac{\partial J_i(W)}{\partial w_1}), though this can introduce significant noise. A more effective strategy is to divide the training data into small batches, which can increase training efficiency and reduce noise. Common batch sizes used during training are 32 or 64.</p><h2 id="Strategies-for-Avoiding-Overfitting">Strategies for Avoiding Overfitting</h2><ol><li>Dropout:<ul><li>randomly set some activate as 0.</li><li>force network not relay on any node</li></ul></li><li>Early stopping:<ul><li>monitor the losing curve and stop the training before it had change to overfit</li></ul></li></ol><h2 id="NW-in-Action">NW in Action</h2><p>Let’s go through an example of using TensorFlow to build a two-layer neural network for a classification task using a dataset from scikit-learn. We will use the Iris dataset, which is a classic dataset for classification.</p><p>Notice: When TensorFlow runs a neural network, it automatically detects and utilizes available GPUs to accelerate the computation. This process is seamless and doesn’t typically require manual intervention.</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> OneHotEncoder<br><br><span class="hljs-comment"># Load the Iris dataset</span><br>iris = datasets.load_iris()<br>X = iris.data<br>y = iris.target.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># One-hot encode the target labels</span><br>encoder = OneHotEncoder(sparse=<span class="hljs-literal">False</span>)<br>y = encoder.fit_transform(y)<br><br><span class="hljs-comment"># Standardize the feature data</span><br>scaler = StandardScaler()<br>X = scaler.fit_transform(X)<br><br><span class="hljs-comment"># Split the dataset into training and test sets</span><br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># Define the model</span><br>model = tf.keras.models.Sequential([<br>    tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(X_train.shape[<span class="hljs-number">1</span>],)),<br>    tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    tf.keras.layers.Dense(<span class="hljs-number">3</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>)<br>])<br><br><span class="hljs-comment"># Compile the model</span><br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PrintLossCallback</span>(<span class="hljs-params">tf.keras.callbacks.Callback</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">on_epoch_end</span>(<span class="hljs-params">self, epoch, logs=<span class="hljs-literal">None</span></span>):</span><br>        print(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span>, Loss: <span class="hljs-subst">&#123;logs[<span class="hljs-string">&#x27;loss&#x27;</span>]&#125;</span>, Accuracy: <span class="hljs-subst">&#123;logs[<span class="hljs-string">&#x27;accuracy&#x27;</span>]&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># Train the model with the callback</span><br>model.fit(X_train, y_train, epochs=<span class="hljs-number">100</span>, batch_size=<span class="hljs-number">32</span>, validation_split=<span class="hljs-number">0.1</span>, callbacks=[PrintLossCallback()])<br><br><br><span class="hljs-comment"># Evaluate the model on the test set</span><br>y_pred = model.predict(X_test)<br>test_loss, test_accuracy = model.evaluate(X_test, y_test)<br>print(<span class="hljs-string">f&quot;Test Accuracy: <span class="hljs-subst">&#123;test_accuracy * <span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f&#125;</span>%&quot;</span>)<br><br></code></pre></td></tr></table></figure></div><pre>Epoch 96/1004/4 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.9532 - val_loss: 0.3162 - val_accuracy: 0.9167Epoch 96, Loss: 0.21823757886886597, Accuracy: 0.9351851940155029Epoch 97/1004/4 [==============================] - 0s 3ms/step - loss: 0.2010 - accuracy: 0.9522 - val_loss: 0.3123 - val_accuracy: 0.9167Epoch 97, Loss: 0.21543042361736298, Accuracy: 0.9351851940155029Epoch 98/1004/4 [==============================] - 0s 3ms/step - loss: 0.2175 - accuracy: 0.9366 - val_loss: 0.3079 - val_accuracy: 0.9167Epoch 98, Loss: 0.21266280114650726, Accuracy: 0.9351851940155029Epoch 99/1004/4 [==============================] - 0s 3ms/step - loss: 0.2047 - accuracy: 0.9428 - val_loss: 0.3040 - val_accuracy: 0.9167Epoch 99, Loss: 0.20983757078647614, Accuracy: 0.9351851940155029Epoch 100/1004/4 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9376 - val_loss: 0.3008 - val_accuracy: 0.9167Epoch 100, Loss: 0.20734088122844696, Accuracy: 0.9351851940155029</pre><table><thead><tr><th style="text-align:center"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://imgur.com/X2jtXE1.png" alt="neural network predicted results"></td><td style="text-align:left"><br><br><br><br>In this group of test data, there are ony one mistake.</td></tr></tbody></table><p>Another regression example write by torch</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torch.optim.lr_scheduler <span class="hljs-keyword">import</span> StepLR<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_sequential_layers</span>():</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Task: Create neural net layers using nn.Sequential.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Requirements: Return an nn.Sequential object, which contains:</span><br><span class="hljs-string">        1. a linear layer (fully connected) with 2 input features and 3 output features,</span><br><span class="hljs-string">        2. a sigmoid activation layer,</span><br><span class="hljs-string">        3. a linear layer with 3 input features and 5 output features.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    block = torch.nn.Sequential(<br>        torch.nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>),<br>        torch.nn.Sigmoid(),<br>        torch.nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>    )<br>    <span class="hljs-keyword">return</span> block<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_loss_function</span>():</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Task: Create a loss function using nn module.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Requirements: Return a loss function from the nn module that is suitable for</span><br><span class="hljs-string">    multi-class classification.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> torch.nn.MSELoss()<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NeuralNet</span>(<span class="hljs-params">torch.nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Initialize your neural network here.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment">################# Your Code Starts Here #################</span><br>        self.conv1 = nn.Conv1d(in_channels=<span class="hljs-number">1</span>, out_channels=<span class="hljs-number">5</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>)<br>        self.relu = nn.LeakyReLU()<br>        <span class="hljs-comment"># Adjust the following layer sizes based on the output of your convolutional layer</span><br>        self.fc1 = nn.Linear(<span class="hljs-number">5</span> * <span class="hljs-number">2883</span>, <span class="hljs-number">69</span>)  <span class="hljs-comment"># Adjusted for flattened conv output</span><br>        self.output = nn.Linear(<span class="hljs-number">69</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-comment">################## Your Code Ends here ##################</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Perform a forward pass through your neural net.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Parameters:</span><br><span class="hljs-string">            x:      an (N, input_size) tensor, where N is arbitrary.</span><br><span class="hljs-string">        Outputs:</span><br><span class="hljs-string">            y:      an (N, output_size) tensor of output from the network</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment">################# Your Code Starts Here #################</span><br>        x = x.view(x.size(<span class="hljs-number">0</span>), <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># Apply Conv1d</span><br>        x = self.conv1(x)<br>        x = self.relu(x)<br>        <span class="hljs-comment"># Flatten the output for the linear layer</span><br>        x = x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        x = self.fc1(x)<br>        x = self.relu(x)<br>        y_pred = self.output(x)<br>        <span class="hljs-keyword">return</span> y_pred<br>        <span class="hljs-comment">################## Your Code Ends here ##################</span><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">train_dataloader, epochs</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    The autograder will call this function and compute the accuracy of the returned model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters:</span><br><span class="hljs-string">        train_dataloader:   a dataloader for the training set and labels</span><br><span class="hljs-string">        test_dataloader:    a dataloader for the testing set and labels</span><br><span class="hljs-string">        epochs:             the number of times to iterate over the training set</span><br><span class="hljs-string">    Outputs:</span><br><span class="hljs-string">        model:              trained model</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment">################# Your Code Starts Here #################</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Implement backward propagation and gradient descent here.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    device = <span class="hljs-string">&quot;cpu&quot;</span><br>    model = NeuralNet().to(device)<br>    loss_fn = torch.nn.CrossEntropyLoss()  <span class="hljs-comment"># Suitable for regression tasks</span><br>    optimizer = torch.optim.Adamax(params=model.parameters(), lr=<span class="hljs-number">0.001</span>)<br>    scheduler = StepLR(optimizer, step_size=<span class="hljs-number">500</span>, gamma=<span class="hljs-number">0.1</span>)  <span class="hljs-comment"># Learning rate scheduler</span><br>    epoch_count = []<br>    train_loss_values = []<br>    test_loss_values = []   <br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):  <span class="hljs-comment"># Loop over the dataset multiple times</span><br>        running_loss = <span class="hljs-number">0.0</span><br>        <span class="hljs-keyword">for</span> inputs, labels <span class="hljs-keyword">in</span> train_dataloader:<br>            train_set, train_labels = inputs.to(device), labels.to(device)  <span class="hljs-comment"># Move inputs and labels to the device</span><br>            model.train()<br>            y_pred = model(train_set)<br>            loss = loss_fn(y_pred, train_labels) <br>            <span class="hljs-comment"># Zero gradients, perform a backward pass, and update the weights</span><br>            optimizer.zero_grad()<br>            loss.backward()<br>            optimizer.step()<br>            <span class="hljs-comment">#scheduler.step()  # Update the learning rate           </span><br>    <span class="hljs-comment">################## Your Code Ends here ##################</span><br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure></div><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Multi-layer Neural Nets</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Hidden Markov Model</title>
    <link href="https://karobben.github.io/2024/02/12/LearnNotes/ai-hmm/"/>
    <id>https://karobben.github.io/2024/02/12/LearnNotes/ai-hmm/</id>
    <published>2024-02-12T06:59:06.000Z</published>
    <updated>2024-02-16T08:43:53.706Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://imgur.com/oMmL1Ln.png" alt=""></p><p>A Hidden Markov Model is a Bayes Network with these assumptions:<br>• <em>Y<sub>t</sub></em> depends only on <em>Y<sub>t-1</sub></em><br>• <em>X<sub>t</sub></em> depends only on <em>Y<sub>t</sub></em></p><p>The belief network conveys the independence assumption:<br>$$<br>for\ all\ i \geq 0, P(S_{i+1}|S_i) = P (S_1|S_0)<br>$$</p><p>$$<br>P(S_i = s) = \sum_{s’} P(S_{i+1} = s \mid S_i = s’) * P(S_i = s’)<br>$$</p><p>In the context of the equation you’re referring to, $ s $ and $ s’ $ represent states in a Markov chain. Typically, $ s $ is used to denote the current state, while $ s’ $ (read as “s prime”) denotes a subsequent or different state that the system can transition into from the current state $ s $.</p><p>The summation over $ s’ $ in the equation indicates that you’re summing over all possible subsequent states that the system can transition to from the current state $ s $. This is part of the definition of a stationary distribution for a Markov chain, where the probability of being in any given state $ s $ is equal to the sum of the probabilities of transitioning to state $ s $ from all possible previous states $ s’ $, weighted by the probability of being in state $ s’ $ at the previous time step.</p><h3 id="Key-advantage-of-a-hidden-Markov-model-Polynomial-time-complexity">Key advantage of a hidden Markov model: Polynomial-time complexity</h3><ul><li>Suppose there are <em>|y|</em> different speech sounds in English, and the length of the utterance is <em>d</em> centiseconds (<em>|y| ≈ 50, d ≈ 100</em>)</li><li>Without the HMM assumptions, to compute <em>f(x)= argmaxP(y<sub>1</sub>, … , y<sub>d</sub>|x<sub>1</sub>, … , x<sub>d</sub>)</em> requires a time complexity of<br><em>O{|y|<sup>d</sup>} ≈ 50<sup>100</sup></em></li><li>With an HMM, each variable has only one parent, so inference is <em>O{|y|<sup>d</sup>} ≈ 50<sup>2</sup></em></li><li>The computationally efficient algorithm that we use to compute <em>f(x)= argmaxP(y<sub>1</sub>, … , y<sub>d</sub>|x<sub>1</sub>, … , x<sub>d</sub>)</em> is called the Viterbi algorithm, named after the electrical engineer who first applied it to error correction coding.</li></ul><div class="admonition note"><p class="admonition-title">it works much better than bayes</p><p>Text generated by a naïve Bayes model (unigram model):</p><ul><li>Representing and speedily is an good apt or come can different natural here he the a in came the to of to expert gray come to furnishes the line message had be these…</li></ul><p>Text generated by a HMM (bigram model):</p><ul><li>The head and in frontal attack on an English writer that the character of this point is therefore another for the letters that the time of who ever told the problem for an unexpected…</li></ul></div><h3 id="Applications-of-HMMs">Applications of HMMs</h3><ul><li>Speech recognition HMMs:<ul><li>Observations are acoustic signals (continuous valued)</li><li>States are specific positions in specific words (so, tens of thousands)</li></ul></li><li>Machine translation HMMs:<ul><li>Observations are words (tens of thousands)</li><li>States are cross-lingual alignments</li></ul></li><li>Robot tracking:<ul><li>Observations are range readings (continuous)</li><li>States are positions on a map</li></ul></li></ul><h3 id="Viterbi-Algorithm">Viterbi Algorithm</h3><p>The Viterbi algorithm is a computationally efficient algorithm for computing the maximum a posteriori (MAP) state sequence<br>$$<br>f(x)= argmax_{y_1, … , y_d}P(y_1, … , y_d|x_1, … , x_d)<br>$$</p><h3 id="Notation">Notation</h3><ul><li><p>Initial State Probability:</p><ul><li>$ \pi_i = P(Y_1 = i)$</li></ul></li><li><p>Transition Probability:</p><ul><li>$a_{i,j} = P(Y_t=j| Y_{t-1} = i)$</li></ul></li><li><p>Observation Probabilities:</p><ul><li>$b_j(x_t) = P(X_t = x_t|Y_t=j)$</li></ul></li><li><p>Node Probability: Probability of the best path until node $ j $ at time $ t $</p></li></ul><p>$$<br>v_t(j) = \max_{y_1,…,y_{t-1}} P(Y_1 = y_1 ,…, Y_{t-1} = y_{t-1}, Y_t = j, X_0 = x_0, …, X_t = x_t)<br>$$</p><ul><li>Backpointer: which node precedes node $ j $ on the best path?</li></ul><p>$$<br>\psi_t(j) = \arg\max_{y_{t-1}} \max_{y_1,…,y_{t-2}} P(Y_1 = y_1 ,…, Y_{t-1} = y_{t-1}, Y_t = j, X_0 = x_0, …, X_t = x_t)<br>$$</p><h3 id="How-HMM-Worked">How HMM Worked</h3><p>Initiation → Iteration → Termination → Back-Tracing</p><ul><li>Initiation<br>$v_1(i) = \pi_ib_i(x_1)$</li><li>Iteration<br>$v_t(j) = max v_{t-1}(i)a_{i,j}b_j(x_t)$</li><li>Termination<br>$y_d = argmax v_d{i}$</li><li>Back-Tracing<br>$y_t = \psi_{t+1}(y_{t+1})$</li></ul><h3 id="Example-Question">Example Question</h3><p>Richard Feynman is an AI. He cannot see the weather, but he can see whether or not his creator, Elspeth Dunsany, brings an umbrella to work. Let $ R_t $ denote the event “it is raining on day $ t $,” and let $ U_t $ denote the event “Dr. Dunsany brings her umbrella on day $ t $.” Dr. Dunsany is an absent-minded professor; she often brings her umbrella when it’s not raining, and often forgets her umbrella when it’s raining. Richard’s model of Dr. Dunsany’s behavior includes the parameter $ P(R_1 = T) = 0.5 $, and the parameters shown in the tables below. What is $ P(R_2 = F, U_1 = T, U_2 = T) $?</p><table><thead><tr><th style="text-align:center">$ P(R_t = T | R_{t-1} = r_{t-1}) $</th><th style="text-align:center">$ r_{t-1} = F $</th><th style="text-align:center">$ r_{t-1} = T $</th></tr></thead><tbody><tr><td style="text-align:center">$ r_t = F $</td><td style="text-align:center">0.4</td><td style="text-align:center">0.1</td></tr><tr><td style="text-align:center">$ r_t = T $</td><td style="text-align:center">0.8</td><td style="text-align:center">0.7</td></tr></tbody></table><table><thead><tr><th style="text-align:left">$P(U_t = T | R_t = r_t)$</th><th style="text-align:center">$r_t = F$</th><th style="text-align:center">$r_t = T$</th></tr></thead><tbody><tr><td style="text-align:left">$ r_{t-1} = F $</td><td style="text-align:center">0.4</td><td style="text-align:center">0.1</td></tr><tr><td style="text-align:left">$ r_{t-1} = T $</td><td style="text-align:center">0.8</td><td style="text-align:center">0.7</td></tr></tbody></table><ul><li><p>Initial State Probability:</p><ul><li>$ P(R_1 = T) = 0.5 $</li></ul></li><li><p>Transition Probabilities:</p><ul><li>$ P(R_t = T | R_{t-1} = F) = 0.8 $</li><li>$ P(R_t = T | R_{t-1} = T) = 0.7 $</li><li>$ P(R_t = F | R_{t-1} = F) = 0.4 $</li><li>$ P(R_t = F | R_{t-1} = T) = 0.1 $</li></ul></li><li><p>Observation Probabilities (Probability of Dr. Dunsany bringing her umbrella given the weather):</p><ul><li>$ P(U_t = T | R_t = F) = 0.4 $ (Probability she brings an umbrella when it’s not raining)</li><li>$ P(U_t = T | R_t = T) = 0.1 $ (Probability she brings an umbrella when it is raining)</li></ul></li></ul><p>These probabilities define the initial state distribution, the transition dynamics, and the observation model of a system, which are essential components of probabilistic models like Hidden Markov Models (HMMs).</p><p>The scenario you’ve presented involves conditional probabilities and is a typical example used in Bayesian inference or probabilistic models. Given the information in the image, we can calculate the probability that Dr. Dunsany brings her umbrella on day 2 given that it’s not raining on day 2, but it was raining on day 1.</p><p>The information given includes:</p><ol><li>The initial probability that it’s raining on day 1: $ P(R_1 = T) = 0.5 $</li><li>The conditional probabilities of bringing an umbrella given the weather:<ul><li>$ P(R_t = T | R_{t-1} = r_{t-1}) $: The probability that Dr. Dunsany brings an umbrella on day $ t $ given the weather on day $ t-1 $.</li><li>$ P(U_t = T | R_t = r_t) $: The probability that Dr. Dunsany brings an umbrella on day $ t $ given the weather on day $ t $.</li></ul></li></ol><p>With the tables provided, we can calculate the probability that Dr. Dunsany brings her umbrella on day 2 given the conditions specified.</p><p>We can apply the law of total probability to consider all possible weather scenarios from the previous day. Here is the formula based on the total probability theorem:</p><p>$ P(R_2 = F, U_1 = T, U_2 = T) = \\<br>\sum_{r_{t-1} \in {T, F}} P(R_2 = F | R_1 = r_{t-1}) \cdot P(U_1 = T | R_1 = r_{t-1}) \cdot P(U_2 = T | R_2 = F)<br>$</p><p>We would then substitute the values from the tables into the formula to calculate the desired probability. Would you like to proceed with this calculation?</p><style>pre {  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Hidden Markov Model</summary>
    
    
    
    <category term="Notes" scheme="https://karobben.github.io/categories/Notes/"/>
    
    <category term="Class" scheme="https://karobben.github.io/categories/Notes/Class/"/>
    
    <category term="UIUC" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/"/>
    
    <category term="AI" scheme="https://karobben.github.io/categories/Notes/Class/UIUC/AI/"/>
    
    
    <category term="Machine Learning" scheme="https://karobben.github.io/tags/Machine-Learning/"/>
    
    <category term="Data Science" scheme="https://karobben.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Basic Mathematics Calculating</title>
    <link href="https://karobben.github.io/2024/02/11/LearnNotes/math-cal/"/>
    <id>https://karobben.github.io/2024/02/11/LearnNotes/math-cal/</id>
    <published>2024-02-12T04:58:46.000Z</published>
    <updated>2024-02-14T06:03:37.897Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sum">Sum</h2><p>The sum symbol, represented by the Greek letter sigma (Σ), is widely used in mathematics to denote the summation of a sequence of numbers or expressions. When you see this symbol, it means you should add up a series of numbers according to the specified rule. Here’s a breakdown of how it’s typically used:</p><h3 id="Basic-Structure">Basic Structure</h3><p>The summation symbol is written as:</p><p>$$<br>\sum_{i=a}^{b} f(i)<br>$$</p><p>where:</p><ul><li>$i$ is the index of summation, which takes on each integer value from $a$ to $b$, inclusive.</li><li>$a$ is the lower limit of summation, the starting value of $i$.</li><li>$b$ is the upper limit of summation, the ending value of $i$.</li><li>$f(i)$ is the function of $i$ to be summed over the range from $a$ to $b$.</li></ul><h3 id="Examples">Examples</h3><ol><li><p><strong>Sum of the first 5 natural numbers</strong>:<br>$$<br>\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15<br>$$<br>Here, $f(i) = i$, and you sum the values of $i$ from 1 to 5.</p></li><li><p><strong>Sum of the squares of the first 3 positive integers</strong>:<br>$$<br>\sum_{i=1}^{3} i^2 = 1^2 + 2^2 + 3^2 = 1 + 4 + 9 = 14<br>$$<br>In this example, $f(i) = i^2$, so you square each $i$ from 1 to 3 and then add them together.</p></li><li><p><strong>Sum of a constant over a range</strong>:<br>Suppose you want to add the number 4, five times. The expression would be:<br>$$<br>\sum_{i=1}^{5} 4 = 4 + 4 + 4 + 4 + 4 = 20<br>$$</p><p>Here, $f(i) = 4$, which doesn’t depend on $i$. You’re essentially multiplying 4 by the number of terms (5 in this case).</p></li><li><p><strong>Two sums</strong><br>$$<br>\sum_{i=1}^ {5}\sum_{j=2}^ {6} ij<br>$$<br>For this, you sum over $j$ from 2 to 6 for each value of $i$ from 1 to 5, and then sum those results. It’s like computing a series within another series. The operation proceeds as follows:</p><ol><li>First, fix $i$ at its starting value, 1.</li><li>Then, for $i = 1$, sum over $j$ from 2 to 6, calculating $1 \cdot j$ for each $j$ and adding them together.</li><li>Repeat this process for each value of $i$ up to 5.</li><li>Finally, sum all the results from the inner summations together.</li></ol><p>Let’s compute this step-by-step to see the result.<br>The result of the double summation $\sum_{i=1}^ {5}\sum_{j=2}^ {6} ij$ is 300. This means that when you sum the product of $i$ and $j$ for each $i$ from 1 to 5 and each $j$ from 2 to 6, the total sum is 300.<br>PS: in python:</p> <div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="code"><pre><code class="hljs python">N= <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>):<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>,<span class="hljs-number">7</span>):<br>        N += i*j<br></code></pre></td></tr></table></figure></div></li></ol><h3 id="How-to-Use">How to Use</h3><ul><li><strong>Identify the sequence</strong> you need to sum. This could be a series of numbers, functions of an index, or even a constant value repeated several times.</li><li><strong>Determine the starting and ending indices</strong> ($a$ and $b$, respectively) for your summation.</li><li><strong>Write down the function or value</strong> to be summed as $f(i)$ for each $i$ in the range from $a$ to $b$.</li><li><strong>Compute each term</strong> in the series and <strong>add them together</strong> to find the total sum.</li></ul><p>Summation notation is a powerful tool in mathematics, especially for dealing with sequences and series, and it’s widely used in various fields such as statistics, physics, and finance.</p><h2 id="Product-Notation">Product Notation</h2><p>Similarly, we have <mark>product notation</mark>, too. The product symbol is represented by the Greek letter pi (Π), not to be confused with the mathematical constant $\pi$ (pi) used for the ratio of a circle’s circumference to its diameter. The product symbol is used to denote the multiplication of a sequence of numbers or expressions, just like the sum symbol is used for addition.</p><p>$$<br>\prod_{i=a}^{b} f(i)<br>$$</p><p>where:</p><ul><li>$i$ is the index of multiplication, taking on each integer value from $a$ to $b$, inclusive.</li><li>$a$ is the lower limit of the product, the starting value of $i$.</li><li>$b$ is the upper limit of the product, the ending value of $i$.</li><li>$f(i)$ is the function of $i$ to be multiplied over the range from $a$ to $b$.</li></ul><h3 id="Examples-v2">Examples</h3><ol><li><p><strong>Product of the first 5 natural numbers</strong> (also known as $5!$, factorial of 5):</p><p>$$<br>\prod_{i=1}^{5} i = 1 \times 2 \times 3 \times 4 \times 5 = 120<br>$$</p><p>This multiplies the values of $i$ from 1 to 5.</p></li></ol><p>In mathematics and particularly in machine learning, besides the summation (Σ) and product (Π) notations, another frequently used notation is the integral symbol (∫). While the summation and product notations deal with discrete sequences, the integral symbol is used for continuous functions and is fundamental in calculus. Integrals play a crucial role in various aspects of machine learning, especially in optimization, probability distributions, and understanding the area under curves (such as ROC curves).</p><h2 id="Integral-Notation">Integral Notation</h2><p>The basic structure of an integral is:</p><p>$$<br>\int_{a}^{b} f(x) , dx<br>$$</p><p>where:</p><ul><li>$a$ and $b$ are the lower and upper limits of integration, respectively, defining the interval over which the function $f(x)$ is integrated.</li><li>$f(x)$ is the function to be integrated over $x$.</li><li>$dx$ represents an infinitesimally small increment of $x$, indicating that the integration is performed with respect to $x$.</li></ul><h3 id="Importance-in-Machine-Learning">Importance in Machine Learning</h3><ol><li><p><strong>Optimization</strong>: Many machine learning models involve optimization problems where the goal is to minimize or maximize some function (e.g., a loss function in neural networks or a cost function in logistic regression). Integrals are essential in solving continuous optimization problems, especially when calculating gradients or understanding the behavior of functions over continuous intervals.</p></li><li><p><strong>Probability Distributions</strong>: In the context of probabilistic models and statistics, integrals are used to calculate probabilities, expected values, and variances of continuous random variables. For example, the area under the probability density function (PDF) of a continuous random variable over an interval gives the probability of the variable falling within that interval.</p></li><li><p><strong>Feature Extraction and Signal Processing</strong>: In machine learning applications involving signal processing or feature extraction from continuous data, integrals are used to calculate various features and transform signals into more useful forms.</p></li><li><p><strong>Kernel Methods</strong>: In machine learning, kernel methods (e.g., support vector machines) utilize integrals in the formulation of kernel functions, which are essential in mapping input data into higher-dimensional spaces for classification or regression tasks.</p></li><li><p><strong>Deep Learning</strong>: In the training of deep neural networks, integrals may not be explicitly visible but are conceptually present in the form of continuous optimization and in the calculation of gradients during backpropagation.</p></li></ol><h3 id="Example">Example</h3><p>Consider the problem of finding the area under a curve, which is a fundamental concept in machine learning for evaluating model performance (e.g., calculating the area under the ROC curve (AUC) for classification problems). If $f(x)$ represents the curve, the area under $f(x)$ from $a$ to $b$ can be computed by the integral:</p><p>$$<br>\text{Area} = \int_{a}^{b} f(x) , dx<br>$$</p><h2 id="Other-Frequently-Used-Notations">Other Frequently Used Notations</h2><p>This integral computes the total area under $f(x)$ between $a$ and $b$, providing a measure of the model’s performance over that interval.</p><p>Integrals, along with summation and product notations, form the backbone of many mathematical operations in machine learning, from theoretical underpinnings to practical applications in data analysis, model evaluation, and optimization strategies.</p><p>Beyond summation (Σ), product (Π), and integral (∫) notations, there are several other mathematical symbols and concepts that are frequently used in machine learning and statistics. These include:</p><h3 id="Gradient-∇">Gradient (∇)</h3><p>The gradient is a vector operation that represents the direction and rate of the fastest increase of a scalar function. In machine learning, the gradient is crucial for optimization algorithms like gradient descent, which is used to minimize loss functions. The gradient of a function $f(x_1, x_2, \ldots, x_n)$ with respect to its variables is denoted by:</p><p>$$<br>\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)<br>$$</p><h3 id="Partial-Derivative-∂">Partial Derivative (∂)</h3><p>The partial derivative represents the rate of change of a function of multiple variables with respect to one of those variables, keeping the others constant. It’s denoted by the symbol ∂. Partial derivatives are essential in the calculation of gradients and in the optimization of machine learning models.</p><h3 id="Expectation-E">Expectation (E)</h3><p>The expectation or expected value of a random variable is a fundamental concept in probability and statistics, denoted by $E[X]$ for a random variable $X$. It represents the average or mean value that $X$ takes over its probability distribution and is crucial in understanding the behavior of models, especially in probabilistic settings.</p><h3 id="Variance-Var-and-Standard-Deviation-σ">Variance (Var) and Standard Deviation (σ)</h3><p>Variance measures the spread of a random variable’s values and is denoted by $Var(X)$ or $\sigma^2$ for a random variable $X$. The standard deviation, $\sigma$, is the square root of the variance and provides a measure of the dispersion of data points around their mean value. These concepts are vital in assessing the reliability and performance of models.</p><h3 id="Covariance-and-Correlation">Covariance and Correlation</h3><p>Covariance and correlation measure the relationship between two random variables. Covariance indicates the direction of the linear relationship between variables, while correlation measures both the strength and direction of this linear relationship. Understanding these relationships is essential in features selection and in modeling the interactions between variables.</p><h3 id="Big-O-Notation-O">Big O Notation (O)</h3><p>Big O notation is used to describe the computational complexity of algorithms, which is crucial in machine learning for understanding the scalability and efficiency of models and algorithms. For example, an algorithm with a complexity of $O(n^2)$ means its execution time or space requirements increase quadratically as the input size $n$ increases.</p><h3 id="Matrix-Notations-and-Operations">Matrix Notations and Operations</h3><p>Matrices and vectors are fundamental in machine learning for representing and manipulating data. Operations such as matrix multiplication, transpose, and inversion are essential for linear algebra, which underpins many machine learning algorithms, including neural networks, PCA (Principal Component Analysis), and SVMs (Support Vector Machines).</p><p>Each of these mathematical concepts plays a crucial role in the formulation, analysis, and implementation of machine learning algorithms. They provide the theoretical foundation for understanding model behavior, optimizing performance, and evaluating outcomes in a wide range of applications.</p><h2 id="Matrix-Calculating">Matrix Calculating</h2><p>Matrix multiplication is a fundamental operation in linear algebra with extensive applications in mathematics, physics, engineering, computer science, and particularly in machine learning and data analysis. The way matrix multiplication is defined—by taking the dot product of rows and columns—might seem arbitrary at first, but it’s designed to capture several important mathematical and practical concepts.</p><p>Understanding how to perform basic operations with matrices—addition, subtraction, multiplication, and division (in a sense)—is crucial in linear algebra, which is foundational for many areas of mathematics, physics, engineering, and especially machine learning. Here’s a brief overview of each operation:</p><p>$$<br>\begin{pmatrix}<br>a_{11} &amp; \cdots &amp; a_{1j} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>a_{i1} &amp; \cdots &amp; a_{ij}<br>\end{pmatrix}<br>$$</p><p>Each element within the matrix is a pair $(i,j)$, where $i$ is the row index and $j$ is the column index.</p><h3 id="Matrix-Addition-and-Subtraction">Matrix Addition and Subtraction</h3><p>Matrix addition and subtraction are straightforward operations that are performed element-wise. This means you add or subtract the corresponding elements of the matrices. For these operations to be defined, the matrices must be of the same dimensions.</p><ul><li><p><strong>Addition</strong>: If $A$ and $B$ are matrices of the same size, their sum $C = A + B$ is a matrix where each element $c_{ij}$ is the sum of $a_{ij} + b_{ij}$.</p></li><li><p><strong>Subtraction</strong>: Similarly, the difference $C = A - B$ is a matrix where each element $c_{ij}$ is the difference $a_{ij} - b_{ij}$.</p></li></ul><h4 id="Example-v2">Example:</h4><p>If $A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}$ and $B = \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix}$, then</p><ul><li>$A + B = \begin{pmatrix} 1+5 &amp; 2+6 \\ 3+7 &amp; 4+8 \end{pmatrix} = \begin{pmatrix} 6 &amp; 8 \\ 10 &amp; 12 \end{pmatrix}$</li><li>$A - B = \begin{pmatrix} 1-5 &amp; 2-6 \\ 3-7 &amp; 4-8 \end{pmatrix} = \begin{pmatrix} -4 &amp; -4 \\ -4 &amp; -4 \end{pmatrix}$</li></ul><h3 id="Matrix-Multiplication">Matrix Multiplication</h3><p>Matrix multiplication is more complex and involves a dot product of rows and columns. For two matrices $A$ and $B$ to be multiplied, the number of columns in $A$ must equal the number of rows in $B$. If $A$ is an $m \times n$ matrix and $B$ is an $n \times p$ matrix, the resulting matrix $C = AB$ will be an $m \times p$ matrix where each element $c_{ij}$ is computed as the dot product of the $i$th row of $A$ and the $j$th column of $B$.</p><h4 id="Example-v3">Example:</h4><p>If $A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}$ and $B = \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix}$, then</p><ul><li>$AB = \begin{pmatrix} (1× 5 + 2× 7) &amp; (1× 6 + 2× 8) \\ (3× 5 + 4× 7) &amp; (3× 6 + 4× 8) \end{pmatrix} = \begin{pmatrix} 19 &amp; 22 \\ 43 &amp; 50 \end{pmatrix}$</li></ul><h3 id="Matrix-Division">Matrix Division</h3><p>Matrix division as such doesn’t exist in the way we think of division for real numbers. Instead, we talk about the inverse of a matrix. For matrix $A$ to “divide” another matrix $B$, you would multiply $B$ by the inverse of $A$, denoted as $A^{-1}$. This operation is only defined for square matrices (same number of rows and columns), and not all square matrices have an inverse.</p><ul><li><strong>Multiplying by the Inverse</strong>: If you want to solve for $X$ in $AX = B$, you can multiply both sides by $A^{-1}$, assuming $A^{-1}$ exists, to get $X = A^{-1}B$.</li></ul><h4 id="Example-v4">Example:</h4><p>If $A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}$, and its inverse $A^{-1} = \begin{pmatrix} -2 &amp; 1 \\ 1.5 &amp; -0.5 \end{pmatrix}$, and you want to “divide” $B = \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix}$ by $A$, you would compute $A^{-1}B$.</p><h3 id="Key-Points">Key Points</h3><ul><li><strong>Addition/Subtraction</strong>: Element-wise operation requiring matrices of the same dimensions.</li><li><strong>Multiplication</strong>: Involves the dot product of rows and columns, requiring the number of columns in the first matrix to equal the number of rows in the second.</li><li><strong>Division</strong>: Not directly defined, but involves multiplying by the inverse of a matrix.</li></ul><style>pre {  background-color:#38393d;  color: #5fd381;}</style>]]></content>
    
    
    <summary type="html">Basic Mathematics Calculating</summary>
    
    
    
    
  </entry>
  
</feed>
